var __index = {"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"Home","text":""},{"location":"index.html#quoptuna-documentation","title":"QuOptuna Documentation","text":"<p>Welcome to QuOptuna, a quantum-enhanced hyperparameter optimization framework that combines quantum computing with Optuna for advanced model tuning and explainable AI.</p>"},{"location":"index.html#overview","title":"Overview","text":"<p>QuOptuna provides a comprehensive platform for: - \ud83c\udfaf Automated hyperparameter optimization for quantum and classical ML models - \ud83d\udd0d SHAP-based explainable AI with rich visualizations - \ud83d\udcca UCI ML Repository integration for easy dataset access - \ud83d\udcdd AI-powered report generation for model analysis - \ud83d\udda5\ufe0f Interactive Streamlit interface for the complete workflow</p>"},{"location":"index.html#quick-start","title":"Quick Start","text":""},{"location":"index.html#installation","title":"Installation","text":"<p>Install QuOptuna using UV (recommended) or pip:</p> <pre><code># Using UV (recommended)\nuv pip install quoptuna\n\n# Using pip\npip install quoptuna\n</code></pre>"},{"location":"index.html#launch-the-application","title":"Launch the Application","text":"<p>Start the interactive Streamlit interface:</p> <pre><code>quoptuna --start\n</code></pre> <p>Or run directly with Python:</p> <pre><code>python -m quoptuna.frontend.app run\n</code></pre>"},{"location":"index.html#basic-python-usage","title":"Basic Python Usage","text":"<pre><code>from quoptuna import DataPreparation, Optimizer\nfrom quoptuna.backend.utils.data_utils.data import mock_csv_data\nimport pandas as pd\n\n# Load and prepare data\ndf = pd.read_csv(\"your_data.csv\")\ndf[\"target\"] = df[\"target\"].replace({0: -1, 1: 1})\n\n# Save data\nfile_path = mock_csv_data(df, tmp_path=\"data\", file_name=\"my_data\")\n\n# Prepare for training\ndata_prep = DataPreparation(\n    file_path=file_path,\n    x_cols=[col for col in df.columns if col != \"target\"],\n    y_col=\"target\"\n)\ndata_dict = data_prep.get_data(output_type=\"2\")\n\n# Run optimization\noptimizer = Optimizer(db_name=\"experiment\", study_name=\"trial_1\", data=data_dict)\nstudy, best_trials = optimizer.optimize(n_trials=100)\n\nprint(f\"Best F1 Score: {best_trials[0].value:.4f}\")\nprint(f\"Best Model: {best_trials[0].params['model_type']}\")\n</code></pre>"},{"location":"index.html#key-features","title":"Key Features","text":""},{"location":"index.html#hyperparameter-optimization","title":"\ud83c\udfaf Hyperparameter Optimization","text":"<p>Automated optimization using Optuna with support for: - Multiple quantum models (Data Reuploading, Circuit-Centric, Quantum Kitchen Sinks, etc.) - Classical baselines (SVC, MLP, Perceptron) - Multi-objective optimization - Parallel trial execution - Persistent storage with SQLite</p>"},{"location":"index.html#explainable-ai","title":"\ud83d\udd0d Explainable AI","text":"<p>Comprehensive SHAP analysis with multiple visualization types: - Bar Plot: Feature importance ranking - Beeswarm Plot: Feature value impact distribution - Violin Plot: SHAP value distributions - Heatmap: Instance-level feature contributions - Waterfall Plot: Individual prediction explanations - Confusion Matrix: Model performance visualization</p>"},{"location":"index.html#dataset-management","title":"\ud83d\udcca Dataset Management","text":"<ul> <li>UCI ML Repository: Direct access to 100+ datasets</li> <li>Custom Upload: Support for CSV files</li> <li>Automatic Preprocessing: Handle missing values, feature scaling</li> <li>Target Transformation: Binary classification support (-1/+1 encoding)</li> </ul>"},{"location":"index.html#ai-powered-reports","title":"\ud83d\udcdd AI-Powered Reports","text":"<p>Generate comprehensive analysis reports using: - Google Gemini - OpenAI GPT - Anthropic Claude</p> <p>Reports include performance metrics, SHAP interpretations, and governance recommendations.</p>"},{"location":"index.html#supported-models","title":"Supported Models","text":""},{"location":"index.html#quantum-models","title":"Quantum Models","text":"<ul> <li>Data Reuploading Classifier: Quantum circuit with data re-uploading</li> <li>Circuit-Centric Classifier: Parameterized quantum circuits</li> <li>Quantum Kitchen Sinks: Quantum feature maps</li> <li>Quantum Metric Learner: Metric learning with quantum circuits</li> <li>Dressed Quantum Circuit Classifier: Hybrid quantum-classical</li> </ul>"},{"location":"index.html#classical-models","title":"Classical Models","text":"<ul> <li>Support Vector Classifier (SVC): With multiple kernels</li> <li>Multi-Layer Perceptron (MLP): Neural network classifier</li> <li>Perceptron: Simple linear classifier</li> </ul>"},{"location":"index.html#documentation","title":"Documentation","text":""},{"location":"index.html#getting-started","title":"\ud83d\ude80 Getting Started","text":"<ul> <li>Quick Start Guide - Get up and running in 5 minutes</li> <li>QuOptuna Next - Modern drag-and-drop workflow interface</li> <li>Running Without Docker - Local development setup</li> </ul>"},{"location":"index.html#user-guides","title":"\ud83d\udcd6 User Guides","text":"<ul> <li>User Guide - Complete walkthrough of the Streamlit interface</li> <li>Streamlit Guide - Streamlit-specific features and tips</li> <li>Workflow Builder Guide - Create and manage workflows</li> <li>Frontend Quick Reference - Quick reference for frontend features</li> </ul>"},{"location":"index.html#architecture-design","title":"\ud83c\udfd7\ufe0f Architecture &amp; Design","text":"<ul> <li>Frontend Architecture - Frontend design and components</li> <li>Frontend Architecture Diagram - Visual architecture overview</li> <li>Optimizer Architecture - Backend optimization system design</li> <li>New Frontend Design - Latest frontend improvements</li> </ul>"},{"location":"index.html#development","title":"\ud83d\udee0\ufe0f Development","text":"<ul> <li>Testing Checklist - Testing best practices</li> <li>Workflow Testing - Test workflow components</li> <li>Implementation Summary - Recent implementation updates</li> </ul>"},{"location":"index.html#configuration","title":"\u2699\ufe0f Configuration","text":"<ul> <li>GitHub Settings Guide - Repository configuration</li> <li>GitHub Pages Setup - Deploy documentation site</li> </ul>"},{"location":"index.html#api-documentation","title":"\ud83d\udcda API Documentation","text":"<ul> <li>API Reference - Detailed API documentation for Python usage</li> <li>API Docs - Additional API documentation</li> </ul>"},{"location":"index.html#examples","title":"\ud83d\udca1 Examples","text":"<ul> <li>Examples - Code examples for common use cases</li> </ul>"},{"location":"index.html#roadmap","title":"\ud83d\uddfa\ufe0f Roadmap","text":"<ul> <li>Implementation Roadmap - Future plans and features</li> </ul>"},{"location":"index.html#changelog","title":"\ud83d\udccb Changelog","text":"<ul> <li>Changelog - Version history and updates</li> </ul>"},{"location":"index.html#workflow","title":"Workflow","text":"<p>QuOptuna provides a structured workflow:</p> <ol> <li>Dataset Selection</li> <li>Load from UCI ML Repository or upload CSV</li> <li>Configure features and target</li> <li> <p>Apply preprocessing</p> </li> <li> <p>Optimization</p> </li> <li>Prepare train/test splits</li> <li>Run hyperparameter optimization</li> <li> <p>Review best performing models</p> </li> <li> <p>Model Training</p> </li> <li>Select best trial</li> <li>Train model with optimized parameters</li> <li> <p>Evaluate performance</p> </li> <li> <p>SHAP Analysis</p> </li> <li>Calculate SHAP values</li> <li>Generate multiple visualization types</li> <li> <p>Understand feature importance</p> </li> <li> <p>Report Generation</p> </li> <li>Create AI-powered analysis</li> <li>Export results</li> <li>Share insights</li> </ol>"},{"location":"index.html#system-requirements","title":"System Requirements","text":"<ul> <li>Python 3.8+</li> <li>4GB+ RAM (8GB recommended for quantum models)</li> <li>Internet connection (for UCI datasets and LLM reports)</li> </ul>"},{"location":"index.html#dependencies","title":"Dependencies","text":"<p>Core dependencies: - <code>optuna</code> - Hyperparameter optimization - <code>streamlit</code> - Web interface - <code>shap</code> - Explainable AI - <code>pennylane</code> - Quantum computing - <code>scikit-learn</code> - Classical ML models - <code>pandas</code>, <code>numpy</code> - Data processing</p>"},{"location":"index.html#development_1","title":"Development","text":""},{"location":"index.html#setup","title":"Setup","text":"<pre><code># Clone repository\ngit clone https://github.com/Qentora/quoptuna.git\ncd quoptuna\n\n# Install development dependencies\nuv pip install -e \".[dev]\"\n\n# Run tests\nuv run pytest\n\n# Run linting\nuv run ruff check .\nuv run mypy .\n</code></pre>"},{"location":"index.html#project-structure","title":"Project Structure","text":"<pre><code>quoptuna/\n\u251c\u2500\u2500 src/quoptuna/\n\u2502   \u251c\u2500\u2500 backend/         # Core optimization and model code\n\u2502   \u2502   \u251c\u2500\u2500 models/      # Model implementations\n\u2502   \u2502   \u251c\u2500\u2500 tuners/      # Optuna integration\n\u2502   \u2502   \u251c\u2500\u2500 xai/         # SHAP analysis\n\u2502   \u2502   \u2514\u2500\u2500 utils/       # Utilities\n\u2502   \u2514\u2500\u2500 frontend/        # Streamlit interface\n\u2502       \u251c\u2500\u2500 pages/       # Multi-page app\n\u2502       \u251c\u2500\u2500 app.py       # Main application\n\u2502       \u2514\u2500\u2500 support.py   # Helper functions\n\u251c\u2500\u2500 docs/                # Documentation\n\u251c\u2500\u2500 experiments/         # Example notebooks\n\u2514\u2500\u2500 tests/              # Unit tests\n</code></pre>"},{"location":"index.html#contributing","title":"Contributing","text":"<p>We welcome contributions! Please see our Contributing Guidelines.</p>"},{"location":"index.html#ways-to-contribute","title":"Ways to Contribute","text":"<ul> <li>\ud83d\udc1b Report bugs and issues</li> <li>\ud83d\udca1 Suggest new features</li> <li>\ud83d\udcdd Improve documentation</li> <li>\ud83d\udd27 Submit pull requests</li> <li>\u2b50 Star the repository</li> </ul>"},{"location":"index.html#support-community","title":"Support &amp; Community","text":"<ul> <li>GitHub Issues: Report bugs or request features</li> <li>Discussions: Join the community</li> <li>Documentation: Full docs</li> </ul>"},{"location":"index.html#license","title":"License","text":"<p>QuOptuna is released under the MIT License. See LICENSE for details.</p>"},{"location":"index.html#citation","title":"Citation","text":"<p>If you use QuOptuna in your research, please cite:</p> <pre><code>@software{quoptuna,\n  title = {QuOptuna: Quantum-Enhanced Machine Learning Optimization},\n  author = {QuOptuna Team},\n  year = {2024},\n  url = {https://github.com/Qentora/quoptuna}\n}\n</code></pre>"},{"location":"index.html#acknowledgments","title":"Acknowledgments","text":"<p>Built with: - Optuna - Hyperparameter optimization framework - PennyLane - Quantum machine learning - SHAP - Explainable AI - Streamlit - Web framework</p> <p>Ready to get started? Check out the Quick Start Guide or User Guide, then launch the app with <code>quoptuna --start</code>!</p>"},{"location":"api/api-docs.html","title":"API Documentation","text":""},{"location":"api/api-docs.html#api-documentation","title":"API Documentation","text":""},{"location":"api/api-docs.html#main-api","title":"Main API","text":""},{"location":"api/api-docs.html#quoptuna.XAI","title":"XAI","text":"<pre><code>XAI(model: BaseEstimator, data: DataSet, config: XAIConfig | None = None)\n</code></pre> METHOD DESCRIPTION <code>__str__</code> <code>_generate_final_report</code> <code>_generate_plot</code> <code>_generate_report_images</code> <code>_get_explainer</code> <code>_get_plot_function</code> <code>_get_plot_values</code> <code>_get_shap_values</code> <code>_get_shap_values_each_class</code> <code>_handle_plot_error</code> <p>Handle plot generation errors.</p> <code>_initialize_chat</code> <code>_save_plot_to_base64</code> <code>_save_plot_to_file</code> <code>_set_shap_values_classes</code> <code>_validate_and_get_data</code> <code>_validate_shap_values_class</code> <p>Validate and get SHAP values for class-specific case.</p> <code>generate_report_with_langchain</code> <p>Generate comprehensive report using LangChain and multimodal LLM.</p> <code>get_average_precision_score</code> <p>Get the average precision score of the model.</p> <code>get_bar_plot</code> <code>get_beeswarm_plot</code> <code>get_classes</code> <p>Get model classes.</p> <code>get_classification_report</code> <p>Get the classification report of the model.</p> <code>get_cohens_kappa</code> <p>Get the cohens kappa of the model.</p> <code>get_confusion_matrix</code> <p>Get the confusion matrix of the model.</p> <code>get_f1_score</code> <p>Get the f1 score of the model.</p> <code>get_heatmap_plot</code> <code>get_log_loss</code> <p>Get the log loss of the model.</p> <code>get_mcc</code> <p>Get the mcc of the model.</p> <code>get_plot</code> <p>Generate plot with given configuration.</p> <code>get_precision</code> <p>Get the precision of the model.</p> <code>get_precision_recall_curve</code> <p>Get the precision recall curve of the model.</p> <code>get_recall</code> <p>Get the recall of the model.</p> <code>get_report</code> <p>Get the report of the model.</p> <code>get_roc_auc_score</code> <p>Get the roc auc score of the model.</p> <code>get_roc_curve</code> <p>Get the roc curve of the model.</p> <code>get_violin_plot</code> <code>get_waterfall_plot</code> <code>load_state</code> <p>Loads the state of the class from a pkl file.</p> <code>plot_confusion_matrix</code> <p>Plot confusion matrix with given configuration.</p> <code>save_state</code> <p>Saves the state of the class and its variables in a pkl file.</p> <code>validate_predict_proba</code> ATTRIBUTE DESCRIPTION <code>_classes</code> <p> </p> <code>_explainer</code> <p> TYPE: <code>Explainer | None</code> </p> <code>_predictions</code> <p> TYPE: <code>Series | None</code> </p> <code>_predictions_proba</code> <p> TYPE: <code>DataFrame | None</code> </p> <code>_shap_values</code> <p> TYPE: <code>Explanation | None</code> </p> <code>_shap_values_each_class</code> <p> TYPE: <code>dict[str, Explanation] | None</code> </p> <code>_x_test</code> <p> TYPE: <code>DataFrame | None</code> </p> <code>_y_test</code> <p> TYPE: <code>Series | None</code> </p> <code>config</code> <p> </p> <code>data</code> <p> </p> <code>data_key</code> <p> TYPE: <code>str</code> </p> <code>explainer</code> <p> TYPE: <code>Explainer</code> </p> <code>feature_names</code> <p> TYPE: <code>list[str] | None</code> </p> <code>max_display</code> <p> TYPE: <code>int</code> </p> <code>model</code> <p> </p> <code>onsubset</code> <p> TYPE: <code>bool</code> </p> <code>predictions</code> <p> TYPE: <code>Series</code> </p> <code>predictions_proba</code> <p> TYPE: <code>DataFrame</code> </p> <code>shap_values</code> <p> TYPE: <code>Explanation</code> </p> <code>shap_values_each_class</code> <p> TYPE: <code>dict[str, Explanation] | None</code> </p> <code>subset_size</code> <p> TYPE: <code>int</code> </p> <code>use_proba</code> <p> TYPE: <code>bool</code> </p> <code>x_test</code> <p> TYPE: <code>DataFrame</code> </p> <code>x_test_key</code> <p> TYPE: <code>str</code> </p> <code>y_test</code> <p> TYPE: <code>Series</code> </p> <code>y_test_key</code> <p> TYPE: <code>str</code> </p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def __init__(\n    self,\n    model: BaseEstimator,\n    data: DataSet,\n    config: XAIConfig | None = None,\n) -&gt; None:\n    if model is None:\n        msg = \"Model cannot be None\"\n        raise TypeError(msg)\n\n    self.config = config or XAIConfig()\n    self.model = model\n    self.data = data\n\n    # Explicitly declare instance attributes\n    self.use_proba: bool = self.config.use_proba\n    self.onsubset: bool = self.config.onsubset\n    self.feature_names: list[str] | None = self.config.feature_names\n    self.subset_size: int = self.config.subset_size\n    self.max_display: int = self.config.max_display\n    self.data_key: str = self.config.data_key\n    self.x_test_key: str = self.config.x_test_key\n    self.y_test_key: str = self.config.y_test_key\n\n    self._classes = self.get_classes\n    data_frame = self.data.get(self.data_key)\n    if self.feature_names is None and isinstance(data_frame, pd.DataFrame):\n        self.feature_names = list(data_frame.columns)\n\n    if self.use_proba:\n        self.validate_predict_proba()\n\n    # Initialize these as None, they'll be computed on demand\n    self._explainer: Explainer | None = None\n    self._shap_values: shap.Explanation | None = None\n    self._shap_values_each_class: dict[str, shap.Explanation] | None = None\n    self._x_test: pd.DataFrame | None = None\n    self._y_test: pd.Series | None = None\n    self._predictions: pd.Series | None = None\n    self._predictions_proba: pd.DataFrame | None = None\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI._classes","title":"_classes  <code>instance-attribute</code>","text":"<pre><code>_classes = get_classes\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI._explainer","title":"_explainer  <code>instance-attribute</code>","text":"<pre><code>_explainer: Explainer | None = None\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI._predictions","title":"_predictions  <code>instance-attribute</code>","text":"<pre><code>_predictions: Series | None = None\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI._predictions_proba","title":"_predictions_proba  <code>instance-attribute</code>","text":"<pre><code>_predictions_proba: DataFrame | None = None\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI._shap_values","title":"_shap_values  <code>instance-attribute</code>","text":"<pre><code>_shap_values: Explanation | None = None\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI._shap_values_each_class","title":"_shap_values_each_class  <code>instance-attribute</code>","text":"<pre><code>_shap_values_each_class: dict[str, Explanation] | None = None\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI._x_test","title":"_x_test  <code>instance-attribute</code>","text":"<pre><code>_x_test: DataFrame | None = None\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI._y_test","title":"_y_test  <code>instance-attribute</code>","text":"<pre><code>_y_test: Series | None = None\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config or XAIConfig()\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data = data\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.data_key","title":"data_key  <code>instance-attribute</code>","text":"<pre><code>data_key: str = data_key\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.explainer","title":"explainer  <code>property</code>","text":"<pre><code>explainer: Explainer\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.feature_names","title":"feature_names  <code>instance-attribute</code>","text":"<pre><code>feature_names: list[str] | None = feature_names\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.max_display","title":"max_display  <code>instance-attribute</code>","text":"<pre><code>max_display: int = max_display\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = model\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.onsubset","title":"onsubset  <code>instance-attribute</code>","text":"<pre><code>onsubset: bool = onsubset\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.predictions","title":"predictions  <code>property</code>","text":"<pre><code>predictions: Series\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.predictions_proba","title":"predictions_proba  <code>property</code>","text":"<pre><code>predictions_proba: DataFrame\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.shap_values","title":"shap_values  <code>property</code>","text":"<pre><code>shap_values: Explanation\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.shap_values_each_class","title":"shap_values_each_class  <code>property</code>","text":"<pre><code>shap_values_each_class: dict[str, Explanation] | None\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.subset_size","title":"subset_size  <code>instance-attribute</code>","text":"<pre><code>subset_size: int = subset_size\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.use_proba","title":"use_proba  <code>instance-attribute</code>","text":"<pre><code>use_proba: bool = use_proba\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.x_test","title":"x_test  <code>property</code>","text":"<pre><code>x_test: DataFrame\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.x_test_key","title":"x_test_key  <code>instance-attribute</code>","text":"<pre><code>x_test_key: str = x_test_key\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.y_test","title":"y_test  <code>property</code>","text":"<pre><code>y_test: Series\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.y_test_key","title":"y_test_key  <code>instance-attribute</code>","text":"<pre><code>y_test_key: str = y_test_key\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def __str__(self):\n    return str(self.get_report())\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI._generate_final_report","title":"_generate_final_report","text":"<pre><code>_generate_final_report(chat, report, images, prompt2)\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def _generate_final_report(self, chat, report, images, prompt2):\n    prompt_template = ChatPromptTemplate(\n        messages=[\n            SystemMessage(content=prompt2),\n            HumanMessage(content=\"Model Evaluation Report:\\n```\\n{report}\\n```\"),\n            MessagesPlaceholder(variable_name=\"images\"),\n        ]\n    )\n    image_messages = []\n    for plot_type, image_url in images.items():\n        image_messages.append(\n            HumanMessage(\n                content=[\n                    {\"type\": \"text\", \"text\": f\"Here is a {plot_type.replace('_', ' ')} plot:\"},\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": image_url, \"detail\": \"auto\"}},\n                ]\n            )\n        )\n\n    final_prompt = prompt_template.format_messages(report=str(report), images=image_messages)\n\n    response = chat(final_prompt)\n    return response.content\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI._generate_plot","title":"_generate_plot","text":"<pre><code>_generate_plot(plot_type: PlotType, values, max_display: int, index: int, save_config: dict | None) -&gt; str\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def _generate_plot(\n    self, plot_type: PlotType, values, max_display: int, index: int, save_config: dict | None\n) -&gt; str:\n    plt.figure()\n    if plot_type != \"waterfall\":\n        plot_func = self._get_plot_function(plot_type)\n        plot_func(values, max_display=max_display, show=False)\n    else:\n        shap.plots.waterfall(values[index], show=False)\n\n    base64_code = self._save_plot_to_base64()\n\n    if save_config is not None:  # Check if save_config exists\n        self._save_plot_to_file(save_config)\n\n    plt.close()\n    return base64_code\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI._generate_report_images","title":"_generate_report_images","text":"<pre><code>_generate_report_images(num_waterfall_plots: int)\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def _generate_report_images(self, num_waterfall_plots: int):\n    images: dict[str, str] = {}  # Change type hint to allow string keys\n    plot_types: list[PlotType] = [\"bar\", \"beeswarm\", \"violin\", \"heatmap\"]\n\n    try:\n        for plot_type in plot_types:\n            images[plot_type] = self.get_plot(plot_type)\n\n        if self.onsubset:\n            num_waterfall_plots = min(num_waterfall_plots, self.subset_size)\n        else:\n            num_waterfall_plots = min(num_waterfall_plots, len(self.x_test))\n\n        indices = sorted(random.sample(range(num_waterfall_plots), num_waterfall_plots))\n        for i in indices:\n            waterfall_plot_type: PlotType = \"waterfall\"\n            images[f\"{waterfall_plot_type}_{i}\"] = self.get_plot(waterfall_plot_type, index=i)\n\n        fig = self.plot_confusion_matrix()\n        img_buf = io.BytesIO()\n        fig.savefig(img_buf, format=\"png\")\n        img_buf.seek(0)\n        img_base64 = base64.b64encode(img_buf.getvalue()).decode(\"utf-8\")\n        images[\"confusion_matrix\"] = f\"data:image/png;base64,{img_base64}\"\n        plt.close(fig)\n\n    except Exception as e:\n        msg = f\"Error generating plots: {e}\"\n        raise ValueError(msg) from e\n\n    return images\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI._get_explainer","title":"_get_explainer","text":"<pre><code>_get_explainer() -&gt; Explainer\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def _get_explainer(self) -&gt; Explainer:\n    predict_method = self.model.predict_proba if self.use_proba else self.model.predict\n    data = self._validate_and_get_data()\n    if self.onsubset:\n        data = data.iloc[: self.subset_size]\n    return Explainer(model=predict_method, masker=data, feature_names=self.feature_names)\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI._get_plot_function","title":"_get_plot_function","text":"<pre><code>_get_plot_function(plot_type: PlotType)\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def _get_plot_function(self, plot_type: PlotType):\n    return {\n        \"bar\": shap.plots.bar,\n        \"beeswarm\": shap.plots.beeswarm,\n        \"heatmap\": shap.plots.heatmap,\n        \"violin\": shap.plots.violin,\n    }[plot_type]\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI._get_plot_values","title":"_get_plot_values","text":"<pre><code>_get_plot_values(class_index: int) -&gt; Explanation\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def _get_plot_values(self, class_index: int) -&gt; shap.Explanation:\n    if class_index == -1:\n        return self.shap_values\n    if self.shap_values.values.ndim &gt; EXPECTED_SHAP_VALUES_DIM:  # noqa: PD011\n        if self.shap_values_each_class is None:\n            msg = \"No class-specific SHAP values available\"\n            raise ValueError(msg)\n        return self.shap_values_each_class[str(class_index)]\n    return self.shap_values\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI._get_shap_values","title":"_get_shap_values","text":"<pre><code>_get_shap_values() -&gt; Explanation\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def _get_shap_values(self) -&gt; shap.Explanation:\n    data = self._validate_and_get_data()\n    if self.onsubset:\n        data = data.iloc[: self.subset_size]\n    return self.explainer(data)\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI._get_shap_values_each_class","title":"_get_shap_values_each_class","text":"<pre><code>_get_shap_values_each_class(shap_values: Explanation) -&gt; dict[str, Explanation]\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def _get_shap_values_each_class(\n    self, shap_values: shap.Explanation\n) -&gt; dict[str, shap.Explanation]:\n    if shap_values.values.ndim &lt; EXPECTED_SHAP_VALUES_DIM:  # noqa: PD011\n        msg = \"shap_values has less than 2 dimensions\"\n        raise TypeError(msg)\n    return {str(i): shap_values[:, :, i] for i in self.get_classes()}\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI._handle_plot_error","title":"_handle_plot_error","text":"<pre><code>_handle_plot_error(plot_type: PlotType, error: Exception) -&gt; None\n</code></pre> <p>Handle plot generation errors.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def _handle_plot_error(self, plot_type: PlotType, error: Exception) -&gt; None:\n    \"\"\"Handle plot generation errors.\"\"\"\n    if isinstance(error, (ValueError, TypeError, KeyError)):\n        raise error\n    msg = f\"Error generating {plot_type} plot: {error!s}\"\n    raise RuntimeError(msg) from error\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI._initialize_chat","title":"_initialize_chat","text":"<pre><code>_initialize_chat(api_key: str, model_name: str, provider: str)\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def _initialize_chat(self, api_key: str, model_name: str, provider: str):\n    if provider == \"google\":\n        return ChatGoogleGenerativeAI(google_api_key=api_key, model=model_name)\n    if provider == \"openai\":\n        return ChatOpenAI(openai_api_key=api_key, model_name=model_name)\n    msg = \"Invalid provider\"\n    raise ValueError(msg)\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI._save_plot_to_base64","title":"_save_plot_to_base64","text":"<pre><code>_save_plot_to_base64() -&gt; str\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def _save_plot_to_base64(self) -&gt; str:\n    img_buf = io.BytesIO()\n    plt.savefig(img_buf, format=\"png\")\n    img_buf.seek(0)\n    img_base64 = base64.b64encode(img_buf.getvalue()).decode(\"utf-8\")\n    return f\"data:image/png;base64,{img_base64}\"\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI._save_plot_to_file","title":"_save_plot_to_file","text":"<pre><code>_save_plot_to_file(save_config: dict) -&gt; None\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def _save_plot_to_file(self, save_config: dict) -&gt; None:\n    save_path = save_config.get(\"save_path\")\n    save_name = save_config.get(\"save_name\")\n    if not save_path or not save_name:\n        return\n\n    plt.savefig(\n        Path(save_path) / save_name,\n        format=save_config.get(\"save_format\", \"png\"),\n        dpi=save_config.get(\"save_dpi\", 300),\n        bbox_inches=\"tight\",\n    )\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI._set_shap_values_classes","title":"_set_shap_values_classes","text":"<pre><code>_set_shap_values_classes() -&gt; dict[str, Explanation] | None\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def _set_shap_values_classes(self) -&gt; dict[str, shap.Explanation] | None:\n    if not (self.shap_values and self.shap_values.values.ndim &gt; EXPECTED_SHAP_VALUES_DIM):  # noqa: PD011\n        return None\n    return self._get_shap_values_each_class(self.shap_values)\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI._validate_and_get_data","title":"_validate_and_get_data","text":"<pre><code>_validate_and_get_data() -&gt; DataFrame\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def _validate_and_get_data(self) -&gt; pd.DataFrame:\n    data = self.data.get(DATA_KEY)\n    if not isinstance(data, pd.DataFrame):\n        msg = f\"Expected {DATA_KEY} to be a pandas DataFrame\"\n        raise TypeError(msg)\n    return data\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI._validate_shap_values_class","title":"_validate_shap_values_class","text":"<pre><code>_validate_shap_values_class() -&gt; Explanation\n</code></pre> <p>Validate and get SHAP values for class-specific case.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def _validate_shap_values_class(self) -&gt; shap.Explanation:\n    \"\"\"Validate and get SHAP values for class-specific case.\"\"\"\n    if self.shap_values_each_class is None:\n        msg = \"No class-specific SHAP values available\"\n        raise ValueError(msg)\n    first_class = next(iter(self.get_classes()))\n    return self.shap_values_each_class[str(first_class)]\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.generate_report_with_langchain","title":"generate_report_with_langchain","text":"<pre><code>generate_report_with_langchain(api_key: str, model_name: str = 'gpt-4o', provider: str = 'google', num_waterfall_plots: int = 5)\n</code></pre> <p>Generate comprehensive report using LangChain and multimodal LLM.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def generate_report_with_langchain(\n    self,\n    api_key: str,\n    model_name: str = \"gpt-4o\",\n    provider: str = \"google\",\n    num_waterfall_plots: int = 5,\n):\n    \"\"\"Generate comprehensive report using LangChain and multimodal LLM.\"\"\"\n    chat = self._initialize_chat(api_key, model_name, provider)\n\n    report = self.get_report()\n    images = self._generate_report_images(num_waterfall_plots)\n\n    prompt2 = Path(\"prompt.txt\").read_text()\n    return self._generate_final_report(chat, report, images, prompt2)\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.get_average_precision_score","title":"get_average_precision_score","text":"<pre><code>get_average_precision_score()\n</code></pre> <p>Get the average precision score of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_average_precision_score(self):\n    \"\"\"Get the average precision score of the model.\"\"\"\n    return average_precision_score(self.y_test, self.predictions_proba)\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.get_bar_plot","title":"get_bar_plot","text":"<pre><code>get_bar_plot(max_display: int = DEFAULT_MAX_DISPLAY, class_index: int = -1)\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_bar_plot(self, max_display: int = DEFAULT_MAX_DISPLAY, class_index: int = -1):\n    return self.get_plot(\"bar\", max_display, class_index=class_index)\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.get_beeswarm_plot","title":"get_beeswarm_plot","text":"<pre><code>get_beeswarm_plot(max_display: int = DEFAULT_MAX_DISPLAY, class_index: int = -1)\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_beeswarm_plot(self, max_display: int = DEFAULT_MAX_DISPLAY, class_index: int = -1):\n    return self.get_plot(\"beeswarm\", max_display, class_index=class_index)\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.get_classes","title":"get_classes","text":"<pre><code>get_classes() -&gt; dict[int, str]\n</code></pre> <p>Get model classes.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_classes(self) -&gt; dict[int, str]:\n    \"\"\"Get model classes.\"\"\"\n    if not hasattr(self.model, \"classes_\"):\n        msg = \"Model does not have a classes_ attribute\"\n        raise TypeError(msg)\n    return self.model.classes_\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.get_classification_report","title":"get_classification_report","text":"<pre><code>get_classification_report()\n</code></pre> <p>Get the classification report of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_classification_report(self):\n    \"\"\"Get the classification report of the model.\"\"\"\n    return classification_report(self.y_test, self.predictions)\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.get_cohens_kappa","title":"get_cohens_kappa","text":"<pre><code>get_cohens_kappa()\n</code></pre> <p>Get the cohens kappa of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_cohens_kappa(self):\n    \"\"\"Get the cohens kappa of the model.\"\"\"\n    return cohen_kappa_score(self.y_test, self.predictions)\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.get_confusion_matrix","title":"get_confusion_matrix","text":"<pre><code>get_confusion_matrix()\n</code></pre> <p>Get the confusion matrix of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_confusion_matrix(self):\n    \"\"\"Get the confusion matrix of the model.\"\"\"\n    return confusion_matrix(self.y_test, self.predictions)\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.get_f1_score","title":"get_f1_score","text":"<pre><code>get_f1_score()\n</code></pre> <p>Get the f1 score of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_f1_score(self):\n    \"\"\"Get the f1 score of the model.\"\"\"\n    return f1_score(self.y_test, self.predictions)\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.get_heatmap_plot","title":"get_heatmap_plot","text":"<pre><code>get_heatmap_plot(max_display: int = DEFAULT_MAX_DISPLAY, class_index: int = -1)\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_heatmap_plot(self, max_display: int = DEFAULT_MAX_DISPLAY, class_index: int = -1):\n    return self.get_plot(\"heatmap\", max_display, class_index=class_index)\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.get_log_loss","title":"get_log_loss","text":"<pre><code>get_log_loss()\n</code></pre> <p>Get the log loss of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_log_loss(self):\n    \"\"\"Get the log loss of the model.\"\"\"\n    return log_loss(self.y_test, self.predictions_proba)\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.get_mcc","title":"get_mcc","text":"<pre><code>get_mcc()\n</code></pre> <p>Get the mcc of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_mcc(self):\n    \"\"\"Get the mcc of the model.\"\"\"\n    return matthews_corrcoef(self.y_test, self.predictions)\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.get_plot","title":"get_plot","text":"<pre><code>get_plot(plot_type: PlotType, max_display: int = DEFAULT_MAX_DISPLAY, class_index: int = -1, index: int = 0, save_config: dict | None = None)\n</code></pre> <p>Generate plot with given configuration.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_plot(\n    self,\n    plot_type: PlotType,\n    max_display: int = DEFAULT_MAX_DISPLAY,\n    class_index: int = -1,\n    index: int = 0,\n    save_config: dict | None = None,\n):\n    \"\"\"Generate plot with given configuration.\"\"\"\n    try:\n        values = self._get_plot_values(class_index)\n        return self._generate_plot(plot_type, values, max_display, index, save_config)\n    except (ValueError, TypeError, KeyError, RuntimeError) as e:\n        self._handle_plot_error(plot_type, e)\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.get_precision","title":"get_precision","text":"<pre><code>get_precision()\n</code></pre> <p>Get the precision of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_precision(self):\n    \"\"\"Get the precision of the model.\"\"\"\n    return precision_score(self.y_test, self.predictions)\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.get_precision_recall_curve","title":"get_precision_recall_curve","text":"<pre><code>get_precision_recall_curve()\n</code></pre> <p>Get the precision recall curve of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_precision_recall_curve(self):\n    \"\"\"Get the precision recall curve of the model.\"\"\"\n    return precision_recall_curve(self.y_test, self.predictions_proba)\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.get_recall","title":"get_recall","text":"<pre><code>get_recall()\n</code></pre> <p>Get the recall of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_recall(self):\n    \"\"\"Get the recall of the model.\"\"\"\n    return recall_score(self.y_test, self.predictions)\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.get_report","title":"get_report","text":"<pre><code>get_report()\n</code></pre> <p>Get the report of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_report(self):\n    \"\"\"Get the report of the model.\"\"\"\n    report = {}\n    metrics = {\n        \"confusion_matrix\": self.get_confusion_matrix,\n        \"classification_report\": self.get_classification_report,\n        \"roc_curve\": self.get_roc_curve,\n        \"roc_auc_score\": self.get_roc_auc_score,\n        \"precision_recall_curve\": self.get_precision_recall_curve,\n        \"average_precision_score\": self.get_average_precision_score,\n        \"f1_score\": self.get_f1_score,\n        \"mcc\": self.get_mcc,\n        \"log_loss\": self.get_log_loss,\n        \"cohens_kappa\": self.get_cohens_kappa,\n        \"precision\": self.get_precision,\n        \"recall\": self.get_recall,\n    }\n\n    try:\n        for key, func in metrics.items():\n            report[key] = func()\n    except (ValueError, TypeError) as e:\n        report[key] = str(e)\n    return report\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.get_roc_auc_score","title":"get_roc_auc_score","text":"<pre><code>get_roc_auc_score()\n</code></pre> <p>Get the roc auc score of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_roc_auc_score(self):\n    \"\"\"Get the roc auc score of the model.\"\"\"\n    return roc_auc_score(self.y_test, self.predictions_proba)\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.get_roc_curve","title":"get_roc_curve","text":"<pre><code>get_roc_curve()\n</code></pre> <p>Get the roc curve of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_roc_curve(self):\n    \"\"\"Get the roc curve of the model.\"\"\"\n    return roc_curve(self.y_test, self.predictions_proba)\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.get_violin_plot","title":"get_violin_plot","text":"<pre><code>get_violin_plot(max_display: int = DEFAULT_MAX_DISPLAY, class_index: int = -1)\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_violin_plot(self, max_display: int = DEFAULT_MAX_DISPLAY, class_index: int = -1):\n    return self.get_plot(\"violin\", max_display, class_index=class_index)\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.get_waterfall_plot","title":"get_waterfall_plot","text":"<pre><code>get_waterfall_plot(max_display: int = DEFAULT_MAX_DISPLAY, index: int = 0, class_index: int = -1)\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_waterfall_plot(\n    self, max_display: int = DEFAULT_MAX_DISPLAY, index: int = 0, class_index: int = -1\n):\n    return self.get_plot(\"waterfall\", max_display, index=index, class_index=class_index)\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.load_state","title":"load_state  <code>classmethod</code>","text":"<pre><code>load_state(file_path: str)\n</code></pre> <p>Loads the state of the class from a pkl file. Warning: Only use this method with trusted data sources as pickle can be unsafe.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>@classmethod\ndef load_state(cls, file_path: str):\n    \"\"\"Loads the state of the class from a pkl file.\n    Warning: Only use this method with trusted data sources as pickle can be unsafe.\n    \"\"\"\n    with Path(file_path).open(\"rb\") as f:\n        return pickle.load(f)  # noqa: S301\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.plot_confusion_matrix","title":"plot_confusion_matrix","text":"<pre><code>plot_confusion_matrix(plot_config: dict | None = None)\n</code></pre> <p>Plot confusion matrix with given configuration.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def plot_confusion_matrix(self, plot_config: dict | None = None):\n    \"\"\"Plot confusion matrix with given configuration.\"\"\"\n    from sklearn.metrics import ConfusionMatrixDisplay\n\n    config = {\n        \"include_values\": True,\n        \"cmap\": \"viridis\",\n        \"xticks_rotation\": \"horizontal\",\n        \"values_format\": None,\n        \"ax\": None,\n        \"colorbar\": True,\n        \"im_kw\": None,\n        \"text_kw\": None,\n        **(plot_config or {}),\n    }\n\n    cm = self.get_confusion_matrix()\n    ConfusionMatrixDisplay(cm).plot(**config)\n\n    if plot_config and plot_config.get(\"save_path\"):\n        plt.savefig(\n            Path(plot_config[\"save_path\"]) / plot_config[\"save_name\"],\n            format=plot_config.get(\"save_format\", \"png\"),\n            dpi=plot_config.get(\"save_dpi\", 300),\n            bbox_inches=\"tight\",\n        )\n\n    return plt.gcf()\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.save_state","title":"save_state","text":"<pre><code>save_state(file_path: str)\n</code></pre> <p>Saves the state of the class and its variables in a pkl file.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def save_state(self, file_path: str):\n    \"\"\"Saves the state of the class and its variables in a pkl file.\"\"\"\n    with Path(file_path).open(\"wb\") as f:\n        pickle.dump(self, f)\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAI.validate_predict_proba","title":"validate_predict_proba","text":"<pre><code>validate_predict_proba() -&gt; bool\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def validate_predict_proba(self) -&gt; bool:\n    if not hasattr(self.model, \"predict_proba\"):\n        msg = \"Model does not have a predict_proba method\"\n        raise TypeError(msg)\n    return True\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAIConfig","title":"XAIConfig  <code>dataclass</code>","text":"<pre><code>XAIConfig(use_proba: bool = True, onsubset: bool = True, feature_names: list[str] | None = None, subset_size: int = DEFAULT_SUBSET_SIZE, max_display: int = DEFAULT_MAX_DISPLAY, data_key: str = DATA_KEY, x_test_key: str = 'x_test', y_test_key: str = 'y_test')\n</code></pre> ATTRIBUTE DESCRIPTION <code>data_key</code> <p> TYPE: <code>str</code> </p> <code>feature_names</code> <p> TYPE: <code>list[str] | None</code> </p> <code>max_display</code> <p> TYPE: <code>int</code> </p> <code>onsubset</code> <p> TYPE: <code>bool</code> </p> <code>subset_size</code> <p> TYPE: <code>int</code> </p> <code>use_proba</code> <p> TYPE: <code>bool</code> </p> <code>x_test_key</code> <p> TYPE: <code>str</code> </p> <code>y_test_key</code> <p> TYPE: <code>str</code> </p>"},{"location":"api/api-docs.html#quoptuna.XAIConfig.data_key","title":"data_key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>data_key: str = DATA_KEY\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAIConfig.feature_names","title":"feature_names  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>feature_names: list[str] | None = None\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAIConfig.max_display","title":"max_display  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_display: int = DEFAULT_MAX_DISPLAY\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAIConfig.onsubset","title":"onsubset  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>onsubset: bool = True\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAIConfig.subset_size","title":"subset_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>subset_size: int = DEFAULT_SUBSET_SIZE\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAIConfig.use_proba","title":"use_proba  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>use_proba: bool = True\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAIConfig.x_test_key","title":"x_test_key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>x_test_key: str = 'x_test'\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.XAIConfig.y_test_key","title":"y_test_key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>y_test_key: str = 'y_test'\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.Optimizer","title":"Optimizer","text":"<pre><code>Optimizer(db_name: str, dataset_name: str = '', data: Optional[dict] = None, study_name: str = '')\n</code></pre> <p>Initialize the Optimizer class.</p> PARAMETER DESCRIPTION <p>The name of the database to be used for storing optimization results.</p> <p> TYPE: <code>str</code> </p> <p>The name of the dataset. If provided, the data will be loaded from a CSV file located in the 'notebook' directory. Defaults to an empty string.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <p>A dictionary containing training and testing data. If not provided, an empty dictionary will be used. Expected keys are 'train_x', 'test_x', 'train_y', and 'test_y'.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> <p>The name of the study for Optuna. Defaults to an empty string.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> ATTRIBUTE DESCRIPTION <code>db_name</code> <p>The name of the database.</p> <p> </p> <code>dataset_name</code> <p>The name of the dataset.</p> <p> </p> <code>data_path</code> <p>The path to the dataset CSV file or an empty string if no dataset name is provided.</p> <p> </p> <code>data</code> <p>The data dictionary containing training and testing data.</p> <p> </p> <code>train_x</code> <p>The training features.</p> <p> </p> <code>test_x</code> <p>The testing features.</p> <p> </p> <code>train_y</code> <p>The training labels.</p> <p> </p> <code>test_y</code> <p>The testing labels.</p> <p> </p> <code>storage_location</code> <p>The storage location for the Optuna study.</p> <p> </p> <code>study_name</code> <p>The name of the Optuna study.</p> <p> </p> <code>study</code> <p>The Optuna study object.</p> <p> </p> METHOD DESCRIPTION <code>load_and_preprocess_data</code> <code>load_study</code> <code>log_user_attributes</code> <code>objective</code> <code>optimize</code> ATTRIBUTE DESCRIPTION <code>data</code> <p> </p> <code>data_path</code> <p> </p> <code>dataset_name</code> <p> </p> <code>db_name</code> <p> </p> <code>storage_location</code> <p> </p> <code>study</code> <p> </p> <code>study_name</code> <p> </p> <code>test_x</code> <p> </p> <code>test_y</code> <p> </p> <code>train_x</code> <p> </p> <code>train_y</code> <p> </p> Source code in <code>src/quoptuna/backend/tuners/optimizer.py</code> <pre><code>def __init__(\n    self,\n    db_name: str,\n    dataset_name: str = \"\",\n    data: Optional[dict] = None,  # noqa: FA100\n    study_name: str = \"\",\n):\n    \"\"\"Initialize the Optimizer class.\n\n    Args:\n        db_name: The name of the database to be used for storing optimization results.\n        dataset_name: The name of the dataset. If provided, the data will be loaded from a\n            CSV file located in the 'notebook' directory. Defaults to an empty string.\n        data: A dictionary containing training and testing data. If not provided, an empty\n            dictionary will be used. Expected keys are 'train_x', 'test_x', 'train_y', and 'test_y'.\n        study_name: The name of the study for Optuna. Defaults to an empty string.\n\n    Attributes:\n        db_name: The name of the database.\n        dataset_name: The name of the dataset.\n        data_path: The path to the dataset CSV file or an empty string if no dataset name is provided.\n        data: The data dictionary containing training and testing data.\n        train_x: The training features.\n        test_x: The testing features.\n        train_y: The training labels.\n        test_y: The testing labels.\n        storage_location: The storage location for the Optuna study.\n        study_name: The name of the Optuna study.\n        study: The Optuna study object.\n    \"\"\"\n    self.db_name = db_name\n    self.dataset_name = dataset_name\n    if len(self.dataset_name) &gt; 0:\n        self.data_path = f\"notebook/{self.dataset_name}.csv\"\n    else:\n        self.data_path = \"\"\n    self.data = data or {}  # Use an empty dictionary if no data is provided\n    self.train_x = self.data.get(\"train_x\")\n    self.test_x = self.data.get(\"test_x\")\n    self.train_y = self.data.get(\"train_y\")\n    self.test_y = self.data.get(\"test_y\")\n    self.data_path = f\"db/{self.db_name}.db\"\n    if not os.path.exists(\"db\"):  # noqa: PTH110\n        os.makedirs(\"db\")  # noqa: PTH103\n    self.storage_location = f\"sqlite:///{self.data_path}\"\n    self.study_name = study_name\n    self.study = None\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.Optimizer(db_name)","title":"<code>db_name</code>","text":""},{"location":"api/api-docs.html#quoptuna.Optimizer(dataset_name)","title":"<code>dataset_name</code>","text":""},{"location":"api/api-docs.html#quoptuna.Optimizer(data)","title":"<code>data</code>","text":""},{"location":"api/api-docs.html#quoptuna.Optimizer(study_name)","title":"<code>study_name</code>","text":""},{"location":"api/api-docs.html#quoptuna.Optimizer.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data = data or {}\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.Optimizer.data_path","title":"data_path  <code>instance-attribute</code>","text":"<pre><code>data_path = f'db/{db_name}.db'\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.Optimizer.dataset_name","title":"dataset_name  <code>instance-attribute</code>","text":"<pre><code>dataset_name = dataset_name\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.Optimizer.db_name","title":"db_name  <code>instance-attribute</code>","text":"<pre><code>db_name = db_name\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.Optimizer.storage_location","title":"storage_location  <code>instance-attribute</code>","text":"<pre><code>storage_location = f'sqlite:///{data_path}'\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.Optimizer.study","title":"study  <code>instance-attribute</code>","text":"<pre><code>study = None\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.Optimizer.study_name","title":"study_name  <code>instance-attribute</code>","text":"<pre><code>study_name = study_name\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.Optimizer.test_x","title":"test_x  <code>instance-attribute</code>","text":"<pre><code>test_x = get('test_x')\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.Optimizer.test_y","title":"test_y  <code>instance-attribute</code>","text":"<pre><code>test_y = get('test_y')\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.Optimizer.train_x","title":"train_x  <code>instance-attribute</code>","text":"<pre><code>train_x = get('train_x')\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.Optimizer.train_y","title":"train_y  <code>instance-attribute</code>","text":"<pre><code>train_y = get('train_y')\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.Optimizer.load_and_preprocess_data","title":"load_and_preprocess_data","text":"<pre><code>load_and_preprocess_data()\n</code></pre> Source code in <code>src/quoptuna/backend/tuners/optimizer.py</code> <pre><code>def load_and_preprocess_data(self):\n    self.X, self.y = load_data(self.data_path)\n    self.train_x, self.test_x, self.train_y, self.test_y = preprocess_data(self.X, self.y)\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.Optimizer.load_study","title":"load_study","text":"<pre><code>load_study()\n</code></pre> Source code in <code>src/quoptuna/backend/tuners/optimizer.py</code> <pre><code>def load_study(self):\n    # load the study from the database\n    self.study = load_study(\n        storage=self.storage_location,\n        study_name=self.study_name,\n    )\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.Optimizer.log_user_attributes","title":"log_user_attributes","text":"<pre><code>log_user_attributes(model_type, eval_scores, trial)\n</code></pre> Source code in <code>src/quoptuna/backend/tuners/optimizer.py</code> <pre><code>def log_user_attributes(self, model_type, eval_scores, trial):\n    if model_type in [\"SVC\", \"SVClinear\", \"MLPClassifier\", \"Perceptron\"]:\n        trial.set_user_attr(\"Classical_accuracy\", eval_scores[\"accuracy\"])\n        trial.set_user_attr(\"Classical_f1_score\", eval_scores[\"f1_score\"])\n        trial.set_user_attr(\"Classical_score\", eval_scores[\"score\"])\n        trial.set_user_attr(\"Quantum_accuracy\", 0)\n        trial.set_user_attr(\"Quantum_f1_score\", 0)\n        trial.set_user_attr(\"Quantum_score\", 0)\n    else:\n        trial.set_user_attr(\"Quantum_accuracy\", eval_scores[\"accuracy\"])\n        trial.set_user_attr(\"Quantum_f1_score\", eval_scores[\"f1_score\"])\n        trial.set_user_attr(\"Quantum_score\", eval_scores[\"score\"])\n        trial.set_user_attr(\"Classical_accuracy\", 0)\n        trial.set_user_attr(\"Classical_f1_score\", 0)\n        trial.set_user_attr(\"Classical_score\", 0)\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.Optimizer.objective","title":"objective","text":"<pre><code>objective(trial: Trial)\n</code></pre> Source code in <code>src/quoptuna/backend/tuners/optimizer.py</code> <pre><code>def objective(self, trial: Trial):\n    try:\n        # Define the hyperparameter search space\n        params = {\n            \"max_vmap\": trial.suggest_categorical(\"max_vmap\", [1]),\n            \"batch_size\": trial.suggest_categorical(\"batch_size\", [32]),\n            \"learning_rate\": trial.suggest_categorical(\"learning_rate\", [0.001, 0.01, 0.1]),\n            \"n_input_copies\": trial.suggest_categorical(\"n_input_copies\", [1, 2, 3]),\n            \"n_layers\": trial.suggest_categorical(\"n_layers\", [1, 5, 10]),\n            \"observable_type\": trial.suggest_categorical(\n                \"observable_type\", [\"single\", \"half\", \"full\"]\n            ),\n            \"repeats\": trial.suggest_categorical(\"repeats\", [1, 5, 10]),\n            \"C\": trial.suggest_categorical(\"C\", [0.1, 1, 10, 100]),\n            \"gamma_factor\": trial.suggest_categorical(\"gamma_factor\", [0.1, 1, 10]),\n            \"trotter_steps\": trial.suggest_categorical(\"trotter_steps\", [1, 3, 5]),\n            \"t\": trial.suggest_categorical(\"t\", [0.01, 0.1, 1.0]),\n            \"n_qfeatures\": trial.suggest_categorical(\"n_qfeatures\", [\"full\", \"half\"]),\n            \"n_episodes\": trial.suggest_categorical(\"n_episodes\", [10, 100, 500, 2000]),\n            \"visible_qubits\": trial.suggest_categorical(\n                \"visible_qubits\", [\"single\", \"half\", \"full\"]\n            ),\n            \"temperature\": trial.suggest_categorical(\"temperature\", [1, 10, 100]),\n            \"encoding_layers\": trial.suggest_categorical(\"encoding_layers\", [1, 3, 5, 10]),\n            \"degree\": trial.suggest_categorical(\"degree\", [2, 3, 4]),\n            \"n_qchannels\": trial.suggest_categorical(\"n_qchannels\", [1, 5, 10]),\n            \"qkernel_shape\": trial.suggest_categorical(\"qkernel_shape\", [2, 3]),\n            \"kernel_shape\": trial.suggest_categorical(\"kernel_shape\", [2, 3, 5]),\n            \"filter_name\": trial.suggest_categorical(\n                \"filter_name\", [\"edge_detect\", \"smooth\", \"sharpen\"]\n            ),\n            \"gamma\": trial.suggest_categorical(\"gamma\", [0.001, 0.01, 0.1, 1]),\n            \"alpha\": trial.suggest_categorical(\"alpha\", [0.01, 0.001, 0.0001]),\n            \"hidden_layer_sizes\": trial.suggest_categorical(\n                \"hidden_layer_sizes\", [\"[100,)\", \"(10, 10, 10, 10)\", \"(50, 10, 5)\"]\n            ),\n            \"eta0\": trial.suggest_categorical(\"eta0\", [0.1, 1, 10]),\n        }\n\n        model_type = trial.suggest_categorical(\n            \"model_type\",\n            [\n                \"CircuitCentricClassifier\",\n                \"DataReuploadingClassifier\",\n                \"DataReuploadingClassifierSeparable\",\n                \"DressedQuantumCircuitClassifier\",\n                \"DressedQuantumCircuitClassifierSeparable\",\n                \"ProjectedQuantumKernel\",\n                \"QuantumKitchenSinks\",\n                \"QuantumMetricLearner\",\n                \"TreeTensorClassifier\",\n                \"SeparableVariationalClassifier\",\n                \"SeparableKernelClassifier\",\n                \"SVC\",\n                \"SVClinear\",\n                \"MLPClassifier\",\n                \"Perceptron\",\n            ],\n        )\n\n        model = create_model(model_type, **params)\n\n        model.fit(self.train_x, self.train_y)\n        score = model.score(self.test_x, self.test_y)\n\n        f_score_ = f1_score(self.test_y, model.predict(self.test_x))\n        acc_ = accuracy_score(self.test_y, model.predict(self.test_x))\n\n        self.log_user_attributes(\n            model_type,\n            {\"accuracy\": acc_, \"f1_score\": f_score_, \"score\": score},\n            trial,\n        )\n\n        return f_score_  # noqa: TRY300\n    except Exception:\n        import logging\n\n        logging.exception(\"An error occurred\")  # Use logging instead of print\n        return 0\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.Optimizer.optimize","title":"optimize","text":"<pre><code>optimize(n_trials=100)\n</code></pre> Source code in <code>src/quoptuna/backend/tuners/optimizer.py</code> <pre><code>def optimize(self, n_trials=100):\n    if (\n        self.train_x is None\n        or self.test_x is None\n        or self.train_y is None\n        or self.test_y is None\n    ):\n        self.load_and_preprocess_data()\n    # database  stored in a db folder \"db\"\n\n    # sqllite database\n\n    study = create_study(\n        storage=self.storage_location,\n        sampler=TPESampler(),\n        directions=[\"maximize\"],\n        study_name=self.study_name,\n    )\n    self.study = study\n    study.optimize(self.objective, n_trials=n_trials)\n    return study, study.best_trials\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.DataPreparation","title":"DataPreparation","text":"<pre><code>DataPreparation(dataset: DataSet | None = None, file_path: str | None = None, x_cols: list[str] | None = None, y_col: str | None = None, scaler=None)\n</code></pre> METHOD DESCRIPTION <code>create_dataset</code> <p>Creates a dataset from raw data.</p> <code>get_data</code> <code>prepare_data</code> <p>Selects columns and preprocesses the data.</p> <code>prepare_data_dict</code> <code>preprocess</code> <p>Preprocess the features and target.</p> <code>read_csv</code> <p>Reads a CSV file and returns a raw dataset.</p> <code>select_columns</code> <p>Selects specified columns and splits the dataset into features and target.</p> <code>set_x_cols</code> <code>set_y_col</code> <code>update_column_names</code> <p>Update column names in x_cols if they are single length after conversion to string.</p> ATTRIBUTE DESCRIPTION <code>dataset</code> <p> </p> <code>scaler</code> <p> </p> <code>x_cols</code> <p> </p> <code>y_col</code> <p> </p> Source code in <code>src/quoptuna/backend/utils/data_utils/prepare.py</code> <pre><code>def __init__(\n    self,\n    dataset: DataSet | None = None,\n    file_path: str | None = None,\n    x_cols: list[str] | None = None,\n    y_col: str | None = None,\n    scaler=None,\n):\n    self.x_cols = x_cols\n    self.y_col = y_col\n    self.scaler = scaler or StandardScaler()\n    if dataset is not None:\n        x = self.update_column_names(dataset.get(\"x\"))\n        self.set_x_cols(x.columns)\n        self.dataset = {\"x\": x, \"y\": dataset.get(\"y\")}\n    elif file_path is not None:\n        if x_cols is None or y_col is None:\n            msg = \"x_cols and y_col must be provided when file_path is used\"\n            raise ValueError(msg)\n        self.dataset = self.create_dataset(self.read_csv(file_path), x_cols, y_col)\n    else:\n        msg = \"Either dataset or file_path must be provided\"\n        raise ValueError(msg)\n    self.x_train, self.x_test, self.y_train, self.y_test = self.prepare_data()\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.DataPreparation.dataset","title":"dataset  <code>instance-attribute</code>","text":"<pre><code>dataset = {'x': x, 'y': get('y')}\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.DataPreparation.scaler","title":"scaler  <code>instance-attribute</code>","text":"<pre><code>scaler = scaler or StandardScaler()\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.DataPreparation.x_cols","title":"x_cols  <code>instance-attribute</code>","text":"<pre><code>x_cols = x_cols\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.DataPreparation.y_col","title":"y_col  <code>instance-attribute</code>","text":"<pre><code>y_col = y_col\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.DataPreparation.create_dataset","title":"create_dataset","text":"<pre><code>create_dataset(raw_data: DataFrame, x_cols: list[str], y_col: str) -&gt; DataSet\n</code></pre> <p>Creates a dataset from raw data.</p> Source code in <code>src/quoptuna/backend/utils/data_utils/prepare.py</code> <pre><code>def create_dataset(self, raw_data: pd.DataFrame, x_cols: list[str], y_col: str) -&gt; DataSet:\n    \"\"\"Creates a dataset from raw data.\"\"\"\n    x = raw_data[x_cols]\n    y = raw_data[y_col]\n    x = self.update_column_names(x)\n    self.set_x_cols(x.columns)\n    return {\"x\": x, \"y\": y}\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.DataPreparation.get_data","title":"get_data","text":"<pre><code>get_data(output_type: Literal['1', '2'] = '1')\n</code></pre> Source code in <code>src/quoptuna/backend/utils/data_utils/prepare.py</code> <pre><code>def get_data(self, output_type: Literal[\"1\", \"2\"] = \"1\"):\n    if output_type == \"1\":\n        return {\n            \"x_train\": self.x_train,\n            \"x_test\": self.x_test,\n            \"y_train\": self.y_train,\n            \"y_test\": self.y_test,\n        }\n    if output_type == \"2\":\n        return {\n            \"train_x\": self.x_train,\n            \"train_y\": self.y_train,\n            \"test_x\": self.x_test,\n            \"test_y\": self.y_test,\n        }\n    return None\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.DataPreparation.prepare_data","title":"prepare_data","text":"<pre><code>prepare_data()\n</code></pre> <p>Selects columns and preprocesses the data.</p> Source code in <code>src/quoptuna/backend/utils/data_utils/prepare.py</code> <pre><code>def prepare_data(self):\n    \"\"\"Selects columns and preprocesses the data.\"\"\"\n    if self.x_cols is None or self.y_col is None:\n        msg = \"x_cols and y_col must be provided\"\n        raise ValueError(msg)\n    x, y = self.select_columns()\n    return self.preprocess(x, y)\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.DataPreparation.prepare_data_dict","title":"prepare_data_dict","text":"<pre><code>prepare_data_dict(x_train, y_train, x_test, y_test)\n</code></pre> Source code in <code>src/quoptuna/backend/utils/data_utils/prepare.py</code> <pre><code>def prepare_data_dict(self, x_train, y_train, x_test, y_test):\n    return {\"x_train\": x_train, \"x_test\": x_test, \"y_train\": y_train, \"y_test\": y_test}\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.DataPreparation.preprocess","title":"preprocess","text":"<pre><code>preprocess(x: DataFrame, y: Series, train_size: float = 0.75)\n</code></pre> <p>Preprocess the features and target.</p> Source code in <code>src/quoptuna/backend/utils/data_utils/prepare.py</code> <pre><code>def preprocess(self, x: pd.DataFrame, y: pd.Series, train_size: float = 0.75):\n    \"\"\"Preprocess the features and target.\"\"\"\n    x = pd.DataFrame(self.scaler.fit_transform(x), columns=x.columns)\n    classes = np.unique(y)\n    y = pd.DataFrame(\n        np.where(y == classes[0], 1, -1),\n        columns=[self.y_col] if not isinstance(self.y_col, list) else self.y_col,\n    )\n    return train_test_split(x, y, train_size=train_size, random_state=42)\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.DataPreparation.read_csv","title":"read_csv  <code>staticmethod</code>","text":"<pre><code>read_csv(file_path: str) -&gt; DataFrame\n</code></pre> <p>Reads a CSV file and returns a raw dataset.</p> Source code in <code>src/quoptuna/backend/utils/data_utils/prepare.py</code> <pre><code>@staticmethod\ndef read_csv(file_path: str) -&gt; pd.DataFrame:\n    \"\"\"Reads a CSV file and returns a raw dataset.\"\"\"\n    return pd.read_csv(file_path)\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.DataPreparation.select_columns","title":"select_columns","text":"<pre><code>select_columns()\n</code></pre> <p>Selects specified columns and splits the dataset into features and target.</p> Source code in <code>src/quoptuna/backend/utils/data_utils/prepare.py</code> <pre><code>def select_columns(self):\n    \"\"\"Selects specified columns and splits the dataset into features and target.\"\"\"\n    if self.x_cols is None or self.y_col is None:\n        msg = \"x_cols and y_col must be provided\"\n        raise ValueError(msg)\n    x = self.dataset.get(\"x\")\n    y = self.dataset.get(\"y\")\n    return x, y\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.DataPreparation.set_x_cols","title":"set_x_cols","text":"<pre><code>set_x_cols(x_cols: list[str])\n</code></pre> Source code in <code>src/quoptuna/backend/utils/data_utils/prepare.py</code> <pre><code>def set_x_cols(self, x_cols: list[str]):\n    self.x_cols = x_cols\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.DataPreparation.set_y_col","title":"set_y_col","text":"<pre><code>set_y_col(y_col: str)\n</code></pre> Source code in <code>src/quoptuna/backend/utils/data_utils/prepare.py</code> <pre><code>def set_y_col(self, y_col: str):\n    self.y_col = y_col\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.DataPreparation.update_column_names","title":"update_column_names","text":"<pre><code>update_column_names(dataframe: DataFrame | None = None)\n</code></pre> <p>Update column names in x_cols if they are single length after conversion to string. Also updates the corresponding DataFrame if provided.</p> Source code in <code>src/quoptuna/backend/utils/data_utils/prepare.py</code> <pre><code>def update_column_names(self, dataframe: pd.DataFrame | None = None):\n    \"\"\"Update column names in x_cols if they are single length after conversion to string.\n    Also updates the corresponding DataFrame if provided.\n    \"\"\"\n    if self.x_cols is not None:\n        for i, col in enumerate(self.x_cols):\n            if len(str(col)) == 1:\n                self.x_cols[i] = f\"feat: {col}\"\n                if dataframe is not None and col in dataframe.columns:\n                    dataframe = dataframe.rename(columns={col: f\"feat: {col}\"})\n    return dataframe\n</code></pre>"},{"location":"api/api-docs.html#quoptuna.create_model","title":"create_model","text":"<pre><code>create_model(model_type, **kwargs)\n</code></pre> Source code in <code>src/quoptuna/backend/models.py</code> <pre><code>def create_model(model_type, **kwargs):\n    model_constructors = {\n        \"CircuitCentricClassifier\": (\n            CircuitCentricClassifier,\n            [\"max_vmap\", \"batch_size\", \"learning_rate\", \"n_input_copies\", \"n_layers\"],\n        ),\n        \"DataReuploadingClassifier\": (\n            DataReuploadingClassifier,\n            [\"max_vmap\", \"batch_size\", \"learning_rate\", \"n_layers\", \"observable_type\"],\n        ),\n        \"DataReuploadingClassifierSeparable\": (\n            DataReuploadingClassifierSeparable,\n            [\"max_vmap\", \"batch_size\", \"learning_rate\", \"n_layers\", \"observable_type\"],\n        ),\n        \"DressedQuantumCircuitClassifier\": (\n            DressedQuantumCircuitClassifier,\n            [\"max_vmap\", \"batch_size\", \"learning_rate\", \"n_layers\"],\n        ),\n        \"DressedQuantumCircuitClassifierSeparable\": (\n            DressedQuantumCircuitClassifierSeparable,\n            [\"max_vmap\", \"batch_size\", \"learning_rate\", \"n_layers\"],\n        ),\n        \"IQPKernelClassifier\": (IQPKernelClassifier, [\"max_vmap\", \"repeats\", \"C\"]),\n        \"ProjectedQuantumKernel\": (\n            ProjectedQuantumKernel,\n            [\"max_vmap\", \"gamma_factor\", \"C\", \"trotter_steps\", \"t\"],\n        ),\n        \"QuantumKitchenSinks\": (\n            QuantumKitchenSinks,\n            [\"max_vmap\", \"n_qfeatures\", \"n_episodes\"],\n        ),\n        \"QuantumMetricLearner\": (\n            QuantumMetricLearner,\n            [\"max_vmap\", \"batch_size\", \"learning_rate\", \"n_layers\"],\n        ),\n        \"QuantumBoltzmannMachine\": (\n            QuantumBoltzmannMachine,\n            [\n                \"max_vmap\",\n                \"batch_size\",\n                \"learning_rate\",\n                \"visible_qubits\",\n                \"temperature\",\n            ],\n        ),\n        \"QuantumBoltzmannMachineSeparable\": (\n            QuantumBoltzmannMachineSeparable,\n            [\n                \"max_vmap\",\n                \"batch_size\",\n                \"learning_rate\",\n                \"visible_qubits\",\n                \"temperature\",\n            ],\n        ),\n        \"TreeTensorClassifier\": (\n            TreeTensorClassifier,\n            [\"max_vmap\", \"batch_size\", \"learning_rate\"],\n        ),\n        \"QuanvolutionalNeuralNetwork\": (\n            QuanvolutionalNeuralNetwork,\n            [\n                \"max_vmap\",\n                \"batch_size\",\n                \"learning_rate\",\n                \"n_qchannels\",\n                \"qkernel_shape\",\n                \"kernel_shape\",\n            ],\n        ),\n        \"WeiNet\": (WeiNet, [\"max_vmap\", \"batch_size\", \"learning_rate\", \"filter_name\"]),\n        \"SeparableVariationalClassifier\": (\n            SeparableVariationalClassifier,\n            [\"batch_size\", \"learning_rate\", \"encoding_layers\"],\n        ),\n        \"SeparableKernelClassifier\": (\n            SeparableKernelClassifier,\n            [\"C\", \"encoding_layers\"],\n        ),\n        \"ConvolutionalNeuralNetwork\": (\n            ConvolutionalNeuralNetwork,\n            [\"batch_size\", \"learning_rate\", \"kernel_shape\"],\n        ),\n        \"SVC\": (SVC, [\"gamma\", \"C\"]),\n        \"SVClinear\": (LinearSVC, [\"C\"]),\n        \"MLPClassifier\": (\n            MLPClassifier,\n            [\"batch_size\", \"learning_rate\", \"hidden_layer_sizes\", \"alpha\"],\n        ),\n        \"Perceptron\": (Perceptron, [\"eta0\"]),\n    }\n\n    if model_type not in model_constructors:\n        raise UnknownModelTypeError(model_type)\n\n    model_class, param_keys = model_constructors[model_type]\n    params = {key: kwargs.get(key) for key in param_keys}\n\n    if model_type == \"MLPClassifier\":\n        params[\"hidden_layer_sizes\"] = ast.literal_eval(params[\"hidden_layer_sizes\"])\n\n    return model_class(**params)\n</code></pre>"},{"location":"api/api-reference.html","title":"API Reference","text":""},{"location":"api/api-reference.html#api-reference","title":"API Reference","text":""},{"location":"api/api-reference.html#overview","title":"Overview","text":"<p>QuOptuna provides a comprehensive Python API for quantum-enhanced machine learning optimization. This reference covers the main classes and functions available for programmatic use.</p>"},{"location":"api/api-reference.html#core-classes","title":"Core Classes","text":""},{"location":"api/api-reference.html#datapreparation","title":"DataPreparation","text":"<p>Handles data loading, preprocessing, and splitting.</p> <pre><code>from quoptuna import DataPreparation\n\ndata_prep = DataPreparation(\n    file_path=\"path/to/data.csv\",\n    x_cols=[\"feature1\", \"feature2\", \"feature3\"],\n    y_col=\"target\"\n)\n\n# Get preprocessed data\ndata_dict = data_prep.get_data(output_type=\"2\")\n</code></pre> <p>Parameters: - <code>file_path</code> (str): Path to the CSV data file - <code>x_cols</code> (list): List of feature column names - <code>y_col</code> (str): Target column name - <code>test_size</code> (float, optional): Proportion of data for testing (default: 0.25) - <code>random_state</code> (int, optional): Random seed for reproducibility</p> <p>Methods:</p>"},{"location":"api/api-reference.html#get_dataoutput_type2","title":"<code>get_data(output_type=\"2\")</code>","text":"<p>Returns preprocessed data dictionary.</p> <p>Parameters: - <code>output_type</code> (str): Format of output   - <code>\"1\"</code>: Returns pandas DataFrames   - <code>\"2\"</code>: Returns numpy arrays (recommended for optimization)</p> <p>Returns: - dict: Dictionary with keys <code>train_x</code>, <code>test_x</code>, <code>train_y</code>, <code>test_y</code></p>"},{"location":"api/api-reference.html#optimizer","title":"Optimizer","text":"<p>Manages hyperparameter optimization using Optuna.</p> <pre><code>from quoptuna import Optimizer\n\noptimizer = Optimizer(\n    db_name=\"my_experiment\",\n    study_name=\"trial_001\",\n    data=data_dict\n)\n\n# Run optimization\nstudy, best_trials = optimizer.optimize(n_trials=100)\n</code></pre> <p>Parameters: - <code>db_name</code> (str): Database name for storing results - <code>study_name</code> (str): Unique study identifier - <code>data</code> (dict): Data dictionary from DataPreparation - <code>dataset_name</code> (str, optional): Human-readable dataset name</p> <p>Attributes: - <code>storage_location</code> (str): SQLite database URI - <code>study</code> (optuna.Study): Optuna study object - <code>best_trials</code> (list): List of best performing trials</p> <p>Methods:</p>"},{"location":"api/api-reference.html#optimizen_trials100-timeoutnone","title":"<code>optimize(n_trials=100, timeout=None)</code>","text":"<p>Run hyperparameter optimization.</p> <p>Parameters: - <code>n_trials</code> (int): Number of optimization trials - <code>timeout</code> (int, optional): Maximum optimization time in seconds</p> <p>Returns: - <code>study</code> (optuna.Study): Completed study - <code>best_trials</code> (list): List of Pareto-optimal trials</p>"},{"location":"api/api-reference.html#load_study","title":"<code>load_study()</code>","text":"<p>Load previously saved study from database.</p> <p>Returns: - <code>study</code> (optuna.Study): Loaded study object</p>"},{"location":"api/api-reference.html#model-creation","title":"Model Creation","text":"<p>Create models with optimized hyperparameters.</p> <pre><code>from quoptuna.backend.models import create_model\n\n# From trial parameters\nmodel = create_model(**trial.params)\n\n# Or specify directly\nmodel = create_model(\n    model_type=\"DataReuploadingClassifier\",\n    n_layers=10,\n    learning_rate=0.1,\n    batch_size=32\n)\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Make predictions\npredictions = model.predict(X_test)\n</code></pre> <p>Supported Models:</p>"},{"location":"api/api-reference.html#quantum-models","title":"Quantum Models","text":"<p>DataReuploadingClassifier - <code>n_layers</code> (int): Number of quantum layers - <code>learning_rate</code> (float): Learning rate for optimization - <code>batch_size</code> (int): Batch size for training - <code>n_input_copies</code> (int): Number of data reuploads - <code>observable_type</code> (str): Type of observable (\"all\" or \"half\")</p> <p>CircuitCentricClassifier - <code>n_layers</code> (int): Circuit depth - <code>learning_rate</code> (float): Learning rate - <code>n_qubits</code> (int): Number of qubits</p> <p>QuantumKitchenSinks - <code>n_episodes</code> (int): Number of training episodes - <code>learning_rate</code> (float): Learning rate - <code>gamma</code> (float): Kernel parameter</p>"},{"location":"api/api-reference.html#classical-models","title":"Classical Models","text":"<p>SVC (Support Vector Classifier) - <code>C</code> (float): Regularization parameter - <code>gamma</code> (str or float): Kernel coefficient - <code>kernel</code> (str): Kernel type</p> <p>MLPClassifier - <code>hidden_layer_sizes</code> (tuple): Hidden layer configuration - <code>learning_rate</code> (str): Learning rate schedule - <code>alpha</code> (float): L2 regularization parameter</p>"},{"location":"api/api-reference.html#xai-explainable-ai","title":"XAI (Explainable AI)","text":"<p>Generate SHAP explanations and visualizations.</p> <pre><code>from quoptuna import XAI\nfrom quoptuna.backend.xai.xai import XAIConfig\n\n# Configure XAI\nconfig = XAIConfig(\n    use_proba=True,\n    onsubset=True,\n    subset_size=50\n)\n\n# Create XAI instance\nxai = XAI(\n    model=trained_model,\n    data=data_dict,\n    config=config\n)\n\n# Get evaluation report\nreport = xai.get_report()\n</code></pre> <p>XAIConfig Parameters: - <code>use_proba</code> (bool): Use probability predictions - <code>onsubset</code> (bool): Use subset of data - <code>subset_size</code> (int): Size of subset</p> <p>XAI Methods:</p>"},{"location":"api/api-reference.html#get_report","title":"<code>get_report()</code>","text":"<p>Generate classification report.</p> <p>Returns: - dict: Contains confusion matrix, classification report, and ROC curve data</p>"},{"location":"api/api-reference.html#get_plotplot_type-kwargs","title":"<code>get_plot(plot_type, **kwargs)</code>","text":"<p>Generate SHAP visualization.</p> <p>Parameters: - <code>plot_type</code> (str): Type of plot   - <code>\"bar\"</code>: Feature importance bar plot   - <code>\"beeswarm\"</code>: SHAP value distribution   - <code>\"violin\"</code>: Violin plot of SHAP values   - <code>\"heatmap\"</code>: Instance-level SHAP heatmap   - <code>\"waterfall\"</code>: Individual prediction explanation - <code>max_display</code> (int): Maximum features to display - <code>class_index</code> (int): Class to explain (for binary: 0 or 1) - <code>index</code> (int): Sample index (for waterfall plot) - <code>save_config</code> (dict, optional): Configuration for saving plot</p> <p>Returns: - str: Base64 encoded image</p>"},{"location":"api/api-reference.html#plot_confusion_matrix","title":"<code>plot_confusion_matrix()</code>","text":"<p>Generate confusion matrix plot.</p> <p>Returns: - matplotlib.figure.Figure: Confusion matrix figure</p>"},{"location":"api/api-reference.html#generate_report_with_langchainprovider-api_key-model_name-dataset_infonone","title":"<code>generate_report_with_langchain(provider, api_key, model_name, dataset_info=None)</code>","text":"<p>Generate AI-powered analysis report.</p> <p>Parameters: - <code>provider</code> (str): LLM provider (\"google\", \"openai\", \"anthropic\") - <code>api_key</code> (str): API key for the provider - <code>model_name</code> (str): Model identifier - <code>dataset_info</code> (dict, optional): Dataset metadata</p> <p>Returns: - str: Markdown formatted report</p>"},{"location":"api/api-reference.html#utility-functions","title":"Utility Functions","text":""},{"location":"api/api-reference.html#mock_csv_data","title":"mock_csv_data","text":"<p>Save DataFrame to CSV file.</p> <pre><code>from quoptuna.backend.utils.data_utils.data import mock_csv_data\n\nfile_path = mock_csv_data(\n    dataframe,\n    tmp_path=\"data\",\n    file_name=\"my_dataset\"\n)\n</code></pre> <p>Parameters: - <code>dataframe</code> (pd.DataFrame): Data to save - <code>tmp_path</code> (str): Directory path - <code>file_name</code> (str): File name (without .csv extension)</p> <p>Returns: - str: Full path to saved file</p>"},{"location":"api/api-reference.html#complete-example","title":"Complete Example","text":"<p>Here's a complete example workflow:</p> <pre><code>import pandas as pd\nfrom ucimlrepo import fetch_ucirepo\nfrom quoptuna import DataPreparation, Optimizer, XAI\nfrom quoptuna.backend.models import create_model\nfrom quoptuna.backend.xai.xai import XAIConfig\nfrom quoptuna.backend.utils.data_utils.data import mock_csv_data\n\n# 1. Load dataset\ndataset = fetch_ucirepo(id=143)  # Statlog dataset\nX = dataset.data.features\ny = dataset.data.targets\ndf = pd.concat([X, y], axis=1)\n\n# 2. Prepare data\ndf[\"target\"] = df[\"A15\"].replace({0: -1, 1: 1})\ndf = df.drop(columns=[\"A15\"])\ndf = df.dropna()\n\n# 3. Save to file\nfile_path = mock_csv_data(df, tmp_path=\"data\", file_name=\"Statlog\")\n\n# 4. Prepare for training\ndata_prep = DataPreparation(\n    file_path=file_path,\n    x_cols=list(df.columns.difference([\"target\"])),\n    y_col=\"target\"\n)\ndata_dict = data_prep.get_data(output_type=\"2\")\n\n# Convert to numpy arrays\ndata_dict[\"train_x\"] = data_dict[\"train_x\"].values\ndata_dict[\"test_x\"] = data_dict[\"test_x\"].values\ndata_dict[\"train_y\"] = data_dict[\"train_y\"].values\ndata_dict[\"test_y\"] = data_dict[\"test_y\"].values\n\n# 5. Run optimization\noptimizer = Optimizer(\n    db_name=\"Statlog\",\n    study_name=\"Statlog\",\n    data=data_dict\n)\nstudy, best_trials = optimizer.optimize(n_trials=100)\n\n# 6. Train best model\nbest_trial = best_trials[0]\nmodel = create_model(**best_trial.params)\nmodel.fit(data_dict[\"train_x\"], data_dict[\"train_y\"])\n\n# 7. SHAP analysis\nconfig = XAIConfig(use_proba=True, onsubset=True, subset_size=50)\nxai = XAI(model=model, data=data_dict, config=config)\n\n# 8. Generate visualizations\nbar_plot = xai.get_plot(\"bar\", max_display=10, class_index=1)\nbeeswarm_plot = xai.get_plot(\"beeswarm\", max_display=10, class_index=1)\n\n# 9. Generate report\nreport = xai.generate_report_with_langchain(\n    provider=\"google\",\n    api_key=\"your-api-key\",\n    model_name=\"models/gemini-2.0-flash-exp\",\n    dataset_info={\n        \"Name\": \"Statlog Credit Approval\",\n        \"URL\": \"https://archive.ics.uci.edu/dataset/143\",\n        \"Description\": \"Credit card application dataset\"\n    }\n)\n\nprint(report)\n</code></pre>"},{"location":"api/api-reference.html#data-format-requirements","title":"Data Format Requirements","text":""},{"location":"api/api-reference.html#input-data","title":"Input Data","text":"<p>CSV Format: - Must have header row with column names - Target column should contain binary values - Features can be numeric or categorical - Missing values will be removed</p> <p>Target Encoding: - Binary classification: Must use <code>-1</code> and <code>1</code> - QuOptuna does not currently support multi-class classification</p>"},{"location":"api/api-reference.html#data-dictionary-format","title":"Data Dictionary Format","text":"<p>After preprocessing, data should be in this format:</p> <pre><code>data_dict = {\n    \"train_x\": np.ndarray,  # Shape: (n_train_samples, n_features)\n    \"test_x\": np.ndarray,   # Shape: (n_test_samples, n_features)\n    \"train_y\": np.ndarray,  # Shape: (n_train_samples,)\n    \"test_y\": np.ndarray    # Shape: (n_test_samples,)\n}\n</code></pre>"},{"location":"api/api-reference.html#advanced-usage","title":"Advanced Usage","text":""},{"location":"api/api-reference.html#custom-optimization-objectives","title":"Custom Optimization Objectives","text":"<p>You can customize the optimization objective:</p> <pre><code>import optuna\n\ndef custom_objective(trial):\n    # Define your custom objective\n    params = {\n        \"model_type\": trial.suggest_categorical(\"model_type\", [\"SVC\", \"MLPClassifier\"]),\n        \"C\": trial.suggest_float(\"C\", 0.1, 10.0)\n    }\n\n    model = create_model(**params)\n    model.fit(train_x, train_y)\n\n    # Return custom metric\n    return custom_metric(model, test_x, test_y)\n\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(custom_objective, n_trials=100)\n</code></pre>"},{"location":"api/api-reference.html#parallel-optimization","title":"Parallel Optimization","text":"<p>Run multiple trials in parallel:</p> <pre><code>optimizer = Optimizer(db_name=\"my_db\", study_name=\"my_study\", data=data_dict)\n\n# Use n_jobs for parallel execution\nstudy, best_trials = optimizer.optimize(\n    n_trials=100,\n    n_jobs=4  # Run 4 trials in parallel\n)\n</code></pre>"},{"location":"api/api-reference.html#saving-and-loading-models","title":"Saving and Loading Models","text":"<pre><code>import joblib\n\n# Save model\njoblib.dump(model, \"model.pkl\")\n\n# Load model\nloaded_model = joblib.load(\"model.pkl\")\n</code></pre>"},{"location":"api/api-reference.html#error-handling","title":"Error Handling","text":"<p>Common errors and solutions:</p> <pre><code>try:\n    optimizer.optimize(n_trials=100)\nexcept ValueError as e:\n    # Handle data validation errors\n    print(f\"Data error: {e}\")\nexcept RuntimeError as e:\n    # Handle optimization errors\n    print(f\"Optimization error: {e}\")\nexcept Exception as e:\n    # Handle unexpected errors\n    print(f\"Unexpected error: {e}\")\n</code></pre>"},{"location":"api/api-reference.html#performance-tips","title":"Performance Tips","text":"<ol> <li>Use Subsets for SHAP: Analyze 50-100 samples for faster computation</li> <li>Increase Trials Gradually: Start with 50 trials, increase as needed</li> <li>Use Caching: Reuse loaded studies when possible</li> <li>Monitor Memory: Large datasets may require subset analysis</li> <li>Parallel Processing: Use <code>n_jobs</code> parameter for faster optimization</li> </ol>"},{"location":"api/api-reference.html#see-also","title":"See Also","text":"<ul> <li>User Guide - Step-by-step usage instructions</li> <li>Examples - Common use cases and tutorials</li> <li>GitHub Repository - Source code and issues</li> </ul>"},{"location":"architecture/frontend-architecture-diagram.html","title":"QuOptuna Frontend - Architecture Diagrams","text":""},{"location":"architecture/frontend-architecture-diagram.html#quoptuna-frontend-architecture-diagrams","title":"QuOptuna Frontend - Architecture Diagrams","text":""},{"location":"architecture/frontend-architecture-diagram.html#1-high-level-component-architecture","title":"1. High-Level Component Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         STREAMLIT APPLICATION                       \u2502\n\u2502                     (src/quoptuna/frontend/)                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                     \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502                      SIDEBAR                                \u2502  \u2502\n\u2502  \u2502  (sidebar.py + support.py)                                 \u2502  \u2502\n\u2502  \u2502                                                             \u2502  \u2502\n\u2502  \u2502  \u251c\u2500 Data Upload Component                                 \u2502  \u2502\n\u2502  \u2502  \u251c\u2500 Database Configuration                               \u2502  \u2502\n\u2502  \u2502  \u251c\u2500 Study Name Setup                                     \u2502  \u2502\n\u2502  \u2502  \u251c\u2500 Trial Count Slider                                  \u2502  \u2502\n\u2502  \u2502  \u2514\u2500 Visualization Control                               \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                                                     \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502                      MAIN CONTENT                            \u2502  \u2502\n\u2502  \u2502                                                              \u2502  \u2502\n\u2502  \u2502  HOME PAGE (main_page.py)                                  \u2502  \u2502\n\u2502  \u2502  \u251c\u2500 Welcome &amp; Overview                                    \u2502  \u2502\n\u2502  \u2502  \u251c\u2500 Feature Highlights (3 columns)                        \u2502  \u2502\n\u2502  \u2502  \u2514\u2500 Getting Started Guide                                 \u2502  \u2502\n\u2502  \u2502                                                             \u2502  \u2502\n\u2502  \u2502  PAGE 1: Dataset Selection (pages/1_dataset_selection.py) \u2502  \u2502\n\u2502  \u2502  \u251c\u2500 UCI Repository Browser                               \u2502  \u2502\n\u2502  \u2502  \u251c\u2500 Custom CSV Upload                                    \u2502  \u2502\n\u2502  \u2502  \u2514\u2500 Data Configuration Panel                             \u2502  \u2502\n\u2502  \u2502                                                             \u2502  \u2502\n\u2502  \u2502  PAGE 2: Optimization (pages/2_optimization.py)           \u2502  \u2502\n\u2502  \u2502  \u251c\u2500 Data Preparation Module                              \u2502  \u2502\n\u2502  \u2502  \u251c\u2500 Optimization Configuration                           \u2502  \u2502\n\u2502  \u2502  \u251c\u2500 Trial Progress Display                               \u2502  \u2502\n\u2502  \u2502  \u2514\u2500 Results Table                                         \u2502  \u2502\n\u2502  \u2502                                                             \u2502  \u2502\n\u2502  \u2502  PAGE 3: SHAP Analysis (pages/3_shap_analysis.py)         \u2502  \u2502\n\u2502  \u2502  \u251c\u2500 Trial Selector                                       \u2502  \u2502\n\u2502  \u2502  \u251c\u2500 Model Training Controls                              \u2502  \u2502\n\u2502  \u2502  \u251c\u2500 SHAP Configuration                                   \u2502  \u2502\n\u2502  \u2502  \u251c\u2500 6 Visualization Tabs                                 \u2502  \u2502\n\u2502  \u2502  \u2514\u2500 AI Report Generator                                  \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2193                                   \u2193                    \u2193\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  Backend    \u2502              \u2502    External      \u2502    \u2502  Storage    \u2502\n    \u2502  Libraries  \u2502              \u2502    Services      \u2502    \u2502  &amp; Files    \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/frontend-architecture-diagram.html#2-data-flow-architecture","title":"2. Data Flow Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  USER INPUT \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u251c\u2500 Dataset Source\n       \u2502  \u251c\u2500 UCI Repository (fetch_ucirepo)\n       \u2502  \u2514\u2500 Custom CSV Upload\n       \u2502\n       \u251c\u2500 Target Selection (Column)\n       \u2502\n       \u251c\u2500 Feature Selection (Multiselect)\n       \u2502\n       \u2514\u2500 Transformation Parameters\n           \u251c\u2500 Missing value handling\n           \u251c\u2500 Target encoding (-1, 1)\n           \u2514\u2500 File path\n\n       \u2193\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  PAGE 1: DATASET SELECTION               \u2502\n\u2502  (1_dataset_selection.py)                \u2502\n\u2502                                          \u2502\n\u2502  fetch_uci_dataset() / upload_custom()  \u2502\n\u2502  \u251c\u2500 Load data (pandas DataFrame)       \u2502\n\u2502  \u251c\u2500 Preview &amp; validate                 \u2502\n\u2502  \u2514\u2500 Save to: data/{dataset_name}.csv   \u2502\n\u2502                                          \u2502\n\u2502  Session State: dataset_df, file_path   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n               \u2502 (file_path)\n               \u2193\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  PAGE 2: OPTIMIZATION                               \u2502\n\u2502  (2_optimization.py)                                \u2502\n\u2502                                                     \u2502\n\u2502  DataPreparation(file_path, x_cols, y_col)         \u2502\n\u2502  \u251c\u2500 Read CSV                                       \u2502\n\u2502  \u251c\u2500 Feature scaling (StandardScaler)               \u2502\n\u2502  \u251c\u2500 Train/test split (75/25)                       \u2502\n\u2502  \u2514\u2500 Output:                                        \u2502\n\u2502      data_dict = {                                 \u2502\n\u2502          'train_x': ndarray,                       \u2502\n\u2502          'test_x': ndarray,                        \u2502\n\u2502          'train_y': ndarray[-1,1],                \u2502\n\u2502          'test_y': ndarray[-1,1]                  \u2502\n\u2502      }                                             \u2502\n\u2502                                                     \u2502\n\u2502  Optimizer(db_name, study_name, data=data_dict)   \u2502\n\u2502  \u251c\u2500 optimize(n_trials)                            \u2502\n\u2502  \u2502   \u251c\u2500 Loop n_trials:                            \u2502\n\u2502  \u2502   \u2502   \u251c\u2500 Suggest hyperparameters (TPE)         \u2502\n\u2502  \u2502   \u2502   \u251c\u2500 create_model(model_type, **params)    \u2502\n\u2502  \u2502   \u2502   \u251c\u2500 Train model                           \u2502\n\u2502  \u2502   \u2502   \u251c\u2500 Evaluate F1, accuracy                 \u2502\n\u2502  \u2502   \u2502   \u251c\u2500 Store metrics in trial.user_attrs     \u2502\n\u2502  \u2502   \u2502   \u2514\u2500 Save to SQLite                        \u2502\n\u2502  \u2502   \u2514\u2500 Return (study, best_trials)               \u2502\n\u2502  \u2514\u2500 Output to: db/{db_name}.db                    \u2502\n\u2502                                                     \u2502\n\u2502  Session State: data_dict, optimizer,              \u2502\n\u2502                 best_trials, study                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n               \u2502 (best_trials, data_dict)\n               \u2193\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  PAGE 3: SHAP ANALYSIS                             \u2502\n\u2502  (3_shap_analysis.py)                              \u2502\n\u2502                                                     \u2502\n\u2502  Step 1: Trial Selection (dropdown)                \u2502\n\u2502  \u2514\u2500 selected_trial = best_trials[idx]              \u2502\n\u2502                                                     \u2502\n\u2502  Step 2: Model Training                            \u2502\n\u2502  \u251c\u2500 params = selected_trial.params                 \u2502\n\u2502  \u251c\u2500 model = create_model(**params)                 \u2502\n\u2502  \u251c\u2500 model.fit(data_dict['train_x'],               \u2502\n\u2502  \u2502           data_dict['train_y'])                \u2502\n\u2502  \u2514\u2500 Session State: trained_model                  \u2502\n\u2502                                                     \u2502\n\u2502  Step 3: SHAP Analysis                             \u2502\n\u2502  \u251c\u2500 XAI(model, data_dict, XAIConfig(...))         \u2502\n\u2502  \u251c\u2500 xai.get_shap_values()                         \u2502\n\u2502  \u251c\u2500 xai.get_explainer()                           \u2502\n\u2502  \u2514\u2500 Session State: xai                            \u2502\n\u2502                                                     \u2502\n\u2502  Step 4: Visualizations                            \u2502\n\u2502  \u251c\u2500 xai.get_plot('bar') \u2192 base64 image            \u2502\n\u2502  \u251c\u2500 xai.get_plot('beeswarm') \u2192 base64 image       \u2502\n\u2502  \u251c\u2500 xai.get_plot('violin') \u2192 base64 image         \u2502\n\u2502  \u251c\u2500 xai.get_plot('heatmap') \u2192 base64 image        \u2502\n\u2502  \u251c\u2500 xai.get_plot('waterfall') \u2192 base64 image      \u2502\n\u2502  \u2514\u2500 xai.plot_confusion_matrix() \u2192 matplotlib fig  \u2502\n\u2502                                                     \u2502\n\u2502  Step 5: AI Report Generation                      \u2502\n\u2502  \u251c\u2500 xai.get_report() \u2192 dict of metrics             \u2502\n\u2502  \u251c\u2500 Generate SHAP plot images                     \u2502\n\u2502  \u251c\u2500 xai.generate_report_with_langchain(           \u2502\n\u2502  \u2502       provider, api_key, model_name,            \u2502\n\u2502  \u2502       dataset_info)                             \u2502\n\u2502  \u2502   \u251c\u2500 Initialize LLM client                     \u2502\n\u2502  \u2502   \u251c\u2500 Create multimodal prompt                  \u2502\n\u2502  \u2502   \u251c\u2500 Include base64-encoded images             \u2502\n\u2502  \u2502   \u2514\u2500 Return markdown report                    \u2502\n\u2502  \u2514\u2500 Session State: report, shap_images            \u2502\n\u2502                                                     \u2502\n\u2502  OUTPUT:                                            \u2502\n\u2502  \u251c\u2500 SHAP visualizations (displayed)                \u2502\n\u2502  \u2514\u2500 Markdown report (downloadable)                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/frontend-architecture-diagram.html#3-module-dependency-graph","title":"3. Module Dependency Graph","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    FRONTEND MODULES                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                             \u2502\n\u2502  app.py                                                    \u2502\n\u2502  \u251c\u2500 imports: main_page, sidebar, support                 \u2502\n\u2502  \u2514\u2500 imports: st (streamlit)                              \u2502\n\u2502                                                             \u2502\n\u2502  main_page.py                                             \u2502\n\u2502  \u251c\u2500 imports: st                                          \u2502\n\u2502  \u2514\u2500 pure display/documentation                            \u2502\n\u2502                                                             \u2502\n\u2502  sidebar.py                                               \u2502\n\u2502  \u251c\u2500 imports: st                                          \u2502\n\u2502  \u251c\u2500 imports: Optimizer                                   \u2502\n\u2502  \u251c\u2500 imports: preprocess_data                             \u2502\n\u2502  \u251c\u2500 imports: support module functions                    \u2502\n\u2502  \u2514\u2500 orchestrates data upload &amp; DB config                \u2502\n\u2502                                                             \u2502\n\u2502  support.py                                               \u2502\n\u2502  \u251c\u2500 imports: st, optuna, pandas                          \u2502\n\u2502  \u251c\u2500 exports: upload_and_display_data()                   \u2502\n\u2502  \u251c\u2500 exports: select_columns()                            \u2502\n\u2502  \u251c\u2500 exports: initialize_session_state()                  \u2502\n\u2502  \u251c\u2500 exports: update_plot()                               \u2502\n\u2502  \u2514\u2500 shared visualization utilities                        \u2502\n\u2502                                                             \u2502\n\u2502  pages/1_dataset_selection.py                             \u2502\n\u2502  \u251c\u2500 imports: st, pandas, ucimlrepo                       \u2502\n\u2502  \u251c\u2500 imports: mock_csv_data (utility)                     \u2502\n\u2502  \u251c\u2500 functions:                                            \u2502\n\u2502  \u2502  \u251c\u2500 fetch_uci_dataset()                              \u2502\n\u2502  \u2502  \u251c\u2500 upload_custom_dataset()                          \u2502\n\u2502  \u2502  \u2514\u2500 configure_dataset()                              \u2502\n\u2502  \u2514\u2500 standalone Streamlit page                            \u2502\n\u2502                                                             \u2502\n\u2502  pages/2_optimization.py                                  \u2502\n\u2502  \u251c\u2500 imports: st                                          \u2502\n\u2502  \u251c\u2500 imports: DataPreparation, Optimizer                  \u2502\n\u2502  \u251c\u2500 functions:                                            \u2502\n\u2502  \u2502  \u251c\u2500 prepare_data()                                   \u2502\n\u2502  \u2502  \u251c\u2500 run_optimization()                               \u2502\n\u2502  \u2502  \u2514\u2500 display_results()                                \u2502\n\u2502  \u2514\u2500 standalone Streamlit page                            \u2502\n\u2502                                                             \u2502\n\u2502  pages/3_shap_analysis.py                                 \u2502\n\u2502  \u251c\u2500 imports: st, base64, io                              \u2502\n\u2502  \u251c\u2500 imports: XAI, XAIConfig                              \u2502\n\u2502  \u251c\u2500 imports: create_model                                \u2502\n\u2502  \u251c\u2500 functions:                                            \u2502\n\u2502  \u2502  \u251c\u2500 select_trial()                                   \u2502\n\u2502  \u2502  \u251c\u2500 train_model()                                    \u2502\n\u2502  \u2502  \u251c\u2500 run_shap_analysis()                              \u2502\n\u2502  \u2502  \u251c\u2500 display_shap_plots()                             \u2502\n\u2502  \u2502  \u251c\u2500 generate_report()                                \u2502\n\u2502  \u2502  \u2514\u2500 save_plot()                                      \u2502\n\u2502  \u2514\u2500 standalone Streamlit page                            \u2502\n\u2502                                                             \u2502\n\u2502  pages/shap.py (legacy)                                   \u2502\n\u2502  \u251c\u2500 imports: various backend modules                     \u2502\n\u2502  \u2514\u2500 old SHAP workflow (not actively used)               \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2193                                  \u2193\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  BACKEND MODULES     \u2502      \u2502  EXTERNAL LIBRARIES  \u2502\n    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524      \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n    \u2502                      \u2502      \u2502                      \u2502\n    \u2502 Optimizer            \u2502      \u2502 streamlit (st)       \u2502\n    \u2502 DataPreparation      \u2502      \u2502 pandas               \u2502\n    \u2502 create_model()       \u2502      \u2502 numpy                \u2502\n    \u2502 XAI                  \u2502      \u2502 optuna               \u2502\n    \u2502 XAIConfig            \u2502      \u2502 sklearn              \u2502\n    \u2502 preprocess_data()    \u2502      \u2502 shap                 \u2502\n    \u2502 mock_csv_data()      \u2502      \u2502 pennylane            \u2502\n    \u2502                      \u2502      \u2502 ucimlrepo            \u2502\n    \u2502                      \u2502      \u2502 langchain            \u2502\n    \u2502                      \u2502      \u2502 matplotlib           \u2502\n    \u2502                      \u2502      \u2502 plotly               \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/frontend-architecture-diagram.html#4-state-management-flow","title":"4. State Management Flow","text":"<pre><code>START: initialize_session_state() [app.py]\n  \u2502\n  \u251c\u2500 Set initial values for all keys\n  \u2514\u2500 Create empty dict for session_state\n\n     \u2193\n\nPAGE 1: Dataset Selection\n  \u2502\n  \u251c\u2500 dataset_loaded = False \u2192 True\n  \u251c\u2500 dataset_df = None \u2192 DataFrame\n  \u251c\u2500 dataset_name = None \u2192 \"Australian_Credit_Approval\"\n  \u251c\u2500 dataset_metadata = None \u2192 UCI metadata dict\n  \u251c\u2500 file_path = None \u2192 \"data/Australian_Credit_Approval.csv\"\n  \u251c\u2500 target_column = None \u2192 \"target\"\n  \u2514\u2500 feature_columns = None \u2192 [\"feat1\", \"feat2\", ...]\n\n     \u2193\n\nPAGE 2: Optimization\n  \u2502\n  \u251c\u2500 data_dict = None \u2192 {\"train_x\": ndarray, ...}\n  \u251c\u2500 db_name = None \u2192 \"Australian_Credit_Approval\"\n  \u251c\u2500 study_name = None \u2192 \"Australian_Credit_Approval\"\n  \u251c\u2500 optimizer = None \u2192 Optimizer instance\n  \u251c\u2500 study = None \u2192 Optuna Study object\n  \u251c\u2500 best_trials = None \u2192 [Trial1, Trial2, ...]\n  \u2514\u2500 optimization_complete = False \u2192 True\n\n     \u2193\n\nPAGE 3: SHAP Analysis\n  \u2502\n  \u251c\u2500 selected_trial = None \u2192 Trial object\n  \u251c\u2500 trained_model = None \u2192 Model instance\n  \u251c\u2500 xai = None \u2192 XAI instance\n  \u251c\u2500 report = None \u2192 \"# Analysis Report\\n...\"\n  \u2514\u2500 shap_images = None \u2192 {\"bar\": \"data:image/png;base64,...\", ...}\n\nEND: All state available across all pages\n</code></pre>"},{"location":"architecture/frontend-architecture-diagram.html#5-backend-integration-points","title":"5. Backend Integration Points","text":"<pre><code>                    FRONTEND (Streamlit)\n                           \u2193\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502    Python Library Imports         \u2502\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2193           \u2193          \u2193\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502  Optimizer  \u2502  \u2502 Data   \u2502  \u2502   XAI       \u2502\n        \u2502             \u2502  \u2502Prepar  \u2502  \u2502             \u2502\n        \u2502  optimize() \u2502  \u2502ation   \u2502  \u2502 get_plot()  \u2502\n        \u2502   objective \u2502  \u2502        \u2502  \u2502 get_report()\u2502\n        \u2502  log_attrs  \u2502  \u2502prepare \u2502  \u2502generate_rpt \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502             \u2502              \u2502\n               \u251c\u2500 Optuna \u2500\u2500\u2500\u2500\u2500\u253c\u2500 PennyLane  \u2502\n               \u2502 (Study)      \u2502 (Quantum)   \u2502\n               \u2502              \u2502             \u2502\n               \u2514\u2500\u2500SkLearn\u2500\u2500\u2500\u2500\u2500\u2534\u2500 SHAP \u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  (Classical)    (Explainability)\n\n  Storage:\n  \u251c\u2500 SQLite: db/{db_name}.db\n  \u251c\u2500 File: data/{dataset_name}.csv\n  \u2514\u2500 Memory: st.session_state\n\n  External APIs:\n  \u251c\u2500 UCI Repository (read-only)\n  \u2514\u2500 LLM APIs (OpenAI, Google, Anthropic) [optional]\n</code></pre>"},{"location":"architecture/frontend-architecture-diagram.html#6-requestresponse-cycle-for-key-operations","title":"6. Request/Response Cycle for Key Operations","text":""},{"location":"architecture/frontend-architecture-diagram.html#operation-load-uci-dataset","title":"Operation: Load UCI Dataset","text":"<pre><code>User Action: Click \"Load UCI Dataset\"\n  \u2502\n  \u251c\u2500 fetch_ucirepo(id=143)\n  \u2502  \u2514\u2500 API call to UCI Repository\n  \u2502     \u2514\u2500 Returns: Dataset object\n  \u2502\n  \u251c\u2500 Extract X, y from dataset\n  \u251c\u2500 Combine: pd.concat([X, y], axis=1)\n  \u2502\n  \u251c\u2500 Store in st.session_state:\n  \u2502  \u251c\u2500 dataset_df\n  \u2502  \u251c\u2500 dataset_name\n  \u2502  \u2514\u2500 dataset_metadata\n  \u2502\n  \u2514\u2500 Display:\n     \u251c\u2500 \u2705 Success message\n     \u251c\u2500 Metadata expander (instances, features, area, tasks)\n     \u2514\u2500 Data preview button\n</code></pre>"},{"location":"architecture/frontend-architecture-diagram.html#operation-run-optimization","title":"Operation: Run Optimization","text":"<pre><code>User Action: Click \"Start Optimization\"\n  \u2502\n  \u251c\u2500 DataPreparation.prepare_data()\n  \u2502  \u251c\u2500 Read CSV from file_path\n  \u2502  \u251c\u2500 StandardScaler.fit_transform()\n  \u2502  \u251c\u2500 train_test_split(75/25)\n  \u2502  \u2514\u2500 Return data_dict\n  \u2502\n  \u251c\u2500 Optimizer(db_name, study_name, data=data_dict)\n  \u2502  \u2514\u2500 create_study(storage, sampler=TPE, study_name)\n  \u2502\n  \u251c\u2500 study.optimize(objective, n_trials=n_trials)\n  \u2502  \u2502\n  \u2502  \u2514\u2500 For each trial:\n  \u2502     \u251c\u2500 trial.suggest_categorical() [hyperparameters]\n  \u2502     \u251c\u2500 create_model(**params)\n  \u2502     \u251c\u2500 model.fit(train_x, train_y)\n  \u2502     \u251c\u2500 f1 = f1_score(test_y, model.predict(test_x))\n  \u2502     \u251c\u2500 trial.set_user_attr(\"Quantum_f1_score\", f1)\n  \u2502     \u2514\u2500 return f1\n  \u2502\n  \u251c\u2500 study.best_trials \u2192 filtered top trials\n  \u2502\n  \u251c\u2500 Store in st.session_state:\n  \u2502  \u251c\u2500 best_trials\n  \u2502  \u251c\u2500 optimizer\n  \u2502  \u2514\u2500 optimization_complete = True\n  \u2502\n  \u2514\u2500 Display:\n     \u251c\u2500 Progress bar (100%)\n     \u2514\u2500 Best trials table with metrics\n</code></pre>"},{"location":"architecture/frontend-architecture-diagram.html#operation-generate-shap-report","title":"Operation: Generate SHAP Report","text":"<pre><code>User Action: Click \"Generate Report\"\n  \u2502\n  \u251c\u2500 Validate API key provided\n  \u2502\n  \u251c\u2500 XAI.get_report()\n  \u2502  \u2514\u2500 Compute all metrics:\n  \u2502     \u251c\u2500 confusion_matrix()\n  \u2502     \u251c\u2500 classification_report()\n  \u2502     \u251c\u2500 roc_auc_score()\n  \u2502     \u251c\u2500 f1_score()\n  \u2502     \u2514\u2500 etc...\n  \u2502\n  \u251c\u2500 Generate plot images:\n  \u2502  \u251c\u2500 For plot_type in [\"bar\", \"beeswarm\", \"violin\", \"heatmap\"]:\n  \u2502  \u2502  \u2514\u2500 xai.get_plot(plot_type) \u2192 base64 image\n  \u2502  \u251c\u2500 For index in range(num_waterfall_plots):\n  \u2502  \u2502  \u2514\u2500 xai.get_plot(\"waterfall\", index=i) \u2192 base64 image\n  \u2502  \u2514\u2500 xai.plot_confusion_matrix() \u2192 base64 image\n  \u2502\n  \u251c\u2500 Initialize LLM client:\n  \u2502  \u2514\u2500 ChatGoogleGenerativeAI(api_key, model_name)\n  \u2502     OR ChatOpenAI(api_key, model_name)\n  \u2502\n  \u251c\u2500 Create multimodal prompt:\n  \u2502  \u251c\u2500 System message: prompt.txt\n  \u2502  \u251c\u2500 Human message: report dict\n  \u2502  \u2514\u2500 Human messages: [image for each plot]\n  \u2502\n  \u251c\u2500 LLM response:\n  \u2502  \u2514\u2500 response = chat(final_prompt)\n  \u2502     \u2514\u2500 Returns: markdown report string\n  \u2502\n  \u251c\u2500 Store in st.session_state:\n  \u2502  \u251c\u2500 report\n  \u2502  \u2514\u2500 shap_images\n  \u2502\n  \u2514\u2500 Display:\n     \u251c\u2500 Report markdown (rendered)\n     \u251c\u2500 Download button (report.md)\n     \u2514\u2500 \u2705 Success message\n</code></pre>"},{"location":"architecture/frontend-architecture-diagram.html#7-file-structure-io-diagram","title":"7. File Structure &amp; I/O Diagram","text":"<pre><code>PROJECT ROOT\n\u2502\n\u251c\u2500 src/quoptuna/\n\u2502  \u251c\u2500 frontend/\n\u2502  \u2502  \u251c\u2500 app.py \u2b50\n\u2502  \u2502  \u251c\u2500 main_page.py\n\u2502  \u2502  \u251c\u2500 sidebar.py\n\u2502  \u2502  \u251c\u2500 support.py\n\u2502  \u2502  \u251c\u2500 test.py\n\u2502  \u2502  \u251c\u2500 __init__.py\n\u2502  \u2502  \u2514\u2500 pages/\n\u2502  \u2502     \u251c\u2500 __init__.py\n\u2502  \u2502     \u251c\u2500 1_dataset_selection.py \u2b50\n\u2502  \u2502     \u251c\u2500 2_optimization.py \u2b50\n\u2502  \u2502     \u251c\u2500 3_shap_analysis.py \u2b50\n\u2502  \u2502     \u2514\u2500 shap.py\n\u2502  \u2502\n\u2502  \u2514\u2500 backend/\n\u2502     \u251c\u2500 __init__.py [exports: Optimizer, DataPreparation, XAI, create_model]\n\u2502     \u251c\u2500 models.py\n\u2502     \u251c\u2500 data.py\n\u2502     \u251c\u2500 tuners/\n\u2502     \u2502  \u251c\u2500 __init__.py\n\u2502     \u2502  \u2514\u2500 optimizer.py\n\u2502     \u251c\u2500 utils/\n\u2502     \u2502  \u251c\u2500 data_utils/\n\u2502     \u2502  \u2502  \u251c\u2500 prepare.py [DataPreparation class]\n\u2502     \u2502  \u2502  \u2514\u2500 data.py [mock_csv_data, preprocess_data]\n\u2502     \u2502  \u2514\u2500 ...\n\u2502     \u2514\u2500 xai/\n\u2502        \u251c\u2500 __init__.py\n\u2502        \u251c\u2500 xai.py [XAI, XAIConfig classes]\n\u2502        \u2514\u2500 constants.py\n\u2502\n\u251c\u2500 .streamlit/\n\u2502  \u2514\u2500 config.toml [theme configuration]\n\u2502\n\u251c\u2500 FRONTEND_ARCHITECTURE.md [Documentation]\n\u251c\u2500 FRONTEND_QUICK_REFERENCE.md [Quick guide]\n\u2514\u2500 FRONTEND_ARCHITECTURE_DIAGRAM.md [This file]\n\nRUNTIME DIRECTORIES (created at runtime):\n\u2502\n\u251c\u2500 uploaded_data/ [Temporary CSV uploads from file_uploader]\n\u2502  \u2514\u2500 *.csv\n\u2502\n\u251c\u2500 data/ [Processed datasets from Page 1]\n\u2502  \u2514\u2500 {dataset_name}.csv\n\u2502\n\u251c\u2500 db/ [Optuna SQLite databases from Page 2]\n\u2502  \u2514\u2500 {db_name}.db\n\u2502\n\u251c\u2500 outputs/ [User-saved SHAP plots from Page 3]\n\u2502  \u2514\u2500 *.png\n\u2502\n\u2514\u2500 .streamlit/\n   \u251c\u2500 .streamlit_cache/ [Streamlit cache]\n   \u2514\u2500 logs/ [Streamlit logs]\n</code></pre>"},{"location":"architecture/frontend-architecture-diagram.html#8-control-flow-diagram-swimlanes","title":"8. Control Flow Diagram (Swimlanes)","text":"<pre><code>User          \u2502    Frontend      \u2502    Backend       \u2502    Storage\n              \u2502                  \u2502                  \u2502\nLoad Dataset  \u2502                  \u2502                  \u2502\n  \u251c\u2500\u2192 Page 1 \u2500\u253c\u2500\u2192 fetch_uci() \u2500\u2500\u253c\u2500\u2192 ucimlrepo API \u2502\n  \u2502           \u2502    (or upload)   \u2502                  \u2502\n  \u2502           \u2502                  \u2502                  \u2502\n  \u2514\u2500\u2192 Configure\n      Target \u2500\u253c\u2500\u2192 Validate data  \u251c\u2500\u2192 mock_csv_data() \u2500\u2192 data/*.csv\n              \u2502    Transformation \u2502                  \u2502\n              \u2502                  \u2502                  \u2502\nPrepare Data  \u2502                  \u2502                  \u2502\n  \u251c\u2500\u2192 Page 2 \u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2192 DataPrep  \u2500\u2500\u2500\u2500\u2524\n  \u2502           \u2502                  \u2502  - Read CSV     \u2502\n  \u2502           \u2502                  \u2502  - Scale        \u2502\n  \u2502           \u2502  (data_dict)      \u2502  - Split        \u2502\n  \u2514\u2500\u2192 Optimize\u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  - Map target   \u2502\n              \u2502                  \u2502                  \u2502\n              \u2502                  \u2502                  \u2502\n              \u2502  Trial 1          \u2502                  \u2502\n              \u2502  \u251c\u2500\u2192 Model create \u251c\u2500\u2192 create_model()\u2502\nRun N Trials  \u2502  \u251c\u2500\u2192 Train        \u2502  \u251c\u2500\u2192 model.fit()\u2502\n              \u2502  \u251c\u2500\u2192 Evaluate     \u2502  \u251c\u2500\u2192 f1_score() \u2502\n              \u2502  \u251c\u2500\u2192 Store metrics\u251c\u2500\u2500\u253c\u2500\u2192 trial.user_attr\n              \u2502  \u2514\u2500\u2192 ...N trials  \u2502                  \u2502\n              \u2502                  \u2502                  \u251c\u2500\u2192 db/*.db\n              \u2502  Best results \u25c4\u2500\u2500\u2534\u2500\u2192 study.best_trials\n              \u2502                  \u2502                  \u2502\nAnalyze Model \u2502                  \u2502                  \u2502\n  \u251c\u2500\u2192 Page 3 \u2500\u253c\u2500\u2192 Train model \u2500\u2500\u2524\u2500\u2192 create_model()\u2502\n  \u2502           \u2502                  \u2502  + model.fit()  \u2502\n  \u2502           \u2502                  \u2502                  \u2502\n  \u2502           \u2502\u2500\u2192 SHAP analysis \u2500\u253c\u2500\u2192 XAI class \u2500\u2500\u2500\u2500\u2524\n  \u2502           \u2502  (calculate      \u2502  \u251c\u2500\u2192 Explainer() \u2502\n  \u2502           \u2502   values)         \u2502  \u251c\u2500\u2192 get_plot()\u2502\n  \u2502           \u2502                  \u2502  \u2514\u2500\u2192 metrics    \u2502\n  \u2502           \u2502                  \u2502                  \u251c\u2500\u2192 outputs/*.png\n  \u2502           \u2502                  \u2502                  \u2502\n  \u2502           \u2502\u2500\u2192 Generate report\u251c\u2500\u2192 LangChain  \u2500\u2500\u2500\u2524\n  \u2502           \u2502  (with LLM)      \u2502  \u251c\u2500\u2192 ChatGPT/  \u2502\n  \u2514\u2500\u2192 Download\u2502  \u25c4\u2500 Markdown    \u2502     Gemini/    \u2502\n     Report   \u2502    Report        \u2502     Claude     \u2502\n              \u2502  (*.md file)     \u2502                  \u2502\n</code></pre>"},{"location":"architecture/frontend-architecture-diagram.html#9-key-metrics-performance-path","title":"9. Key Metrics &amp; Performance Path","text":"<pre><code>Optimization Trial Loop:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 for trial in 1..n_trials:                           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                     \u2502\n\u2502  1. Suggest Hyperparameters (TPE Sampler)          \u2502\n\u2502     \u251c\u2500 max_vmap: [1]                              \u2502\n\u2502     \u251c\u2500 batch_size: [32]                           \u2502\n\u2502     \u251c\u2500 learning_rate: [0.001, 0.01, 0.1]         \u2502\n\u2502     \u251c\u2500 n_layers: [1, 5, 10]                       \u2502\n\u2502     \u251c\u2500 model_type: [15 options]                   \u2502\n\u2502     \u2514\u2500 [20+ other params]                          \u2502\n\u2502                                                     \u2502\n\u2502  2. Create Model                                   \u2502\n\u2502     \u2514\u2500 create_model(model_type, **params)         \u2502\n\u2502                                                     \u2502\n\u2502  3. Train Model                                    \u2502\n\u2502     \u2514\u2500 model.fit(train_x, train_y)                \u2502\n\u2502        \u2514\u2500 Training time: 1-30 seconds (varies)    \u2502\n\u2502                                                     \u2502\n\u2502  4. Evaluate Model                                 \u2502\n\u2502     \u251c\u2500 predictions = model.predict(test_x)        \u2502\n\u2502     \u251c\u2500 f1 = f1_score(test_y, predictions)         \u2502\n\u2502     \u251c\u2500 accuracy = accuracy_score(...)             \u2502\n\u2502     \u2514\u2500 Return f1 as objective value                \u2502\n\u2502                                                     \u2502\n\u2502  5. Log Metrics                                    \u2502\n\u2502     \u251c\u2500 trial.set_user_attr(\"Quantum_f1_score\", f1)\u2502\n\u2502     \u251c\u2500 trial.set_user_attr(\"Classical_f1_score\",0)\u2502\n\u2502     \u251c\u2500 trial.set_user_attr(\"Quantum_accuracy\", ...)\n\u2502     \u2514\u2500 trial.set_user_attr(\"Classical_accuracy\",0)\u2502\n\u2502                                                     \u2502\n\u2502  6. Persist to SQLite                              \u2502\n\u2502     \u2514\u2500 Storage: db/{db_name}.db                   \u2502\n\u2502                                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u2502 After N trials\n       \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Best Trials Selection                               \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                     \u2502\n\u2502 study.best_trials:                                 \u2502\n\u2502 \u251c\u2500 Sorted by F1 score (descending)                \u2502\n\u2502 \u251c\u2500 Top 5 displayed in results                      \u2502\n\u2502 \u2514\u2500 Selected for SHAP analysis                      \u2502\n\u2502                                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/frontend-architecture-diagram.html#10-technology-stack-summary","title":"10. Technology Stack Summary","text":"<pre><code>FRONTEND FRAMEWORK:\n\u251c\u2500 Streamlit 1.x\n\u2502  \u251c\u2500 Multi-page app (st.set_page_config)\n\u2502  \u251c\u2500 Session state persistence\n\u2502  \u251c\u2500 Responsive UI components\n\u2502  \u251c\u2500 Tabs, expanders, columns\n\u2502  \u251c\u2500 File uploaders\n\u2502  \u251c\u2500 Chart rendering (native + plotly)\n\u2502  \u2514\u2500 CSS/HTML customization\n\nCORE ML LIBRARIES:\n\u251c\u2500 scikit-learn\n\u2502  \u251c\u2500 Classification models (SVC, MLP, Perceptron)\n\u2502  \u251c\u2500 Metrics (F1, precision, recall, AUC, etc.)\n\u2502  \u251c\u2500 Preprocessing (StandardScaler, train_test_split)\n\u2502  \u2514\u2500 Confusion matrix\n\n\u251c\u2500 PennyLane\n\u2502  \u251c\u2500 Quantum circuit definition\n\u2502  \u251c\u2500 18 quantum model types\n\u2502  \u251c\u2500 Quantum simulators\n\u2502  \u2514\u2500 Hybrid quantum-classical training\n\n\u251c\u2500 Optuna\n\u2502  \u251c\u2500 Hyperparameter optimization\n\u2502  \u251c\u2500 TPE sampler\n\u2502  \u251c\u2500 Study management\n\u2502  \u2514\u2500 SQLite backend\n\nDATA HANDLING:\n\u251c\u2500 pandas\n\u2502  \u251c\u2500 DataFrame operations\n\u2502  \u251c\u2500 CSV I/O\n\u2502  \u2514\u2500 Data transformation\n\n\u251c\u2500 numpy\n\u2502  \u251c\u2500 Array operations\n\u2502  \u251c\u2500 Numerical computations\n\u2502  \u2514\u2500 Data storage (train/test splits)\n\nEXPLAINABILITY:\n\u251c\u2500 SHAP\n\u2502  \u251c\u2500 SHAP Explainer\n\u2502  \u251c\u2500 Feature importance calculation\n\u2502  \u251c\u2500 5 plot types (bar, beeswarm, violin, heatmap, waterfall)\n\u2502  \u2514\u2500 Confusion matrix visualization\n\nAI/LLM INTEGRATION:\n\u251c\u2500 LangChain\n\u2502  \u251c\u2500 Chat model abstractions\n\u2502  \u251c\u2500 Prompt templating\n\u2502  \u251c\u2500 Multimodal message handling\n\u2502  \u2514\u2500 Chain orchestration\n\n\u251c\u2500 langchain-google-genai\n\u2502  \u251c\u2500 Google Generative AI (Gemini)\n\u2502  \u2514\u2500 Vision capability for images\n\n\u251c\u2500 langchain-openai\n\u2502  \u251c\u2500 OpenAI API integration\n\u2502  \u251c\u2500 GPT-4, GPT-4V (vision)\n\u2502  \u2514\u2500 Embedding support\n\nDATA SOURCE:\n\u251c\u2500 ucimlrepo\n\u2502  \u251c\u2500 UCI ML Repository API\n\u2502  \u251c\u2500 400+ datasets\n\u2502  \u2514\u2500 Metadata retrieval\n\nVISUALIZATION:\n\u251c\u2500 matplotlib\n\u2502  \u251c\u2500 SHAP plot rendering\n\u2502  \u251c\u2500 Figure saving\n\u2502  \u2514\u2500 Confusion matrix plots\n\n\u251c\u2500 plotly\n\u2502  \u251c\u2500 Interactive charts\n\u2502  \u251c\u2500 Optuna visualizations\n\u2502  \u2514\u2500 Timeline, parameter importance, history\n\nSTORAGE:\n\u251c\u2500 SQLite 3\n\u2502  \u251c\u2500 Optuna trial database\n\u2502  \u251c\u2500 db/{db_name}.db\n\u2502  \u2514\u2500 ACID compliance\n\n\u251c\u2500 File System\n\u2502  \u251c\u2500 CSV files (data/)\n\u2502  \u251c\u2500 PNG outputs (outputs/)\n\u2502  \u2514\u2500 Markdown reports (download)\n</code></pre>"},{"location":"architecture/frontend-architecture-diagram.html#summary-architecture-points","title":"Summary Architecture Points","text":"<ol> <li>Multi-Page Design: Each page is a self-contained Streamlit app with its own workflow</li> <li>Session-Based State: Streamlit session_state maintains data across pages</li> <li>Backend Separation: Clean separation between frontend UI and backend logic</li> <li>Factory Pattern: Models created via <code>create_model()</code> factory function</li> <li>Lazy Loading: SHAP Explainer and values computed on-demand</li> <li>Modular Libraries: Each external library handles one responsibility</li> <li>File-Based Persistence: SQLite for trials, CSV for prepared data</li> <li>API Abstraction: LangChain abstracts multiple LLM providers</li> <li>Error Handling: Try/except blocks with user-friendly error messages</li> <li>Progressive Disclosure: Information revealed step-by-step through pages</li> </ol>"},{"location":"architecture/frontend-architecture.html","title":"QuOptuna Frontend Architecture - Comprehensive Overview","text":""},{"location":"architecture/frontend-architecture.html#quoptuna-frontend-architecture-comprehensive-overview","title":"QuOptuna Frontend Architecture - Comprehensive Overview","text":""},{"location":"architecture/frontend-architecture.html#1-project-structure","title":"1. Project Structure","text":"<p>The QuOptuna frontend is built with Streamlit and follows a modular, multi-page architecture.</p>"},{"location":"architecture/frontend-architecture.html#directory-layout","title":"Directory Layout","text":"<pre><code>src/quoptuna/frontend/\n\u251c\u2500\u2500 app.py                      # Main entry point &amp; page configuration\n\u251c\u2500\u2500 main_page.py               # Welcome/home page\n\u251c\u2500\u2500 sidebar.py                 # Sidebar with data upload &amp; database configuration\n\u251c\u2500\u2500 support.py                 # Shared utilities &amp; visualization helpers\n\u2514\u2500\u2500 pages/\n    \u251c\u2500\u2500 1_dataset_selection.py # Dataset loading &amp; preparation\n    \u251c\u2500\u2500 2_optimization.py      # Hyperparameter optimization\n    \u251c\u2500\u2500 3_shap_analysis.py     # SHAP analysis &amp; report generation\n    \u2514\u2500\u2500 shap.py               # Legacy SHAP utilities\n</code></pre>"},{"location":"architecture/frontend-architecture.html#2-pagescomponents-overview","title":"2. Pages/Components Overview","text":""},{"location":"architecture/frontend-architecture.html#21-main-application-page-apppy","title":"2.1 Main Application Page (app.py)","text":"<p>Purpose: Application entry point and page configuration</p> <p>Features: - Initializes Streamlit page config (wide layout, atom icon) - Applies custom CSS styling (dark theme with purple accents) - Initializes session state - Coordinates sidebar and main page rendering - Manages visualization updates</p> <p>Key Components: </p><pre><code>st.set_page_config(page_title=\"QuOptuna\", page_icon=\":atom_symbol:\", layout=\"wide\")\n# Purple theme: #9b59b6\n# Dark background: #121212\n</code></pre><p></p>"},{"location":"architecture/frontend-architecture.html#22-mainhome-page-main_pagepy","title":"2.2 Main/Home Page (main_page.py)","text":"<p>Purpose: Welcome page with overview and getting started guide</p> <p>Displays: - Welcome heading with project description - 3-column feature highlights:   - Optimization capabilities   - Explainability features (SHAP)   - Analytics &amp; reporting - Step-by-step workflow guide - Tips &amp; best practices for users - Links to GitHub &amp; issue tracking</p> <p>Supported Models Advertised: - Quantum Models (7+):   - Data Reuploading Classifier   - Circuit-Centric Classifier   - Quantum Kitchen Sinks   - Quantum Metric Learner   - Dressed Quantum Circuit Classifier   - And others...</p> <ul> <li>Classical Models:</li> <li>Support Vector Classifier (SVC)</li> <li>Multi-Layer Perceptron (MLP)</li> <li>Perceptron</li> </ul>"},{"location":"architecture/frontend-architecture.html#23-page-1-dataset-selection-pages1_dataset_selectionpy","title":"2.3 Page 1: Dataset Selection (pages/1_dataset_selection.py)","text":"<p>Purpose: Load and prepare datasets for optimization</p> <p>Two Data Sources: 1. UCI ML Repository (via <code>ucimlrepo</code> library)    - Popular datasets with pre-configured IDs    - Options: Australian Credit, Blood Transfusion, Banknote Auth, Heart Disease, Ionosphere    - Custom dataset ID support    - Displays metadata (instances, features, area, tasks)</p> <ol> <li>Custom CSV Upload</li> <li>File uploader for user datasets</li> <li>Validates CSV format</li> </ol> <p>Configuration Features: - Data preview with statistics - Target &amp; feature column selection - Target value transformation to -1 and 1 (binary classification requirement) - Missing value handling (removal) - Column renaming (target \u2192 \"target\") - Data saved to <code>data/</code> directory via <code>mock_csv_data()</code> utility</p> <p>Session State Tracking: - <code>dataset_df</code>: Loaded DataFrame - <code>file_path</code>: Path to saved processed data - <code>target_column</code>, <code>feature_columns</code>: Selected columns - <code>dataset_name</code>: Dataset identifier</p>"},{"location":"architecture/frontend-architecture.html#24-page-2-optimization-pages2_optimizationpy","title":"2.4 Page 2: Optimization (pages/2_optimization.py)","text":"<p>Purpose: Data preparation and hyperparameter optimization</p> <p>Two-Stage Workflow:</p>"},{"location":"architecture/frontend-architecture.html#stage-1-data-preparation","title":"Stage 1: Data Preparation","text":"<ul> <li>Uses <code>DataPreparation</code> class</li> <li>Reads prepared CSV from Dataset Selection page</li> <li>Performs train/test split (75/25 ratio)</li> <li>StandardScaler normalization</li> <li>Returns data dict with keys: <code>train_x</code>, <code>test_x</code>, <code>train_y</code>, <code>test_y</code></li> <li>Shows train/test split metrics</li> </ul>"},{"location":"architecture/frontend-architecture.html#stage-2-optimization","title":"Stage 2: Optimization","text":"<ul> <li>Uses <code>Optimizer</code> class from backend</li> <li>Configuration inputs:</li> <li>Database name</li> <li>Study name</li> <li>Number of trials (10-200 range)</li> <li>Runs hyperparameter tuning with Optuna</li> <li>Displays progress bar</li> <li>Shows best trials with metrics:</li> <li>Trial number</li> <li>Model type</li> <li>Quantum F1 Score</li> <li>Classical F1 Score</li> <li>Key parameters (learning_rate, n_layers, batch_size, C, gamma)</li> </ul> <p>Key Optimized Hyperparameters: - Model selection from 15 model types - Learning rates: [0.001, 0.01, 0.1] - Layers: [1, 5, 10] - Batch sizes, C values, gamma factors - Quantum-specific: observable type, n_qfeatures, n_episodes, etc.</p> <p>Session State: - <code>data_dict</code>: Prepared training/test data - <code>optimizer</code>: Optimizer instance - <code>study</code>: Optuna study object - <code>best_trials</code>: Top performing trials - <code>optimization_complete</code>: Boolean flag</p>"},{"location":"architecture/frontend-architecture.html#25-page-3-shap-analysis-pages3_shap_analysispy","title":"2.5 Page 3: SHAP Analysis (pages/3_shap_analysis.py)","text":"<p>Purpose: Model explainability with SHAP and AI-powered reporting</p> <p>Four-Stage Workflow:</p>"},{"location":"architecture/frontend-architecture.html#stage-1-trial-selection","title":"Stage 1: Trial Selection","text":"<ul> <li>Dropdown to select from best trials</li> <li>Displays trial details, performance metrics, all parameters</li> <li>Session state: <code>selected_trial</code></li> </ul>"},{"location":"architecture/frontend-architecture.html#stage-2-model-training","title":"Stage 2: Model Training","text":"<ul> <li>Instantiates model with selected trial's hyperparameters</li> <li>Trains on full training dataset</li> <li>Session state: <code>trained_model</code></li> </ul>"},{"location":"architecture/frontend-architecture.html#stage-3-shap-analysis","title":"Stage 3: SHAP Analysis","text":"<ul> <li>Configurable SHAP parameters:</li> <li>Use probability predictions vs class predictions</li> <li>Use data subset vs full dataset</li> <li>Subset size slider (10-200 samples)</li> <li>Creates <code>XAI</code> instance with model and config</li> <li>Session state: <code>xai</code></li> </ul>"},{"location":"architecture/frontend-architecture.html#stage-4-visualizations-report","title":"Stage 4: Visualizations &amp; Report","text":"<ul> <li>5 SHAP Plot Types (in tabs):</li> <li>Bar Plot - Feature importance ranking</li> <li>Beeswarm Plot - Feature impact distribution</li> <li>Violin Plot - SHAP value distributions</li> <li>Heatmap - Instance-level analysis (up to 50 instances)</li> <li> <p>Waterfall Plot - Individual prediction explanation</p> </li> <li> <p>Additional Visualization:</p> </li> <li> <p>Confusion Matrix - Model classification performance</p> </li> <li> <p>AI Report Generation:</p> </li> <li>LLM provider selection: Google, OpenAI, Anthropic</li> <li>Model name customization</li> <li>Dataset context (optional):<ul> <li>URL, description, features, target</li> </ul> </li> <li>Report configuration with API key input</li> <li>LangChain integration for multimodal report generation</li> <li>Markdown report download</li> </ul> <p>Plot Features: - All plots saved as base64-encoded images - Save individual plots to <code>outputs/</code> directory - Max display config for feature limiting</p>"},{"location":"architecture/frontend-architecture.html#3-backend-apis-and-services","title":"3. Backend APIs and Services","text":""},{"location":"architecture/frontend-architecture.html#31-core-backend-modules","title":"3.1 Core Backend Modules","text":""},{"location":"architecture/frontend-architecture.html#quoptunaoptimizer","title":"<code>quoptuna.Optimizer</code>","text":"<p>Location: <code>backend/tuners/optimizer.py</code></p> <p>Responsibilities: - Hyperparameter optimization with Optuna - SQLite database management (<code>db/</code> folder) - Trial execution and metric logging</p> <p>Key Methods: - <code>optimize(n_trials)</code> - Run optimization loop - <code>objective(trial)</code> - Objective function for each trial - <code>log_user_attributes()</code> - Store quantum vs classical metrics - <code>load_study()</code> - Load existing optimization study</p> <p>Data Flow: 1. Receives train/test data (numpy arrays) 2. Creates Optuna study with TPE sampler 3. Iteratively:    - Suggests hyperparameters    - Creates model with <code>create_model()</code>    - Trains and evaluates    - Records F1 scores, accuracy    - Stores metrics in trial user_attrs</p> <p>Storage: SQLite at <code>sqlite:///db/{db_name}.db</code></p>"},{"location":"architecture/frontend-architecture.html#quoptunadatapreparation","title":"<code>quoptuna.DataPreparation</code>","text":"<p>Location: <code>backend/utils/data_utils/prepare.py</code></p> <p>Responsibilities: - Load and normalize data - Train/test splitting - Feature scaling</p> <p>Key Methods: - <code>__init__(file_path, x_cols, y_col)</code> - Load from CSV - <code>prepare_data()</code> - Full preprocessing pipeline - <code>preprocess(x, y)</code> - Scaling and train/test split - <code>get_data(output_type)</code> - Return formatted data dict</p> <p>Processing Pipeline: 1. Read CSV file 2. Select feature &amp; target columns 3. StandardScaler fit on training data 4. Train/test split (75/25, random_state=42) 5. Transform target: unique classes \u2192 {-1, 1}</p>"},{"location":"architecture/frontend-architecture.html#quoptunacreate_modelmodel_type-kwargs","title":"<code>quoptuna.create_model(model_type, **kwargs)</code>","text":"<p>Location: <code>backend/models.py</code></p> <p>Supports 26 Models: - Quantum (18): CircuitCentric, DataReuploading, DressedQuantumCircuit, etc. - Classical (8): SVC, MLPClassifier, Perceptron, etc.</p> <p>Factory Pattern: </p><pre><code>create_model(\n    model_type=\"DataReuploadingClassifier\",\n    max_vmap=1,\n    batch_size=32,\n    learning_rate=0.01,\n    n_layers=5,\n    observable_type=\"full\"\n)\n</code></pre><p></p>"},{"location":"architecture/frontend-architecture.html#quoptunaxai-explainability","title":"<code>quoptuna.XAI</code> (Explainability)","text":"<p>Location: <code>backend/xai/xai.py</code></p> <p>Responsibilities: - SHAP value calculation and visualization - Model performance metrics - AI-powered report generation</p> <p>Key Features:</p> <p>SHAP Methods: - <code>get_plot(type, class_index, index)</code> - Generate plot (bar, beeswarm, violin, heatmap, waterfall) - <code>get_shap_values</code> - Calculate SHAP values for data - <code>explainer</code> - SHAP Explainer instance (lazy-loaded)</p> <p>Metrics Methods: - Classification: precision, recall, F1, MCC, Cohen's Kappa - ROC/AUC: roc_curve, roc_auc_score - Confusion matrix &amp; classification report - Log loss, average precision</p> <p>Report Generation: - <code>generate_report_with_langchain(api_key, model_name, provider)</code> - LLM providers: Google GenAI, OpenAI - Multimodal prompt with base64-encoded images - Configurable system prompt from <code>prompt.txt</code></p> <p>Config: </p><pre><code>XAIConfig(\n    use_proba=True,           # Use probability predictions\n    onsubset=True,            # Use data subset\n    subset_size=100,          # Subset size\n    max_display=20,           # Max features to show\n    feature_names=None        # Auto-detect from data\n)\n</code></pre><p></p>"},{"location":"architecture/frontend-architecture.html#32-data-flow-diagram","title":"3.2 Data Flow Diagram","text":"<pre><code>USER INPUT\n    \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Page 1: Dataset Selection           \u2502\n\u2502 - Load from UCI or CSV upload       \u2502\n\u2502 - Select target &amp; features          \u2502\n\u2502 - Transform binary target (-1, 1)   \u2502\n\u2502 - Save to data/ folder              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2193 (file_path)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Page 2: Optimization                \u2502\n\u2502 - DataPreparation loads &amp; splits    \u2502\n\u2502 - Optimizer.optimize() runs trials  \u2502\n\u2502 - Tests 15 model types              \u2502\n\u2502 - Records quantum/classical scores  \u2502\n\u2502 - Stores in SQLite DB               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2193 (best_trials)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Page 3: SHAP Analysis               \u2502\n\u2502 - Train model with best params      \u2502\n\u2502 - XAI.get_shap_values() calculate   \u2502\n\u2502 - 5 visualization types             \u2502\n\u2502 - LangChain generate AI report      \u2502\n\u2502 - Download markdown report          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/frontend-architecture.html#4-communication-external-services","title":"4. Communication &amp; External Services","text":""},{"location":"architecture/frontend-architecture.html#41-backend-api-calls","title":"4.1 Backend API Calls","text":"<p>Type: In-process Python library calls (no HTTP API)</p> <p>All backend functionality accessed via direct Python imports: </p><pre><code>from quoptuna import Optimizer, DataPreparation, XAI, create_model\n</code></pre><p></p>"},{"location":"architecture/frontend-architecture.html#42-external-services-used","title":"4.2 External Services Used","text":""},{"location":"architecture/frontend-architecture.html#uci-ml-repository","title":"UCI ML Repository","text":"<ul> <li>Library: <code>ucimlrepo</code></li> <li>Purpose: Download datasets</li> <li>Datasets Available: 400+</li> <li>Integration: Pages 1 - fetch_uci_dataset()</li> </ul>"},{"location":"architecture/frontend-architecture.html#optuna","title":"Optuna","text":"<ul> <li>Purpose: Hyperparameter optimization framework</li> <li>Storage: SQLite database</li> <li>Sampler: Tree-structured Parzen Estimator (TPE)</li> <li>Integration: Page 2 - Optimizer class</li> </ul>"},{"location":"architecture/frontend-architecture.html#langchain","title":"LangChain","text":"<ul> <li>Purpose: LLM integration for report generation</li> <li>Providers Supported:</li> <li>Google GenAI (Gemini models)</li> <li>OpenAI (GPT models)</li> <li>Anthropic (Claude models)</li> <li>Features: Multimodal prompting with images</li> <li>Integration: Page 3 - generate_report_with_langchain()</li> </ul>"},{"location":"architecture/frontend-architecture.html#shap","title":"SHAP","text":"<ul> <li>Purpose: Model explainability library</li> <li>Features: Explainer, plot generation</li> <li>Visualizations: bar, beeswarm, waterfall, violin, heatmap</li> <li>Integration: Page 3 - XAI class</li> </ul>"},{"location":"architecture/frontend-architecture.html#scikit-learn","title":"Scikit-learn","text":"<ul> <li>Purpose: Classical ML models &amp; metrics</li> <li>Models: SVC, MLPClassifier, Perceptron</li> <li>Metrics: F1, precision, recall, confusion matrix</li> <li>Integration: Page 2 (optimization), Page 3 (evaluation)</li> </ul>"},{"location":"architecture/frontend-architecture.html#pennylane","title":"PennyLane","text":"<ul> <li>Purpose: Quantum ML framework</li> <li>Models: Quantum circuit classifiers</li> <li>Integration: Page 2 (optimization) - 18 quantum model types</li> </ul>"},{"location":"architecture/frontend-architecture.html#5-session-state-management","title":"5. Session State Management","text":""},{"location":"architecture/frontend-architecture.html#session-state-keys","title":"Session State Keys","text":"<pre><code># Sidebar\nuploaded_file                  # Uploaded CSV file object\nuploaded_file_name            # Original filename\nfile_location                 # Path to uploaded file\nx_columns, y_column          # Selected columns\nDB_NAME, study_name          # Optuna configuration\nn_trials                      # Number of trials\noptimizer                     # Optimizer instance\nprocess_running               # Background optimization flag\nstart_visualization           # Visualization start flag\n\n# Dataset Selection Page\ndataset_loaded                # Boolean flag\ndataset_df                    # DataFrame\ndataset_name                  # Display name\ndataset_metadata              # UCI metadata\nfile_path                     # Processed data path\ntarget_column, feature_columns # Selected columns\n\n# Optimization Page\ndata_dict                     # {train_x, test_x, train_y, test_y}\nstudy                         # Optuna study object\nbest_trials                   # List of best trials\noptimization_complete         # Boolean flag\ndb_name, study_name          # Study identifiers\n\n# SHAP Page\nselected_trial                # Selected trial from dropdown\ntrained_model                 # Trained model instance\nxai                          # XAI instance\nreport                       # Generated markdown report\nshap_images                  # Dict of base64-encoded plots\n</code></pre>"},{"location":"architecture/frontend-architecture.html#6-data-formats-transformations","title":"6. Data Formats &amp; Transformations","text":""},{"location":"architecture/frontend-architecture.html#input-data-format-csv","title":"Input Data Format (CSV)","text":"<pre><code>feature_1, feature_2, ..., feature_n, target_column\n0.5,       0.3,       ..., 0.8,       1\n...\n</code></pre>"},{"location":"architecture/frontend-architecture.html#processed-data-format-in-memory-dict","title":"Processed Data Format (In-Memory Dict)","text":"<pre><code>{\n    \"train_x\": ndarray (n_samples, n_features),\n    \"test_x\": ndarray (m_samples, n_features),\n    \"train_y\": ndarray (n_samples,) with values {-1, 1},\n    \"test_y\": ndarray (m_samples,) with values {-1, 1}\n}\n</code></pre>"},{"location":"architecture/frontend-architecture.html#trial-storage-format","title":"Trial Storage Format","text":"<pre><code>Trial:\n  number: int\n  state: TrialState\n  value: float (F1 score)\n  params: dict (hyperparameters)\n  user_attrs: {\n    \"Quantum_f1_score\": float,\n    \"Classical_f1_score\": float,\n    \"Quantum_accuracy\": float,\n    \"Classical_accuracy\": float,\n    ...\n  }\n  datetime_start, datetime_complete: timestamps\n</code></pre>"},{"location":"architecture/frontend-architecture.html#report-format","title":"Report Format","text":"<p>Markdown with: - Model &amp; dataset information - Performance metrics (F1, precision, recall, etc.) - SHAP visualizations (base64-encoded PNG) - AI-generated interpretation of results</p>"},{"location":"architecture/frontend-architecture.html#7-key-features-capabilities","title":"7. Key Features &amp; Capabilities","text":""},{"location":"architecture/frontend-architecture.html#frontend-strengths","title":"Frontend Strengths","text":"<p>\u2713 Multi-page Streamlit app - Clean workflow separation \u2713 Session persistence - State maintained across pages \u2713 Interactive visualization - Real-time trial monitoring \u2713 Multiple data sources - UCI + custom uploads \u2713 Comprehensive ML suite - 26 model types \u2713 Explainability focus - 5 SHAP visualization types \u2713 AI reporting - LLM-powered analysis \u2713 Dark theme - Purple accent color scheme</p>"},{"location":"architecture/frontend-architecture.html#visualization-capabilities","title":"Visualization Capabilities","text":"<ul> <li>Optuna plots: Timeline, parameter importance, optimization history</li> <li>SHAP plots: Bar, beeswarm, violin, heatmap, waterfall</li> <li>Performance: Confusion matrix, classification report</li> <li>Real-time updates: 10-second refresh interval during optimization</li> </ul>"},{"location":"architecture/frontend-architecture.html#integration-points","title":"Integration Points","text":"<ul> <li>Database: SQLite for trial persistence</li> <li>Data source: UCI ML Repository API</li> <li>LLM APIs: Google, OpenAI, Anthropic</li> <li>ML frameworks: Scikit-learn, PennyLane, SHAP</li> </ul>"},{"location":"architecture/frontend-architecture.html#8-architecture-patterns","title":"8. Architecture Patterns","text":""},{"location":"architecture/frontend-architecture.html#design-patterns-used","title":"Design Patterns Used","text":"<ol> <li>Factory Pattern - <code>create_model(model_type, **kwargs)</code></li> <li>Session State Pattern - Streamlit <code>st.session_state</code></li> <li>Configuration Classes - <code>XAIConfig</code> dataclass</li> <li>Lazy Loading - Properties in XAI (<code>@property explainer</code>)</li> <li>Strategy Pattern - Multiple plot types in SHAP</li> <li>Pipeline Pattern - Data preparation workflow</li> </ol>"},{"location":"architecture/frontend-architecture.html#file-organization","title":"File Organization","text":"<ul> <li>Pages: Stateless, self-contained workflows</li> <li>Support: Shared utilities (<code>support.py</code>)</li> <li>Sidebar: Persistent UI element across pages</li> <li>Backend: Python library (no web API)</li> </ul>"},{"location":"architecture/frontend-architecture.html#9-known-limitations-edge-cases","title":"9. Known Limitations &amp; Edge Cases","text":""},{"location":"architecture/frontend-architecture.html#considerations","title":"Considerations","text":"<ul> <li>Binary Classification Only - Requires target transformation to {-1, 1}</li> <li>Memory Usage - Full data subsets for SHAP can be memory-intensive</li> <li>Quantum Models - Requires PennyLane with quantum simulator</li> <li>Optimization Time - Trials run sequentially, no parallelization in frontend</li> <li>Report Generation - Requires external LLM API keys</li> <li>Database Concurrency - SQLite has single-writer limitation</li> </ul>"},{"location":"architecture/frontend-architecture.html#error-handling","title":"Error Handling","text":"<ul> <li>Try/except blocks around model training</li> <li>Graceful degradation for SHAP visualizations</li> <li>API key validation for report generation</li> <li>Data validation at each stage</li> </ul>"},{"location":"architecture/frontend-architecture.html#10-configuration-customization","title":"10. Configuration &amp; Customization","text":""},{"location":"architecture/frontend-architecture.html#streamlit-config-streamlitconfigtoml","title":"Streamlit Config (<code>.streamlit/config.toml</code>)","text":"<pre><code>[theme]\nbase = \"dark\"\nprimaryColor = \"#8e44ad\"        # Purple\nbackgroundColor = \"#121212\"     # Dark background\nsecondaryBackgroundColor = \"#1c1c1c\"\ntextColor = \"#dcdcdc\"           # Light grey\nfont = \"sans serif\"\n</code></pre>"},{"location":"architecture/frontend-architecture.html#customizable-parameters","title":"Customizable Parameters","text":"<ul> <li>Dataset: Any CSV with binary target</li> <li>Models: Choice of 26 model types</li> <li>Optimization: Trial count, database name, study name</li> <li>SHAP: Subset size, probability vs class mode</li> <li>Reporting: LLM provider, model name, dataset context</li> </ul>"},{"location":"architecture/new-frontend-design.html","title":"QuOptuna Next: Modern Full-Stack Architecture","text":""},{"location":"architecture/new-frontend-design.html#quoptuna-next-modern-full-stack-architecture","title":"QuOptuna Next: Modern Full-Stack Architecture","text":""},{"location":"architecture/new-frontend-design.html#overview","title":"Overview","text":"<p>A modern, intuitive full-stack application for quantum machine learning optimization with drag-and-drop workflow building, inspired by langflow's architecture.</p>"},{"location":"architecture/new-frontend-design.html#tech-stack","title":"Tech Stack","text":""},{"location":"architecture/new-frontend-design.html#frontend","title":"Frontend","text":"<ul> <li>Framework: React 18 + TypeScript</li> <li>Build Tool: Vite (fast HMR, optimized builds)</li> <li>UI Library: shadcn/ui + Tailwind CSS</li> <li>Drag &amp; Drop: React Flow (visual workflow builder)</li> <li>State Management: Zustand (lightweight, modern)</li> <li>Charts: Recharts + Plotly.js</li> <li>API Client: TanStack Query (React Query)</li> <li>Form Handling: React Hook Form + Zod validation</li> <li>File Upload: react-dropzone</li> </ul>"},{"location":"architecture/new-frontend-design.html#backend","title":"Backend","text":"<ul> <li>Framework: FastAPI (async, high-performance)</li> <li>API Docs: Auto-generated OpenAPI/Swagger</li> <li>WebSockets: FastAPI WebSocket for real-time updates</li> <li>Task Queue: Celery + Redis (for long-running optimizations)</li> <li>Database: SQLite (Optuna) + PostgreSQL (metadata)</li> <li>CORS: FastAPI middleware</li> </ul>"},{"location":"architecture/new-frontend-design.html#infrastructure","title":"Infrastructure","text":"<ul> <li>Monorepo: pnpm workspace (frontend) + Python packages (backend)</li> <li>Containerization: Docker + docker-compose</li> <li>Dev Server: Vite dev server + uvicorn --reload</li> <li>Type Safety: TypeScript (frontend) + Pydantic (backend)</li> </ul>"},{"location":"architecture/new-frontend-design.html#architecture-design","title":"Architecture Design","text":""},{"location":"architecture/new-frontend-design.html#1-visual-workflow-builder-drag-drop","title":"1. Visual Workflow Builder (Drag &amp; Drop)","text":"<p>Users can visually build ML pipelines by dragging and connecting nodes:</p> <p>Node Types: 1. Data Nodes    - Upload CSV    - Fetch from UCI    - Data Preview    - Feature Selection</p> <ol> <li>Preprocessing Nodes</li> <li>Train/Test Split</li> <li>StandardScaler</li> <li>Label Encoding</li> <li> <p>Feature Engineering</p> </li> <li> <p>Model Nodes</p> </li> <li>Quantum Models (18 types)</li> <li>Classical Models (8 types)</li> <li> <p>Ensemble Methods</p> </li> <li> <p>Optimization Nodes</p> </li> <li>Optuna Study Config</li> <li>Hyperparameter Ranges</li> <li>Objective Function</li> <li> <p>Run Optimization</p> </li> <li> <p>Analysis Nodes</p> </li> <li>SHAP Analysis</li> <li>Confusion Matrix</li> <li>ROC Curve</li> <li> <p>Feature Importance</p> </li> <li> <p>Output Nodes</p> </li> <li>Export Model</li> <li>Generate Report</li> <li>Save Results</li> </ol> <p>Workflow Example: </p><pre><code>[Upload CSV] \u2192 [Feature Selection] \u2192 [Train/Test Split] \u2192 [StandardScaler]\n                                                               \u2193\n                                                          [Quantum Model]\n                                                               \u2193\n                                                       [Optuna Optimization]\n                                                               \u2193\n                                                          [SHAP Analysis]\n                                                               \u2193\n                                                        [Generate Report]\n</code></pre><p></p>"},{"location":"architecture/new-frontend-design.html#2-page-structure","title":"2. Page Structure","text":""},{"location":"architecture/new-frontend-design.html#dashboard","title":"Dashboard (/)","text":"<ul> <li>Recent workflows</li> <li>Quick actions</li> <li>System status</li> <li>Performance metrics</li> </ul>"},{"location":"architecture/new-frontend-design.html#workflow-builder-workflow","title":"Workflow Builder (/workflow)","text":"<ul> <li>Canvas with React Flow</li> <li>Node palette (left sidebar)</li> <li>Property panel (right sidebar)</li> <li>Toolbar (top): Save, Run, Export, Share</li> </ul>"},{"location":"architecture/new-frontend-design.html#data-explorer-data","title":"Data Explorer (/data)","text":"<ul> <li>Uploaded datasets</li> <li>UCI repository browser</li> <li>Data preview with statistics</li> <li>Feature correlation heatmap</li> </ul>"},{"location":"architecture/new-frontend-design.html#experiments-experiments","title":"Experiments (/experiments)","text":"<ul> <li>List of optimization runs</li> <li>Filter by status, model type, dataset</li> <li>Comparison view (side-by-side)</li> <li>Export results</li> </ul>"},{"location":"architecture/new-frontend-design.html#models-models","title":"Models (/models)","text":"<ul> <li>Saved models library</li> <li>Model card with metadata</li> <li>Performance metrics</li> <li>Download/deploy options</li> </ul>"},{"location":"architecture/new-frontend-design.html#analytics-analytics","title":"Analytics (/analytics)","text":"<ul> <li>SHAP visualizations</li> <li>Interactive plots</li> <li>AI-powered insights</li> <li>Report builder</li> </ul>"},{"location":"architecture/new-frontend-design.html#settings-settings","title":"Settings (/settings)","text":"<ul> <li>API keys (OpenAI, Anthropic, Google)</li> <li>Database configuration</li> <li>Compute preferences</li> <li>Theme customization</li> </ul>"},{"location":"architecture/new-frontend-design.html#3-key-features","title":"3. Key Features","text":""},{"location":"architecture/new-frontend-design.html#modern-uiux","title":"\ud83c\udfa8 Modern UI/UX","text":"<ul> <li>Dark/Light Mode: System preference or manual toggle</li> <li>Responsive Design: Mobile, tablet, desktop optimized</li> <li>Keyboard Shortcuts: Power user workflows</li> <li>Drag &amp; Drop: Intuitive workflow building</li> <li>Live Preview: Real-time data/result updates</li> <li>Toast Notifications: Success, error, info messages</li> </ul>"},{"location":"architecture/new-frontend-design.html#performance","title":"\u26a1 Performance","text":"<ul> <li>Code Splitting: Lazy load routes and components</li> <li>Virtual Scrolling: Handle large datasets</li> <li>Debounced Search: Optimized filtering</li> <li>WebSocket Updates: Real-time optimization progress</li> <li>Caching: React Query for smart data caching</li> </ul>"},{"location":"architecture/new-frontend-design.html#type-safety","title":"\ud83d\udd12 Type Safety","text":"<ul> <li>End-to-End Types: TypeScript \u2194 Pydantic</li> <li>Auto-generated API Client: From OpenAPI spec</li> <li>Runtime Validation: Zod schemas</li> <li>Type Guards: Safer data handling</li> </ul>"},{"location":"architecture/new-frontend-design.html#api-design","title":"API Design","text":""},{"location":"architecture/new-frontend-design.html#rest-endpoints","title":"REST Endpoints","text":"<pre><code>// Data Management\nPOST   /api/v1/data/upload           // Upload CSV\nGET    /api/v1/data/uci              // List UCI datasets\nGET    /api/v1/data/uci/{id}         // Fetch specific dataset\nGET    /api/v1/data/{id}             // Get dataset info\nDELETE /api/v1/data/{id}             // Delete dataset\n\n// Workflows\nPOST   /api/v1/workflows             // Create workflow\nGET    /api/v1/workflows             // List workflows\nGET    /api/v1/workflows/{id}        // Get workflow\nPUT    /api/v1/workflows/{id}        // Update workflow\nDELETE /api/v1/workflows/{id}        // Delete workflow\nPOST   /api/v1/workflows/{id}/run    // Execute workflow\n\n// Optimization\nPOST   /api/v1/optimize              // Start optimization\nGET    /api/v1/optimize/{id}         // Get optimization status\nGET    /api/v1/optimize/{id}/trials  // Get trial history\nDELETE /api/v1/optimize/{id}         // Cancel optimization\n\n// Models\nGET    /api/v1/models                // List available models\nGET    /api/v1/models/{type}         // Get model info\nPOST   /api/v1/models/save           // Save trained model\nGET    /api/v1/models/saved          // List saved models\n\n// Analysis\nPOST   /api/v1/analysis/shap         // Generate SHAP analysis\nPOST   /api/v1/analysis/report       // Generate AI report\nGET    /api/v1/analysis/{id}         // Get analysis results\n\n// System\nGET    /api/v1/health                // Health check\nGET    /api/v1/info                  // System info\n</code></pre>"},{"location":"architecture/new-frontend-design.html#websocket-endpoints","title":"WebSocket Endpoints","text":"<pre><code>WS /ws/optimize/{optimization_id}    // Real-time optimization updates\nWS /ws/workflow/{workflow_id}        // Workflow execution status\n</code></pre>"},{"location":"architecture/new-frontend-design.html#websocket-message-format","title":"WebSocket Message Format","text":"<pre><code>interface OptimizationUpdate {\n  type: 'trial_start' | 'trial_complete' | 'study_complete' | 'error';\n  data: {\n    trial_number: number;\n    params: Record&lt;string, any&gt;;\n    value: number;\n    state: 'running' | 'complete' | 'pruned' | 'failed';\n    timestamp: string;\n  };\n}\n</code></pre>"},{"location":"architecture/new-frontend-design.html#directory-structure","title":"Directory Structure","text":"<pre><code>quoptuna/\n\u251c\u2500\u2500 frontend/                    # React frontend\n\u2502   \u251c\u2500\u2500 src/\n\u2502   \u2502   \u251c\u2500\u2500 components/         # Reusable components\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ui/            # shadcn/ui components\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 workflow/      # React Flow nodes\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 charts/        # Visualization components\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 layout/        # Layout components\n\u2502   \u2502   \u251c\u2500\u2500 pages/             # Route pages\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 Dashboard.tsx\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 WorkflowBuilder.tsx\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 DataExplorer.tsx\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 Experiments.tsx\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 Models.tsx\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 Analytics.tsx\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 Settings.tsx\n\u2502   \u2502   \u251c\u2500\u2500 lib/               # Utilities\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 api.ts         # API client\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 websocket.ts   # WebSocket client\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 utils.ts       # Helper functions\n\u2502   \u2502   \u251c\u2500\u2500 stores/            # Zustand stores\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 workflow.ts\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 data.ts\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 settings.ts\n\u2502   \u2502   \u251c\u2500\u2500 types/             # TypeScript types\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 api.ts\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 workflow.ts\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 models.ts\n\u2502   \u2502   \u251c\u2500\u2500 hooks/             # Custom hooks\n\u2502   \u2502   \u251c\u2500\u2500 App.tsx\n\u2502   \u2502   \u2514\u2500\u2500 main.tsx\n\u2502   \u251c\u2500\u2500 public/\n\u2502   \u251c\u2500\u2500 package.json\n\u2502   \u251c\u2500\u2500 vite.config.ts\n\u2502   \u2514\u2500\u2500 tsconfig.json\n\u2502\n\u251c\u2500\u2500 backend/                     # FastAPI backend\n\u2502   \u251c\u2500\u2500 app/\n\u2502   \u2502   \u251c\u2500\u2500 api/               # API routes\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 v1/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 data.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 workflows.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 optimize.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 models.py\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 analysis.py\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 system.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 deps.py        # Dependencies\n\u2502   \u2502   \u251c\u2500\u2500 core/              # Core functionality\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 config.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 security.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 websocket.py\n\u2502   \u2502   \u251c\u2500\u2500 services/          # Business logic\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 data_service.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 optimization_service.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 analysis_service.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 workflow_service.py\n\u2502   \u2502   \u251c\u2500\u2500 schemas/           # Pydantic schemas\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 data.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 workflow.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 optimization.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 analysis.py\n\u2502   \u2502   \u251c\u2500\u2500 models/            # Database models\n\u2502   \u2502   \u251c\u2500\u2500 tasks/             # Celery tasks\n\u2502   \u2502   \u2514\u2500\u2500 main.py            # FastAPI app\n\u2502   \u2514\u2500\u2500 pyproject.toml\n\u2502\n\u251c\u2500\u2500 docker-compose.yml\n\u2514\u2500\u2500 README_NEXT.md\n</code></pre>"},{"location":"architecture/new-frontend-design.html#implementation-phases","title":"Implementation Phases","text":""},{"location":"architecture/new-frontend-design.html#phase-1-foundation-week-1","title":"Phase 1: Foundation (Week 1)","text":"<ul> <li>[x] Architecture design</li> <li>[ ] Setup Vite + React + TypeScript</li> <li>[ ] Setup FastAPI backend structure</li> <li>[ ] Basic routing (React Router)</li> <li>[ ] API client setup (React Query)</li> <li>[ ] Docker configuration</li> </ul>"},{"location":"architecture/new-frontend-design.html#phase-2-core-features-week-2","title":"Phase 2: Core Features (Week 2)","text":"<ul> <li>[ ] Data upload/preview UI</li> <li>[ ] UCI dataset browser</li> <li>[ ] FastAPI data endpoints</li> <li>[ ] Basic workflow canvas (React Flow)</li> <li>[ ] Node palette and types</li> </ul>"},{"location":"architecture/new-frontend-design.html#phase-3-workflow-builder-week-3","title":"Phase 3: Workflow Builder (Week 3)","text":"<ul> <li>[ ] Complete node library (all 6 types)</li> <li>[ ] Node connection validation</li> <li>[ ] Property panel (node configuration)</li> <li>[ ] Workflow save/load</li> <li>[ ] Workflow execution engine</li> </ul>"},{"location":"architecture/new-frontend-design.html#phase-4-optimization-week-4","title":"Phase 4: Optimization (Week 4)","text":"<ul> <li>[ ] Optuna integration</li> <li>[ ] WebSocket real-time updates</li> <li>[ ] Optimization dashboard</li> <li>[ ] Trial visualization</li> <li>[ ] Parameter importance plots</li> </ul>"},{"location":"architecture/new-frontend-design.html#phase-5-analysis-reporting-week-5","title":"Phase 5: Analysis &amp; Reporting (Week 5)","text":"<ul> <li>[ ] SHAP integration</li> <li>[ ] Interactive plots (Plotly.js)</li> <li>[ ] AI report generation (LangChain)</li> <li>[ ] Export functionality</li> <li>[ ] Model comparison tools</li> </ul>"},{"location":"architecture/new-frontend-design.html#phase-6-polish-deploy-week-6","title":"Phase 6: Polish &amp; Deploy (Week 6)","text":"<ul> <li>[ ] Dark/light mode</li> <li>[ ] Keyboard shortcuts</li> <li>[ ] Performance optimization</li> <li>[ ] Error handling</li> <li>[ ] User documentation</li> <li>[ ] Deployment guide</li> </ul>"},{"location":"architecture/new-frontend-design.html#benefits-over-streamlit","title":"Benefits Over Streamlit","text":"Feature Streamlit QuOptuna Next UX Linear page navigation Drag-and-drop visual workflow Interactivity Limited Fully interactive React app Real-time Polling/rerun WebSocket live updates Customization Limited theming Full design system control Performance Page reloads SPA with smart caching Type Safety Python only End-to-end TypeScript + Python Workflow Manual steps Reusable visual workflows Collaboration Single session Multi-user (future) API None Full REST API + WebSocket Mobile Poor Responsive design"},{"location":"architecture/new-frontend-design.html#development-commands","title":"Development Commands","text":"<pre><code># Frontend development\ncd frontend\npnpm install\npnpm dev              # http://localhost:5173\n\n# Backend development\ncd backend\nuv sync\nuv run uvicorn app.main:app --reload  # http://localhost:8000\n\n# Full stack (Docker)\ndocker-compose up     # Frontend + Backend + Redis + PostgreSQL\n\n# Type generation (OpenAPI \u2192 TypeScript)\npnpm generate:api\n\n# Build for production\npnpm build            # Frontend\nuv build              # Backend\n</code></pre>"},{"location":"architecture/new-frontend-design.html#next-steps","title":"Next Steps","text":"<ol> <li>Get approval on architecture design</li> <li>Initialize projects (Vite + FastAPI)</li> <li>Setup development environment (Docker compose)</li> <li>Implement Phase 1 (Foundation)</li> <li>Iterative development following phases</li> <li>User testing and feedback</li> <li>Production deployment</li> </ol> <p>This design provides: - \u2728 Modern, intuitive UI/UX - \ud83c\udfaf Drag-and-drop workflow building - \u26a1 Real-time updates - \ud83d\udd12 Type-safe development - \ud83d\udce6 Modular, scalable architecture - \ud83d\ude80 Production-ready stack</p>"},{"location":"architecture/optimizer-architecture.html","title":"QuOptuna Optimizer Architecture Documentation","text":""},{"location":"architecture/optimizer-architecture.html#quoptuna-optimizer-architecture-documentation","title":"QuOptuna Optimizer Architecture Documentation","text":""},{"location":"architecture/optimizer-architecture.html#current-state-simulated-vs-real-implementation","title":"Current State: Simulated vs Real Implementation","text":""},{"location":"architecture/optimizer-architecture.html#important-the-optimizer-page-is-currently-simulated","title":"\u26a0\ufe0f Important: The Optimizer Page is Currently Simulated","text":"<p>Yes, you're absolutely right! The optimizer and SHAP analysis are currently simulated - they're not actually running real computations. This is why the process feels too fast.</p>"},{"location":"architecture/optimizer-architecture.html#whats-real-vs-simulated","title":"What's Real vs Simulated","text":""},{"location":"architecture/optimizer-architecture.html#real-backend-services-exist-and-work","title":"\u2705 Real Backend Services (Exist and Work)","text":"<p>Located in <code>/backend/app/services/workflow_service.py</code>:</p> <ol> <li>Real Optuna Optimization (lines 270-311)</li> <li>Uses <code>quoptuna.Optimizer</code> class</li> <li>Runs actual hyperparameter optimization</li> <li>Saves results to SQLite database</li> <li> <p>Returns best trial, parameters, and study info</p> </li> <li> <p>Real SHAP Analysis (lines 313-344)</p> </li> <li>Uses <code>quoptuna.XAI</code> class</li> <li>Generates actual SHAP plots (bar, beeswarm, violin, waterfall)</li> <li>Calculates real feature importance</li> <li> <p>Creates visual plots</p> </li> <li> <p>UCI Dataset Fetching (lines 131-147)</p> </li> <li>\u2705 THIS IS REAL NOW - Uses <code>ucimlrepo</code> library</li> <li>Fetches actual datasets from UCI repository</li> <li> <p>Returns real column names and data</p> </li> <li> <p>Data Preparation (lines 195-219)</p> </li> <li>Real train/test split</li> <li>Real data scaling</li> <li>Real label encoding</li> </ol>"},{"location":"architecture/optimizer-architecture.html#simulated-frontend-code","title":"\u274c Simulated Frontend Code","text":"<p>Located in <code>/frontend/src/pages/Optimizer.tsx</code>:</p> <ol> <li> <p>Simulated Optimization (lines 259-301)    </p><pre><code>// This is FAKE - just a setTimeout loop\nfor (let i = 1; i &lt;= totalTrials; i++) {\n  await new Promise((resolve) =&gt; setTimeout(resolve, 50));\n  setCurrentTrial(i);\n  setProgress((i / totalTrials) * 100);\n}\n\n// Mock results - not real\nconst mockResults = {\n  bestValue: 0.9234,  // Hardcoded!\n  bestParams: { ... } // Hardcoded!\n};\n</code></pre><p></p> </li> <li> <p>Simulated SHAP Analysis (lines 455-477)    </p><pre><code>// This is FAKE - just random numbers\nconst mockSHAPData = {\n  featureImportance: features.map(feature =&gt; ({\n    feature,\n    importance: Math.random() * 0.5 + 0.1  // Random!\n  }))\n};\n</code></pre><p></p> </li> <li> <p>Simulated Report Generation (lines 623-686)    </p><pre><code>// This is a template string - not AI generated\nconst report = `# Optimization Analysis Report...`;\n</code></pre><p></p> </li> </ol>"},{"location":"architecture/optimizer-architecture.html#api-endpoints-are-stubs","title":"\ud83d\udea7 API Endpoints are Stubs","text":"<p>Located in <code>/backend/app/api/v1/</code>:</p> <ul> <li><code>/api/v1/optimize</code> - All TODOs, not implemented</li> <li><code>/api/v1/analysis/shap</code> - All TODOs, not implemented</li> <li><code>/api/v1/analysis/report</code> - All TODOs, not implemented</li> </ul>"},{"location":"architecture/optimizer-architecture.html#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        FRONTEND                                  \u2502\n\u2502  /frontend/src/pages/Optimizer.tsx                              \u2502\n\u2502                                                                   \u2502\n\u2502  Step 1: Dataset Selection                                       \u2502\n\u2502    \u251c\u2500 Upload CSV         \u2500\u2500\u2500\u2500\u2510                                  \u2502\n\u2502    \u2514\u2500 Select UCI Dataset \u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500 \u2705 REAL: Calls backend API   \u2502\n\u2502                               \u2502                                  \u2502\n\u2502  Step 2: Features Selection   \u2502                                  \u2502\n\u2502    \u2514\u2500 Select columns      \u2500\u2500\u2500\u2500\u2524 \u274c STORED IN UI STATE ONLY     \u2502\n\u2502                               \u2502                                  \u2502\n\u2502  Step 3: Configuration        \u2502                                  \u2502\n\u2502    \u2514\u2500 Study name, trials  \u2500\u2500\u2500\u2500\u2524 \u274c STORED IN UI STATE ONLY     \u2502\n\u2502                               \u2502                                  \u2502\n\u2502  Step 4: Optimization         \u2502                                  \u2502\n\u2502    \u2514\u2500 Start Optimization  \u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500 \u274c SIMULATED - setTimeout   \u2502\n\u2502                               \u2502                                  \u2502\n\u2502  Step 5: SHAP Analysis        \u2502                                  \u2502\n\u2502    \u2514\u2500 Generate SHAP       \u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500 \u274c SIMULATED - Random data  \u2502\n\u2502                               \u2502                                  \u2502\n\u2502  Step 6: Generate Report      \u2502                                  \u2502\n\u2502    \u2514\u2500 AI Report           \u2500\u2500\u2500\u2500\u2518\u2500\u2500\u2500 \u274c SIMULATED - Template     \u2502\n\u2502                                                                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u2193 HTTP\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     BACKEND API LAYER                            \u2502\n\u2502  /backend/app/api/v1/                                            \u2502\n\u2502                                                                   \u2502\n\u2502  \u2705 /data/uci/{id}        \u2192 fetch_uci_dataset()                 \u2502\n\u2502  \u2705 /data/upload          \u2192 upload_dataset()                    \u2502\n\u2502  \ud83d\udea7 /optimize             \u2192 start_optimization() [TODO]         \u2502\n\u2502  \ud83d\udea7 /optimize/{id}        \u2192 get_optimization_status() [TODO]    \u2502\n\u2502  \ud83d\udea7 /analysis/shap        \u2192 generate_shap_analysis() [TODO]     \u2502\n\u2502  \ud83d\udea7 /analysis/report      \u2192 generate_ai_report() [TODO]         \u2502\n\u2502                                                                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    BACKEND SERVICES LAYER                        \u2502\n\u2502  /backend/app/services/workflow_service.py                      \u2502\n\u2502                                                                   \u2502\n\u2502  \u2705 REAL IMPLEMENTATION EXISTS:                                 \u2502\n\u2502                                                                   \u2502\n\u2502  WorkflowExecutor                                                \u2502\n\u2502    \u251c\u2500 _execute_data_uci()           [REAL: Uses ucimlrepo]     \u2502\n\u2502    \u251c\u2500 _execute_optimization()       [REAL: Uses quoptuna.Optimizer]\n\u2502    \u251c\u2500 _execute_shap_analysis()      [REAL: Uses quoptuna.XAI]  \u2502\n\u2502    \u251c\u2500 _execute_train_test_split()   [REAL: sklearn]            \u2502\n\u2502    \u2514\u2500 _execute_generate_report()    [TODO: Needs LLM]          \u2502\n\u2502                                                                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    CORE QUOPTUNA LIBRARY                         \u2502\n\u2502  (Installed as dependency)                                       \u2502\n\u2502                                                                   \u2502\n\u2502  \u2705 quoptuna.Optimizer                                           \u2502\n\u2502     \u2514\u2500 optimize() \u2192 Runs Optuna study with quantum models       \u2502\n\u2502                                                                   \u2502\n\u2502  \u2705 quoptuna.XAI                                                 \u2502\n\u2502     \u2514\u2500 SHAP analysis, plots (bar, beeswarm, violin, waterfall)  \u2502\n\u2502                                                                   \u2502\n\u2502  \u2705 quoptuna.DataPreparation                                     \u2502\n\u2502     \u2514\u2500 Train/test split, scaling, encoding                      \u2502\n\u2502                                                                   \u2502\n\u2502  \u2705 quoptuna.create_model()                                      \u2502\n\u2502     \u2514\u2500 Quantum and classical models                             \u2502\n\u2502                                                                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/optimizer-architecture.html#how-ui-triggers-backend-services","title":"How UI Triggers Backend Services","text":""},{"location":"architecture/optimizer-architecture.html#current-flow-what-actually-happens","title":"Current Flow (What Actually Happens)","text":""},{"location":"architecture/optimizer-architecture.html#dataset-selection-working","title":"\u2705 Dataset Selection (Working)","text":"<pre><code>User clicks \"Iris\" in modal\n    \u2193\nfrontend/src/pages/Optimizer.tsx:handleUCISelect(53)\n    \u2193\nfetchUCIDataset(53) in frontend/src/lib/api.ts\n    \u2193\nHTTP POST to /api/v1/data/uci/53\n    \u2193\nbackend/app/api/v1/data.py:fetch_uci_dataset(53)\n    \u2193\nUses ucimlrepo.fetch_ucirepo(id=53)\n    \u2193\nReturns: { dataset_id: \"53\", name: \"Iris\", rows: 150, columns: [...] }\n    \u2193\nStored in workflowData.dataset state\n</code></pre>"},{"location":"architecture/optimizer-architecture.html#optimization-currently-simulated","title":"\u274c Optimization (Currently Simulated)","text":"<pre><code>User clicks \"Start Optimization\"\n    \u2193\nfrontend/src/pages/Optimizer.tsx:startOptimization()\n    \u2193\n[CURRENTLY: setTimeout loop creating fake progress]\n    \u2193\n[SHOULD BE: HTTP POST to /api/v1/optimize]\n    \u2193\n[SHOULD BE: Backend runs real Optuna optimization]\n</code></pre>"},{"location":"architecture/optimizer-architecture.html#shap-analysis-currently-simulated","title":"\u274c SHAP Analysis (Currently Simulated)","text":"<pre><code>User clicks \"Generate SHAP Analysis\"\n    \u2193\nfrontend/src/pages/Optimizer.tsx:generateSHAP()\n    \u2193\n[CURRENTLY: Random numbers for feature importance]\n    \u2193\n[SHOULD BE: HTTP POST to /api/v1/analysis/shap]\n    \u2193\n[SHOULD BE: Backend generates real SHAP plots]\n</code></pre>"},{"location":"architecture/optimizer-architecture.html#what-services-backend-uses","title":"What Services Backend Uses","text":""},{"location":"architecture/optimizer-architecture.html#1-optuna-hyperparameter-optimization","title":"1. Optuna (Hyperparameter Optimization)","text":"<ul> <li>Library: <code>optuna&gt;=4.0.0</code></li> <li>Purpose: Bayesian optimization framework</li> <li>Used in: <code>workflow_service.py:_execute_optimization()</code></li> <li>Features:</li> <li>Pruning (early stopping of bad trials)</li> <li>Visualization of optimization history</li> <li>SQLite database storage</li> <li>Multiple samplers (TPE, CMA-ES, etc.)</li> </ul>"},{"location":"architecture/optimizer-architecture.html#2-shap-explainable-ai","title":"2. SHAP (Explainable AI)","text":"<ul> <li>Library: <code>shap&gt;=0.46.0</code></li> <li>Purpose: Explain model predictions</li> <li>Used in: <code>workflow_service.py:_execute_shap_analysis()</code></li> <li>Features:</li> <li>Feature importance calculation</li> <li>Multiple plot types (bar, beeswarm, violin, waterfall)</li> <li>Works with any ML model</li> </ul>"},{"location":"architecture/optimizer-architecture.html#3-pennylane-quantum-machine-learning","title":"3. PennyLane (Quantum Machine Learning)","text":"<ul> <li>Library: <code>pennylane&gt;=0.39.0</code></li> <li>Purpose: Quantum computing and quantum ML</li> <li>Used in: <code>quoptuna.create_model()</code></li> <li>Features:</li> <li>Variational Quantum Circuits (VQC)</li> <li>Data Reuploading</li> <li>Quantum kernels</li> <li>Hybrid quantum-classical models</li> </ul>"},{"location":"architecture/optimizer-architecture.html#4-scikit-learn-classical-ml","title":"4. Scikit-learn (Classical ML)","text":"<ul> <li>Library: <code>scikit-learn&gt;=1.5.0</code></li> <li>Purpose: Classical machine learning</li> <li>Used in: <code>quoptuna.DataPreparation</code>, models</li> <li>Features:</li> <li>Train/test split</li> <li>Data scaling (StandardScaler)</li> <li>Label encoding</li> <li>Classical models (SVM, RandomForest, etc.)</li> </ul>"},{"location":"architecture/optimizer-architecture.html#5-uci-ml-repository","title":"5. UCI ML Repository","text":"<ul> <li>Library: <code>ucimlrepo&gt;=0.0.3</code></li> <li>Purpose: Access to 600+ datasets</li> <li>Used in: <code>data.py:fetch_uci_dataset()</code>, <code>workflow_service.py</code></li> <li>Features:</li> <li>Fetch datasets by ID</li> <li>Automatic feature/target separation</li> <li>Metadata included</li> </ul>"},{"location":"architecture/optimizer-architecture.html#6-pandas-numpy","title":"6. Pandas &amp; NumPy","text":"<ul> <li>Libraries: <code>pandas&gt;=2.2.0</code>, <code>numpy&gt;=1.24.0</code></li> <li>Purpose: Data manipulation</li> <li>Used in: All data processing steps</li> <li>Features:</li> <li>DataFrame operations</li> <li>Data cleaning</li> <li>Statistical analysis</li> </ul>"},{"location":"architecture/optimizer-architecture.html#implementation-status-summary","title":"Implementation Status Summary","text":"Component Status Location Notes Dataset Selection \u2705 Complete <code>frontend/Optimizer.tsx</code>, <code>backend/data.py</code> Fully working Features Selection \u2705 UI Only <code>frontend/Optimizer.tsx</code> Works but not persisted to backend Configuration \u2705 UI Only <code>frontend/Optimizer.tsx</code> Works but not persisted to backend Optimization \u26a0\ufe0f Backend Only <code>backend/workflow_service.py</code> Real code exists but UI uses mock SHAP Analysis \u26a0\ufe0f Backend Only <code>backend/workflow_service.py</code> Real code exists but UI uses mock Report Generation \u274c Partial <code>frontend/Optimizer.tsx</code> UI has template, backend needs LLM Optimize API \ud83d\udea7 TODO <code>backend/optimize.py</code> Endpoints exist but not implemented Analysis API \ud83d\udea7 TODO <code>backend/analysis.py</code> Endpoints exist but not implemented"},{"location":"architecture/optimizer-architecture.html#why-its-fast-simulated","title":"Why It's Fast (Simulated)","text":"<p>The optimizer completes in ~5 seconds because:</p> <ol> <li>Optimization: Just a <code>setTimeout(50ms)</code> per \"trial\"</li> <li>Real Optuna: 1-10 minutes for 100 trials</li> <li> <p>Simulated: 5 seconds (50ms \u00d7 100 trials)</p> </li> <li> <p>SHAP Analysis: Just <code>Math.random()</code></p> </li> <li>Real SHAP: 30 seconds to 5 minutes</li> <li> <p>Simulated: Instant</p> </li> <li> <p>No Model Training: No actual ML models are trained</p> </li> <li>Real training: Minutes to hours</li> <li>Simulated: 0 seconds</li> </ol>"},{"location":"architecture/optimizer-architecture.html#next-steps-to-connect-ui-to-real-backend","title":"Next Steps to Connect UI to Real Backend","text":"<p>See <code>IMPLEMENTATION_ROADMAP.md</code> for detailed steps to: 1. Implement <code>/api/v1/optimize</code> endpoints 2. Implement <code>/api/v1/analysis</code> endpoints 3. Update frontend to call real APIs 4. Add progress tracking with WebSockets 5. Implement AI report generation with LLM</p> <p>Generated: 2025-11-16 Status: Current implementation uses simulated data in UI, but real services exist in backend</p>"},{"location":"configuration/github-pages-setup.html","title":"GitHub Pages Setup Guide","text":""},{"location":"configuration/github-pages-setup.html#github-pages-setup-guide","title":"GitHub Pages Setup Guide","text":""},{"location":"configuration/github-pages-setup.html#overview","title":"Overview","text":"<p>QuOptuna uses GitHub Actions to automatically build and deploy documentation to GitHub Pages. The setup supports multiple versions:</p> <ul> <li>Latest (Stable): Documentation from <code>main</code> or <code>master</code> branch</li> <li>Development: Documentation from <code>development</code> branch</li> </ul>"},{"location":"configuration/github-pages-setup.html#architecture","title":"Architecture","text":"<p>We use mike for documentation versioning, which allows us to maintain multiple versions of the documentation simultaneously.</p>"},{"location":"configuration/github-pages-setup.html#workflow-files","title":"Workflow Files","text":"<p>Three GitHub Actions workflows handle documentation deployment:</p> <ol> <li><code>docs-versioned.yml</code> (Recommended):</li> <li>Deploys versioned docs using mike</li> <li>Supports both <code>main/master</code> and <code>development</code> branches</li> <li> <p>URLs:</p> <ul> <li>Stable: <code>https://&lt;org&gt;.github.io/&lt;repo&gt;/latest/</code></li> <li>Dev: <code>https://&lt;org&gt;.github.io/&lt;repo&gt;/dev/</code></li> </ul> </li> <li> <p><code>docs.yml</code>:</p> </li> <li>Simple deployment for main/master branch only</li> <li> <p>Single version deployment</p> </li> <li> <p><code>docs-dev.yml</code>:</p> </li> <li>Development branch specific deployment</li> <li>Builds on PRs but only deploys on push</li> </ol>"},{"location":"configuration/github-pages-setup.html#initial-setup","title":"Initial Setup","text":""},{"location":"configuration/github-pages-setup.html#1-enable-github-pages","title":"1. Enable GitHub Pages","text":"<ol> <li>Go to your repository on GitHub</li> <li>Navigate to Settings \u2192 Pages</li> <li>Under Source, select:</li> <li>Source: GitHub Actions</li> <li>(No need to select a branch when using GitHub Actions)</li> </ol>"},{"location":"configuration/github-pages-setup.html#2-configure-branch-protection-optional-but-recommended","title":"2. Configure Branch Protection (Optional but Recommended)","text":"<p>For the <code>development</code> branch:</p> <ol> <li>Go to Settings \u2192 Branches</li> <li>Add a branch protection rule for <code>development</code></li> <li>Enable:</li> <li>\u2705 Require status checks to pass before merging</li> <li>\u2705 Require branches to be up to date before merging</li> <li>Select the \"build\" check from the docs workflow</li> </ol>"},{"location":"configuration/github-pages-setup.html#how-it-works","title":"How It Works","text":""},{"location":"configuration/github-pages-setup.html#versioned-deployment-docs-versionedyml","title":"Versioned Deployment (docs-versioned.yml)","text":"<p>When you push to <code>main/master</code> or <code>development</code>:</p> <ol> <li>Build Phase:</li> <li>Checkout code</li> <li>Install Python and dependencies (mkdocs-material, mike, etc.)</li> <li> <p>Determine version based on branch:</p> <ul> <li><code>main/master</code> \u2192 version: \"latest\"</li> <li><code>development</code> \u2192 version: \"dev\"</li> </ul> </li> <li> <p>Deploy Phase:</p> </li> <li>Use <code>mike deploy</code> to publish versioned docs</li> <li>Update the <code>gh-pages</code> branch with the new version</li> <li>Set default version to \"latest\" (for main/master only)</li> </ol>"},{"location":"configuration/github-pages-setup.html#version-selector","title":"Version Selector","text":"<p>Users can switch between versions using the version selector in the docs (top of the page).</p>"},{"location":"configuration/github-pages-setup.html#directory-structure-on-gh-pages","title":"Directory Structure on gh-pages","text":"<pre><code>gh-pages/\n\u251c\u2500\u2500 latest/           # Stable docs from main/master\n\u2502   \u251c\u2500\u2500 index.html\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 dev/              # Development docs\n\u2502   \u251c\u2500\u2500 index.html\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 versions.json     # Version metadata for mike\n</code></pre>"},{"location":"configuration/github-pages-setup.html#manual-deployment","title":"Manual Deployment","text":""},{"location":"configuration/github-pages-setup.html#using-mike-locally","title":"Using Mike Locally","text":"<p>You can also deploy documentation manually:</p> <pre><code># Install mike\npip install mike\n\n# Deploy a new version\nmike deploy &lt;version&gt; &lt;alias&gt; --push\n\n# Examples:\nmike deploy 1.0.0 latest --push          # Deploy version 1.0.0 as latest\nmike deploy dev --push                    # Deploy dev version\nmike deploy 2.0.0 stable --push --update-aliases  # Deploy and update alias\n\n# Set default version\nmike set-default latest --push\n\n# List all versions\nmike list\n\n# Delete a version\nmike delete &lt;version&gt; --push\n\n# Serve locally to preview\nmike serve\n</code></pre>"},{"location":"configuration/github-pages-setup.html#customization","title":"Customization","text":""},{"location":"configuration/github-pages-setup.html#change-version-names","title":"Change Version Names","text":"<p>Edit <code>.github/workflows/docs-versioned.yml</code>:</p> <pre><code>- name: Determine version name\n  id: version\n  run: |\n    if [[ \"${{ github.ref }}\" == \"refs/heads/main\" ]]; then\n      echo \"version=v1.0\" &gt;&gt; $GITHUB_OUTPUT    # Change version name\n      echo \"alias=stable\" &gt;&gt; $GITHUB_OUTPUT    # Change alias\n    fi\n</code></pre>"},{"location":"configuration/github-pages-setup.html#add-more-branches","title":"Add More Branches","text":"<p>Add more branches to the workflow trigger:</p> <pre><code>on:\n  push:\n    branches:\n      - main\n      - master\n      - development\n      - staging        # Add new branch\n      - beta          # Add another branch\n</code></pre> <p>Then add logic in the \"Determine version name\" step:</p> <pre><code>elif [[ \"${{ github.ref }}\" == \"refs/heads/staging\" ]]; then\n  echo \"version=staging\" &gt;&gt; $GITHUB_OUTPUT\n  echo \"alias=staging\" &gt;&gt; $GITHUB_OUTPUT\n</code></pre>"},{"location":"configuration/github-pages-setup.html#custom-domain","title":"Custom Domain","text":"<p>To use a custom domain:</p> <ol> <li> <p>Add a <code>CNAME</code> file to your <code>docs/</code> directory:    </p><pre><code>docs.yourdomain.com\n</code></pre><p></p> </li> <li> <p>Configure DNS:</p> </li> <li> <p>Add a CNAME record pointing to <code>&lt;org&gt;.github.io</code></p> </li> <li> <p>Update <code>mkdocs.yml</code>:    </p><pre><code>site_url: https://docs.yourdomain.com\n</code></pre><p></p> </li> </ol>"},{"location":"configuration/github-pages-setup.html#workflow-triggers","title":"Workflow Triggers","text":""},{"location":"configuration/github-pages-setup.html#on-push-to-development","title":"On Push to Development","text":"<pre><code>on:\n  push:\n    branches:\n      - development\n    paths:\n      - 'docs/**'\n      - 'mkdocs.yml'\n</code></pre> <p>This triggers when: - Changes are pushed to the <code>development</code> branch - Files in <code>docs/</code> or <code>mkdocs.yml</code> are modified</p>"},{"location":"configuration/github-pages-setup.html#on-pull-request","title":"On Pull Request","text":"<pre><code>on:\n  pull_request:\n    branches:\n      - development\n</code></pre> <p>This builds (but doesn't deploy) docs when a PR is created to <code>development</code>, allowing you to catch build errors before merging.</p>"},{"location":"configuration/github-pages-setup.html#permissions","title":"Permissions","text":"<p>The workflows require these permissions:</p> <pre><code>permissions:\n  contents: write    # For pushing to gh-pages branch\n  pages: write       # For deploying to GitHub Pages\n  id-token: write    # For authentication\n</code></pre>"},{"location":"configuration/github-pages-setup.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"configuration/github-pages-setup.html#build-fails","title":"Build Fails","text":"<p>Error: <code>mkdocs build</code> fails</p> <p>Check the build logs: 1. Go to Actions tab 2. Click on the failed workflow 3. Review the \"Build documentation\" step</p> <p>Common issues: - Missing dependencies in workflow - Broken links in markdown - Invalid mkdocs.yml syntax</p> <p>Fix: </p><pre><code># Test locally\nmkdocs build --strict --verbose\n</code></pre><p></p>"},{"location":"configuration/github-pages-setup.html#deployment-fails","title":"Deployment Fails","text":"<p>Error: Permission denied</p> <p>Check repository settings: 1. Settings \u2192 Actions \u2192 General 2. Under Workflow permissions, select:    - \u2705 Read and write permissions</p> <p>Error: Pages deployment failed</p> <p>Ensure GitHub Pages is enabled: 1. Settings \u2192 Pages 2. Source should be GitHub Actions</p>"},{"location":"configuration/github-pages-setup.html#version-not-showing","title":"Version Not Showing","text":"<p>Version selector is empty</p> <ol> <li>Ensure <code>extra.version.provider: mike</code> is in <code>mkdocs.yml</code></li> <li>Check that <code>mike</code> is installed in the workflow</li> <li>Verify <code>versions.json</code> exists in gh-pages branch</li> </ol>"},{"location":"configuration/github-pages-setup.html#testing","title":"Testing","text":""},{"location":"configuration/github-pages-setup.html#test-locally","title":"Test Locally","text":"<pre><code># Install dependencies\npip install mkdocs-material mike mkdocs-autorefs mkdocstrings[python] mkdocs-glightbox\n\n# Build locally\nmkdocs build\n\n# Serve locally\nmkdocs serve\n\n# Test versioned deployment locally\nmike deploy test --push\nmike serve\n</code></pre>"},{"location":"configuration/github-pages-setup.html#test-workflow","title":"Test Workflow","text":"<p>Create a test branch:</p> <pre><code>git checkout -b test-docs\n# Make changes to docs\ngit add docs/\ngit commit -m \"Test docs changes\"\ngit push -u origin test-docs\n</code></pre> <p>Then create a PR to <code>development</code> to trigger the build workflow.</p>"},{"location":"configuration/github-pages-setup.html#best-practices","title":"Best Practices","text":""},{"location":"configuration/github-pages-setup.html#1-always-review-locally-first","title":"1. Always Review Locally First","text":"<pre><code>mkdocs serve\n# Visit http://127.0.0.1:8000\n</code></pre>"},{"location":"configuration/github-pages-setup.html#2-use-strict-mode","title":"2. Use Strict Mode","text":"<p>This catches issues early:</p> <pre><code>mkdocs build --strict\n</code></pre>"},{"location":"configuration/github-pages-setup.html#3-keep-versions-clean","title":"3. Keep Versions Clean","text":"<p>Delete old versions when no longer needed:</p> <pre><code>mike delete old-version --push\n</code></pre>"},{"location":"configuration/github-pages-setup.html#4-use-aliases-wisely","title":"4. Use Aliases Wisely","text":"<p>Aliases make it easy to update versions:</p> <pre><code># Users can always go to /latest/ to get current stable version\nmike deploy 1.2.0 latest --update-aliases --push\n</code></pre>"},{"location":"configuration/github-pages-setup.html#5-document-breaking-changes","title":"5. Document Breaking Changes","text":"<p>When deploying a new major version, consider: - Keeping old version accessible - Adding migration guide - Using version warnings</p>"},{"location":"configuration/github-pages-setup.html#urls","title":"URLs","text":"<p>After setup, your documentation will be available at:</p> <ul> <li>Stable/Latest: <code>https://&lt;org&gt;.github.io/&lt;repo&gt;/latest/</code></li> <li>Development: <code>https://&lt;org&gt;.github.io/&lt;repo&gt;/dev/</code></li> <li>Root: <code>https://&lt;org&gt;.github.io/&lt;repo&gt;/</code> (redirects to default version)</li> </ul> <p>For QuOptuna: - Stable: <code>https://qentora.github.io/quoptuna/latest/</code> - Development: <code>https://qentora.github.io/quoptuna/dev/</code></p>"},{"location":"configuration/github-pages-setup.html#additional-resources","title":"Additional Resources","text":"<ul> <li>MkDocs Documentation</li> <li>Material for MkDocs</li> <li>Mike Documentation</li> <li>GitHub Pages Documentation</li> <li>GitHub Actions Documentation</li> </ul>"},{"location":"configuration/github-pages-setup.html#support","title":"Support","text":"<p>If you encounter issues:</p> <ol> <li>Check workflow logs in the Actions tab</li> <li>Review the GitHub Pages Status</li> <li>Test locally with <code>mkdocs build --strict</code></li> <li>Open an issue in the repository</li> </ol>"},{"location":"configuration/github-pages-setup.html#license","title":"License","text":"<p>This setup is part of QuOptuna and follows the same MIT license.</p>"},{"location":"configuration/github-settings-guide.html","title":"GitHub Settings Configuration Guide","text":""},{"location":"configuration/github-settings-guide.html#github-settings-configuration-guide","title":"GitHub Settings Configuration Guide","text":"<p>This guide walks you through the required GitHub repository settings to enable automatic documentation deployment.</p>"},{"location":"configuration/github-settings-guide.html#prerequisites","title":"Prerequisites","text":"<ul> <li>You must have admin access to the repository</li> <li>The workflows must be committed to the repository</li> <li>You need to be on GitHub.com (not local)</li> </ul>"},{"location":"configuration/github-settings-guide.html#step-1-enable-github-pages","title":"Step 1: Enable GitHub Pages","text":""},{"location":"configuration/github-settings-guide.html#11-navigate-to-repository-settings","title":"1.1 Navigate to Repository Settings","text":"<ol> <li>Go to your repository on GitHub: <code>https://github.com/Qentora/quoptuna</code></li> <li>Click the \u2699\ufe0f Settings tab (top right of the repository page)</li> <li>If you don't see this tab, you don't have admin access</li> </ol>"},{"location":"configuration/github-settings-guide.html#12-access-pages-settings","title":"1.2 Access Pages Settings","text":"<ol> <li>In the left sidebar, scroll down to the Code and automation section</li> <li>Click on Pages</li> </ol>"},{"location":"configuration/github-settings-guide.html#13-configure-pages-source","title":"1.3 Configure Pages Source","text":"<ol> <li>Under Build and deployment section:</li> <li>Source: Select GitHub Actions from the dropdown      <pre><code>Before: Deploy from a branch\nAfter:  GitHub Actions \u2713\n</code></pre></li> <li> <p>You should see: \"Your site is ready to be published at https://qentora.github.io/quoptuna/\"</p> </li> <li> <p>Do NOT select a branch - GitHub Actions will handle deployment</p> </li> <li> <p>The page should now show:    </p><pre><code>Source\nGitHub Actions\n\nYour GitHub Pages site is currently being built from the GitHub Actions workflow.\n</code></pre><p></p> </li> <li> <p>Click anywhere else to auto-save (no save button needed)</p> </li> </ol>"},{"location":"configuration/github-settings-guide.html#14-optional-custom-domain-skip-if-not-needed","title":"1.4 Optional: Custom Domain (Skip if not needed)","text":"<p>If you want to use a custom domain like <code>docs.quoptuna.com</code>:</p> <ol> <li>Under Custom domain, enter your domain</li> <li>Click Save</li> <li>Wait for DNS check to complete</li> <li>Enable Enforce HTTPS after DNS verification</li> </ol>"},{"location":"configuration/github-settings-guide.html#step-2-configure-workflow-permissions","title":"Step 2: Configure Workflow Permissions","text":"<p>This is critical - without this, the workflows cannot deploy to GitHub Pages.</p>"},{"location":"configuration/github-settings-guide.html#21-navigate-to-actions-settings","title":"2.1 Navigate to Actions Settings","text":"<ol> <li>Still in \u2699\ufe0f Settings, find Code and automation section in left sidebar</li> <li>Click on Actions</li> <li>Click on General (under Actions)</li> </ol>"},{"location":"configuration/github-settings-guide.html#22-set-workflow-permissions","title":"2.2 Set Workflow Permissions","text":"<p>Scroll down to the Workflow permissions section:</p> <ol> <li> <p>Select: Read and write permissions (this is critical!)    </p><pre><code>\u25cb Read repository contents and packages permissions (default)\n\u25cf Read and write permissions \u2190 SELECT THIS\n</code></pre><p></p> </li> <li> <p>Keep checked: \u2705 Allow GitHub Actions to create and approve pull requests</p> </li> <li> <p>Click Save button at the bottom</p> </li> </ol>"},{"location":"configuration/github-settings-guide.html#23-why-this-is-important","title":"2.3 Why This Is Important","text":"<p>The workflows need write permissions to: - Push to the <code>gh-pages</code> branch - Deploy documentation to GitHub Pages - Update version information with mike</p>"},{"location":"configuration/github-settings-guide.html#step-3-verify-branch-protection-optional-but-recommended","title":"Step 3: Verify Branch Protection (Optional but Recommended)","text":""},{"location":"configuration/github-settings-guide.html#31-navigate-to-branches","title":"3.1 Navigate to Branches","text":"<ol> <li>In \u2699\ufe0f Settings, click Branches (under Code and automation)</li> </ol>"},{"location":"configuration/github-settings-guide.html#32-add-branch-protection-rule-for-development","title":"3.2 Add Branch Protection Rule for Development","text":"<ol> <li> <p>Click Add branch protection rule</p> </li> <li> <p>Configure:</p> </li> <li> <p>Branch name pattern: <code>development</code></p> </li> <li> <p>Enable these settings:</p> </li> <li> <p>\u2705 Require a pull request before merging</p> <ul> <li>\u2705 Require approvals: 1 (or more)</li> </ul> </li> <li> <p>\u2705 Require status checks to pass before merging</p> <ul> <li>\u2705 Require branches to be up to date before merging</li> <li>Search and select: <code>build</code> (from docs workflows)</li> </ul> </li> <li> <p>\u2705 Require conversation resolution before merging</p> </li> <li> <p>\u2705 Do not allow bypassing the above settings</p> </li> <li> <p>Click Create or Save changes</p> </li> </ol>"},{"location":"configuration/github-settings-guide.html#33-branch-protection-for-mainmaster-recommended","title":"3.3 Branch Protection for Main/Master (Recommended)","text":"<p>Repeat the same process for your main branch: - Branch name pattern: <code>main</code> (or <code>master</code>) - Same settings as development - This prevents accidental direct pushes to production</p>"},{"location":"configuration/github-settings-guide.html#step-4-enable-github-actions-if-disabled","title":"Step 4: Enable GitHub Actions (if disabled)","text":""},{"location":"configuration/github-settings-guide.html#41-check-if-actions-are-enabled","title":"4.1 Check if Actions Are Enabled","text":"<ol> <li>In \u2699\ufe0f Settings \u2192 Actions \u2192 General</li> <li>Under Actions permissions, ensure one of these is selected:</li> <li>\u2705 Allow all actions and reusable workflows (recommended for public repos)</li> <li> <p>\u2705 Allow enterprise, and select non-enterprise, actions and reusable workflows</p> </li> <li> <p>If actions were disabled, click Save</p> </li> </ol>"},{"location":"configuration/github-settings-guide.html#step-5-configure-environments-automatic","title":"Step 5: Configure Environments (Automatic)","text":"<p>GitHub will automatically create these environments when workflows run: - <code>github-pages</code> (for stable docs from main) - <code>github-pages-dev</code> (for dev docs from development)</p> <p>You can view/configure them later at: - \u2699\ufe0f Settings \u2192 Environments</p>"},{"location":"configuration/github-settings-guide.html#optional-environment-configuration","title":"Optional Environment Configuration","text":"<p>After the first deployment, you can add protection rules:</p> <ol> <li>Go to Settings \u2192 Environments</li> <li>Click on <code>github-pages</code> or <code>github-pages-dev</code></li> <li>Add Deployment protection rules:</li> <li>Required reviewers</li> <li>Wait timer</li> <li>Deployment branches (restrict to specific branches)</li> </ol>"},{"location":"configuration/github-settings-guide.html#step-6-trigger-your-first-deployment","title":"Step 6: Trigger Your First Deployment","text":""},{"location":"configuration/github-settings-guide.html#option-a-push-to-development-branch","title":"Option A: Push to Development Branch","text":"<pre><code># If you're on the claude branch, merge to development\ngit checkout development\ngit merge claude/jupyter-dataset-experiments-011CUsbYuf2cF6t4P6NWtFfg\ngit push origin development\n</code></pre>"},{"location":"configuration/github-settings-guide.html#option-b-merge-pull-request","title":"Option B: Merge Pull Request","text":"<ol> <li>Create a PR from your claude branch to <code>development</code></li> <li>The workflow will build (but not deploy) the docs</li> <li>After PR is merged, the workflow will deploy to <code>/dev/</code></li> </ol>"},{"location":"configuration/github-settings-guide.html#option-c-manual-trigger","title":"Option C: Manual Trigger","text":"<ol> <li>Go to Actions tab in your repository</li> <li>Select Deploy Versioned Docs workflow</li> <li>Click Run workflow button</li> <li>Select branch: <code>development</code></li> <li>Click Run workflow</li> </ol>"},{"location":"configuration/github-settings-guide.html#step-7-monitor-deployment","title":"Step 7: Monitor Deployment","text":""},{"location":"configuration/github-settings-guide.html#71-check-workflow-status","title":"7.1 Check Workflow Status","text":"<ol> <li>Go to Actions tab</li> <li>You should see a workflow running: Deploy Versioned Docs</li> <li>Click on it to see progress</li> <li>Wait for all jobs to complete (green checkmarks)</li> </ol>"},{"location":"configuration/github-settings-guide.html#72-check-for-errors","title":"7.2 Check for Errors","text":"<p>If the workflow fails:</p> <ol> <li>Click on the failed job</li> <li>Expand the step that failed (red X)</li> <li>Read the error message</li> <li>Common issues:</li> <li>Permission denied: Go back to Step 2, ensure \"Read and write permissions\" is selected</li> <li>Pages not enabled: Go back to Step 1</li> <li>Build errors: Check the documentation locally with <code>mkdocs build --strict</code></li> </ol>"},{"location":"configuration/github-settings-guide.html#73-verify-deployment","title":"7.3 Verify Deployment","text":"<p>After workflow succeeds:</p> <ol> <li>Go back to Settings \u2192 Pages</li> <li>You should see:    <pre><code>Your site is live at https://qentora.github.io/quoptuna/\n</code></pre></li> <li>Click Visit site to view your documentation</li> </ol>"},{"location":"configuration/github-settings-guide.html#step-8-access-your-documentation","title":"Step 8: Access Your Documentation","text":""},{"location":"configuration/github-settings-guide.html#development-documentation","title":"Development Documentation","text":"<p>After pushing to <code>development</code> branch: - URL: <code>https://qentora.github.io/quoptuna/dev/</code> - Updates automatically on every push to development</p>"},{"location":"configuration/github-settings-guide.html#stable-documentation","title":"Stable Documentation","text":"<p>After merging to <code>main</code> or <code>master</code>: - URL: <code>https://qentora.github.io/quoptuna/latest/</code> - Also available at: <code>https://qentora.github.io/quoptuna/</code></p>"},{"location":"configuration/github-settings-guide.html#version-selector","title":"Version Selector","text":"<p>Users can switch between versions using the selector in the documentation header.</p>"},{"location":"configuration/github-settings-guide.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"configuration/github-settings-guide.html#issue-1-workflow-not-permitted-to-deploy-to-environment","title":"Issue 1: \"Workflow not permitted to deploy to environment\"","text":"<p>Solution: 1. Settings \u2192 Actions \u2192 General 2. Ensure \"Read and write permissions\" is selected 3. Click Save</p>"},{"location":"configuration/github-settings-guide.html#issue-2-pages-build-and-deployment-failed","title":"Issue 2: \"Pages build and deployment failed\"","text":"<p>Solution: 1. Settings \u2192 Pages 2. Ensure source is set to GitHub Actions (not \"Deploy from a branch\")</p>"},{"location":"configuration/github-settings-guide.html#issue-3-workflow-runs-but-docs-dont-update","title":"Issue 3: Workflow runs but docs don't update","text":"<p>Solution: 1. Check if the <code>gh-pages</code> branch exists 2. Go to repository \u2192 Branches 3. If <code>gh-pages</code> doesn't exist, the first deployment creates it 4. Wait a few minutes and check again</p>"},{"location":"configuration/github-settings-guide.html#issue-4-404-error-when-visiting-docs-url","title":"Issue 4: 404 error when visiting docs URL","text":"<p>Possible causes: 1. Workflow hasn't completed yet - wait and refresh 2. GitHub Pages not enabled - go to Step 1 3. Path is wrong - use <code>/dev/</code> or <code>/latest/</code> not just root</p> <p>Solution: 1. Verify workflow completed successfully (green checkmark) 2. Wait 2-3 minutes for GitHub Pages to propagate 3. Clear browser cache 4. Try incognito/private browsing mode</p>"},{"location":"configuration/github-settings-guide.html#issue-5-old-version-still-showing","title":"Issue 5: Old version still showing","text":"<p>Solution: </p><pre><code># Clear version cache with mike\npip install mike\nmike delete old-version --push\nmike deploy new-version latest --push --update-aliases\n</code></pre><p></p>"},{"location":"configuration/github-settings-guide.html#quick-reference-checklist","title":"Quick Reference Checklist","text":"<p>Before workflows can deploy docs, ensure:</p> <ul> <li>[ ] GitHub Pages enabled (Settings \u2192 Pages \u2192 Source: GitHub Actions)</li> <li>[ ] Workflow permissions set to \"Read and write\" (Settings \u2192 Actions \u2192 General)</li> <li>[ ] Workflows committed and pushed to repository</li> <li>[ ] Branch exists (<code>development</code> or <code>main</code>)</li> <li>[ ] Changes to <code>docs/</code> or <code>mkdocs.yml</code> pushed</li> </ul>"},{"location":"configuration/github-settings-guide.html#summary-of-settings","title":"Summary of Settings","text":"Setting Location Required Value Pages Source Settings \u2192 Pages GitHub Actions Workflow Permissions Settings \u2192 Actions \u2192 General Read and write permissions Actions Settings \u2192 Actions \u2192 General Allow all actions Branch N/A <code>development</code> or <code>main</code> exists"},{"location":"configuration/github-settings-guide.html#next-steps-after-setup","title":"Next Steps After Setup","text":"<ol> <li>Test the deployment:</li> <li>Make a small change to any file in <code>docs/</code></li> <li>Commit and push to <code>development</code></li> <li>Check Actions tab for workflow run</li> <li> <p>Visit the dev docs URL</p> </li> <li> <p>Set up branch protection (recommended)</p> </li> <li>Protect <code>development</code> and <code>main</code> branches</li> <li>Require PR reviews</li> <li> <p>Require status checks</p> </li> <li> <p>Customize documentation:</p> </li> <li>Update <code>docs/index.md</code></li> <li>Add more documentation pages</li> <li> <p>Customize <code>mkdocs.yml</code> theme</p> </li> <li> <p>Monitor deployments:</p> </li> <li>Subscribe to workflow notifications</li> <li>Check Actions tab regularly</li> <li>Set up deployment notifications</li> </ol>"},{"location":"configuration/github-settings-guide.html#getting-help","title":"Getting Help","text":"<p>If you encounter issues not covered here:</p> <ol> <li>Check workflow logs in Actions tab</li> <li>Review GITHUB_PAGES_SETUP.md for detailed troubleshooting</li> <li>Consult GitHub Pages documentation</li> <li>Check mike documentation</li> <li>Open an issue in the repository</li> </ol>"},{"location":"configuration/github-settings-guide.html#visual-guide","title":"Visual Guide","text":""},{"location":"configuration/github-settings-guide.html#settings-pages","title":"Settings \u2192 Pages","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Build and deployment                \u2502\n\u2502                                     \u2502\n\u2502 Source                              \u2502\n\u2502 [GitHub Actions \u25bc]  \u2190 SELECT THIS  \u2502\n\u2502                                     \u2502\n\u2502 Your site is live at:               \u2502\n\u2502 https://qentora.github.io/quoptuna/ \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"configuration/github-settings-guide.html#settings-actions-general-workflow-permissions","title":"Settings \u2192 Actions \u2192 General \u2192 Workflow permissions","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Workflow permissions                \u2502\n\u2502                                     \u2502\n\u2502 \u25cb Read repository contents and     \u2502\n\u2502   packages permissions              \u2502\n\u2502                                     \u2502\n\u2502 \u25cf Read and write permissions        \u2502\n\u2502   \u2190 SELECT THIS                     \u2502\n\u2502                                     \u2502\n\u2502 \u2713 Allow GitHub Actions to create   \u2502\n\u2502   and approve pull requests         \u2502\n\u2502                                     \u2502\n\u2502 [Save]                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Last Updated: November 2024 Repository: https://github.com/Qentora/quoptuna</p>"},{"location":"development/implementation-summary.html","title":"QuOptuna Workflow Builder - Implementation Summary","text":""},{"location":"development/implementation-summary.html#quoptuna-workflow-builder-implementation-summary","title":"QuOptuna Workflow Builder - Implementation Summary","text":""},{"location":"development/implementation-summary.html#overview","title":"Overview","text":"<p>This document summarizes the comprehensive implementation of the QuOptuna Workflow Builder, a visual drag-and-drop interface for creating quantum and classical machine learning pipelines.</p>"},{"location":"development/implementation-summary.html#what-was-implemented","title":"What Was Implemented","text":""},{"location":"development/implementation-summary.html#1-backend-fixes-and-enhancements","title":"1. Backend Fixes and Enhancements","text":""},{"location":"development/implementation-summary.html#dependency-compatibility-fix","title":"Dependency Compatibility Fix","text":"<ul> <li>Problem: NumPy 2.0 incompatibility with SHAP 0.46.0 causing <code>TypeError</code> on import</li> <li>Solution: Downgraded NumPy to <code>&gt;=1.24.0,&lt;2.0.0</code> in <code>backend/pyproject.toml</code></li> <li>File: <code>backend/pyproject.toml:16</code></li> <li>Impact: Backend now starts successfully without import errors</li> </ul>"},{"location":"development/implementation-summary.html#docker-build-fix","title":"Docker Build Fix","text":"<ul> <li>Problem: <code>crosshair-tool</code> compilation failure due to missing gcc</li> <li>Solution: Added build dependencies (gcc, g++) to Dockerfile</li> <li>File: <code>backend/Dockerfile:8-12</code></li> <li>Impact: Docker builds complete successfully</li> </ul>"},{"location":"development/implementation-summary.html#file-upload-api-implementation","title":"File Upload API Implementation","text":"<ul> <li>What: Complete CSV file upload endpoint with validation and metadata</li> <li>Features:</li> <li>UUID-based file naming for uniqueness</li> <li>Automatic upload directory creation</li> <li>CSV validation and column detection</li> <li>File cleanup on errors</li> <li>Returns file path, rows, and column names</li> <li>File: <code>backend/app/api/v1/data.py:22-58</code></li> <li>Endpoint: <code>POST /api/v1/data/upload</code></li> <li>Response:   <pre><code>{\n  \"message\": \"Dataset uploaded successfully\",\n  \"filename\": \"iris.csv\",\n  \"id\": \"uuid-here\",\n  \"file_path\": \"/app/uploads/uuid.csv\",\n  \"rows\": 150,\n  \"columns\": [\"sepal_length\", \"sepal_width\", ...]\n}\n</code></pre></li> </ul>"},{"location":"development/implementation-summary.html#2-frontend-implementation","title":"2. Frontend Implementation","text":""},{"location":"development/implementation-summary.html#node-configuration-panel-component","title":"Node Configuration Panel Component","text":"<ul> <li>What: Complete UI panel for configuring all 16 node types</li> <li>File: <code>frontend/src/components/workflow/NodeConfigPanel.tsx</code></li> <li>Features:</li> <li>Slide-out panel on right side</li> <li>Different configuration forms per node type</li> <li>File upload dialog for CSV upload nodes</li> <li>Model selection dropdowns</li> <li>Feature column input fields</li> <li>Optuna parameter inputs (trials, study name, database)</li> <li>SHAP plot type checkboxes</li> <li>Real-time file upload with progress indicator</li> <li>Save/Cancel buttons</li> <li>Lines of Code: 364</li> </ul>"},{"location":"development/implementation-summary.html#upload-api-integration","title":"Upload API Integration","text":"<ul> <li>What: Frontend API client for file uploads</li> <li>File: <code>frontend/src/lib/api.ts:101-128</code></li> <li>Function: <code>uploadDataset(file: File)</code></li> <li>Features:</li> <li>FormData handling for multipart uploads</li> <li>Error handling and type safety</li> <li>Returns TypeScript-typed response</li> </ul>"},{"location":"development/implementation-summary.html#workflow-builder-integration","title":"Workflow Builder Integration","text":"<ul> <li>What: Connected configuration panel to workflow builder</li> <li>File: <code>frontend/src/pages/WorkflowBuilder.tsx</code></li> <li>Changes:</li> <li>Added <code>selectedNode</code> state</li> <li>Added <code>handleNodeClick</code> handler</li> <li>Added <code>handleConfigSave</code> function</li> <li>Integrated NodeConfigPanel component</li> <li>Click on any node to open configuration</li> <li>Configuration saved to node's <code>data.config</code> object</li> </ul>"},{"location":"development/implementation-summary.html#3-documentation","title":"3. Documentation","text":""},{"location":"development/implementation-summary.html#user-guide","title":"User Guide","text":"<ul> <li>File: <code>frontend/WORKFLOW_BUILDER_GUIDE.md</code></li> <li>Length: 700+ lines</li> <li>Contents:</li> <li>Getting started instructions</li> <li>Detailed documentation for all 16 node types</li> <li>Configuration parameters for each node</li> <li>3 complete example workflows</li> <li>Troubleshooting section</li> <li>API integration guide</li> <li>Best practices</li> <li>Future enhancements roadmap</li> </ul> <p>Example workflows documented: 1. Simple Classification (UCI \u2192 Features \u2192 Split \u2192 Quantum \u2192 Optimize \u2192 SHAP) 2. Upload and Analyze (Upload \u2192 Preview \u2192 Features \u2192 Split \u2192 Classical \u2192 Optimize) 3. Quantum vs Classical Comparison (parallel paths)</p>"},{"location":"development/implementation-summary.html#testing-checklist","title":"Testing Checklist","text":"<ul> <li>File: <code>TESTING_CHECKLIST.md</code></li> <li>Length: 650+ lines</li> <li>Contents:</li> <li>37 systematic test cases</li> <li>Backend API tests (upload, execution, status)</li> <li>Frontend UI tests (navigation, canvas, nodes)</li> <li>Node functionality tests for all types</li> <li>Complete end-to-end workflow tests</li> <li>Error handling tests</li> <li>Performance tests</li> <li>Browser compatibility checklist</li> <li>Test data requirements</li> <li>Results tracking table</li> </ul>"},{"location":"development/implementation-summary.html#node-configuration-details","title":"Node Configuration Details","text":""},{"location":"development/implementation-summary.html#nodes-with-configuration-ui","title":"Nodes with Configuration UI","text":"Node Type Configuration Fields Description Upload CSV File upload dialog Browse and upload CSV files UCI Dataset dataset_id (text) Enter UCI repository ID Feature Selection x_columns (list), y_column (text) Select features and target Quantum Model model_name (dropdown) Choose from 10 quantum models Classical Model model_name (dropdown) Choose from 8 classical models Optuna Config study_name, n_trials, db_name Configure optimization SHAP Analysis plot_types (checkboxes) Select bar, beeswarm, violin Export Model export_path (text) Specify save location Generate Report llm_provider (dropdown) Choose OpenAI/Anthropic/Google"},{"location":"development/implementation-summary.html#nodes-without-configuration-pass-through","title":"Nodes Without Configuration (Pass-through)","text":"<ul> <li>Data Preview</li> <li>Train/Test Split</li> <li>Standard Scaler</li> <li>Label Encoding</li> <li>Run Optimization</li> <li>Confusion Matrix</li> <li>Feature Importance</li> </ul>"},{"location":"development/implementation-summary.html#how-to-use","title":"How to Use","text":""},{"location":"development/implementation-summary.html#starting-the-application","title":"Starting the Application","text":"<pre><code>cd /home/user/quoptuna\ndocker compose up --build\n</code></pre> <p>Access: - Frontend: http://localhost:5173 - Backend: http://localhost:8000 - API Docs: http://localhost:8000/docs</p>"},{"location":"development/implementation-summary.html#creating-a-workflow","title":"Creating a Workflow","text":"<ol> <li>Navigate to Workflow Builder from sidebar</li> <li>Drag nodes from the palette onto the canvas</li> <li>Connect nodes by dragging edges between them</li> <li>Click on a node to open configuration panel</li> <li>Configure node parameters:</li> <li>For Upload CSV: Click \"Upload CSV File\" button</li> <li>For UCI Dataset: Enter dataset ID (e.g., 53)</li> <li>For Feature Selection: Enter column names</li> <li>For Models: Select from dropdown</li> <li>For Optuna: Set trials (start with 10-20 for testing)</li> <li>Click \"Save Configuration\" to apply changes</li> <li>Click \"Run\" to execute the workflow</li> <li>Monitor execution progress in status bar</li> <li>View results in alert dialog and browser console</li> </ol>"},{"location":"development/implementation-summary.html#example-quick-test-workflow","title":"Example: Quick Test Workflow","text":"<pre><code>UCI Dataset (ID: 53)\n  \u2193\nFeature Selection (X: sepal_length,sepal_width,petal_length,petal_width | y: species)\n  \u2193\nTrain/Test Split\n  \u2193\nQuantum Model (DataReuploading)\n  \u2193\nOptuna Config (trials: 10)\n  \u2193\nRun Optimization\n</code></pre> <p>Expected result: Optimization completes in ~5-10 minutes with best accuracy reported.</p>"},{"location":"development/implementation-summary.html#testing-checklist_1","title":"Testing Checklist","text":""},{"location":"development/implementation-summary.html#quick-smoke-tests","title":"Quick Smoke Tests","text":"<ol> <li> <p>Backend Health:    </p><pre><code>curl http://localhost:8000/docs\n</code></pre>    \u2705 Should see Swagger UI<p></p> </li> <li> <p>File Upload:    </p><pre><code>curl -X POST http://localhost:8000/api/v1/data/upload \\\n  -F \"file=@test.csv\"\n</code></pre>    \u2705 Should return 201 with file metadata<p></p> </li> <li> <p>Frontend Loading:</p> </li> <li>Open http://localhost:5173</li> <li>\u2705 Should see dashboard</li> <li>Navigate to Workflow Builder</li> <li> <p>\u2705 Should see node palette and canvas</p> </li> <li> <p>Node Configuration:</p> </li> <li>Drag UCI Dataset node onto canvas</li> <li>Click the node</li> <li>\u2705 Configuration panel opens on right</li> <li>Enter dataset ID: 53</li> <li>Click \"Save Configuration\"</li> <li> <p>\u2705 Panel closes, config saved</p> </li> <li> <p>Workflow Execution:</p> </li> <li>Build simple workflow (see example above)</li> <li>Click \"Run\"</li> <li>\u2705 Status shows \"Starting workflow execution...\"</li> <li>\u2705 Status updates to \"Running...\"</li> <li>\u2705 Eventually shows \"completed successfully\"</li> </ol>"},{"location":"development/implementation-summary.html#full-test-suite","title":"Full Test Suite","text":"<p>See <code>TESTING_CHECKLIST.md</code> for comprehensive test plan covering: - All 16 node types - Error handling - Edge cases - Performance scenarios</p>"},{"location":"development/implementation-summary.html#architecture","title":"Architecture","text":""},{"location":"development/implementation-summary.html#data-flow","title":"Data Flow","text":"<pre><code>User Action (UI)\n  \u2193\nWorkflowBuilder Component\n  \u2193\nAPI Client (frontend/src/lib/api.ts)\n  \u2193\nFastAPI Backend (backend/app/api/v1/)\n  \u2193\nWorkflowExecutor Service (backend/app/services/workflow_service.py)\n  \u2193\nQuOptuna Services (Optimizer, DataPreparation, XAI)\n  \u2193\nResults returned through polling\n</code></pre>"},{"location":"development/implementation-summary.html#node-configuration-flow","title":"Node Configuration Flow","text":"<pre><code>User clicks node\n  \u2193\nWorkflowBuilder.handleNodeClick()\n  \u2193\nNodeConfigPanel opens with node data\n  \u2193\nUser modifies configuration\n  \u2193\nUser clicks \"Save Configuration\"\n  \u2193\nhandleConfigSave() updates node.data.config\n  \u2193\nConfiguration available for execution\n</code></pre>"},{"location":"development/implementation-summary.html#file-upload-flow","title":"File Upload Flow","text":"<pre><code>User selects CSV file\n  \u2193\nNodeConfigPanel.handleFileUpload()\n  \u2193\nuploadDataset() API call\n  \u2193\nBackend saves file to /uploads\n  \u2193\nBackend validates CSV and extracts metadata\n  \u2193\nFile path and metadata returned\n  \u2193\nStored in node config\n  \u2193\nUsed during workflow execution\n</code></pre>"},{"location":"development/implementation-summary.html#key-files-modifiedcreated","title":"Key Files Modified/Created","text":""},{"location":"development/implementation-summary.html#backend","title":"Backend","text":"<ul> <li><code>backend/Dockerfile</code> - Added build dependencies</li> <li><code>backend/pyproject.toml</code> - Fixed numpy version</li> <li><code>backend/app/api/v1/data.py</code> - Implemented upload endpoint</li> </ul>"},{"location":"development/implementation-summary.html#frontend","title":"Frontend","text":"<ul> <li><code>frontend/src/components/workflow/NodeConfigPanel.tsx</code> - NEW Configuration UI</li> <li><code>frontend/src/pages/WorkflowBuilder.tsx</code> - Integrated config panel</li> <li><code>frontend/src/lib/api.ts</code> - Added upload function</li> </ul>"},{"location":"development/implementation-summary.html#documentation","title":"Documentation","text":"<ul> <li><code>frontend/WORKFLOW_BUILDER_GUIDE.md</code> - NEW User guide</li> <li><code>TESTING_CHECKLIST.md</code> - NEW Test plan</li> <li><code>IMPLEMENTATION_SUMMARY.md</code> - NEW This file</li> </ul>"},{"location":"development/implementation-summary.html#remaining-work-future-enhancements","title":"Remaining Work (Future Enhancements)","text":""},{"location":"development/implementation-summary.html#not-yet-implemented","title":"Not Yet Implemented","text":"<ol> <li>Workflow Persistence: Currently workflows are lost on page refresh</li> <li>Need to add database storage</li> <li> <p>Save/Load functionality exists but stores in memory only</p> </li> <li> <p>Real-time Progress: Currently polls every 2 seconds</p> </li> <li>Could add WebSocket for live updates</li> <li> <p>Show per-node execution status</p> </li> <li> <p>Result Visualization: Results shown in alert</p> </li> <li>Should add dedicated results panel</li> <li>Display SHAP plots, confusion matrices in UI</li> <li> <p>Download reports and exports</p> </li> <li> <p>Advanced Features:</p> </li> <li>Workflow templates</li> <li>Node search in palette</li> <li>Undo/Redo</li> <li>Workflow validation before execution</li> <li> <p>Parameter suggestions based on data</p> </li> <li> <p>Partial Implementations:</p> </li> <li>Export Model: Returns placeholder, doesn't actually save model</li> <li>Generate Report: Requires LLM API keys, not tested</li> <li>Confusion Matrix: Returns placeholder</li> </ol>"},{"location":"development/implementation-summary.html#known-limitations","title":"Known Limitations","text":"<ol> <li>Configuration UI: Some advanced parameters not exposed in UI</li> <li>Test size for train/test split (defaults to 0.2)</li> <li>Random state for reproducibility (defaults to 42)</li> <li> <p>Specific model hyperparameters</p> </li> <li> <p>Validation: No client-side validation before execution</p> </li> <li>Can create invalid workflows (e.g., missing required config)</li> <li> <p>Errors only discovered during execution</p> </li> <li> <p>State Management: Node config updates may not trigger re-render</p> </li> <li> <p>Workaround: Panel shows current state on open</p> </li> <li> <p>File Management: No cleanup of uploaded files</p> </li> <li>Files accumulate in /uploads directory</li> <li>Should add file lifecycle management</li> </ol>"},{"location":"development/implementation-summary.html#performance-considerations","title":"Performance Considerations","text":""},{"location":"development/implementation-summary.html#optimization-timing","title":"Optimization Timing","text":"<ul> <li>Classical models: 10 trials \u2248 1-2 minutes</li> <li>Quantum models: 10 trials \u2248 5-10 minutes</li> <li>SHAP analysis: +2-5 minutes for quantum models</li> </ul> <p>Recommendation: Start with 10-20 trials for testing, increase to 100+ for production.</p>"},{"location":"development/implementation-summary.html#data-size-limits","title":"Data Size Limits","text":"<ul> <li>Upload limit: 100 MB (configured in backend)</li> <li>Large datasets (&gt;10k rows) may slow optimization</li> <li>SHAP computation scales with data size</li> </ul>"},{"location":"development/implementation-summary.html#security-considerations","title":"Security Considerations","text":""},{"location":"development/implementation-summary.html#file-upload","title":"File Upload","text":"<ul> <li>Only .csv files accepted</li> <li>Files stored with random UUID names</li> <li>No path traversal vulnerabilities</li> <li>File size limited to 100 MB</li> </ul>"},{"location":"development/implementation-summary.html#areas-for-improvement","title":"Areas for Improvement","text":"<ul> <li>Add virus scanning for uploaded files</li> <li>Implement user authentication</li> <li>Add rate limiting on upload endpoint</li> <li>Sanitize file content before processing</li> </ul>"},{"location":"development/implementation-summary.html#deployment-readiness","title":"Deployment Readiness","text":""},{"location":"development/implementation-summary.html#ready-for-testing","title":"Ready for Testing","text":"<p>\u2705 Docker Compose setup works \u2705 All core features implemented \u2705 Documentation complete \u2705 Error handling in place</p>"},{"location":"development/implementation-summary.html#before-production","title":"Before Production","text":"<p>\u26a0\ufe0f Add database for workflow persistence \u26a0\ufe0f Implement user authentication \u26a0\ufe0f Add monitoring and logging \u26a0\ufe0f Set up proper secrets management \u26a0\ufe0f Configure CORS for production domains \u26a0\ufe0f Add rate limiting and resource quotas \u26a0\ufe0f Implement file cleanup jobs \u26a0\ufe0f Add comprehensive error tracking</p>"},{"location":"development/implementation-summary.html#success-metrics","title":"Success Metrics","text":""},{"location":"development/implementation-summary.html#what-works-now","title":"What Works Now","text":"<p>\u2705 Visual workflow creation with drag-and-drop \u2705 16 different node types, all executable \u2705 Node configuration through UI \u2705 File upload with validation \u2705 Workflow execution with status polling \u2705 Integration with QuOptuna services \u2705 Topological sorting for execution order \u2705 Error handling and user feedback \u2705 SHAP analysis and explainability \u2705 Model optimization with Optuna</p>"},{"location":"development/implementation-summary.html#user-experience","title":"User Experience","text":"<p>\u2705 Intuitive node palette organization \u2705 Click-to-configure workflow \u2705 Visual status indicators during execution \u2705 Comprehensive documentation and examples \u2705 Clear error messages</p>"},{"location":"development/implementation-summary.html#conclusion","title":"Conclusion","text":"<p>The QuOptuna Workflow Builder is now fully functional with: - Complete node configuration UI - File upload capability - End-to-end workflow execution - Comprehensive documentation - Systematic test plan</p> <p>Next Steps: 1. Run through the testing checklist 2. Fix any issues discovered during testing 3. Test with real datasets and use cases 4. Gather user feedback 5. Implement persistence and advanced features 6. Prepare for production deployment</p> <p>All code has been committed and pushed to branch: <code>claude/fix-missing-backend-data-module-01NxSyL2aeak91CokAegVzUi</code></p> <p>Ready for user testing and feedback!</p>"},{"location":"development/testing-checklist.html","title":"QuOptuna Workflow Builder Testing Checklist","text":""},{"location":"development/testing-checklist.html#quoptuna-workflow-builder-testing-checklist","title":"QuOptuna Workflow Builder Testing Checklist","text":""},{"location":"development/testing-checklist.html#test-environment-setup","title":"Test Environment Setup","text":""},{"location":"development/testing-checklist.html#prerequisites","title":"Prerequisites","text":"<ul> <li>[ ] Docker and Docker Compose installed</li> <li>[ ] Ports 5173 and 8000 available</li> <li>[ ] Sample CSV file prepared for upload testing</li> </ul>"},{"location":"development/testing-checklist.html#start-services","title":"Start Services","text":"<pre><code>cd /home/user/quoptuna\ndocker compose up --build\n</code></pre> <ul> <li>[ ] Backend starts without errors</li> <li>[ ] Frontend starts without errors</li> <li>[ ] Can access http://localhost:5173</li> <li>[ ] Can access http://localhost:8000/docs</li> </ul>"},{"location":"development/testing-checklist.html#backend-api-tests","title":"Backend API Tests","text":""},{"location":"development/testing-checklist.html#data-upload-endpoint","title":"Data Upload Endpoint","text":"<p>Test 1: Upload Valid CSV </p><pre><code>curl -X POST http://localhost:8000/api/v1/data/upload \\\n  -F \"file=@test_data.csv\"\n</code></pre> - [ ] Returns 201 status code - [ ] Returns file_id, file_path, rows, and columns - [ ] File is saved in ./uploads directory - [ ] Can read uploaded file<p></p> <p>Test 2: Upload Invalid File </p><pre><code>curl -X POST http://localhost:8000/api/v1/data/upload \\\n  -F \"file=@test_data.txt\"\n</code></pre> - [ ] Returns 400 error - [ ] Error message: \"Only CSV files are allowed\"<p></p> <p>Test 3: Upload Malformed CSV </p><pre><code># Create a CSV with invalid format\ncurl -X POST http://localhost:8000/api/v1/data/upload \\\n  -F \"file=@malformed.csv\"\n</code></pre> - [ ] Returns 400 error - [ ] Error message indicates CSV parsing failure - [ ] No file is left in uploads directory<p></p>"},{"location":"development/testing-checklist.html#workflow-execution-endpoint","title":"Workflow Execution Endpoint","text":"<p>Test 4: Execute Simple Workflow </p><pre><code>curl -X POST http://localhost:8000/api/v1/workflows/execute \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"Test Workflow\",\n    \"nodes\": [\n      {\n        \"id\": \"node-1\",\n        \"data\": {\n          \"type\": \"data-uci\",\n          \"label\": \"UCI Dataset\",\n          \"config\": {\"dataset_id\": \"53\"}\n        }\n      }\n    ],\n    \"edges\": []\n  }'\n</code></pre> - [ ] Returns execution_id - [ ] Status is \"pending\" - [ ] Can poll execution status<p></p> <p>Test 5: Get Execution Status </p><pre><code>curl http://localhost:8000/api/v1/workflows/executions/{execution_id}\n</code></pre> - [ ] Returns execution details - [ ] Status transitions: pending \u2192 running \u2192 completed - [ ] Result contains workflow output<p></p>"},{"location":"development/testing-checklist.html#frontend-ui-tests","title":"Frontend UI Tests","text":""},{"location":"development/testing-checklist.html#navigation-and-layout","title":"Navigation and Layout","text":"<ul> <li>[ ] Sidebar menu is visible</li> <li>[ ] Can navigate to Dashboard</li> <li>[ ] Can navigate to Workflow Builder</li> <li>[ ] Can navigate to Data Explorer</li> <li>[ ] Can navigate to Models</li> <li>[ ] Can navigate to Analytics</li> <li>[ ] Can navigate to Settings</li> <li>[ ] Current page is highlighted in sidebar</li> </ul>"},{"location":"development/testing-checklist.html#node-palette","title":"Node Palette","text":"<ul> <li>[ ] Node Palette is visible on left side</li> <li>[ ] All 6 categories are displayed:</li> <li>[ ] Data (4 nodes)</li> <li>[ ] Preprocessing (3 nodes)</li> <li>[ ] Models (2 nodes)</li> <li>[ ] Optimization (2 nodes)</li> <li>[ ] Analysis (3 nodes)</li> <li>[ ] Output (2 nodes)</li> <li>[ ] Each node shows label and description</li> <li>[ ] Nodes are draggable</li> <li>[ ] Click on node also adds it to canvas</li> </ul>"},{"location":"development/testing-checklist.html#canvas-operations","title":"Canvas Operations","text":"<p>Test 6: Add Nodes - [ ] Drag \"Upload CSV\" node onto canvas - [ ] Node appears at drop location - [ ] Node has unique ID - [ ] Node displays correct label - [ ] Can drag multiple nodes - [ ] Each node is independently selectable</p> <p>Test 7: Connect Nodes - [ ] Add \"UCI Dataset\" node - [ ] Add \"Data Preview\" node - [ ] Drag from UCI Dataset output to Data Preview input - [ ] Edge is created and visible - [ ] Edge has arrow showing direction - [ ] Edge connects correct handles</p> <p>Test 8: Delete Elements - [ ] Select a node - [ ] Press Delete key (or Backspace) - [ ] Node is removed from canvas - [ ] Connected edges are also removed - [ ] Select an edge - [ ] Press Delete key - [ ] Edge is removed</p> <p>Test 9: Canvas Controls - [ ] Zoom in with mouse wheel - [ ] Zoom out with mouse wheel - [ ] Pan by dragging on empty canvas - [ ] Fit view button centers all nodes - [ ] Mini-map shows overview (if implemented)</p>"},{"location":"development/testing-checklist.html#toolbar-actions","title":"Toolbar Actions","text":"<p>Test 10: Clear Canvas - [ ] Add several nodes to canvas - [ ] Click \"Clear\" button - [ ] Confirmation dialog appears - [ ] All nodes and edges are removed</p> <p>Test 11: Save Workflow (if implemented) - [ ] Create a workflow - [ ] Click \"Save\" button - [ ] Workflow is saved - [ ] Can reload saved workflow</p> <p>Test 12: Load Workflow (if implemented) - [ ] Click \"Load\" button - [ ] Saved workflows are listed - [ ] Select a workflow - [ ] Canvas is populated with nodes and edges</p>"},{"location":"development/testing-checklist.html#node-functionality-tests","title":"Node Functionality Tests","text":""},{"location":"development/testing-checklist.html#data-nodes","title":"Data Nodes","text":"<p>Test 13: UCI Dataset Node 1. Add UCI Dataset node 2. Configure with dataset_id: 53 (Iris) 3. Connect to Data Preview node 4. Run workflow - [ ] Dataset is fetched successfully - [ ] Data Preview shows 150 rows, 5 columns - [ ] Execution completes without errors</p> <p>Test 14: Upload CSV Node 1. Add Upload CSV node 2. Upload a test CSV file 3. Connect to Data Preview node 4. Run workflow - [ ] File uploads successfully - [ ] File path is stored in node config - [ ] Data Preview shows correct dimensions - [ ] Execution completes without errors</p> <p>Test 15: Data Preview Node 1. Connect data source to Data Preview 2. Run workflow - [ ] Preview shows shape (rows, columns) - [ ] Shows data types for each column - [ ] Shows statistical summary - [ ] Shows first few rows</p> <p>Test 16: Feature Selection Node 1. Connect dataset to Feature Selection 2. Configure x_columns and y_column 3. Connect to Train/Test Split 4. Run workflow - [ ] Features are correctly separated - [ ] X contains specified columns - [ ] y contains target column - [ ] Execution continues to next node</p>"},{"location":"development/testing-checklist.html#preprocessing-nodes","title":"Preprocessing Nodes","text":"<p>Test 17: Train/Test Split Node 1. Connect Feature Selection to Train/Test Split 2. Run workflow - [ ] Data is split into train and test sets - [ ] x_train, x_test, y_train, y_test are created - [ ] Split ratio is approximately 80/20 - [ ] Data is automatically scaled</p> <p>Test 18: Scaler and Encoding Nodes 1. Add Scaler and Label Encoding nodes 2. Connect in preprocessing pipeline 3. Run workflow - [ ] Nodes execute successfully - [ ] Data passes through (handled by DataPreparation) - [ ] No errors occur</p>"},{"location":"development/testing-checklist.html#model-and-optimization-nodes","title":"Model and Optimization Nodes","text":"<p>Test 19: Quantum Model Node 1. Add Quantum Model node after preprocessing 2. Configure model_name: \"DataReuploading\" 3. Connect to Optuna Config 4. Run workflow - [ ] Model configuration is stored - [ ] model_type is \"quantum\" - [ ] Execution continues</p> <p>Test 20: Classical Model Node 1. Add Classical Model node after preprocessing 2. Configure model_name: \"RandomForest\" 3. Connect to Optuna Config 4. Run workflow - [ ] Model configuration is stored - [ ] model_type is \"classical\" - [ ] Execution continues</p> <p>Test 21: Optuna Config Node 1. Add Optuna Config node 2. Configure:    - study_name: \"test_study\"    - n_trials: 10    - db_name: \"test.db\" 3. Connect to Run Optimization 4. Run workflow - [ ] Configuration is merged with model config - [ ] Parameters are passed to optimizer - [ ] Execution continues</p> <p>Test 22: Run Optimization Node 1. Complete workflow: Data \u2192 Features \u2192 Split \u2192 Model \u2192 Optuna \u2192 Optimization 2. Configure for 10 trials (for speed) 3. Run workflow - [ ] Optimization starts - [ ] Status updates show \"running\" - [ ] Trials are executed (check backend logs) - [ ] Best parameters are found - [ ] best_value, best_params, best_trial_number returned - [ ] SQLite database is created - [ ] Execution completes successfully</p>"},{"location":"development/testing-checklist.html#analysis-nodes","title":"Analysis Nodes","text":"<p>Test 23: SHAP Analysis Node 1. Connect Optimization results to SHAP Analysis 2. Configure plot_types: [\"bar\", \"beeswarm\"] 3. Run workflow - [ ] SHAP analysis executes - [ ] Plots are generated (check result) - [ ] Feature importance is calculated - [ ] No errors occur</p> <p>Test 24: Feature Importance Node 1. Connect SHAP results to Feature Importance 2. Run workflow - [ ] Feature rankings are displayed - [ ] Uses SHAP data if available - [ ] Execution completes</p>"},{"location":"development/testing-checklist.html#complete-workflow-tests","title":"Complete Workflow Tests","text":""},{"location":"development/testing-checklist.html#test-25-end-to-end-classification-workflow","title":"Test 25: End-to-End Classification Workflow","text":"<p>Workflow Structure: </p><pre><code>UCI Dataset (Iris)\n  \u2193\nSelect Features (all but target \u2192 species)\n  \u2193\nTrain/Test Split\n  \u2193\nQuantum Model (DataReuploading)\n  \u2193\nOptuna Config (20 trials)\n  \u2193\nRun Optimization\n  \u2193\nSHAP Analysis\n</code></pre><p></p> <p>Execution: 1. Build workflow by connecting nodes 2. Click Run 3. Monitor execution progress</p> <p>Verification: - [ ] All nodes execute in correct order - [ ] Status updates show progress - [ ] Dataset is loaded (150 rows) - [ ] Features are selected and split - [ ] Optimization runs for 20 trials - [ ] SHAP analysis completes - [ ] Final result contains all node outputs - [ ] Execution time is reasonable (&lt; 10 minutes for 20 trials) - [ ] All nodes show green (completed) status</p>"},{"location":"development/testing-checklist.html#test-26-classical-model-comparison","title":"Test 26: Classical Model Comparison","text":"<p>Create two parallel paths: 1. Path 1: Quantum Model (DataReuploading) 2. Path 2: Classical Model (RandomForest) 3. Both paths from same data source 4. Both with 20 trials optimization</p> <p>Execution: - [ ] Workflow executes both paths - [ ] Results can be compared - [ ] No conflicts between parallel executions - [ ] Both optimizations complete successfully</p>"},{"location":"development/testing-checklist.html#test-27-upload-custom-dataset","title":"Test 27: Upload Custom Dataset","text":"<p>Workflow: </p><pre><code>Upload CSV\n  \u2193\nData Preview\n  \u2193\nSelect Features\n  \u2193\nTrain/Test Split\n  \u2193\nClassical Model (SVC)\n  \u2193\nOptuna Config (10 trials)\n  \u2193\nRun Optimization\n</code></pre><p></p> <p>Steps: 1. Prepare a custom classification dataset CSV 2. Upload via Upload CSV node 3. Preview to verify columns 4. Select appropriate features and target 5. Run complete workflow</p> <p>Verification: - [ ] File uploads successfully - [ ] Preview shows correct data - [ ] Features are selected properly - [ ] Optimization runs with custom data - [ ] Results are meaningful for the dataset</p>"},{"location":"development/testing-checklist.html#error-handling-tests","title":"Error Handling Tests","text":"<p>Test 28: Circular Dependency 1. Create nodes: A \u2192 B \u2192 C \u2192 A 2. Run workflow - [ ] Error: \"Workflow contains cycles\" - [ ] Execution stops - [ ] Clear error message displayed</p> <p>Test 29: Missing Input 1. Add Optimization node without connecting data 2. Run workflow - [ ] Error: \"No input configuration for optimization\" - [ ] Node status shows failed - [ ] Error message is clear</p> <p>Test 30: Invalid Configuration 1. Feature Selection without specifying columns 2. Run workflow - [ ] Error: \"Must specify x_columns and y_column\" - [ ] Execution fails at that node - [ ] Subsequent nodes don't execute</p>"},{"location":"development/testing-checklist.html#performance-tests","title":"Performance Tests","text":"<p>Test 31: Large Dataset 1. Upload CSV with 10,000+ rows 2. Run optimization with 50 trials - [ ] Execution doesn't timeout - [ ] Progress updates continue - [ ] Memory usage is acceptable - [ ] Execution completes (may take a while)</p> <p>Test 32: Multiple Simultaneous Workflows 1. Start workflow execution 2. Immediately start another workflow - [ ] Both execute in background - [ ] No conflicts between executions - [ ] Both complete successfully - [ ] Execution IDs are unique</p> <p>Test 33: Rapid Node Addition 1. Quickly drag 20+ nodes onto canvas 2. Connect them in sequence 3. Run workflow - [ ] UI remains responsive - [ ] All nodes are tracked - [ ] Execution proceeds correctly</p>"},{"location":"development/testing-checklist.html#integration-tests","title":"Integration Tests","text":"<p>Test 34: Backend Restart During Execution 1. Start a long-running optimization 2. Restart backend container 3. Check execution status - [ ] Execution state is handled gracefully - [ ] Error message if execution is lost - [ ] New executions work after restart</p> <p>Test 35: Frontend Refresh During Execution 1. Start workflow execution 2. Refresh browser page 3. Check execution status - [ ] Execution continues in backend - [ ] Can query status by execution_id (if stored) - [ ] UI state can be recovered</p>"},{"location":"development/testing-checklist.html#browser-compatibility","title":"Browser Compatibility","text":"<p>Test in different browsers: - [ ] Chrome/Chromium - All features work - [ ] Firefox - All features work - [ ] Safari - All features work - [ ] Edge - All features work</p>"},{"location":"development/testing-checklist.html#accessibility-tests","title":"Accessibility Tests","text":"<ul> <li>[ ] Keyboard navigation works</li> <li>[ ] Tab order is logical</li> <li>[ ] Can operate without mouse</li> <li>[ ] Screen reader compatibility (basic)</li> <li>[ ] Sufficient color contrast</li> <li>[ ] Focus indicators are visible</li> </ul>"},{"location":"development/testing-checklist.html#documentation-tests","title":"Documentation Tests","text":"<p>Test 36: User Guide - [ ] Guide is accurate for all features - [ ] Examples can be reproduced - [ ] API documentation matches implementation - [ ] Troubleshooting section is helpful</p> <p>Test 37: API Documentation - [ ] Access http://localhost:8000/docs - [ ] All endpoints are documented - [ ] Can test endpoints from Swagger UI - [ ] Request/response schemas are correct</p>"},{"location":"development/testing-checklist.html#known-issues-limitations","title":"Known Issues / Limitations","text":"<p>Document any issues found during testing:</p> <ol> <li>Node Configuration UI: Not yet implemented - config must be set programmatically</li> <li>Export Model: Returns placeholder - actual export not implemented</li> <li>Generate Report: Requires LLM API keys - returns placeholder</li> <li>Confusion Matrix: Not fully implemented - returns placeholder</li> <li>Workflow Persistence: No database - workflows lost on reload</li> <li>Parallel Execution: Workflows run in sequence, not true parallel processing</li> </ol>"},{"location":"development/testing-checklist.html#test-data-requirements","title":"Test Data Requirements","text":""},{"location":"development/testing-checklist.html#sample-csvs-needed","title":"Sample CSVs Needed","text":"<p>1. iris.csv - Small classification dataset </p><pre><code>sepal_length,sepal_width,petal_length,petal_width,species\n5.1,3.5,1.4,0.2,setosa\n4.9,3.0,1.4,0.2,setosa\n...\n</code></pre><p></p> <p>2. binary_classification.csv - Binary target </p><pre><code>feature1,feature2,feature3,target\n1.2,3.4,5.6,0\n2.3,4.5,6.7,1\n...\n</code></pre><p></p> <p>3. multi_feature.csv - Many features (10+) </p><pre><code>f1,f2,f3,f4,f5,f6,f7,f8,f9,f10,target\n...\n</code></pre><p></p>"},{"location":"development/testing-checklist.html#test-results-summary","title":"Test Results Summary","text":"Test # Feature Status Notes 1 Upload Valid CSV \u23f3 Pending test 2 Upload Invalid File \u23f3 Pending test ... ... ... ... <p>Legend: - \u2705 Passed - \u274c Failed - \u23f3 Pending - \u26a0\ufe0f Partial</p>"},{"location":"development/testing-checklist.html#sign-off","title":"Sign-off","text":"<ul> <li>Tested By: ___</li> <li>Date: ___</li> <li>Environment: Docker Compose / Local</li> <li>Overall Status: \u23f3 Pending / \u2705 Passed / \u274c Failed</li> <li>Ready for Production: Yes / No</li> </ul>"},{"location":"development/testing-checklist.html#next-steps","title":"Next Steps","text":"<p>After completing all tests:</p> <ol> <li>Fix any failing tests</li> <li>Document known limitations</li> <li>Create user acceptance tests</li> <li>Plan production deployment</li> <li>Set up monitoring and logging</li> <li>Create backup and recovery procedures</li> </ol>"},{"location":"development/workflow-testing.html","title":"Workflow Testing Guide","text":""},{"location":"development/workflow-testing.html#workflow-testing-guide","title":"Workflow Testing Guide","text":""},{"location":"development/workflow-testing.html#overview","title":"Overview","text":"<p>This document describes the current state of the optimizer workflow, known issues, and how to test it properly.</p>"},{"location":"development/workflow-testing.html#recent-fixes","title":"Recent Fixes","text":""},{"location":"development/workflow-testing.html#1-data-format-handling-2025-11-16","title":"1. Data Format Handling (2025-11-16)","text":"<p>Fixed critical data format mismatches between optimization and SHAP analysis:</p> <ol> <li>Optimizer expects numpy arrays for <code>train_x</code>, <code>train_y</code>, <code>test_x</code>, <code>test_y</code></li> <li>XAI (SHAP) expects pandas DataFrames for the same data</li> <li>Model.fit() requires numpy arrays</li> </ol> <p>Solution: The workflow now: - Stores DataFrames from DataPreparation - Converts to numpy arrays when passing to Optimizer - Retains DataFrames in optimization results - Converts to numpy arrays only for model.fit() in SHAP analysis - Passes DataFrames to XAI constructor</p>"},{"location":"development/workflow-testing.html#2-pennylane-quantum-device-deprecation-2025-11-16-fixed","title":"2. PennyLane Quantum Device Deprecation (2025-11-16) \u2705 FIXED","text":"<p>Previous Error: <code>pennylane.exceptions.DeviceError: Device default.qubit.jax does not exist</code></p> <p>Fix: Updated all 14 quantum model classes to use <code>default.qubit</code> instead of deprecated <code>default.qubit.jax</code>: - DataReuploadingClassifier &amp; DataReuploadingClassifierSeparable - DressedQuantumCircuitClassifier &amp; DressedQuantumCircuitClassifierSeparable - CircuitCentricClassifier - ProjectedQuantumKernel - QuantumKitchenSinks - QuantumMetricLearner - IQPVariationalClassifier &amp; IQPKernelClassifier - TreeTensorClassifier - SeparableVariationalClassifier &amp; SeparableKernelClassifier - QuanvolutionalNeuralNetwork, VanillaQNN, WEINet</p> <p>Result: All quantum models now work correctly during optimization! \ud83c\udf89</p>"},{"location":"development/workflow-testing.html#3-binary-label-encoding-2025-11-16-fixed","title":"3. Binary Label Encoding (2025-11-16) \u2705 FIXED","text":"<p>Previous Error: <code>IndexError: boolean index did not match indexed array</code></p> <p>Cause: Quantum models expect binary classification labels as -1 and 1, but UCI datasets often have labels as 0/1 or other values (e.g., \"g\"/\"b\" for Ionosphere).</p> <p>Fix: Added automatic label encoding step in workflow: - Detects binary classification (2 unique classes) - Automatically maps first class \u2192 -1, second class \u2192 1 - Logs the mapping for transparency - Works with any binary dataset (0/1, g/b, yes/no, etc.)</p> <p>Example: Breast Cancer dataset with labels [0, 1] is now automatically mapped to [-1, 1]</p> <p>Result: All binary classification datasets now work with quantum models! \ud83c\udf89</p>"},{"location":"development/workflow-testing.html#known-issues","title":"Known Issues","text":""},{"location":"development/workflow-testing.html#1-sklearn-parameter-validation","title":"1. sklearn Parameter Validation","text":"<p>Error: <code>The 'learning_rate' parameter of MLPClassifier must be a str among {'constant', 'adaptive', 'invscaling'}</code></p> <p>Cause: Optuna search space suggests numeric learning rates, but MLPClassifier expects string values</p> <p>Impact: Some MLPClassifier trials fail with 0.0 value</p> <p>Workaround: Ignored - Optuna will try other parameters</p>"},{"location":"development/workflow-testing.html#testing-the-workflow","title":"Testing the Workflow","text":""},{"location":"development/workflow-testing.html#recommended-test-process","title":"Recommended Test Process","text":"<ol> <li>Select UCI Dataset:</li> <li>Choose a simple dataset (Wine, Iris recommended)</li> <li>Use small number of features (&lt; 15)</li> <li> <p>Binary classification preferred</p> </li> <li> <p>Configure Optimization:</p> </li> <li>Study name: <code>test_wine</code> or similar</li> <li>Database name: <code>test_wine</code> or similar</li> <li>Num trials: 10-100 (quantum models now work!)</li> <li>Start with 10-20 trials for quick testing</li> <li> <p>Use 50-100 trials for better results</p> </li> <li> <p>Run Optimization:</p> </li> <li>Both quantum and classical models now work</li> <li>Some MLPClassifier trials may fail (parameter validation)</li> <li>Should complete successfully with mixed model trials</li> <li> <p>Expect 1-5 minutes for 10-20 trials</p> </li> <li> <p>Generate SHAP Analysis:</p> </li> <li>Works with both quantum and classical models</li> <li>Generates bar, beeswarm, and waterfall plots</li> <li>Returns feature importance rankings</li> </ol>"},{"location":"development/workflow-testing.html#example-working-configuration","title":"Example Working Configuration","text":"<pre><code>{\n  \"dataset\": {\n    \"id\": \"109\",\n    \"name\": \"Wine\",\n    \"source\": \"uci\"\n  },\n  \"features\": {\n    \"selectedFeatures\": [\"Alcohol\", \"Malic_acid\", \"Ash\", \"Alcalinity_of_ash\"],\n    \"targetColumn\": \"class\"\n  },\n  \"configuration\": {\n    \"studyName\": \"wine_test\",\n    \"databaseName\": \"wine_test\",\n    \"numTrials\": 10\n  }\n}\n</code></pre>"},{"location":"development/workflow-testing.html#reference-notebooks","title":"Reference Notebooks","text":"<p>The workflow implementation is based on these working examples:</p> <ol> <li><code>experiments/basic_dataset_test/test_shap.ipynb</code></li> <li>Shows complete workflow: load study \u2192 create model \u2192 fit \u2192 XAI \u2192 plots</li> <li>Demonstrates correct data format handling</li> <li> <p>Example with Statlog dataset</p> </li> <li> <p><code>experiments/basic_dataset_test/test_new_data_test.ipynb</code></p> </li> <li>Shows UCI dataset fetching and preparation</li> <li>Demonstrates DataPreparation usage</li> <li>Shows conversion to numpy arrays for Optimizer</li> </ol>"},{"location":"development/workflow-testing.html#workflow-steps-technical","title":"Workflow Steps (Technical)","text":""},{"location":"development/workflow-testing.html#1-dataset-selection","title":"1. Dataset Selection","text":"<pre><code>Frontend \u2192 POST /api/v1/data/uci/{dataset_id}\nBackend: fetch_ucirepo(id) \u2192 return columns\n</code></pre>"},{"location":"development/workflow-testing.html#2-feature-selection","title":"2. Feature Selection","text":"<pre><code>Frontend: User selects features + target\nState stored in workflowData.features\n</code></pre>"},{"location":"development/workflow-testing.html#3-optimization","title":"3. Optimization","text":"<pre><code>Frontend \u2192 POST /api/v1/optimize\nBackend:\n  1. Create workflow nodes (data, features, split, model, optuna, optimize)\n  2. Execute workflow in topological order:\n     - data-uci: Fetch dataset\n     - feature-selection: Select specified features\n     - train-test-split: Use DataPreparation \u2192 returns DataFrames\n     - optimize: Convert to numpy \u2192 create Optimizer \u2192 run trials\n  3. Return optimization results with DataFrames intact\n</code></pre>"},{"location":"development/workflow-testing.html#4-shap-analysis","title":"4. SHAP Analysis","text":"<pre><code>Frontend \u2192 POST /api/v1/analysis/shap\nBackend:\n  1. Load best trial from Optuna study\n  2. Recreate model with best parameters\n  3. Convert DataFrames to numpy \u2192 fit model\n  4. Create XAI with model + DataFrames\n  5. Generate SHAP plots\n  6. Calculate feature importance\n  7. Return plots + importance\n</code></pre>"},{"location":"development/workflow-testing.html#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Frontend UI   \u2502\n\u2502  (React/TS)     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u2193 API Calls\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  FastAPI        \u2502\n\u2502  /api/v1/       \u2502\n\u2502  - optimize     \u2502\n\u2502  - analysis     \u2502\n\u2502  - data         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 WorkflowService \u2502\n\u2502  Executes nodes \u2502\n\u2502  in order       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2193         \u2193         \u2193          \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Optuna  \u2502 \u2502 XAI \u2502 \u2502 Data \u2502 \u2502 Models \u2502\n\u2502 Optimizer\u2502 \u2502SHAP \u2502 \u2502 Prep \u2502 \u2502        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"development/workflow-testing.html#success-criteria","title":"Success Criteria","text":"<p>\u2705 Optimization completes with best_value &gt; 0 \u2705 Multiple trials succeed (both quantum and classical models) \u2705 SHAP analysis generates without 500 error \u2705 Feature importance is non-empty array \u2705 Plots are base64-encoded images</p>"},{"location":"development/workflow-testing.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"development/workflow-testing.html#all-trials-returning-00","title":"\"All trials returning 0.0\"","text":"<ul> <li>Check if data has NaN values</li> <li>Verify target column has correct labels</li> <li>Try different dataset</li> <li>Check backend logs for specific model errors</li> </ul>"},{"location":"development/workflow-testing.html#shap-500-error","title":"\"SHAP 500 error\"","text":"<ul> <li>\u2705 Should be fixed now (data format issue resolved)</li> <li>If still occurring, check backend logs for specific error</li> <li>Verify optimization completed successfully</li> <li>Confirm database file exists in <code>backend/db/</code></li> </ul>"},{"location":"development/workflow-testing.html#no-plots-generated","title":"\"No plots generated\"","text":"<ul> <li>Check if model has predict_proba method</li> <li>Verify XAI config subset_size is reasonable</li> <li>Check backend logs for plot generation errors</li> </ul>"},{"location":"development/workflow-testing.html#quantum-models-still-failing","title":"\"Quantum models still failing\"","text":"<ul> <li>\u2705 Should be fixed now (PennyLane device updated)</li> <li>If still occurring, verify PennyLane version is up to date</li> <li>Check that backend has correct dependencies installed</li> </ul>"},{"location":"development/workflow-testing.html#summary","title":"Summary","text":"<p>The optimizer workflow is now fully functional with both quantum and classical models! \ud83c\udf89</p> <p>What works: - \u2705 UCI dataset selection and loading - \u2705 Feature selection - \u2705 Optuna hyperparameter optimization with 15+ model types - \u2705 Both quantum models (DataReuploading, DressedQuantumCircuit, etc.) - \u2705 Classical models (SVC, MLP, Perceptron) - \u2705 SHAP explainability analysis - \u2705 Feature importance visualization - \u2705 Complete end-to-end workflow</p> <p>Minor issues remaining: - Some MLPClassifier trials may fail due to parameter validation (doesn't affect workflow)</p> <p>The workflow successfully demonstrates quantum machine learning optimization and explainable AI!</p>"},{"location":"examples/examples.html","title":"Examples","text":""},{"location":"examples/examples.html#examples","title":"Examples","text":""},{"location":"examples/examples.html#introduction","title":"Introduction","text":"<p>This page provides practical examples for common QuOptuna use cases. Each example includes complete, runnable code.</p>"},{"location":"examples/examples.html#table-of-contents","title":"Table of Contents","text":"<ol> <li>Basic Workflow</li> <li>UCI Dataset Analysis</li> <li>Custom Dataset</li> <li>SHAP Analysis</li> <li>Comparing Models</li> <li>Report Generation</li> <li>Batch Processing</li> </ol>"},{"location":"examples/examples.html#basic-workflow","title":"Basic Workflow","text":"<p>Complete workflow from data loading to SHAP analysis:</p> <pre><code>from quoptuna import DataPreparation, Optimizer, XAI\nfrom quoptuna.backend.models import create_model\nfrom quoptuna.backend.xai.xai import XAIConfig\nfrom quoptuna.backend.utils.data_utils.data import mock_csv_data\nimport pandas as pd\n\n# Load your data\ndf = pd.read_csv(\"your_data.csv\")\n\n# Ensure target is -1 and 1\ndf[\"target\"] = df[\"target\"].replace({0: -1, 1: 1})\n\n# Save to file\nfile_path = mock_csv_data(df, tmp_path=\"data\", file_name=\"my_data\")\n\n# Prepare data\ndata_prep = DataPreparation(\n    file_path=file_path,\n    x_cols=[col for col in df.columns if col != \"target\"],\n    y_col=\"target\"\n)\ndata_dict = data_prep.get_data(output_type=\"2\")\n\n# Convert to numpy\nfor key in [\"train_x\", \"test_x\", \"train_y\", \"test_y\"]:\n    data_dict[key] = data_dict[key].values\n\n# Optimize\noptimizer = Optimizer(db_name=\"my_experiment\", study_name=\"run_1\", data=data_dict)\nstudy, best_trials = optimizer.optimize(n_trials=50)\n\n# Train best model\nbest_model = create_model(**best_trials[0].params)\nbest_model.fit(data_dict[\"train_x\"], data_dict[\"train_y\"])\n\n# SHAP analysis\nxai = XAI(\n    model=best_model,\n    data=data_dict,\n    config=XAIConfig(use_proba=True, onsubset=True, subset_size=50)\n)\n\n# Generate plots\nbar_plot = xai.get_plot(\"bar\", max_display=10, class_index=1)\nprint(\"Analysis complete!\")\n</code></pre>"},{"location":"examples/examples.html#uci-dataset-analysis","title":"UCI Dataset Analysis","text":"<p>Working with UCI ML Repository datasets:</p> <pre><code>from ucimlrepo import fetch_ucirepo\nfrom quoptuna import DataPreparation, Optimizer\nfrom quoptuna.backend.utils.data_utils.data import mock_csv_data\nimport pandas as pd\n\n# Fetch dataset from UCI\ndataset = fetch_ucirepo(id=143)  # Statlog Credit Approval\n\n# Get metadata\nprint(\"Dataset:\", dataset.metadata[\"name\"])\nprint(\"Instances:\", dataset.metadata[\"num_instances\"])\nprint(\"Features:\", dataset.metadata[\"num_features\"])\n\n# Prepare data\nX = dataset.data.features\ny = dataset.data.targets\ndf = pd.concat([X, y], axis=1)\n\n# Transform target\ntarget_col = dataset.metadata[\"target_col\"][0]\ndf[\"target\"] = df[target_col].replace({0: -1, 1: 1})\ndf = df.drop(columns=[target_col])\ndf = df.dropna()\n\n# Save and prepare\nfile_path = mock_csv_data(df, tmp_path=\"data\", file_name=\"Statlog\")\n\ndata_prep = DataPreparation(\n    file_path=file_path,\n    x_cols=list(df.columns.difference([\"target\"])),\n    y_col=\"target\"\n)\ndata_dict = data_prep.get_data(output_type=\"2\")\n\n# Convert to numpy\nfor key in data_dict.keys():\n    data_dict[key] = data_dict[key].values\n\n# Run optimization\noptimizer = Optimizer(db_name=\"Statlog\", study_name=\"Statlog\", data=data_dict)\nstudy, best_trials = optimizer.optimize(n_trials=100)\n\n# Show results\nfor i, trial in enumerate(best_trials[:3]):\n    print(f\"\\n=== Best Trial {i+1} ===\")\n    print(f\"Model: {trial.params['model_type']}\")\n    print(f\"Quantum F1: {trial.user_attrs.get('Quantum_f1_score', 0):.4f}\")\n    print(f\"Classical F1: {trial.user_attrs.get('Classical_f1_score', 0):.4f}\")\n</code></pre>"},{"location":"examples/examples.html#custom-dataset","title":"Custom Dataset","text":"<p>Loading and processing a custom CSV file:</p> <pre><code>import pandas as pd\nfrom quoptuna import DataPreparation, Optimizer\nfrom quoptuna.backend.utils.data_utils.data import mock_csv_data\n\n# Load custom dataset\ndf = pd.read_csv(\"my_dataset.csv\")\n\n# Explore data\nprint(\"Shape:\", df.shape)\nprint(\"Columns:\", df.columns.tolist())\nprint(\"Missing values:\", df.isnull().sum().sum())\n\n# Handle missing values\ndf = df.dropna()\n\n# Transform target to -1 and 1\n# Example: If target is 'Yes'/'No'\ndf[\"target\"] = df[\"outcome\"].map({\"Yes\": 1, \"No\": -1})\n\n# Drop original target column\ndf = df.drop(columns=[\"outcome\"])\n\n# Select features\nfeature_cols = [\"age\", \"income\", \"credit_score\", \"debt_ratio\"]\n\n# Keep only selected columns\ndf = df[feature_cols + [\"target\"]]\n\n# Save processed data\nfile_path = mock_csv_data(df, tmp_path=\"data\", file_name=\"custom_dataset\")\n\n# Prepare for training\ndata_prep = DataPreparation(\n    file_path=file_path,\n    x_cols=feature_cols,\n    y_col=\"target\"\n)\ndata_dict = data_prep.get_data(output_type=\"2\")\n\n# Convert to numpy\nfor key in data_dict.keys():\n    data_dict[key] = data_dict[key].values\n\n# Optimize\noptimizer = Optimizer(\n    db_name=\"custom_experiment\",\n    study_name=\"trial_001\",\n    data=data_dict\n)\nstudy, best_trials = optimizer.optimize(n_trials=100)\n\nprint(f\"\\nFound {len(best_trials)} best trials\")\n</code></pre>"},{"location":"examples/examples.html#shap-analysis","title":"SHAP Analysis","text":"<p>Comprehensive SHAP analysis with all plot types:</p> <pre><code>from quoptuna import XAI\nfrom quoptuna.backend.models import create_model\nfrom quoptuna.backend.xai.xai import XAIConfig\nimport os\n\n# Assuming you have optimized model and data_dict from previous steps\n# Load best trial parameters\nbest_params = best_trials[0].params\n\n# Train model\nmodel = create_model(**best_params)\nmodel.fit(data_dict[\"train_x\"], data_dict[\"train_y\"])\n\n# Configure XAI\nconfig = XAIConfig(\n    use_proba=True,\n    onsubset=True,\n    subset_size=100\n)\n\n# Create XAI instance\nxai = XAI(model=model, data=data_dict, config=config)\n\n# Create output directory\nos.makedirs(\"outputs/shap_plots\", exist_ok=True)\n\n# Generate and save all plot types\nplot_types = [\"bar\", \"beeswarm\", \"violin\", \"heatmap\"]\n\nfor plot_type in plot_types:\n    print(f\"Generating {plot_type} plot...\")\n\n    plot = xai.get_plot(\n        plot_type,\n        max_display=10,\n        class_index=1,\n        save_config={\n            \"save_path\": \"outputs/shap_plots\",\n            \"save_name\": f\"{plot_type}_plot\",\n            \"save_format\": \"png\",\n            \"save_dpi\": 300\n        }\n    )\n\n    print(f\"Saved {plot_type} plot\")\n\n# Generate waterfall plots for first 5 samples\nfor i in range(5):\n    waterfall = xai.get_plot(\n        \"waterfall\",\n        index=i,\n        class_index=1,\n        save_config={\n            \"save_path\": \"outputs/shap_plots\",\n            \"save_name\": f\"waterfall_sample_{i}\",\n            \"save_format\": \"png\",\n            \"save_dpi\": 300\n        }\n    )\n\n    print(f\"Saved waterfall plot for sample {i}\")\n\n# Get classification report\nreport = xai.get_report()\n\nprint(\"\\n=== Classification Report ===\")\nprint(report[\"classification_report\"])\n\n# Plot confusion matrix\nimport matplotlib.pyplot as plt\n\nfig = xai.plot_confusion_matrix()\nplt.savefig(\"outputs/shap_plots/confusion_matrix.png\", dpi=300, bbox_inches=\"tight\")\nplt.close()\n\nprint(\"\\nAll SHAP plots saved to outputs/shap_plots/\")\n</code></pre>"},{"location":"examples/examples.html#comparing-models","title":"Comparing Models","text":"<p>Compare quantum vs classical models:</p> <pre><code>from quoptuna import Optimizer\nfrom quoptuna.backend.models import create_model\nimport pandas as pd\n\n# Run optimization (assumes data_dict is prepared)\noptimizer = Optimizer(db_name=\"comparison\", study_name=\"quantum_vs_classical\", data=data_dict)\nstudy, best_trials = optimizer.optimize(n_trials=100)\n\n# Separate quantum and classical trials\nquantum_trials = []\nclassical_trials = []\n\nfor trial in study.get_trials():\n    model_type = trial.params.get(\"model_type\", \"\")\n\n    # Determine if quantum or classical\n    if \"Classifier\" in model_type and any(\n        q in model_type\n        for q in [\"Reuploading\", \"Circuit\", \"Quantum\", \"Kitchen\", \"Dressed\"]\n    ):\n        quantum_trials.append(trial)\n    else:\n        classical_trials.append(trial)\n\n# Compare performance\ndef get_f1_score(trial):\n    q_f1 = trial.user_attrs.get(\"Quantum_f1_score\", 0)\n    c_f1 = trial.user_attrs.get(\"Classical_f1_score\", 0)\n    return max(q_f1, c_f1)\n\n# Get best from each category\nbest_quantum = max(quantum_trials, key=get_f1_score) if quantum_trials else None\nbest_classical = max(classical_trials, key=get_f1_score) if classical_trials else None\n\nprint(\"=== Model Comparison ===\\n\")\n\nif best_quantum:\n    print(\"Best Quantum Model:\")\n    print(f\"  Type: {best_quantum.params['model_type']}\")\n    print(f\"  F1 Score: {get_f1_score(best_quantum):.4f}\")\n    print(f\"  Trial: {best_quantum.number}\")\n\nif best_classical:\n    print(\"\\nBest Classical Model:\")\n    print(f\"  Type: {best_classical.params['model_type']}\")\n    print(f\"  F1 Score: {get_f1_score(best_classical):.4f}\")\n    print(f\"  Trial: {best_classical.number}\")\n\n# Create comparison DataFrame\ncomparison_data = []\n\nfor trial in quantum_trials + classical_trials:\n    comparison_data.append({\n        \"Trial\": trial.number,\n        \"Model Type\": trial.params[\"model_type\"],\n        \"Category\": \"Quantum\" if trial in quantum_trials else \"Classical\",\n        \"F1 Score\": get_f1_score(trial),\n        \"State\": trial.state.name\n    })\n\ndf_comparison = pd.DataFrame(comparison_data)\ndf_comparison = df_comparison.sort_values(\"F1 Score\", ascending=False)\n\nprint(\"\\n=== Top 10 Models ===\")\nprint(df_comparison.head(10))\n\n# Save results\ndf_comparison.to_csv(\"outputs/model_comparison.csv\", index=False)\n</code></pre>"},{"location":"examples/examples.html#report-generation","title":"Report Generation","text":"<p>Generate comprehensive AI reports:</p> <pre><code>from quoptuna import XAI\nfrom quoptuna.backend.xai.xai import XAIConfig\nimport os\n\n# Train model (from previous steps)\nmodel = create_model(**best_trials[0].params)\nmodel.fit(data_dict[\"train_x\"], data_dict[\"train_y\"])\n\n# Create XAI instance\nxai = XAI(\n    model=model,\n    data=data_dict,\n    config=XAIConfig(use_proba=True, onsubset=True, subset_size=50)\n)\n\n# Dataset information for better reports\ndataset_info = {\n    \"Name\": \"Credit Card Approval\",\n    \"URL\": \"https://archive.ics.uci.edu/dataset/143\",\n    \"Description\": \"\"\"\n        This dataset concerns credit card applications.\n        It contains a mix of continuous and categorical features\n        for predicting credit approval decisions.\n    \"\"\",\n    \"Features\": [\"Age\", \"Income\", \"Credit Score\", \"Employment Status\"],\n    \"Target\": \"Approval Decision\",\n    \"Instances\": 690,\n    \"Task\": \"Binary Classification\"\n}\n\n# Generate report with Google Gemini\nreport = xai.generate_report_with_langchain(\n    provider=\"google\",\n    api_key=os.getenv(\"GOOGLE_API_KEY\"),\n    model_name=\"models/gemini-2.0-flash-exp\",\n    dataset_info=dataset_info\n)\n\n# Save report\nwith open(\"outputs/analysis_report.md\", \"w\") as f:\n    f.write(report)\n\nprint(\"Report saved to outputs/analysis_report.md\")\n\n# Generate with OpenAI GPT\nreport_gpt = xai.generate_report_with_langchain(\n    provider=\"openai\",\n    api_key=os.getenv(\"OPENAI_API_KEY\"),\n    model_name=\"gpt-4\",\n    dataset_info=dataset_info\n)\n\nwith open(\"outputs/analysis_report_gpt4.md\", \"w\") as f:\n    f.write(report_gpt)\n\nprint(\"GPT-4 report saved to outputs/analysis_report_gpt4.md\")\n</code></pre>"},{"location":"examples/examples.html#batch-processing","title":"Batch Processing","text":"<p>Process multiple datasets:</p> <pre><code>from quoptuna import DataPreparation, Optimizer\nfrom quoptuna.backend.utils.data_utils.data import mock_csv_data\nimport pandas as pd\nimport os\n\n# List of datasets to process\ndatasets = [\n    {\"id\": 143, \"name\": \"Statlog\"},\n    {\"id\": 176, \"name\": \"Blood\"},\n    {\"id\": 267, \"name\": \"Banknote\"},\n]\n\nresults = []\n\nfor dataset_info in datasets:\n    print(f\"\\n{'='*50}\")\n    print(f\"Processing: {dataset_info['name']}\")\n    print('='*50)\n\n    try:\n        # Fetch dataset\n        from ucimlrepo import fetch_ucirepo\n        dataset = fetch_ucirepo(id=dataset_info[\"id\"])\n\n        # Prepare data\n        X = dataset.data.features\n        y = dataset.data.targets\n        df = pd.concat([X, y], axis=1)\n\n        # Get target column name\n        target_col = dataset.metadata[\"target_col\"][0]\n        df[\"target\"] = df[target_col].replace({0: -1, 1: 1})\n        df = df.drop(columns=[target_col])\n        df = df.dropna()\n\n        # Save\n        file_path = mock_csv_data(\n            df,\n            tmp_path=\"data/batch\",\n            file_name=dataset_info[\"name\"]\n        )\n\n        # Prepare\n        data_prep = DataPreparation(\n            file_path=file_path,\n            x_cols=list(df.columns.difference([\"target\"])),\n            y_col=\"target\"\n        )\n        data_dict = data_prep.get_data(output_type=\"2\")\n\n        # Convert to numpy\n        for key in data_dict.keys():\n            data_dict[key] = data_dict[key].values\n\n        # Optimize\n        optimizer = Optimizer(\n            db_name=f\"batch_{dataset_info['name']}\",\n            study_name=dataset_info[\"name\"],\n            data=data_dict\n        )\n        study, best_trials = optimizer.optimize(n_trials=50)\n\n        # Record results\n        best_f1 = max(\n            best_trials[0].user_attrs.get(\"Quantum_f1_score\", 0),\n            best_trials[0].user_attrs.get(\"Classical_f1_score\", 0)\n        )\n\n        results.append({\n            \"Dataset\": dataset_info[\"name\"],\n            \"Best Model\": best_trials[0].params[\"model_type\"],\n            \"Best F1\": best_f1,\n            \"Trials\": len(study.trials),\n            \"Status\": \"Success\"\n        })\n\n        print(f\"\u2713 Completed: {dataset_info['name']}\")\n        print(f\"  Best F1: {best_f1:.4f}\")\n        print(f\"  Model: {best_trials[0].params['model_type']}\")\n\n    except Exception as e:\n        print(f\"\u2717 Failed: {dataset_info['name']}\")\n        print(f\"  Error: {e}\")\n\n        results.append({\n            \"Dataset\": dataset_info[\"name\"],\n            \"Best Model\": None,\n            \"Best F1\": None,\n            \"Trials\": 0,\n            \"Status\": f\"Failed: {str(e)}\"\n        })\n\n# Save summary\ndf_results = pd.DataFrame(results)\ndf_results.to_csv(\"outputs/batch_processing_results.csv\", index=False)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"BATCH PROCESSING COMPLETE\")\nprint(\"=\"*50)\nprint(df_results)\n</code></pre>"},{"location":"examples/examples.html#advanced-custom-objective-function","title":"Advanced: Custom Objective Function","text":"<p>Define custom optimization objectives:</p> <pre><code>import optuna\nfrom quoptuna.backend.models import create_model\nfrom sklearn.metrics import f1_score, precision_score, recall_score\n\ndef custom_objective(trial, data_dict):\n    \"\"\"Custom objective balancing F1 score and model complexity.\"\"\"\n\n    # Suggest model type\n    model_type = trial.suggest_categorical(\n        \"model_type\",\n        [\"SVC\", \"MLPClassifier\", \"DataReuploadingClassifier\"]\n    )\n\n    # Suggest hyperparameters based on model type\n    if model_type == \"SVC\":\n        params = {\n            \"model_type\": model_type,\n            \"C\": trial.suggest_float(\"C\", 0.1, 10.0),\n            \"gamma\": trial.suggest_categorical(\"gamma\", [\"scale\", \"auto\"])\n        }\n    elif model_type == \"MLPClassifier\":\n        params = {\n            \"model_type\": model_type,\n            \"hidden_layer_sizes\": trial.suggest_categorical(\n                \"hidden_layer_sizes\",\n                [\"(10,)\", \"(50,)\", \"(10, 10)\"]\n            ),\n            \"alpha\": trial.suggest_float(\"alpha\", 0.0001, 0.1, log=True)\n        }\n    else:  # DataReuploadingClassifier\n        params = {\n            \"model_type\": model_type,\n            \"n_layers\": trial.suggest_int(\"n_layers\", 2, 10),\n            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.5)\n        }\n\n    # Create and train model\n    model = create_model(**params)\n    model.fit(data_dict[\"train_x\"], data_dict[\"train_y\"])\n\n    # Evaluate\n    y_pred = model.predict(data_dict[\"test_x\"])\n    y_true = data_dict[\"test_y\"]\n\n    # Calculate metrics\n    f1 = f1_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred)\n    recall = recall_score(y_true, y_pred)\n\n    # Store metrics as user attributes\n    trial.set_user_attr(\"precision\", precision)\n    trial.set_user_attr(\"recall\", recall)\n\n    # Return weighted score\n    # Prefer higher F1 but penalize complex models\n    complexity_penalty = 0.01 if model_type == \"DataReuploadingClassifier\" else 0\n    return f1 - complexity_penalty\n\n# Create study\nstudy = optuna.create_study(direction=\"maximize\")\n\n# Optimize\nstudy.optimize(\n    lambda trial: custom_objective(trial, data_dict),\n    n_trials=100\n)\n\n# Show results\nprint(f\"Best F1 Score: {study.best_value:.4f}\")\nprint(f\"Best Parameters: {study.best_params}\")\nprint(f\"Precision: {study.best_trial.user_attrs['precision']:.4f}\")\nprint(f\"Recall: {study.best_trial.user_attrs['recall']:.4f}\")\n</code></pre>"},{"location":"examples/examples.html#next-steps","title":"Next Steps","text":"<ul> <li>Review the API Reference for detailed class documentation</li> <li>Check the User Guide for the Streamlit interface</li> <li>Visit GitHub for more examples</li> </ul>"},{"location":"getting-started/quickstart.html","title":"QuOptuna Next - Quick Start Guide \ud83d\ude80","text":""},{"location":"getting-started/quickstart.html#quoptuna-next-quick-start-guide","title":"QuOptuna Next - Quick Start Guide \ud83d\ude80","text":"<p>Get up and running with QuOptuna Next in under 5 minutes!</p>"},{"location":"getting-started/quickstart.html#what-youll-get","title":"\ud83c\udfaf What You'll Get","text":"<p>A modern, drag-and-drop interface for building quantum ML workflows with real-time optimization tracking.</p>"},{"location":"getting-started/quickstart.html#prerequisites","title":"\ud83d\udccb Prerequisites","text":"<p>Choose one: - Docker (easiest) - Just Docker and docker-compose - Local Development - Node.js 18+ and Python 3.10+</p>"},{"location":"getting-started/quickstart.html#option-1-docker-recommended","title":"\ud83d\ude80 Option 1: Docker (Recommended)","text":""},{"location":"getting-started/quickstart.html#step-1-start-the-services","title":"Step 1: Start the Services","text":"<pre><code>cd /path/to/quoptuna\ndocker-compose up\n</code></pre> <p>That's it! \ud83c\udf89</p>"},{"location":"getting-started/quickstart.html#step-2-access-the-application","title":"Step 2: Access the Application","text":"<ul> <li>Frontend: http://localhost:5173</li> <li>Backend API: http://localhost:8000</li> <li>API Docs: http://localhost:8000/api/docs</li> </ul>"},{"location":"getting-started/quickstart.html#option-2-local-development","title":"\ud83d\udcbb Option 2: Local Development","text":""},{"location":"getting-started/quickstart.html#step-1-start-backend","title":"Step 1: Start Backend","text":"<pre><code>cd backend\n\n# Create virtual environment\npython -m venv .venv\nsource .venv/bin/activate  # Windows: .venv\\Scripts\\activate\n\n# Install dependencies\npip install -e .\n\n# Start server\nuvicorn app.main:app --reload\n</code></pre> <p>Backend running at http://localhost:8000 \u2713</p>"},{"location":"getting-started/quickstart.html#step-2-start-frontend","title":"Step 2: Start Frontend","text":"<pre><code># New terminal\ncd frontend\n\n# Install dependencies\nnpm install\n\n# Start dev server\nnpm run dev\n</code></pre> <p>Frontend running at http://localhost:5173 \u2713</p>"},{"location":"getting-started/quickstart.html#using-the-application","title":"\ud83c\udfa8 Using the Application","text":""},{"location":"getting-started/quickstart.html#1-create-your-first-workflow","title":"1. Create Your First Workflow","text":"<ol> <li>Navigate to Workflow Builder</li> <li>Drag nodes from the left palette onto the canvas</li> <li>Connect nodes by dragging from output (bottom) to input (top) handles</li> </ol> <p>Example Workflow: </p><pre><code>Upload CSV \u2192 Feature Selection \u2192 Train/Test Split \u2192 Quantum Model \u2192 Optimization \u2192 SHAP Analysis\n</code></pre><p></p>"},{"location":"getting-started/quickstart.html#2-configure-nodes","title":"2. Configure Nodes","text":"<p>Click any node to configure its parameters: - Dataset selection - Model type - Hyperparameter ranges - Analysis options</p>"},{"location":"getting-started/quickstart.html#3-run-the-workflow","title":"3. Run the Workflow","text":"<p>Click Run in the toolbar to execute your pipeline!</p>"},{"location":"getting-started/quickstart.html#example-iris-classification","title":"\ud83d\udcca Example: Iris Classification","text":""},{"location":"getting-started/quickstart.html#using-the-interface","title":"Using the Interface:","text":"<ol> <li>Data Explorer \u2192 Upload <code>iris.csv</code> or fetch from UCI</li> <li>Workflow Builder \u2192 Create this pipeline:    <pre><code>[UCI Dataset: Iris]\n   \u2193\n[Feature Selection: sepal_length, sepal_width, petal_length, petal_width \u2192 species]\n   \u2193\n[Train/Test Split: 75/25]\n   \u2193\n[Standard Scaler]\n   \u2193\n[Quantum Model: Data Reuploading]\n   \u2193\n[Optuna Config: 50 trials]\n   \u2193\n[SHAP Analysis: bar, beeswarm, violin]\n   \u2193\n[Generate Report: GPT-4]\n</code></pre></li> <li>Run \u2192 Monitor real-time progress</li> <li>Analytics \u2192 View SHAP insights and AI report</li> </ol>"},{"location":"getting-started/quickstart.html#development-commands","title":"\ud83d\udd27 Development Commands","text":""},{"location":"getting-started/quickstart.html#frontend","title":"Frontend","text":"<pre><code>cd frontend\n\n# Development\nnpm run dev          # Start dev server\nnpm run build        # Build for production\nnpm run preview      # Preview production build\nnpm run type-check   # TypeScript check\n\n# Clean install\nrm -rf node_modules package-lock.json &amp;&amp; npm install\n</code></pre>"},{"location":"getting-started/quickstart.html#backend","title":"Backend","text":"<pre><code>cd backend\n\n# Development\nuvicorn app.main:app --reload        # Start with auto-reload\npython -m pytest                      # Run tests\nruff check .                          # Lint code\nruff format .                         # Format code\n\n# Clean environment\nrm -rf .venv &amp;&amp; python -m venv .venv\n</code></pre>"},{"location":"getting-started/quickstart.html#docker","title":"Docker","text":"<pre><code># Start services\ndocker-compose up              # Foreground\ndocker-compose up -d           # Background\n\n# View logs\ndocker-compose logs -f         # All services\ndocker-compose logs -f frontend  # Frontend only\ndocker-compose logs -f backend   # Backend only\n\n# Stop and clean\ndocker-compose down            # Stop services\ndocker-compose down -v         # Stop and remove volumes\ndocker-compose build           # Rebuild images\n</code></pre>"},{"location":"getting-started/quickstart.html#next-steps","title":"\ud83c\udfaf Next Steps","text":""},{"location":"getting-started/quickstart.html#explore-features","title":"Explore Features","text":"<ol> <li>Model Library \u2192 Browse 26 available models</li> <li>Data Explorer \u2192 Upload datasets or fetch from UCI</li> <li>Analytics \u2192 Generate SHAP visualizations</li> <li>Settings \u2192 Configure API keys for LLM reports</li> </ol>"},{"location":"getting-started/quickstart.html#customize","title":"Customize","text":"<ul> <li>Add Custom Nodes: Edit <code>frontend/src/components/workflow/NodePalette.tsx</code></li> <li>Add Models: Extend <code>src/quoptuna/backend/models.py</code></li> <li>Modify Theme: Update <code>frontend/src/index.css</code></li> <li>Add Endpoints: Create in <code>backend/app/api/v1/</code></li> </ul>"},{"location":"getting-started/quickstart.html#learn-more","title":"Learn More","text":"<ul> <li>\ud83d\udcd6 Full Documentation: See <code>README_NEXT.md</code></li> <li>\ud83c\udfd7\ufe0f Architecture Guide: See <code>NEW_FRONTEND_DESIGN.md</code></li> <li>\ud83d\udcda API Reference: http://localhost:8000/api/docs</li> </ul>"},{"location":"getting-started/quickstart.html#troubleshooting","title":"\ud83d\udc1b Troubleshooting","text":""},{"location":"getting-started/quickstart.html#port-already-in-use","title":"Port Already in Use?","text":"<pre><code># Change frontend port\ncd frontend\n# Edit vite.config.ts: server: { port: 3000 }\n\n# Change backend port\ncd backend\n# Start with: uvicorn app.main:app --port 8001\n</code></pre>"},{"location":"getting-started/quickstart.html#dependencies-not-installing","title":"Dependencies Not Installing?","text":"<pre><code># Frontend\ncd frontend\nrm -rf node_modules package-lock.json\nnpm cache clean --force\nnpm install\n\n# Backend\ncd backend\nrm -rf .venv\npython -m venv .venv\nsource .venv/bin/activate\npip install --upgrade pip\npip install -e .\n</code></pre>"},{"location":"getting-started/quickstart.html#docker-issues","title":"Docker Issues?","text":"<pre><code># Full clean restart\ndocker-compose down -v\ndocker system prune -a\ndocker-compose build --no-cache\ndocker-compose up\n</code></pre>"},{"location":"getting-started/quickstart.html#tips","title":"\ud83d\udca1 Tips","text":"<ol> <li>Save Workflows - Click Save to reuse your pipelines</li> <li>Keyboard Shortcuts -</li> <li><code>Delete</code> - Remove selected node</li> <li><code>Ctrl/Cmd + S</code> - Save workflow</li> <li><code>Ctrl/Cmd + Z</code> - Undo</li> <li>Node Status - Watch nodes change color during execution:</li> <li>Gray = Idle</li> <li>Blue = Running (animated)</li> <li>Green = Complete</li> <li>Red = Error</li> <li>Use Templates - Start with example workflows in Dashboard</li> </ol>"},{"location":"getting-started/quickstart.html#tutorial-your-first-quantum-ml-workflow","title":"\ud83c\udf93 Tutorial: Your First Quantum ML Workflow","text":"<p>Goal: Train a quantum model on the Iris dataset</p> <p>Time: 5 minutes</p> <ol> <li>Upload Data</li> <li>Go to Data Explorer</li> <li>Click \"UCI Repository\"</li> <li> <p>Select \"Iris Dataset\"</p> </li> <li> <p>Build Workflow</p> </li> <li>Go to Workflow Builder</li> <li>Drag: UCI Dataset \u2192 Feature Selection \u2192 Train/Test Split \u2192 Standard Scaler \u2192 Quantum Model \u2192 Optuna Config \u2192 SHAP Analysis</li> <li> <p>Connect all nodes</p> </li> <li> <p>Configure</p> </li> <li>UCI Dataset: Select \"iris\"</li> <li>Feature Selection: X = [sepal_length, sepal_width, petal_length, petal_width], y = species</li> <li>Quantum Model: Type = \"Data Reuploading\"</li> <li> <p>Optuna Config: Trials = 20</p> </li> <li> <p>Execute</p> </li> <li>Click Run</li> <li>Watch real-time progress</li> <li> <p>View results in Analytics</p> </li> <li> <p>Analyze</p> </li> <li>Go to Analytics</li> <li>See SHAP plots</li> <li>Generate AI report</li> </ol> <p>Congratulations! \ud83c\udf89 You've built your first quantum ML workflow!</p>"},{"location":"getting-started/quickstart.html#get-help","title":"\ud83d\udcde Get Help","text":"<ul> <li>Issues: https://github.com/Qentora/quoptuna/issues</li> <li>Discussions: https://github.com/Qentora/quoptuna/discussions</li> <li>API Docs: http://localhost:8000/api/docs</li> </ul> <p>Happy Quantum Machine Learning! \ud83c\udf1f</p>"},{"location":"getting-started/quoptuna-next.html","title":"QuOptuna Next \ud83d\ude80","text":""},{"location":"getting-started/quoptuna-next.html#quoptuna-next","title":"QuOptuna Next \ud83d\ude80","text":"<p>A modern, full-stack application for quantum machine learning optimization with drag-and-drop workflow building, inspired by langflow's architecture.</p>"},{"location":"getting-started/quoptuna-next.html#features","title":"\u2728 Features","text":""},{"location":"getting-started/quoptuna-next.html#modern-uiux","title":"\ud83c\udfa8 Modern UI/UX","text":"<ul> <li>Drag &amp; Drop Workflow Builder - Visually design ML pipelines with React Flow</li> <li>Dark/Light Mode - Beautiful design system with Tailwind CSS</li> <li>Responsive Design - Works seamlessly on desktop, tablet, and mobile</li> <li>Real-time Updates - Live optimization progress via WebSocket</li> <li>Interactive Dashboards - Rich data visualizations with Recharts and Plotly</li> </ul>"},{"location":"getting-started/quoptuna-next.html#powerful-ml-capabilities","title":"\ud83e\udde0 Powerful ML Capabilities","text":"<ul> <li>26 ML Models - 18 quantum models + 8 classical models</li> <li>Hyperparameter Optimization - Powered by Optuna</li> <li>Explainability Analysis - SHAP integration for model insights</li> <li>AI-Powered Reports - Generate insights with GPT-4, Claude, or Gemini</li> </ul>"},{"location":"getting-started/quoptuna-next.html#modern-tech-stack","title":"\u26a1 Modern Tech Stack","text":""},{"location":"getting-started/quoptuna-next.html#frontend","title":"Frontend","text":"<ul> <li>React 18 + TypeScript</li> <li>Vite (lightning-fast HMR)</li> <li>React Flow (drag &amp; drop)</li> <li>Tailwind CSS + shadcn/ui</li> <li>Zustand (state management)</li> <li>TanStack Query (data fetching)</li> </ul>"},{"location":"getting-started/quoptuna-next.html#backend","title":"Backend","text":"<ul> <li>FastAPI (async, high-performance)</li> <li>Pydantic (type safety)</li> <li>Optuna (optimization)</li> <li>PennyLane (quantum ML)</li> <li>SHAP (explainability)</li> <li>LangChain (LLM integration)</li> </ul>"},{"location":"getting-started/quoptuna-next.html#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"getting-started/quoptuna-next.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Node.js 18+ and npm</li> <li>Python 3.10+</li> <li>Docker (optional, recommended)</li> </ul>"},{"location":"getting-started/quoptuna-next.html#option-1-docker-recommended","title":"Option 1: Docker (Recommended)","text":"<pre><code># Clone the repository\ngit clone https://github.com/Qentora/quoptuna.git\ncd quoptuna\n\n# Start all services\ndocker-compose up\n\n# Access the application\n# Frontend: http://localhost:5173\n# Backend API: http://localhost:8000\n# API Docs: http://localhost:8000/api/docs\n</code></pre>"},{"location":"getting-started/quoptuna-next.html#option-2-local-development","title":"Option 2: Local Development","text":""},{"location":"getting-started/quoptuna-next.html#frontend_1","title":"Frontend","text":"<pre><code>cd frontend\n\n# Install dependencies\nnpm install\n\n# Start dev server\nnpm run dev\n\n# Open http://localhost:5173\n</code></pre>"},{"location":"getting-started/quoptuna-next.html#backend_1","title":"Backend","text":"<pre><code>cd backend\n\n# Create virtual environment\npython -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install dependencies\npip install -e .\n\n# Start server\nuvicorn app.main:app --reload\n\n# Open http://localhost:8000/api/docs\n</code></pre>"},{"location":"getting-started/quoptuna-next.html#usage-guide","title":"\ud83d\udcd6 Usage Guide","text":""},{"location":"getting-started/quoptuna-next.html#1-create-a-workflow","title":"1. Create a Workflow","text":"<p>Navigate to Workflow Builder and design your ML pipeline:</p> <pre><code>[Upload CSV] \u2192 [Feature Selection] \u2192 [Train/Test Split] \u2192 [StandardScaler]\n                                                               \u2193\n                                                          [Quantum Model]\n                                                               \u2193\n                                                       [Optuna Optimization]\n                                                               \u2193\n                                                          [SHAP Analysis]\n                                                               \u2193\n                                                        [Generate Report]\n</code></pre> <p>Drag nodes from the palette \u2192 Connect them \u2192 Configure each node \u2192 Run!</p>"},{"location":"getting-started/quoptuna-next.html#2-upload-data","title":"2. Upload Data","text":"<p>Go to Data Explorer to: - Upload CSV files - Browse UCI ML Repository - Preview dataset statistics - Manage your datasets</p>"},{"location":"getting-started/quoptuna-next.html#3-run-optimization","title":"3. Run Optimization","text":"<p>Click Run on your workflow to: - Start Optuna hyperparameter tuning - Monitor progress in real-time - View trial results - Explore optimization history</p>"},{"location":"getting-started/quoptuna-next.html#4-analyze-results","title":"4. Analyze Results","text":"<p>Navigate to Analytics to: - Generate SHAP visualizations - Create AI-powered reports - Compare model performance - Export results</p>"},{"location":"getting-started/quoptuna-next.html#project-structure","title":"\ud83d\uddc2\ufe0f Project Structure","text":"<pre><code>quoptuna/\n\u251c\u2500\u2500 frontend/                    # React frontend\n\u2502   \u251c\u2500\u2500 src/\n\u2502   \u2502   \u251c\u2500\u2500 components/         # Reusable components\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ui/            # shadcn/ui components\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 workflow/      # Workflow builder nodes\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 charts/        # Visualization components\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 layout/        # Layout components\n\u2502   \u2502   \u251c\u2500\u2500 pages/             # Route pages\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 Dashboard.tsx\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 WorkflowBuilder.tsx\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 DataExplorer.tsx\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 Models.tsx\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 Analytics.tsx\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 Settings.tsx\n\u2502   \u2502   \u251c\u2500\u2500 stores/            # Zustand stores\n\u2502   \u2502   \u251c\u2500\u2500 types/             # TypeScript types\n\u2502   \u2502   \u2514\u2500\u2500 lib/               # Utilities &amp; API client\n\u2502   \u251c\u2500\u2500 package.json\n\u2502   \u2514\u2500\u2500 vite.config.ts\n\u2502\n\u251c\u2500\u2500 backend/                     # FastAPI backend\n\u2502   \u251c\u2500\u2500 app/\n\u2502   \u2502   \u251c\u2500\u2500 api/v1/            # API endpoints\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 data.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 workflows.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 optimize.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 analysis.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 system.py\n\u2502   \u2502   \u251c\u2500\u2500 core/              # Core configuration\n\u2502   \u2502   \u251c\u2500\u2500 services/          # Business logic\n\u2502   \u2502   \u251c\u2500\u2500 schemas/           # Pydantic models\n\u2502   \u2502   \u2514\u2500\u2500 main.py            # FastAPI app\n\u2502   \u2514\u2500\u2500 pyproject.toml\n\u2502\n\u251c\u2500\u2500 docker-compose.yml\n\u251c\u2500\u2500 NEW_FRONTEND_DESIGN.md      # Architecture design doc\n\u2514\u2500\u2500 README_NEXT.md              # This file\n</code></pre>"},{"location":"getting-started/quoptuna-next.html#available-node-types","title":"\ud83c\udfaf Available Node Types","text":""},{"location":"getting-started/quoptuna-next.html#data-nodes","title":"Data Nodes","text":"<ul> <li>Upload CSV - Import local datasets</li> <li>UCI Dataset - Fetch from repository</li> <li>Data Preview - View statistics</li> <li>Feature Selection - Choose X and y</li> </ul>"},{"location":"getting-started/quoptuna-next.html#preprocessing-nodes","title":"Preprocessing Nodes","text":"<ul> <li>Train/Test Split - Split data (default 75/25)</li> <li>Standard Scaler - Normalize features</li> <li>Label Encoding - Encode target labels</li> </ul>"},{"location":"getting-started/quoptuna-next.html#model-nodes","title":"Model Nodes","text":"<ul> <li>Quantum Models - 18 PennyLane-based models</li> <li>Classical Models - 8 Scikit-learn models</li> </ul>"},{"location":"getting-started/quoptuna-next.html#optimization-nodes","title":"Optimization Nodes","text":"<ul> <li>Optuna Config - Configure study parameters</li> <li>Run Optimization - Execute hyperparameter tuning</li> </ul>"},{"location":"getting-started/quoptuna-next.html#analysis-nodes","title":"Analysis Nodes","text":"<ul> <li>SHAP Analysis - Generate explainability plots</li> <li>Confusion Matrix - Classification metrics</li> <li>Feature Importance - Rank features</li> </ul>"},{"location":"getting-started/quoptuna-next.html#output-nodes","title":"Output Nodes","text":"<ul> <li>Export Model - Save trained models</li> <li>Generate Report - AI-powered insights</li> </ul>"},{"location":"getting-started/quoptuna-next.html#quantum-models-18","title":"\ud83d\udcca Quantum Models (18)","text":"<ol> <li>Data Reuploading</li> <li>Circuit Centric</li> <li>Dressed Quantum Circuit</li> <li>Quantum Kitchen Sinks</li> <li>IQP Variational</li> <li>IQP Kernel</li> <li>Projected Quantum Kernel</li> <li>Quantum Metric Learning</li> <li>Vanilla QNN</li> <li>Quantum Boltzmann Machine</li> <li>Tree Tensor Network</li> <li>WeiNet</li> <li>Quanvolutional Neural Network</li> <li>Separable</li> <li>Convolutional Neural Network</li> <li>(and more...)</li> </ol>"},{"location":"getting-started/quoptuna-next.html#classical-models-8","title":"\ud83d\udda5\ufe0f Classical Models (8)","text":"<ol> <li>Support Vector Classifier</li> <li>Multi-layer Perceptron</li> <li>Perceptron</li> <li>Random Forest</li> <li>Gradient Boosting</li> <li>AdaBoost</li> <li>Logistic Regression</li> <li>Decision Tree</li> </ol>"},{"location":"getting-started/quoptuna-next.html#api-documentation","title":"\ud83d\udd0c API Documentation","text":"<p>Access the interactive API docs at: - Swagger UI: http://localhost:8000/api/docs - ReDoc: http://localhost:8000/api/redoc</p>"},{"location":"getting-started/quoptuna-next.html#key-endpoints","title":"Key Endpoints","text":"<pre><code>POST   /api/v1/data/upload           # Upload CSV\nGET    /api/v1/data/uci              # List UCI datasets\nPOST   /api/v1/workflows             # Create workflow\nPOST   /api/v1/workflows/{id}/run    # Execute workflow\nPOST   /api/v1/optimize              # Start optimization\nGET    /api/v1/optimize/{id}/trials  # Get trial history\nPOST   /api/v1/analysis/shap         # Generate SHAP analysis\nPOST   /api/v1/analysis/report       # Generate AI report\nGET    /api/v1/health                # Health check\n</code></pre>"},{"location":"getting-started/quoptuna-next.html#development","title":"\ud83d\udee0\ufe0f Development","text":""},{"location":"getting-started/quoptuna-next.html#frontend-development","title":"Frontend Development","text":"<pre><code>cd frontend\n\n# Install dependencies\nnpm install\n\n# Start dev server with HMR\nnpm run dev\n\n# Type check\nnpm run type-check\n\n# Build for production\nnpm run build\n\n# Preview production build\nnpm run preview\n</code></pre>"},{"location":"getting-started/quoptuna-next.html#backend-development","title":"Backend Development","text":"<pre><code>cd backend\n\n# Install with dev dependencies\npip install -e \".[dev]\"\n\n# Run with auto-reload\nuvicorn app.main:app --reload\n\n# Run tests\npytest\n\n# Format code\nruff format .\n\n# Lint code\nruff check .\n</code></pre>"},{"location":"getting-started/quoptuna-next.html#docker-commands","title":"\ud83d\udc33 Docker Commands","text":"<pre><code># Start all services\ndocker-compose up\n\n# Start in background\ndocker-compose up -d\n\n# View logs\ndocker-compose logs -f\n\n# Stop services\ndocker-compose down\n\n# Rebuild images\ndocker-compose build\n\n# Remove volumes\ndocker-compose down -v\n</code></pre>"},{"location":"getting-started/quoptuna-next.html#key-differences-from-streamlit-version","title":"\ud83c\udf1f Key Differences from Streamlit Version","text":"Feature Streamlit QuOptuna Next UX Linear page navigation Drag-and-drop visual workflows Interactivity Limited, page reloads Fully interactive React SPA Real-time Polling/rerun WebSocket live updates Customization Basic theming Full design system control Performance Server-side rendering Client-side with smart caching Type Safety Python only End-to-end TypeScript + Pydantic API None Full REST API + WebSocket Workflows Manual multi-page steps Reusable visual workflows Mobile Poor responsiveness Fully responsive design State Session-based Persistent with Zustand"},{"location":"getting-started/quoptuna-next.html#configuration","title":"\ud83d\udcdd Configuration","text":""},{"location":"getting-started/quoptuna-next.html#environment-variables","title":"Environment Variables","text":"<p>Create <code>.env</code> files in frontend and backend directories:</p> <p>Backend <code>.env</code>: </p><pre><code>OPENAI_API_KEY=sk-...\nANTHROPIC_API_KEY=sk-ant-...\nGOOGLE_API_KEY=AIza...\nDATABASE_URL=sqlite:///./quoptuna.db\nUPLOAD_DIR=./uploads\n</code></pre><p></p> <p>Frontend <code>.env</code>: </p><pre><code>VITE_API_URL=http://localhost:8000\n</code></pre><p></p>"},{"location":"getting-started/quoptuna-next.html#customization","title":"\ud83c\udfa8 Customization","text":""},{"location":"getting-started/quoptuna-next.html#add-custom-nodes","title":"Add Custom Nodes","text":"<ol> <li>Define node type in <code>frontend/src/types/workflow.ts</code></li> <li>Add to palette in <code>frontend/src/components/workflow/NodePalette.tsx</code></li> <li>Implement backend logic in <code>backend/app/services/</code></li> </ol>"},{"location":"getting-started/quoptuna-next.html#add-custom-models","title":"Add Custom Models","text":"<ol> <li>Create model class in <code>src/quoptuna/backend/models.py</code></li> <li>Register in model factory</li> <li>Add to frontend model list</li> </ol>"},{"location":"getting-started/quoptuna-next.html#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"getting-started/quoptuna-next.html#frontend-not-connecting-to-backend","title":"Frontend not connecting to backend?","text":"<ul> <li>Check CORS settings in <code>backend/app/core/config.py</code></li> <li>Verify <code>VITE_API_URL</code> in frontend <code>.env</code></li> <li>Ensure backend is running on correct port</li> </ul>"},{"location":"getting-started/quoptuna-next.html#workflow-execution-failing","title":"Workflow execution failing?","text":"<ul> <li>Check backend logs: <code>docker-compose logs backend</code></li> <li>Verify dataset is uploaded correctly</li> <li>Ensure all nodes are connected properly</li> </ul>"},{"location":"getting-started/quoptuna-next.html#build-errors","title":"Build errors?","text":"<ul> <li>Clear node_modules: <code>rm -rf node_modules &amp;&amp; npm install</code></li> <li>Clear Python cache: <code>find . -type d -name __pycache__ -delete</code></li> </ul>"},{"location":"getting-started/quoptuna-next.html#contributing","title":"\ud83e\udd1d Contributing","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch: <code>git checkout -b feature/amazing-feature</code></li> <li>Commit changes: <code>git commit -m 'Add amazing feature'</code></li> <li>Push to branch: <code>git push origin feature/amazing-feature</code></li> <li>Open a Pull Request</li> </ol>"},{"location":"getting-started/quoptuna-next.html#license","title":"\ud83d\udcc4 License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"getting-started/quoptuna-next.html#acknowledgments","title":"\ud83d\ude4f Acknowledgments","text":"<ul> <li>Langflow - Inspiration for drag-and-drop architecture</li> <li>React Flow - Excellent workflow builder library</li> <li>shadcn/ui - Beautiful component library</li> <li>FastAPI - Modern Python web framework</li> <li>Optuna - Hyperparameter optimization</li> <li>PennyLane - Quantum machine learning</li> </ul>"},{"location":"getting-started/quoptuna-next.html#support","title":"\ud83d\udce7 Support","text":"<ul> <li>GitHub Issues: https://github.com/Qentora/quoptuna/issues</li> <li>Documentation: See <code>NEW_FRONTEND_DESIGN.md</code> for architecture details</li> <li>API Docs: http://localhost:8000/api/docs</li> </ul> <p>Built with \u2764\ufe0f by the QuOptuna Team</p> <p>Quantum Machine Learning Made Visual</p>"},{"location":"getting-started/run-without-docker.html","title":"Running QuOptuna Without Docker","text":""},{"location":"getting-started/run-without-docker.html#running-quoptuna-without-docker","title":"Running QuOptuna Without Docker","text":""},{"location":"getting-started/run-without-docker.html#backend-setup","title":"Backend Setup","text":""},{"location":"getting-started/run-without-docker.html#1-create-virtual-environment","title":"1. Create Virtual Environment","text":"<pre><code>cd /home/user/quoptuna\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n</code></pre>"},{"location":"getting-started/run-without-docker.html#2-install-main-quoptuna-package","title":"2. Install Main QuOptuna Package","text":"<pre><code># Install the main quoptuna package in editable mode\npip install -e .\n</code></pre>"},{"location":"getting-started/run-without-docker.html#3-install-backend-dependencies","title":"3. Install Backend Dependencies","text":"<pre><code>cd backend\npip install -e .\n</code></pre>"},{"location":"getting-started/run-without-docker.html#4-run-backend-server","title":"4. Run Backend Server","text":"<pre><code># From the backend directory\ncd /home/user/quoptuna/backend\nuvicorn app.main:app --host 0.0.0.0 --port 8000 --reload\n</code></pre> <p>Backend will be available at: - API: http://localhost:8000 - Docs: http://localhost:8000/docs</p>"},{"location":"getting-started/run-without-docker.html#frontend-setup","title":"Frontend Setup","text":""},{"location":"getting-started/run-without-docker.html#1-install-node-dependencies","title":"1. Install Node Dependencies","text":"<pre><code>cd /home/user/quoptuna/frontend\nnpm install\n</code></pre>"},{"location":"getting-started/run-without-docker.html#2-run-frontend-dev-server","title":"2. Run Frontend Dev Server","text":"<pre><code>npm run dev\n</code></pre> <p>Frontend will be available at: - UI: http://localhost:5173</p>"},{"location":"getting-started/run-without-docker.html#quick-start-script","title":"Quick Start Script","text":"<p>Create a file <code>start-dev.sh</code>:</p> <pre><code>#!/bin/bash\n\n# Start backend in background\necho \"Starting backend...\"\ncd /home/user/quoptuna\nsource venv/bin/activate\ncd backend\nuvicorn app.main:app --host 0.0.0.0 --port 8000 --reload &amp;\nBACKEND_PID=$!\n\n# Start frontend in background\necho \"Starting frontend...\"\ncd /home/user/quoptuna/frontend\nnpm run dev &amp;\nFRONTEND_PID=$!\n\necho \"Backend PID: $BACKEND_PID\"\necho \"Frontend PID: $FRONTEND_PID\"\necho \"\"\necho \"Backend running at: http://localhost:8000\"\necho \"Frontend running at: http://localhost:5173\"\necho \"\"\necho \"Press Ctrl+C to stop both services\"\n\n# Wait for Ctrl+C\ntrap \"kill $BACKEND_PID $FRONTEND_PID\" EXIT\nwait\n</code></pre> <p>Make it executable: </p><pre><code>chmod +x start-dev.sh\n./start-dev.sh\n</code></pre><p></p>"},{"location":"getting-started/run-without-docker.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/run-without-docker.html#backend-issues","title":"Backend Issues","text":"<p>Import errors: </p><pre><code># Make sure both packages are installed\npip install -e .  # From /home/user/quoptuna\npip install -e .  # From /home/user/quoptuna/backend\n</code></pre><p></p> <p>Port already in use: </p><pre><code># Find and kill process on port 8000\nlsof -ti:8000 | xargs kill -9\n</code></pre><p></p>"},{"location":"getting-started/run-without-docker.html#frontend-issues","title":"Frontend Issues","text":"<p>Module not found: </p><pre><code>cd frontend\nrm -rf node_modules package-lock.json\nnpm install\n</code></pre><p></p> <p>Port already in use: </p><pre><code># Find and kill process on port 5173\nlsof -ti:5173 | xargs kill -9\n</code></pre><p></p>"},{"location":"getting-started/run-without-docker.html#environment-variables","title":"Environment Variables","text":""},{"location":"getting-started/run-without-docker.html#backend-env-in-homeuserquoptunabackend","title":"Backend (.env in /home/user/quoptuna/backend)","text":"<pre><code>DATABASE_URL=sqlite:///./quoptuna.db\nCORS_ORIGINS=http://localhost:5173,http://localhost:3000\nUPLOAD_DIR=./uploads\n</code></pre>"},{"location":"getting-started/run-without-docker.html#frontend-env-in-homeuserquoptunafrontend","title":"Frontend (.env in /home/user/quoptuna/frontend)","text":"<pre><code>VITE_API_URL=http://localhost:8000\n</code></pre>"},{"location":"guides/frontend-quick-reference.html","title":"QuOptuna Frontend - Quick Reference Guide","text":""},{"location":"guides/frontend-quick-reference.html#quoptuna-frontend-quick-reference-guide","title":"QuOptuna Frontend - Quick Reference Guide","text":""},{"location":"guides/frontend-quick-reference.html#application-flow-at-a-glance","title":"Application Flow at a Glance","text":"<pre><code>START (http://localhost:8501)\n  \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    HOME PAGE (main_page.py)                \u2502\n\u2502                     Welcome &amp; Overview                      \u2502\n\u2502  - Project description                                     \u2502\n\u2502  - 3-column feature highlights                             \u2502\n\u2502  - Step-by-step workflow guide                             \u2502\n\u2502  - Tips &amp; best practices                                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n  \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502        PAGE 1: DATASET SELECTION (1_dataset_selection.py)  \u2502\n\u2502                 Upload &amp; Prepare Data                      \u2502\n\u2502                                                             \u2502\n\u2502  TAB 1: UCI ML Repository                                 \u2502\n\u2502    - Select popular dataset or custom ID                  \u2502\n\u2502    - Metadata display (instances, features, area)         \u2502\n\u2502                                                             \u2502\n\u2502  TAB 2: Custom CSV Upload                                 \u2502\n\u2502    - File uploader widget                                 \u2502\n\u2502                                                             \u2502\n\u2502  Configuration (appears after load):                      \u2502\n\u2502    \u251c\u2500 Data Preview: Head (10 rows) + statistics          \u2502\n\u2502    \u251c\u2500 Target Selection: Dropdown from columns             \u2502\n\u2502    \u251c\u2500 Feature Selection: Multiselect (auto-exclude target)\u2502\n\u2502    \u251c\u2500 Target Transformation:                              \u2502\n\u2502    \u2502   \u251c\u2500 Map unique_value[0] \u2192 -1                       \u2502\n\u2502    \u2502   \u2514\u2500 Map unique_value[1] \u2192 1                        \u2502\n\u2502    \u251c\u2500 Missing value handling: Drop NaN rows              \u2502\n\u2502    \u2514\u2500 Save Configuration \u2192 data/{dataset_name}.csv       \u2502\n\u2502                                                             \u2502\n\u2502  Session State:                                            \u2502\n\u2502    dataset_df, file_path, target_column, feature_columns  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n  \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      PAGE 2: OPTIMIZATION (2_optimization.py)              \u2502\n\u2502        Data Preparation &amp; Hyperparameter Tuning            \u2502\n\u2502                                                             \u2502\n\u2502  SECTION 1: Data Preparation                              \u2502\n\u2502    \u251c\u2500 Load CSV from file_path (Page 1)                   \u2502\n\u2502    \u251c\u2500 DataPreparation(file_path, x_cols, y_col)         \u2502\n\u2502    \u2502   \u251c\u2500 Read CSV                                       \u2502\n\u2502    \u2502   \u251c\u2500 StandardScaler.fit_transform(X_train)          \u2502\n\u2502    \u2502   \u251c\u2500 Train/test split (75/25, random_state=42)      \u2502\n\u2502    \u2502   \u2514\u2500 Map y to {-1, 1}                               \u2502\n\u2502    \u251c\u2500 Return: data_dict = {                              \u2502\n\u2502    \u2502   train_x: array, test_x: array,                    \u2502\n\u2502    \u2502   train_y: array, test_y: array                     \u2502\n\u2502    \u2502 }                                                     \u2502\n\u2502    \u2514\u2500 Display: Train samples, Test samples               \u2502\n\u2502                                                             \u2502\n\u2502  SECTION 2: Optimization Setup                            \u2502\n\u2502    \u251c\u2500 Database Name: text_input (default: dataset_name)  \u2502\n\u2502    \u251c\u2500 Study Name: text_input (default: dataset_name)     \u2502\n\u2502    \u251c\u2500 Number of Trials: slider (10-200, default: 100)    \u2502\n\u2502    \u2514\u2500 Run Button: Start Optimization                      \u2502\n\u2502                                                             \u2502\n\u2502  SECTION 3: Optimizer Execution                           \u2502\n\u2502    \u251c\u2500 Optimizer(db_name, study_name, data=data_dict)    \u2502\n\u2502    \u251c\u2500 optimize(n_trials=n_trials)                        \u2502\n\u2502    \u2502   \u251c\u2500 Loop n_trials times:                           \u2502\n\u2502    \u2502   \u2502   \u251c\u2500 Suggest hyperparameters (TPE sampler)      \u2502\n\u2502    \u2502   \u2502   \u251c\u2500 Create model from 15 types                 \u2502\n\u2502    \u2502   \u2502   \u251c\u2500 model.fit(train_x, train_y)               \u2502\n\u2502    \u2502   \u2502   \u251c\u2500 Evaluate: F1, accuracy                     \u2502\n\u2502    \u2502   \u2502   \u251c\u2500 Store metrics in trial.user_attrs          \u2502\n\u2502    \u2502   \u2502   \u2514\u2500 Save to SQLite db/{db_name}.db             \u2502\n\u2502    \u2502   \u2514\u2500 Return: (study, best_trials)                   \u2502\n\u2502    \u2514\u2500 Display: Best 5 trials with metrics                \u2502\n\u2502                                                             \u2502\n\u2502  Session State:                                            \u2502\n\u2502    data_dict, optimizer, study, best_trials, db_name      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n  \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         PAGE 3: SHAP ANALYSIS (3_shap_analysis.py)         \u2502\n\u2502      Model Explanation &amp; AI-Powered Reporting              \u2502\n\u2502                                                             \u2502\n\u2502  STEP 1: Trial Selection                                  \u2502\n\u2502    \u251c\u2500 Dropdown: Select from best_trials                  \u2502\n\u2502    \u251c\u2500 Display: F1 scores, model type, all parameters     \u2502\n\u2502    \u2514\u2500 Session State: selected_trial                      \u2502\n\u2502                                                             \u2502\n\u2502  STEP 2: Model Training                                   \u2502\n\u2502    \u251c\u2500 Extract: params = selected_trial.params             \u2502\n\u2502    \u251c\u2500 Create: model = create_model(**params)             \u2502\n\u2502    \u251c\u2500 Train: model.fit(train_x, train_y)                 \u2502\n\u2502    \u2514\u2500 Session State: trained_model                       \u2502\n\u2502                                                             \u2502\n\u2502  STEP 3: SHAP Configuration &amp; Analysis                    \u2502\n\u2502    \u251c\u2500 Checkbox: Use Probability (default: True)          \u2502\n\u2502    \u251c\u2500 Checkbox: Use Subset (default: True)               \u2502\n\u2502    \u251c\u2500 Slider: Subset Size (10-200, default: 50)          \u2502\n\u2502    \u251c\u2500 Create: XAI(model, data, XAIConfig(...))           \u2502\n\u2502    \u2502   \u251c\u2500 Initialize SHAP Explainer (lazy-loaded)        \u2502\n\u2502    \u2502   \u251c\u2500 Calculate SHAP values                          \u2502\n\u2502    \u2502   \u2514\u2500 Compute performance metrics                    \u2502\n\u2502    \u2514\u2500 Session State: xai                                 \u2502\n\u2502                                                             \u2502\n\u2502  STEP 4: Visualizations (6 Tabs)                          \u2502\n\u2502    \u251c\u2500 TAB 1: Bar Plot                                    \u2502\n\u2502    \u2502   \u2514\u2500 xai.get_plot(\"bar\", max_display=10)           \u2502\n\u2502    \u251c\u2500 TAB 2: Beeswarm Plot                               \u2502\n\u2502    \u2502   \u2514\u2500 xai.get_plot(\"beeswarm\", max_display=10)      \u2502\n\u2502    \u251c\u2500 TAB 3: Violin Plot                                 \u2502\n\u2502    \u2502   \u2514\u2500 xai.get_plot(\"violin\", max_display=10)        \u2502\n\u2502    \u251c\u2500 TAB 4: Heatmap                                     \u2502\n\u2502    \u2502   \u2514\u2500 xai.get_plot(\"heatmap\", max_display=50)       \u2502\n\u2502    \u251c\u2500 TAB 5: Waterfall Plot                              \u2502\n\u2502    \u2502   \u251c\u2500 Slider: Select sample index                   \u2502\n\u2502    \u2502   \u2514\u2500 xai.get_plot(\"waterfall\", index=idx)          \u2502\n\u2502    \u2514\u2500 TAB 6: Confusion Matrix                            \u2502\n\u2502        \u2514\u2500 xai.plot_confusion_matrix() \u2192 matplotlib fig   \u2502\n\u2502                                                             \u2502\n\u2502  STEP 5: AI-Powered Report Generation                     \u2502\n\u2502    \u251c\u2500 LLM Provider: Selectbox (google/openai/anthropic)  \u2502\n\u2502    \u251c\u2500 API Key: password_input                            \u2502\n\u2502    \u251c\u2500 Model Name: text_input                             \u2502\n\u2502    \u251c\u2500 Dataset Context (optional):                        \u2502\n\u2502    \u2502   \u251c\u2500 URL, Description, Features, Target            \u2502\n\u2502    \u2502   \u2514\u2500 Stored in dataset_info dict                    \u2502\n\u2502    \u251c\u2500 Generate Report:                                   \u2502\n\u2502    \u2502   \u251c\u2500 xai.get_report() \u2192 dict of metrics             \u2502\n\u2502    \u2502   \u251c\u2500 Generate SHAP plot images                      \u2502\n\u2502    \u2502   \u251c\u2500 xai.generate_report_with_langchain(...)        \u2502\n\u2502    \u2502   \u2502   \u251c\u2500 Initialize ChatGoogleGenerativeAI/OpenAI   \u2502\n\u2502    \u2502   \u2502   \u251c\u2500 Create multimodal prompt                   \u2502\n\u2502    \u2502   \u2502   \u251c\u2500 Include base64-encoded plot images         \u2502\n\u2502    \u2502   \u2502   \u2514\u2500 Return: markdown report string             \u2502\n\u2502    \u2502   \u2514\u2500 Display report + Download button               \u2502\n\u2502    \u2514\u2500 Session State: report, shap_images                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/frontend-quick-reference.html#page-by-page-functionality-matrix","title":"Page-by-Page Functionality Matrix","text":"Page File Primary Class Inputs Outputs Key Operations 1 <code>1_dataset_selection.py</code> N/A (utilities) CSV or UCI ID <code>file_path</code>, <code>dataset_df</code> Load, preview, transform, save 2 <code>2_optimization.py</code> <code>DataPreparation</code>, <code>Optimizer</code> <code>file_path</code>, trial count <code>best_trials</code>, <code>db_name</code> Split, normalize, optimize 3 <code>3_shap_analysis.py</code> <code>XAI</code> <code>best_trials</code>, <code>data_dict</code> <code>report</code>, <code>shap_images</code> Explain, visualize, generate"},{"location":"guides/frontend-quick-reference.html#model-support-matrix","title":"Model Support Matrix","text":""},{"location":"guides/frontend-quick-reference.html#26-total-models","title":"26 Total Models","text":"Category Count Examples Quantum Models 18 DataReuploading, CircuitCentric, QuantumKitchenSinks, DressedQuantumCircuit, QuantumMetricLearner, QuantumBoltzmannMachine, TreeTensorClassifier, QuanvolutionalNeuralNetwork, WeiNet, Separable variants, Convolutional variants, IQP, ProjectedQuantumKernel Classical Models 8 SVC, LinearSVC, MLPClassifier, Perceptron"},{"location":"guides/frontend-quick-reference.html#key-parameters-by-model-type","title":"Key Parameters by Model Type","text":"<p>All Quantum Models: - <code>max_vmap</code>: [1] - <code>batch_size</code>: [32] - <code>learning_rate</code>: [0.001, 0.01, 0.1] - <code>n_layers</code>: [1, 5, 10] - Model-specific: observable_type, n_qfeatures, repeats, C, gamma, etc.</p> <p>Classical Models: - <code>C</code> (SVC): [0.1, 1, 10, 100] - <code>gamma</code> (SVC): [0.001, 0.01, 0.1, 1] - <code>alpha</code> (MLP): [0.01, 0.001, 0.0001] - <code>hidden_layer_sizes</code> (MLP): [(100,), (10,10,10,10), (50,10,5)]</p>"},{"location":"guides/frontend-quick-reference.html#external-service-integration-points","title":"External Service Integration Points","text":"Service Library Purpose Page Used API Required UCI ML Repository <code>ucimlrepo</code> Dataset download 1 No Optuna <code>optuna</code> Hyperparameter tuning 2 No SQLite <code>sqlite3</code> Trial persistence 2 No Scikit-learn <code>sklearn</code> Classical ML &amp; metrics 2, 3 No PennyLane <code>pennylane</code> Quantum circuits 2 No SHAP <code>shap</code> Explainability 3 No LangChain <code>langchain</code> LLM integration 3 Yes Google GenAI <code>langchain_google_genai</code> Gemini models 3 Yes OpenAI <code>langchain_openai</code> GPT models 3 Yes"},{"location":"guides/frontend-quick-reference.html#session-state-initialization-flow","title":"Session State Initialization &amp; Flow","text":""},{"location":"guides/frontend-quick-reference.html#initialization-apppy","title":"Initialization (app.py)","text":"<pre><code>initialize_session_state()\nsession_defaults = {\n    \"uploaded_file\": None,\n    \"file_location\": None,\n    \"x_columns\": None,\n    \"y_column\": None,\n    \"DB_NAME\": None,\n    \"study_name\": None,\n    \"n_trials\": 100,\n    \"optimizer\": None,\n    \"process_running\": False,\n    \"start_visualization\": False,\n}\n</code></pre>"},{"location":"guides/frontend-quick-reference.html#per-page-updates","title":"Per-Page Updates","text":"<ul> <li>Page 1: Adds dataset_loaded, dataset_df, file_path, target_column, feature_columns</li> <li>Page 2: Adds data_dict, study, best_trials, optimization_complete</li> <li>Page 3: Adds selected_trial, trained_model, xai, report, shap_images</li> </ul>"},{"location":"guides/frontend-quick-reference.html#data-transformation-pipeline","title":"Data Transformation Pipeline","text":"<pre><code>INPUT: CSV File\n  \u2193\nSTEP 1: Read CSV\n  \u2193\nSTEP 2: Select Columns (X_cols, y_col)\n  \u2193\nSTEP 3: Handle Missing Values (dropna)\n  \u2193\nSTEP 4: Transform Target\n  unique_class[0] \u2192 -1\n  unique_class[1] \u2192 1\n  \u2193\nSTEP 5: Normalize Features\n  StandardScaler.fit_transform(X)\n  \u2193\nSTEP 6: Train/Test Split\n  X_train (75%), X_test (25%)\n  y_train (75%), y_test (25%)\n  \u2193\nOUTPUT: data_dict = {\n    \"train_x\": numpy array,\n    \"test_x\": numpy array,\n    \"train_y\": numpy array with {-1, 1},\n    \"test_y\": numpy array with {-1, 1}\n}\n</code></pre>"},{"location":"guides/frontend-quick-reference.html#visualization-types-reference","title":"Visualization Types Reference","text":""},{"location":"guides/frontend-quick-reference.html#shap-plots-page-3","title":"SHAP Plots (Page 3)","text":"Plot Type Method Purpose Parameters Bar <code>xai.get_plot(\"bar\")</code> Global feature importance max_display=10 Beeswarm <code>xai.get_plot(\"beeswarm\")</code> Feature value impact max_display=10 Violin <code>xai.get_plot(\"violin\")</code> SHAP value distribution max_display=10 Heatmap <code>xai.get_plot(\"heatmap\")</code> Instance-level analysis max_display=50 Waterfall <code>xai.get_plot(\"waterfall\")</code> Single prediction breakdown index=0 Confusion Matrix <code>xai.plot_confusion_matrix()</code> Classification performance standard"},{"location":"guides/frontend-quick-reference.html#optuna-plots-visualization-in-supportpy","title":"Optuna Plots (Visualization in support.py)","text":"Plot Type Method Purpose Timeline <code>optuna.visualization.plot_timeline()</code> Trial execution timeline Parameter Importance <code>optuna.visualization.plot_param_importances()</code> Which params matter most Optimization History <code>optuna.visualization.plot_optimization_history()</code> F1 score over trials"},{"location":"guides/frontend-quick-reference.html#performance-metrics-display","title":"Performance Metrics Display","text":"Metric Method Formula F1 Score <code>f1_score(y_true, y_pred)</code> 2 * (precision * recall) / (precision + recall) Accuracy <code>accuracy_score(y_true, y_pred)</code> Correct / Total Precision <code>precision_score(y_true, y_pred)</code> TP / (TP + FP) Recall <code>recall_score(y_true, y_pred)</code> TP / (TP + FN) MCC <code>matthews_corrcoef(y_true, y_pred)</code> Correlation coefficient AUC <code>roc_auc_score(y_true, y_proba)</code> Area under ROC curve"},{"location":"guides/frontend-quick-reference.html#error-handling-validation","title":"Error Handling &amp; Validation","text":""},{"location":"guides/frontend-quick-reference.html#key-validation-points","title":"Key Validation Points","text":"<ol> <li>Page 1 (Dataset Selection)</li> <li>CSV format validation</li> <li>Binary target verification</li> <li>Missing value detection</li> <li> <p>Feature column selection (minimum 1 required)</p> </li> <li> <p>Page 2 (Optimization)</p> </li> <li>File path verification</li> <li>Data dict completeness check</li> <li>Model instantiation safety (try/except)</li> <li> <p>Training error recovery</p> </li> <li> <p>Page 3 (SHAP Analysis)</p> </li> <li>Trial selection requirement</li> <li>Model training exception handling</li> <li>SHAP computation fallback</li> <li>API key validation for report generation</li> </ol>"},{"location":"guides/frontend-quick-reference.html#exception-handling-pattern","title":"Exception Handling Pattern","text":"<pre><code>try:\n    # Operation\nexcept Exception as e:\n    st.error(f\"Error message: {e}\")\n    st.exception(e)  # Debug info\n    return False\n</code></pre>"},{"location":"guides/frontend-quick-reference.html#file-io-operations","title":"File I/O Operations","text":"Operation Location Path Purpose Upload CSV support.py <code>./uploaded_data/{filename}</code> Temporary data storage Prepared Dataset 1_dataset_selection.py <code>data/{dataset_name}.csv</code> Processed data input for Opt Optuna Database optimizer.py <code>db/{db_name}.db</code> SQLite trial persistence Plot Output 3_shap_analysis.py <code>outputs/{plot_name}.png</code> User-saved visualizations Report Download 3_shap_analysis.py Browser download Markdown file"},{"location":"guides/frontend-quick-reference.html#key-metrics-tracked-during-optimization","title":"Key Metrics Tracked During Optimization","text":"<p>For each trial, stored in <code>trial.user_attrs</code>:</p> <pre><code>{\n    \"Quantum_f1_score\": float,      # F1 for quantum models\n    \"Quantum_accuracy\": float,      # Accuracy for quantum models\n    \"Quantum_score\": float,         # Generic score for quantum\n    \"Classical_f1_score\": float,    # F1 for classical models\n    \"Classical_accuracy\": float,    # Accuracy for classical models\n    \"Classical_score\": float,       # Generic score for classical\n}\n</code></pre> <p>Plus in trial object: - <code>number</code>: Trial ID - <code>state</code>: TrialState (COMPLETE, FAIL, RUNNING, etc.) - <code>value</code>: Objective value (F1 score) - <code>params</code>: All hyperparameters - <code>datetime_start</code>, <code>datetime_complete</code>: Execution timing</p>"},{"location":"guides/frontend-quick-reference.html#configuration-files","title":"Configuration Files","text":""},{"location":"guides/frontend-quick-reference.html#streamlitconfigtoml","title":"<code>.streamlit/config.toml</code>","text":"<pre><code>[theme]\nbase = \"dark\"\nprimaryColor = \"#8e44ad\"              # Purple accent\nbackgroundColor = \"#121212\"           # Dark background\nsecondaryBackgroundColor = \"#1c1c1c\"  # Panel background\ntextColor = \"#dcdcdc\"                 # Light text\nfont = \"sans serif\"\n</code></pre>"},{"location":"guides/frontend-quick-reference.html#custom-css-apppy","title":"Custom CSS (app.py)","text":"<ul> <li><code>.main-title</code>: Purple, 3em, center-aligned</li> <li><code>.description</code>: Light grey, 1.2em, center-aligned</li> <li><code>.stButton&gt;button</code>: Purple background, hover effect</li> </ul>"},{"location":"guides/frontend-quick-reference.html#typical-workflow-timing","title":"Typical Workflow Timing","text":"Stage Operation Typical Duration 1 Load UCI dataset 1-5 seconds 1 Configure columns Instant 2 Prepare data &lt;1 second 2 Optimize (100 trials) 5-60 minutes (model-dependent) 3 Train model 1-10 seconds 3 Calculate SHAP values 10-60 seconds 3 Generate report (with LLM) 30-120 seconds"},{"location":"guides/frontend-quick-reference.html#common-use-cases","title":"Common Use Cases","text":""},{"location":"guides/frontend-quick-reference.html#use-case-1-quick-exploration","title":"Use Case 1: Quick Exploration","text":"<ol> <li>Load popular UCI dataset</li> <li>Run 20-50 trials</li> <li>View best trial's SHAP plots</li> </ol>"},{"location":"guides/frontend-quick-reference.html#use-case-2-comprehensive-analysis","title":"Use Case 2: Comprehensive Analysis","text":"<ol> <li>Upload custom CSV</li> <li>Run 100-200 trials</li> <li>Generate AI report with all SHAP visualizations</li> <li>Download markdown report</li> </ol>"},{"location":"guides/frontend-quick-reference.html#use-case-3-quantum-vs-classical-comparison","title":"Use Case 3: Quantum vs Classical Comparison","text":"<ol> <li>Load dataset</li> <li>Run optimization (tracks both model types)</li> <li>Compare F1 scores in results table</li> <li>Train and analyze best quantum model</li> <li>Generate report with quantum-specific insights</li> </ol>"},{"location":"guides/frontend-quick-reference.html#tips-for-users","title":"Tips for Users","text":""},{"location":"guides/frontend-quick-reference.html#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Use smaller subset for SHAP (50 samples) instead of full dataset</li> <li>Start with 50 trials, then run more if needed</li> <li>Use probability predictions for faster SHAP calculation</li> </ul>"},{"location":"guides/frontend-quick-reference.html#better-results","title":"Better Results","text":"<ul> <li>Ensure sufficient data (100+ samples minimum)</li> <li>Balance dataset classes when possible</li> <li>Run 100+ trials for reliable optimization</li> </ul>"},{"location":"guides/frontend-quick-reference.html#report-generation","title":"Report Generation","text":"<ul> <li>Use faster models (Gemini Flash) for quick feedback</li> <li>Use advanced models (GPT-4, Claude 3) for detailed analysis</li> <li>Provide dataset description for context</li> </ul>"},{"location":"guides/streamlit-guide.html","title":"QuOptuna Streamlit Interface Guide","text":""},{"location":"guides/streamlit-guide.html#quoptuna-streamlit-interface-guide","title":"QuOptuna Streamlit Interface Guide","text":""},{"location":"guides/streamlit-guide.html#overview","title":"Overview","text":"<p>The QuOptuna Streamlit interface provides a comprehensive, user-friendly workflow for quantum-enhanced machine learning optimization. This guide covers the enhanced multi-page application structure and features.</p>"},{"location":"guides/streamlit-guide.html#new-features","title":"New Features","text":""},{"location":"guides/streamlit-guide.html#multi-page-application","title":"\ud83c\udfa8 Multi-Page Application","text":"<p>The interface is now organized into dedicated pages for each workflow stage:</p> <ol> <li>Home - Welcome page with quick start guide</li> <li>\ud83d\udcca Dataset Selection - Load and configure datasets</li> <li>\ud83c\udfaf Optimization - Data preparation and hyperparameter optimization</li> <li>\ud83d\udd0d SHAP Analysis - Model training and explainable AI</li> </ol>"},{"location":"guides/streamlit-guide.html#dataset-selection-page","title":"\ud83d\udcca Dataset Selection Page","text":"<p>Location: <code>src/quoptuna/frontend/pages/1_dataset_selection.py</code></p>"},{"location":"guides/streamlit-guide.html#features","title":"Features:","text":"<p>UCI ML Repository Integration - Popular datasets pre-configured (Statlog, Blood, Banknote, Heart Disease, Ionosphere) - Custom UCI ID support for accessing any UCI dataset - Automatic metadata display (instances, features, description)</p> <p>Custom Dataset Upload - CSV file upload support - Automatic data preview - Missing value detection</p> <p>Data Configuration - Interactive column selection for features and target - Target transformation to -1/+1 encoding - Automatic preprocessing (missing value removal) - Data validation before proceeding</p> <p>Example Usage: </p><pre><code># Programmatically access the same functionality\nfrom ucimlrepo import fetch_ucirepo\n\ndataset = fetch_ucirepo(id=143)  # Statlog dataset\nX = dataset.data.features\ny = dataset.data.targets\n</code></pre><p></p>"},{"location":"guides/streamlit-guide.html#optimization-page","title":"\ud83c\udfaf Optimization Page","text":"<p>Location: <code>src/quoptuna/frontend/pages/2_optimization.py</code></p>"},{"location":"guides/streamlit-guide.html#features_1","title":"Features:","text":"<p>Data Preparation - Automatic train/test splitting - Feature scaling - Numpy array conversion for model compatibility - Dataset summary statistics</p> <p>Hyperparameter Optimization - Configurable number of trials (10-200) - Custom database and study names - Progress tracking - Real-time status updates</p> <p>Results Visualization - Best trial ranking - Performance metrics (Quantum F1, Classical F1) - Parameter inspection - Model type comparison</p> <p>Example Usage: </p><pre><code>from quoptuna import DataPreparation, Optimizer\n\n# Prepare data\ndata_prep = DataPreparation(file_path=\"data/my_data.csv\", x_cols=features, y_col=\"target\")\ndata_dict = data_prep.get_data(output_type=\"2\")\n\n# Run optimization\noptimizer = Optimizer(db_name=\"my_exp\", study_name=\"trial_1\", data=data_dict)\nstudy, best_trials = optimizer.optimize(n_trials=100)\n</code></pre><p></p>"},{"location":"guides/streamlit-guide.html#shap-analysis-page","title":"\ud83d\udd0d SHAP Analysis Page","text":"<p>Location: <code>src/quoptuna/frontend/pages/3_shap_analysis.py</code></p>"},{"location":"guides/streamlit-guide.html#features_2","title":"Features:","text":"<p>Trial Selection - Dropdown with formatted trial information - Performance metrics display - Full parameter inspection</p> <p>Model Training - One-click model training with optimized parameters - Training status feedback - Model storage in session state</p> <p>SHAP Configuration - Probability vs class prediction toggle - Subset analysis option - Configurable subset size</p> <p>Visualization Suite Six types of SHAP visualizations:</p> <ol> <li>Bar Plot - Overall feature importance</li> <li>Beeswarm Plot - Feature value impact distribution</li> <li>Violin Plot - SHAP value distributions</li> <li>Heatmap - Instance-level contributions</li> <li>Waterfall Plot - Individual prediction explanations</li> <li>Confusion Matrix - Model performance</li> </ol> <p>AI Report Generation - Support for multiple LLM providers (Google, OpenAI, Anthropic) - Configurable model selection - Optional dataset context - Markdown export</p> <p>Example Usage: </p><pre><code>from quoptuna import XAI\nfrom quoptuna.backend.xai.xai import XAIConfig\n\n# Configure XAI\nconfig = XAIConfig(use_proba=True, onsubset=True, subset_size=50)\nxai = XAI(model=trained_model, data=data_dict, config=config)\n\n# Generate visualizations\nbar_plot = xai.get_plot(\"bar\", max_display=10, class_index=1)\nbeeswarm = xai.get_plot(\"beeswarm\", max_display=10, class_index=1)\n\n# Generate AI report\nreport = xai.generate_report_with_langchain(\n    provider=\"google\",\n    api_key=\"your-api-key\",\n    model_name=\"models/gemini-2.0-flash-exp\"\n)\n</code></pre><p></p>"},{"location":"guides/streamlit-guide.html#application-structure","title":"Application Structure","text":"<pre><code>src/quoptuna/frontend/\n\u251c\u2500\u2500 app.py                  # Main application entry point\n\u251c\u2500\u2500 main_page.py            # Home page with quick start guide\n\u251c\u2500\u2500 sidebar.py              # Legacy sidebar (kept for compatibility)\n\u251c\u2500\u2500 support.py              # Helper functions\n\u2514\u2500\u2500 pages/\n    \u251c\u2500\u2500 1_dataset_selection.py    # Dataset loading and configuration\n    \u251c\u2500\u2500 2_optimization.py         # Data prep and optimization\n    \u2514\u2500\u2500 3_shap_analysis.py        # SHAP analysis and reporting\n</code></pre>"},{"location":"guides/streamlit-guide.html#session-state-management","title":"Session State Management","text":"<p>The application uses Streamlit's session state to maintain data across pages:</p> <pre><code># Key session state variables\nst.session_state = {\n    # Dataset Selection\n    \"dataset_loaded\": bool,\n    \"dataset_df\": pd.DataFrame,\n    \"dataset_name\": str,\n    \"file_path\": str,\n    \"target_column\": str,\n    \"feature_columns\": list,\n\n    # Optimization\n    \"data_dict\": dict,\n    \"optimizer\": Optimizer,\n    \"study\": optuna.Study,\n    \"best_trials\": list,\n    \"optimization_complete\": bool,\n\n    # SHAP Analysis\n    \"selected_trial\": FrozenTrial,\n    \"trained_model\": Model,\n    \"xai\": XAI,\n    \"report\": str,\n}\n</code></pre>"},{"location":"guides/streamlit-guide.html#workflow-example","title":"Workflow Example","text":""},{"location":"guides/streamlit-guide.html#complete-workflow-through-ui","title":"Complete Workflow Through UI","text":"<ol> <li> <p>Start Application </p><pre><code>quoptuna --start\n</code></pre><p></p> </li> <li> <p>Dataset Selection</p> </li> <li>Navigate to \"\ud83d\udcca Dataset Selection\"</li> <li>Select \"UCI ML Repository\" tab</li> <li>Choose \"Statlog (Australian Credit Approval)\"</li> <li>Click \"Load UCI Dataset\"</li> <li>Review metadata</li> <li>Select target column (will auto-select features)</li> <li>Enable \"Transform target values to -1 and 1\"</li> <li> <p>Click \"Save Configuration\"</p> </li> <li> <p>Optimization</p> </li> <li>Navigate to \"\ud83c\udfaf Optimization\"</li> <li>Click \"Prepare Data for Training\"</li> <li>Review dataset summary</li> <li>Set number of trials (e.g., 100)</li> <li>Enter database name (e.g., \"Statlog\")</li> <li>Enter study name (e.g., \"Statlog_Trial_1\")</li> <li>Click \"Start Optimization\"</li> <li>Wait for completion</li> <li> <p>Review best trials</p> </li> <li> <p>SHAP Analysis</p> </li> <li>Navigate to \"\ud83d\udd0d SHAP Analysis\"</li> <li>Select best trial from dropdown</li> <li>Review trial details</li> <li>Click \"Train Model\"</li> <li>Configure SHAP settings</li> <li>Click \"Run SHAP Analysis\"</li> <li>Navigate through visualization tabs</li> <li>(Optional) Generate AI report with LLM</li> </ol>"},{"location":"guides/streamlit-guide.html#customization","title":"Customization","text":""},{"location":"guides/streamlit-guide.html#adding-new-pages","title":"Adding New Pages","text":"<p>Create a new page in <code>src/quoptuna/frontend/pages/</code>:</p> <pre><code># src/quoptuna/frontend/pages/4_my_feature.py\n\nimport streamlit as st\n\ndef main():\n    st.set_page_config(\n        page_title=\"My Feature - QuOptuna\",\n        page_icon=\"\ud83c\udfa8\",\n        layout=\"wide\"\n    )\n\n    st.title(\"\ud83c\udfa8 My Feature\")\n    # Your implementation here\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Streamlit automatically detects and adds pages to the sidebar.</p>"},{"location":"guides/streamlit-guide.html#modifying-visualizations","title":"Modifying Visualizations","text":"<p>Edit <code>src/quoptuna/frontend/pages/3_shap_analysis.py</code>:</p> <pre><code>def display_shap_plots():\n    # Add custom visualization\n    with tabs[6]:  # Add new tab\n        st.markdown(\"### Custom Visualization\")\n        # Your custom plot code\n</code></pre>"},{"location":"guides/streamlit-guide.html#customizing-styles","title":"Customizing Styles","text":"<p>Edit <code>src/quoptuna/frontend/app.py</code>:</p> <pre><code>st.markdown(\n    \"\"\"\n    &lt;style&gt;\n    /* Your custom CSS */\n    .main-title {\n        color: #your-color;\n    }\n    &lt;/style&gt;\n    \"\"\",\n    unsafe_allow_html=True\n)\n</code></pre>"},{"location":"guides/streamlit-guide.html#best-practices","title":"Best Practices","text":""},{"location":"guides/streamlit-guide.html#performance","title":"Performance","text":"<ol> <li>Use Data Subsets for SHAP</li> <li>Limit subset size to 50-100 samples for faster computation</li> <li> <p>Use full dataset only when necessary</p> </li> <li> <p>Optimize Trial Count</p> </li> <li>Start with 50-100 trials for initial exploration</li> <li> <p>Increase to 100-200 for production models</p> </li> <li> <p>Cache Results</p> </li> <li>Session state persists data during the session</li> <li>Database stores optimization results permanently</li> </ol>"},{"location":"guides/streamlit-guide.html#user-experience","title":"User Experience","text":"<ol> <li>Clear Navigation</li> <li>Use descriptive page titles</li> <li>Provide progress indicators</li> <li> <p>Show clear error messages</p> </li> <li> <p>Data Validation</p> </li> <li>Validate inputs before processing</li> <li>Provide helpful warning messages</li> <li> <p>Guide users to correct issues</p> </li> <li> <p>Documentation</p> </li> <li>Use expanders for detailed information</li> <li>Provide tooltips for parameters</li> <li>Link to external documentation</li> </ol>"},{"location":"guides/streamlit-guide.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/streamlit-guide.html#common-issues","title":"Common Issues","text":"<p>\"Please complete Dataset Selection first\" - Ensure you've saved the dataset configuration on the Dataset Selection page - Check that <code>file_path</code> is in session state</p> <p>\"Please prepare data first\" - Click \"Prepare Data for Training\" on the Optimization page - Verify <code>data_dict</code> is populated in session state</p> <p>\"Please run SHAP analysis first\" - Ensure model is trained - Click \"Run SHAP Analysis\" button - Check for error messages</p> <p>Plots Not Displaying - Check internet connection (for base64 encoded images) - Verify SHAP calculation completed successfully - Try reducing subset size</p> <p>Report Generation Fails - Verify API key is valid and active - Check model name format (e.g., \"models/gemini-2.0-flash-exp\") - Ensure internet connection is stable - Try a different LLM provider</p>"},{"location":"guides/streamlit-guide.html#debug-mode","title":"Debug Mode","text":"<p>Enable debug information:</p> <pre><code>import streamlit as st\n\n# Add to any page\nwith st.expander(\"Debug Info\"):\n    st.write(\"Session State:\", st.session_state)\n</code></pre>"},{"location":"guides/streamlit-guide.html#api-integration","title":"API Integration","text":"<p>The Streamlit interface can be used alongside the Python API:</p> <pre><code># Load data from Streamlit session\nimport streamlit as st\nfrom quoptuna import Optimizer\n\n# Use optimizer created in UI\nif \"optimizer\" in st.session_state:\n    optimizer = st.session_state[\"optimizer\"]\n    study = optimizer.study\n\n    # Continue with Python API\n    for trial in study.best_trials:\n        print(trial.params)\n</code></pre>"},{"location":"guides/streamlit-guide.html#contributing","title":"Contributing","text":"<p>To contribute to the Streamlit interface:</p> <ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Add/modify pages in <code>src/quoptuna/frontend/pages/</code></li> <li>Test thoroughly with different datasets</li> <li>Update this documentation</li> <li>Submit a pull request</li> </ol>"},{"location":"guides/streamlit-guide.html#support","title":"Support","text":"<p>For issues or questions: - GitHub Issues: Report bugs - Documentation: Full docs - Examples: See <code>docs/examples.md</code></p>"},{"location":"guides/streamlit-guide.html#license","title":"License","text":"<p>MIT License - See LICENSE file for details</p>"},{"location":"guides/user-guide.html","title":"User Guide","text":""},{"location":"guides/user-guide.html#user-guide","title":"User Guide","text":""},{"location":"guides/user-guide.html#introduction","title":"Introduction","text":"<p>QuOptuna is a comprehensive platform for quantum-enhanced machine learning optimization. This guide will walk you through the complete workflow from dataset selection to model analysis and report generation.</p>"},{"location":"guides/user-guide.html#workflow-overview","title":"Workflow Overview","text":"<p>The QuOptuna workflow consists of four main stages:</p> <ol> <li>Dataset Selection - Load and prepare your data</li> <li>Optimization - Find the best hyperparameters</li> <li>Model Training - Train models with optimized parameters</li> <li>SHAP Analysis - Understand and explain model behavior</li> </ol>"},{"location":"guides/user-guide.html#getting-started","title":"Getting Started","text":""},{"location":"guides/user-guide.html#installation","title":"Installation","text":"<p>Install QuOptuna using UV (recommended) or pip:</p> <pre><code># Using UV (recommended)\nuv pip install quoptuna\n\n# Using pip\npip install quoptuna\n</code></pre>"},{"location":"guides/user-guide.html#launching-the-application","title":"Launching the Application","text":"<p>Start the Streamlit interface:</p> <pre><code>quoptuna --start\n</code></pre> <p>Or using Python:</p> <pre><code>python -m quoptuna.frontend.app run\n</code></pre>"},{"location":"guides/user-guide.html#dataset-selection","title":"Dataset Selection","text":""},{"location":"guides/user-guide.html#uci-ml-repository","title":"UCI ML Repository","text":"<p>QuOptuna provides easy access to datasets from the UCI Machine Learning Repository:</p> <ol> <li>Navigate to the Dataset Selection page</li> <li>Select UCI ML Repository tab</li> <li>Choose from popular datasets or enter a custom UCI ID</li> <li>Click Load UCI Dataset</li> </ol> <p>Popular Datasets: - Statlog (Australian Credit Approval) - ID: 143 - Blood Transfusion Service Center - ID: 176 - Banknote Authentication - ID: 267 - Heart Disease - ID: 45 - Ionosphere - ID: 225</p>"},{"location":"guides/user-guide.html#custom-dataset-upload","title":"Custom Dataset Upload","text":"<p>To use your own dataset:</p> <ol> <li>Navigate to the Upload Custom Dataset tab</li> <li>Upload a CSV file</li> <li>Configure target and feature columns</li> <li>Apply target transformation if needed</li> </ol>"},{"location":"guides/user-guide.html#data-configuration","title":"Data Configuration","text":"<p>Important: QuOptuna requires binary classification targets to be encoded as <code>-1</code> and <code>1</code>.</p> <ol> <li>Select Target Column: Choose the column you want to predict</li> <li>Select Features: Choose the features to use for prediction</li> <li>Target Transformation: Map your target values to -1 and 1</li> <li>Handle Missing Values: QuOptuna will automatically remove rows with missing values</li> </ol> <p>Click Save Configuration to proceed to the next step.</p>"},{"location":"guides/user-guide.html#data-preparation-optimization","title":"Data Preparation &amp; Optimization","text":""},{"location":"guides/user-guide.html#data-preparation","title":"Data Preparation","text":"<p>Once your dataset is configured:</p> <ol> <li>Review the dataset summary (rows, columns, target distribution)</li> <li>Click Prepare Data for Training</li> <li>QuOptuna will automatically:</li> <li>Split data into training and test sets</li> <li>Scale features</li> <li>Convert to the format required by models</li> </ol>"},{"location":"guides/user-guide.html#hyperparameter-optimization","title":"Hyperparameter Optimization","text":"<p>Configure and run optimization:</p> <ol> <li>Database Name: Name for storing optimization results</li> <li>Study Name: Unique identifier for this optimization study</li> <li>Number of Trials: How many hyperparameter combinations to try (recommended: 50-200)</li> </ol> <p>Click Start Optimization to begin. This will: - Test multiple model types (both quantum and classical) - Try different hyperparameter combinations - Track the best performing configurations</p> <p>Model Types Tested: - Data Reuploading Classifier (Quantum) - Circuit-Centric Classifier (Quantum) - Quantum Kitchen Sinks (Quantum) - Support Vector Classifier (Classical) - Multi-Layer Perceptron (Classical) - And more...</p>"},{"location":"guides/user-guide.html#understanding-results","title":"Understanding Results","text":"<p>After optimization completes, you'll see: - Best Trials: Top performing configurations - F1 Scores: Performance metrics for quantum and classical approaches - Hyperparameters: The configuration for each trial</p>"},{"location":"guides/user-guide.html#shap-analysis-reporting","title":"SHAP Analysis &amp; Reporting","text":""},{"location":"guides/user-guide.html#trial-selection","title":"Trial Selection","text":"<ol> <li>Navigate to the SHAP Analysis page</li> <li>Select a trial from the dropdown (sorted by performance)</li> <li>Review the trial details and parameters</li> </ol>"},{"location":"guides/user-guide.html#model-training","title":"Model Training","text":"<ol> <li>Click Train Model to train the selected model</li> <li>The model will be trained on your data with the optimized hyperparameters</li> </ol>"},{"location":"guides/user-guide.html#shap-analysis","title":"SHAP Analysis","text":"<p>Configure SHAP analysis:</p> <ul> <li>Use Probability Predictions: Use probability outputs instead of class predictions</li> <li>Use Subset of Data: Analyze a subset for faster computation</li> <li>Subset Size: Number of samples to analyze (recommended: 50-100)</li> </ul> <p>Click Run SHAP Analysis to calculate SHAP values.</p>"},{"location":"guides/user-guide.html#shap-visualizations","title":"SHAP Visualizations","text":"<p>QuOptuna provides multiple visualization types:</p>"},{"location":"guides/user-guide.html#bar-plot","title":"Bar Plot","text":"<p>Shows the mean absolute SHAP value for each feature, indicating overall importance.</p> <p>Use Case: Quick overview of feature importance</p>"},{"location":"guides/user-guide.html#beeswarm-plot","title":"Beeswarm Plot","text":"<p>Shows the distribution of SHAP values, with color indicating feature value (red = high, blue = low).</p> <p>Use Case: Understanding how feature values affect predictions</p>"},{"location":"guides/user-guide.html#violin-plot","title":"Violin Plot","text":"<p>Shows the distribution of SHAP values for each feature.</p> <p>Use Case: Understanding the variability in feature impact</p>"},{"location":"guides/user-guide.html#heatmap","title":"Heatmap","text":"<p>Shows SHAP values for individual instances.</p> <p>Use Case: Instance-level analysis, finding patterns in predictions</p>"},{"location":"guides/user-guide.html#waterfall-plot","title":"Waterfall Plot","text":"<p>Explains how features contribute to a single prediction.</p> <p>Use Case: Understanding individual predictions in detail</p>"},{"location":"guides/user-guide.html#confusion-matrix","title":"Confusion Matrix","text":"<p>Shows classification performance.</p> <p>Use Case: Evaluating overall model accuracy</p>"},{"location":"guides/user-guide.html#report-generation","title":"Report Generation","text":"<p>Generate comprehensive AI-powered reports:</p> <ol> <li>Select LLM Provider: Google (Gemini), OpenAI (GPT), or Anthropic (Claude)</li> <li>Enter API Key: Your API key for the selected provider</li> <li>Model Name: Specific model to use (e.g., \"models/gemini-2.0-flash-exp\")</li> <li>Dataset Information (optional): Add context about your dataset</li> </ol> <p>Click Generate Report to create a detailed analysis report.</p> <p>Report Includes: - Performance metrics analysis - SHAP value interpretation - Feature importance ranking - Risk and fairness assessment - Governance recommendations</p>"},{"location":"guides/user-guide.html#best-practices","title":"Best Practices","text":""},{"location":"guides/user-guide.html#optimization","title":"Optimization","text":"<ul> <li>Start Small: Begin with 50-100 trials to get quick results</li> <li>Increase Gradually: Use 100-200 trials for production models</li> <li>Monitor Performance: Check both quantum and classical model scores</li> <li>Save Studies: Use descriptive names for databases and studies</li> </ul>"},{"location":"guides/user-guide.html#shap-analysis_1","title":"SHAP Analysis","text":"<ul> <li>Use Subsets: Analyze 50-100 samples for faster computation</li> <li>Multiple Plots: Generate several plot types for comprehensive understanding</li> <li>Document Findings: Save plots and reports for future reference</li> <li>Understand Context: Consider domain knowledge when interpreting SHAP values</li> </ul>"},{"location":"guides/user-guide.html#report-generation_1","title":"Report Generation","text":"<ul> <li>Provide Context: Add dataset URL and description for better AI insights</li> <li>Choose Appropriate Models:</li> <li>Fast models (Gemini Flash): Quick exploratory reports</li> <li>Advanced models (GPT-4, Gemini Pro): Detailed production reports</li> <li>Review Carefully: AI-generated reports should be reviewed by domain experts</li> </ul>"},{"location":"guides/user-guide.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/user-guide.html#common-issues","title":"Common Issues","text":"<p>Dataset Loading Fails - Check UCI dataset ID is correct - Ensure CSV file is properly formatted - Verify file encoding (UTF-8 recommended)</p> <p>Optimization Errors - Ensure data has no missing values - Check target column has exactly 2 unique values - Verify sufficient samples for train/test split</p> <p>SHAP Analysis Slow - Reduce subset size - Use simpler model types - Check available memory</p> <p>Report Generation Fails - Verify API key is valid - Check internet connection - Ensure model name is correct - Try a different LLM provider</p>"},{"location":"guides/user-guide.html#advanced-features","title":"Advanced Features","text":""},{"location":"guides/user-guide.html#loading-previous-studies","title":"Loading Previous Studies","text":"<p>You can load and analyze previously run optimizations:</p> <ol> <li>Go to the Optimization page</li> <li>Enter the database name and study name</li> <li>Click Load Optimizer</li> <li>Results will be available for analysis</li> </ol>"},{"location":"guides/user-guide.html#batch-processing","title":"Batch Processing","text":"<p>For multiple datasets, you can: 1. Use the Python API directly (see API documentation) 2. Script the workflow using QuOptuna classes 3. Save results to different databases</p>"},{"location":"guides/user-guide.html#custom-models","title":"Custom Models","text":"<p>Advanced users can integrate custom models by: 1. Following the model interface in <code>quoptuna.backend.models</code> 2. Adding model configurations to the optimizer 3. See API documentation for details</p>"},{"location":"guides/user-guide.html#next-steps","title":"Next Steps","text":"<ul> <li>Explore the API Documentation for programmatic usage</li> <li>Check out Examples for common use cases</li> <li>Contribute on GitHub</li> </ul>"},{"location":"guides/user-guide.html#support","title":"Support","text":"<ul> <li>GitHub Issues: Report bugs or request features</li> <li>Documentation: Full documentation</li> <li>Community: Join our discussions on GitHub</li> </ul>"},{"location":"guides/workflow-builder-guide.html","title":"QuOptuna Workflow Builder User Guide","text":""},{"location":"guides/workflow-builder-guide.html#quoptuna-workflow-builder-user-guide","title":"QuOptuna Workflow Builder User Guide","text":""},{"location":"guides/workflow-builder-guide.html#overview","title":"Overview","text":"<p>The QuOptuna Workflow Builder is a visual drag-and-drop interface for creating machine learning pipelines with quantum and classical models. It integrates with the QuOptuna optimization framework to provide hyperparameter tuning and explainability analysis.</p>"},{"location":"guides/workflow-builder-guide.html#getting-started","title":"Getting Started","text":""},{"location":"guides/workflow-builder-guide.html#running-the-application","title":"Running the Application","text":"<ol> <li> <p>Start the services: </p><pre><code>docker compose up --build\n</code></pre><p></p> </li> <li> <p>Access the application:</p> </li> <li>Frontend: http://localhost:5173</li> <li>Backend API: http://localhost:8000</li> <li>API Documentation: http://localhost:8000/docs</li> </ol>"},{"location":"guides/workflow-builder-guide.html#creating-your-first-workflow","title":"Creating Your First Workflow","text":"<ol> <li>Navigate to the Workflow Builder page from the sidebar</li> <li>Drag nodes from the Node Palette onto the canvas</li> <li>Connect nodes by dragging from one node's output handle to another's input handle</li> <li>Configure each node by clicking on it (future feature)</li> <li>Click Run to execute the workflow</li> </ol>"},{"location":"guides/workflow-builder-guide.html#node-types","title":"Node Types","text":""},{"location":"guides/workflow-builder-guide.html#data-nodes","title":"Data Nodes","text":""},{"location":"guides/workflow-builder-guide.html#upload-csv","title":"Upload CSV","text":"<ul> <li>Purpose: Upload a dataset from a local CSV file</li> <li>Configuration:</li> <li>Select a CSV file from your computer</li> <li>Backend validates and stores the file</li> <li>Output: Dataset with columns and row count</li> <li>Usage: Use this as the starting point for workflows with custom data</li> </ul>"},{"location":"guides/workflow-builder-guide.html#uci-dataset","title":"UCI Dataset","text":"<ul> <li>Purpose: Fetch datasets from the UCI Machine Learning Repository</li> <li>Configuration:</li> <li>Dataset ID (e.g., 53 for Iris dataset)</li> <li>Output: Dataset loaded from UCI repository</li> <li>Popular Dataset IDs:</li> <li>53: Iris</li> <li>109: Wine Quality</li> <li>17: Breast Cancer Wisconsin</li> </ul>"},{"location":"guides/workflow-builder-guide.html#data-preview","title":"Data Preview","text":"<ul> <li>Purpose: View basic statistics and information about your dataset</li> <li>Input: Connected dataset</li> <li>Output:</li> <li>Shape (rows, columns)</li> <li>Data types</li> <li>Statistical summary (mean, std, min, max)</li> <li>First few rows</li> </ul>"},{"location":"guides/workflow-builder-guide.html#select-features","title":"Select Features","text":"<ul> <li>Purpose: Choose which columns to use as features (X) and target (y)</li> <li>Configuration:</li> <li><code>x_columns</code>: List of feature column names</li> <li><code>y_column</code>: Target column name</li> <li>Input: Dataset</li> <li>Output: Separated X and y data</li> </ul>"},{"location":"guides/workflow-builder-guide.html#preprocessing-nodes","title":"Preprocessing Nodes","text":""},{"location":"guides/workflow-builder-guide.html#traintest-split","title":"Train/Test Split","text":"<ul> <li>Purpose: Split data into training and testing sets</li> <li>Input: Feature-selected data (X and y)</li> <li>Configuration:</li> <li>Test size: 0.2 (default, 20% for testing)</li> <li>Random state: 42 (for reproducibility)</li> <li>Output:</li> <li><code>x_train</code>, <code>x_test</code></li> <li><code>y_train</code>, <code>y_test</code></li> <li>Note: Automatically handles scaling and encoding via QuOptuna's DataPreparation class</li> </ul>"},{"location":"guides/workflow-builder-guide.html#standard-scaler","title":"Standard Scaler","text":"<ul> <li>Purpose: Normalize features to zero mean and unit variance</li> <li>Input: Split data</li> <li>Output: Scaled data</li> <li>Note: Scaling is already handled by DataPreparation, this node is a pass-through for visual clarity</li> </ul>"},{"location":"guides/workflow-builder-guide.html#label-encoding","title":"Label Encoding","text":"<ul> <li>Purpose: Encode categorical target labels as integers</li> <li>Input: Data with target labels</li> <li>Output: Encoded labels</li> <li>Note: Encoding is already handled by DataPreparation, this node is a pass-through for visual clarity</li> </ul>"},{"location":"guides/workflow-builder-guide.html#model-nodes","title":"Model Nodes","text":""},{"location":"guides/workflow-builder-guide.html#quantum-model","title":"Quantum Model","text":"<ul> <li>Purpose: Configure a quantum machine learning model</li> <li>Configuration:</li> <li><code>model_name</code>: Choose from 18 quantum models</li> <li>Available Quantum Models:</li> <li>DataReuploading</li> <li>QuantumKitchen</li> <li>SeparableTwoDesign</li> <li>BasicEntanglerLayers</li> <li>StronglyEntanglingLayers</li> <li>QuantumMetricLearning</li> <li>SimplifiedTwoDesign</li> <li>QCNN</li> <li>TreeTensorNetwork</li> <li>MERA</li> <li>And 8 more...</li> <li>Input: Preprocessed data</li> <li>Output: Model configuration for optimization</li> </ul>"},{"location":"guides/workflow-builder-guide.html#classical-model","title":"Classical Model","text":"<ul> <li>Purpose: Configure a classical machine learning model</li> <li>Configuration:</li> <li><code>model_name</code>: Choose from 8 classical models</li> <li>Available Classical Models:</li> <li>RandomForest</li> <li>LogisticRegression</li> <li>SVC</li> <li>GradientBoosting</li> <li>AdaBoost</li> <li>KNN</li> <li>DecisionTree</li> <li>NaiveBayes</li> <li>Input: Preprocessed data</li> <li>Output: Model configuration for optimization</li> </ul>"},{"location":"guides/workflow-builder-guide.html#optimization-nodes","title":"Optimization Nodes","text":""},{"location":"guides/workflow-builder-guide.html#optuna-config","title":"Optuna Config","text":"<ul> <li>Purpose: Configure hyperparameter optimization settings</li> <li>Configuration:</li> <li><code>study_name</code>: Name for the optimization study (default: \"workflow_study\")</li> <li><code>n_trials</code>: Number of optimization trials (default: 100)</li> <li><code>db_name</code>: SQLite database for storing results (default: \"workflow_optimization.db\")</li> <li>Input: Model configuration</li> <li>Output: Optimization parameters merged with model config</li> </ul>"},{"location":"guides/workflow-builder-guide.html#run-optimization","title":"Run Optimization","text":"<ul> <li>Purpose: Execute Optuna hyperparameter optimization</li> <li>Input: Optuna configuration with model and data</li> <li>Process:</li> <li>Creates Optimizer instance with configured parameters</li> <li>Runs <code>n_trials</code> optimization trials</li> <li>Finds best hyperparameters</li> <li>Returns best model and performance metrics</li> <li>Output:</li> <li><code>best_value</code>: Best accuracy/metric achieved</li> <li><code>best_params</code>: Optimal hyperparameters found</li> <li><code>best_trial_number</code>: Which trial was best</li> <li>Study name and database for analysis</li> <li>Duration: Can take several minutes to hours depending on:</li> <li>Number of trials (more trials = better results but longer time)</li> <li>Model complexity (quantum models are slower)</li> <li>Dataset size</li> </ul>"},{"location":"guides/workflow-builder-guide.html#analysis-nodes","title":"Analysis Nodes","text":""},{"location":"guides/workflow-builder-guide.html#shap-analysis","title":"SHAP Analysis","text":"<ul> <li>Purpose: Generate explainability visualizations using SHAP (SHapley Additive exPlanations)</li> <li>Input: Optimization results with trained model</li> <li>Configuration:</li> <li><code>plot_types</code>: List of plot types to generate<ul> <li>\"bar\": Feature importance bar chart</li> <li>\"beeswarm\": Feature impact distribution</li> <li>\"violin\": Feature value distribution</li> </ul> </li> <li>Output: SHAP plots and feature importance rankings</li> <li>Use Case: Understanding which features impact model predictions</li> </ul>"},{"location":"guides/workflow-builder-guide.html#confusion-matrix","title":"Confusion Matrix","text":"<ul> <li>Purpose: Visualize classification performance</li> <li>Input: Model predictions and true labels</li> <li>Output: Confusion matrix visualization</li> <li>Status: Placeholder (implementation pending)</li> </ul>"},{"location":"guides/workflow-builder-guide.html#feature-importance","title":"Feature Importance","text":"<ul> <li>Purpose: Rank features by their impact on predictions</li> <li>Input: Trained model or SHAP results</li> <li>Output: Feature importance scores</li> <li>Note: Can use SHAP bar plot if available</li> </ul>"},{"location":"guides/workflow-builder-guide.html#output-nodes","title":"Output Nodes","text":""},{"location":"guides/workflow-builder-guide.html#export-model","title":"Export Model","text":"<ul> <li>Purpose: Save the trained model to disk</li> <li>Configuration:</li> <li><code>export_path</code>: Where to save the model (default: <code>./models/{study_name}.pkl</code>)</li> <li>Input: Optimization results</li> <li>Output: Saved model file</li> <li>Status: Placeholder (implementation pending)</li> </ul>"},{"location":"guides/workflow-builder-guide.html#generate-report","title":"Generate Report","text":"<ul> <li>Purpose: Create AI-powered analysis report</li> <li>Configuration:</li> <li><code>llm_provider</code>: \"openai\", \"anthropic\", or \"google\"</li> <li>Input: Workflow results and metrics</li> <li>Output: Comprehensive markdown report</li> <li>Status: Requires LLM API keys to be configured</li> <li>Requirements: Set environment variables:</li> <li><code>OPENAI_API_KEY</code></li> <li><code>ANTHROPIC_API_KEY</code></li> <li><code>GOOGLE_API_KEY</code></li> </ul>"},{"location":"guides/workflow-builder-guide.html#example-workflows","title":"Example Workflows","text":""},{"location":"guides/workflow-builder-guide.html#simple-classification-workflow","title":"Simple Classification Workflow","text":"<ol> <li>UCI Dataset \u2192 Choose Iris dataset (ID: 53)</li> <li>Select Features \u2192 X: all except target, y: species</li> <li>Train/Test Split \u2192 80/20 split</li> <li>Quantum Model \u2192 DataReuploading</li> <li>Optuna Config \u2192 50 trials</li> <li>Run Optimization \u2192 Execute training</li> <li>SHAP Analysis \u2192 Explain predictions</li> </ol>"},{"location":"guides/workflow-builder-guide.html#upload-and-analyze-workflow","title":"Upload and Analyze Workflow","text":"<ol> <li>Upload CSV \u2192 Your custom dataset</li> <li>Data Preview \u2192 Inspect the data</li> <li>Select Features \u2192 Choose relevant columns</li> <li>Train/Test Split \u2192 Prepare data</li> <li>Classical Model \u2192 RandomForest</li> <li>Optuna Config \u2192 100 trials</li> <li>Run Optimization \u2192 Find best hyperparameters</li> <li>Feature Importance \u2192 Understand key features</li> <li>Export Model \u2192 Save for deployment</li> </ol>"},{"location":"guides/workflow-builder-guide.html#quantum-vs-classical-comparison","title":"Quantum vs Classical Comparison","text":"<p>Create two parallel paths:</p> <p>Path 1 (Quantum): - Data \u2192 Features \u2192 Split \u2192 Quantum Model \u2192 Optuna \u2192 Optimization \u2192 SHAP</p> <p>Path 2 (Classical): - Data \u2192 Features \u2192 Split \u2192 Classical Model \u2192 Optuna \u2192 Optimization \u2192 SHAP</p> <p>Compare the results to see if quantum models provide advantages for your dataset.</p>"},{"location":"guides/workflow-builder-guide.html#workflow-execution","title":"Workflow Execution","text":""},{"location":"guides/workflow-builder-guide.html#execution-process","title":"Execution Process","text":"<ol> <li>Topological Sort: Nodes are sorted into execution order based on dependencies</li> <li>Sequential Execution: Each node runs in order, passing data to connected nodes</li> <li>Background Processing: Workflows run in the background, allowing you to continue working</li> <li>Status Updates: UI polls for updates every 2 seconds</li> <li>Results: Final results are displayed when execution completes</li> </ol>"},{"location":"guides/workflow-builder-guide.html#node-status-indicators","title":"Node Status Indicators","text":"<ul> <li>Gray: Not yet executed</li> <li>Blue: Currently running</li> <li>Green: Completed successfully</li> <li>Red: Failed with error</li> </ul>"},{"location":"guides/workflow-builder-guide.html#monitoring-execution","title":"Monitoring Execution","text":"<ul> <li>Check the status message at the top of the canvas</li> <li>Watch node colors change as they execute</li> <li>View detailed results in the execution response</li> </ul>"},{"location":"guides/workflow-builder-guide.html#best-practices","title":"Best Practices","text":""},{"location":"guides/workflow-builder-guide.html#data-preparation","title":"Data Preparation","text":"<ol> <li>Clean Data: Ensure CSV files are properly formatted</li> <li>Handle Missing Values: Remove or impute missing data before upload</li> <li>Feature Selection: Start with fewer features for faster optimization</li> <li>Test Size: Use 20-30% test size for better generalization</li> </ol>"},{"location":"guides/workflow-builder-guide.html#optimization","title":"Optimization","text":"<ol> <li>Start Small: Begin with 10-20 trials for testing</li> <li>Increase Gradually: Use 100+ trials for final optimization</li> <li>Quantum Models: Require more time, use fewer trials initially</li> <li>Classical Baseline: Always compare with classical models</li> </ol>"},{"location":"guides/workflow-builder-guide.html#workflow-design","title":"Workflow Design","text":"<ol> <li>Linear Flows: Start simple with linear pipelines</li> <li>Validation: Use Data Preview to verify data loading</li> <li>Save Work: Export models and results regularly</li> <li>Documentation: Use descriptive workflow names</li> </ol>"},{"location":"guides/workflow-builder-guide.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/workflow-builder-guide.html#common-issues","title":"Common Issues","text":""},{"location":"guides/workflow-builder-guide.html#node-not-executing","title":"Node Not Executing","text":"<ul> <li>Check connections: Ensure all input edges are connected</li> <li>Verify order: Parent nodes must complete before children</li> <li>Review config: Some nodes require configuration</li> </ul>"},{"location":"guides/workflow-builder-guide.html#upload-fails","title":"Upload Fails","text":"<ul> <li>File format: Only CSV files are supported</li> <li>File size: Must be under 100 MB</li> <li>Format: Ensure proper CSV structure (comma-separated)</li> </ul>"},{"location":"guides/workflow-builder-guide.html#optimization-slow","title":"Optimization Slow","text":"<ul> <li>Reduce trials: Lower n_trials for faster results</li> <li>Use classical: Quantum models are inherently slower</li> <li>Check dataset: Larger datasets take longer</li> </ul>"},{"location":"guides/workflow-builder-guide.html#shap-analysis-fails","title":"SHAP Analysis Fails","text":"<ul> <li>Model compatibility: Not all models support SHAP</li> <li>Data size: Very large datasets may timeout</li> <li>Try subsampling: Use fewer samples for SHAP</li> </ul>"},{"location":"guides/workflow-builder-guide.html#error-messages","title":"Error Messages","text":"<ul> <li>\"Workflow contains cycles\": Remove circular dependencies</li> <li>\"No input data\": Connect required input nodes</li> <li>\"No model name provided\": Configure model selection</li> <li>\"Execution failed\": Check backend logs for details</li> </ul>"},{"location":"guides/workflow-builder-guide.html#api-integration","title":"API Integration","text":""},{"location":"guides/workflow-builder-guide.html#rest-api-endpoints","title":"REST API Endpoints","text":"<p>The workflow builder uses these backend endpoints:</p> <pre><code>POST   /api/v1/data/upload              # Upload CSV file\nGET    /api/v1/data/uci                 # List UCI datasets\nPOST   /api/v1/workflows/execute        # Execute workflow\nGET    /api/v1/workflows/executions/:id # Get execution status\n</code></pre>"},{"location":"guides/workflow-builder-guide.html#execution-response-format","title":"Execution Response Format","text":"<pre><code>{\n  \"execution_id\": \"exec-1\",\n  \"status\": \"pending\",\n  \"message\": \"Workflow execution started\"\n}\n</code></pre>"},{"location":"guides/workflow-builder-guide.html#status-response-format","title":"Status Response Format","text":"<pre><code>{\n  \"id\": \"exec-1\",\n  \"status\": \"completed\",\n  \"started_at\": \"2025-11-15T00:00:00\",\n  \"completed_at\": \"2025-11-15T00:05:00\",\n  \"result\": {\n    \"status\": \"completed\",\n    \"workflow_name\": \"My Workflow\",\n    \"node_results\": { /* ... */ }\n  }\n}\n</code></pre>"},{"location":"guides/workflow-builder-guide.html#advanced-features","title":"Advanced Features","text":""},{"location":"guides/workflow-builder-guide.html#custom-configurations","title":"Custom Configurations","text":"<p>Nodes can be configured by modifying their <code>config</code> property in the workflow JSON:</p> <pre><code>{\n  \"id\": \"node-1\",\n  \"type\": \"optuna-config\",\n  \"data\": {\n    \"type\": \"optuna-config\",\n    \"label\": \"Optuna Config\",\n    \"config\": {\n      \"study_name\": \"my_study\",\n      \"n_trials\": 200,\n      \"db_name\": \"custom.db\"\n    }\n  }\n}\n</code></pre>"},{"location":"guides/workflow-builder-guide.html#programmatic-access","title":"Programmatic Access","text":"<p>You can also execute workflows programmatically via the API:</p> <pre><code>import { executeWorkflow, pollExecutionStatus } from './lib/api';\n\nconst result = await executeWorkflow({\n  name: 'Automated Workflow',\n  nodes: [...],\n  edges: [...]\n});\n\nconst status = await pollExecutionStatus(result.execution_id);\nconsole.log(status.result);\n</code></pre>"},{"location":"guides/workflow-builder-guide.html#future-enhancements","title":"Future Enhancements","text":"<p>Planned features for future releases:</p> <ul> <li>[ ] Node configuration UI panels</li> <li>[ ] Workflow templates and examples</li> <li>[ ] Real-time progress streaming</li> <li>[ ] Workflow sharing and export</li> <li>[ ] Custom node plugins</li> <li>[ ] Parallel execution paths</li> <li>[ ] Model versioning and tracking</li> <li>[ ] Interactive result visualization</li> <li>[ ] Workflow scheduling</li> <li>[ ] Multi-objective optimization</li> </ul>"},{"location":"guides/workflow-builder-guide.html#support","title":"Support","text":"<p>For issues, questions, or feature requests:</p> <ul> <li>Check the backend logs: <code>docker compose logs backend</code></li> <li>View frontend console for client errors</li> <li>API documentation: http://localhost:8000/docs</li> <li>Submit issues on GitHub</li> </ul>"},{"location":"guides/workflow-builder-guide.html#license","title":"License","text":"<p>QuOptuna is licensed under the Apache-2.0 License.</p>"},{"location":"roadmap/implementation-roadmap.html","title":"Implementation Roadmap: Connect Optimizer UI to Real Backend","text":""},{"location":"roadmap/implementation-roadmap.html#implementation-roadmap-connect-optimizer-ui-to-real-backend","title":"Implementation Roadmap: Connect Optimizer UI to Real Backend","text":"<p>This document outlines the steps needed to replace the simulated optimization/SHAP in the UI with real backend services.</p>"},{"location":"roadmap/implementation-roadmap.html#phase-1-implement-backend-api-endpoints","title":"Phase 1: Implement Backend API Endpoints","text":""},{"location":"roadmap/implementation-roadmap.html#task-11-implement-optimization-endpoints","title":"Task 1.1: Implement Optimization Endpoints","text":"<p>File: <code>/backend/app/api/v1/optimize.py</code></p> <pre><code># Current: All TODOs\n# Needed: Real implementation\n\nfrom fastapi import APIRouter, HTTPException, BackgroundTasks\nfrom pydantic import BaseModel\nfrom typing import Dict, Any, List\nimport asyncio\nfrom pathlib import Path\n\nfrom app.services.workflow_service import WorkflowExecutor\n\nrouter = APIRouter()\n\n# In-memory storage (replace with Redis/database in production)\noptimization_jobs = {}\n\nclass OptimizationRequest(BaseModel):\n    dataset_id: str\n    dataset_source: str  # 'uci' or 'upload'\n    selected_features: List[str]\n    target_column: str\n    study_name: str\n    database_name: str\n    num_trials: int\n    model_name: str = \"DataReuploading\"\n\nclass OptimizationStatus(BaseModel):\n    id: str\n    status: str  # 'pending', 'running', 'completed', 'failed'\n    current_trial: int\n    total_trials: int\n    best_value: float | None\n    best_params: Dict[str, Any] | None\n    started_at: str\n    completed_at: str | None\n    error: str | None\n\ndef run_optimization_background(job_id: str, request: OptimizationRequest):\n    \"\"\"Background task to run optimization\"\"\"\n    try:\n        optimization_jobs[job_id]['status'] = 'running'\n\n        # Build workflow\n        workflow = {\n            \"id\": job_id,\n            \"name\": request.study_name,\n            \"nodes\": [\n                {\n                    \"id\": \"data\",\n                    \"data\": {\n                        \"type\": \"data-uci\" if request.dataset_source == \"uci\" else \"data-upload\",\n                        \"config\": {\"dataset_id\": request.dataset_id}\n                    }\n                },\n                {\n                    \"id\": \"features\",\n                    \"data\": {\n                        \"type\": \"feature-selection\",\n                        \"config\": {\n                            \"x_columns\": request.selected_features,\n                            \"y_column\": request.target_column\n                        }\n                    }\n                },\n                {\n                    \"id\": \"split\",\n                    \"data\": {\"type\": \"train-test-split\", \"config\": {}}\n                },\n                {\n                    \"id\": \"model\",\n                    \"data\": {\n                        \"type\": \"quantum-model\",\n                        \"config\": {\"model_name\": request.model_name}\n                    }\n                },\n                {\n                    \"id\": \"optuna\",\n                    \"data\": {\n                        \"type\": \"optuna-config\",\n                        \"config\": {\n                            \"study_name\": request.study_name,\n                            \"n_trials\": request.num_trials,\n                            \"db_name\": request.database_name\n                        }\n                    }\n                },\n                {\n                    \"id\": \"optimize\",\n                    \"data\": {\"type\": \"optimization\", \"config\": {}}\n                }\n            ],\n            \"edges\": [\n                {\"source\": \"data\", \"target\": \"features\"},\n                {\"source\": \"features\", \"target\": \"split\"},\n                {\"source\": \"split\", \"target\": \"model\"},\n                {\"source\": \"model\", \"target\": \"optuna\"},\n                {\"source\": \"optuna\", \"target\": \"optimize\"}\n            ]\n        }\n\n        # Execute workflow\n        executor = WorkflowExecutor(workflow)\n        result = executor.execute()\n\n        # Extract optimization results\n        opt_result = result['node_results']['optimize']\n\n        optimization_jobs[job_id].update({\n            'status': 'completed',\n            'current_trial': request.num_trials,\n            'best_value': opt_result['best_value'],\n            'best_params': opt_result['best_params'],\n            'completed_at': datetime.now().isoformat(),\n            'result': opt_result\n        })\n\n    except Exception as e:\n        optimization_jobs[job_id].update({\n            'status': 'failed',\n            'error': str(e),\n            'completed_at': datetime.now().isoformat()\n        })\n\n@router.post(\"\", response_model=dict)\nasync def start_optimization(\n    request: OptimizationRequest,\n    background_tasks: BackgroundTasks\n):\n    \"\"\"Start a new optimization study\"\"\"\n    job_id = f\"opt_{uuid.uuid4().hex[:8]}\"\n\n    optimization_jobs[job_id] = {\n        'id': job_id,\n        'status': 'pending',\n        'current_trial': 0,\n        'total_trials': request.num_trials,\n        'best_value': None,\n        'best_params': None,\n        'started_at': datetime.now().isoformat(),\n        'completed_at': None,\n        'error': None\n    }\n\n    background_tasks.add_task(run_optimization_background, job_id, request)\n\n    return {'id': job_id, 'status': 'pending'}\n\n@router.get(\"/{optimization_id}\", response_model=OptimizationStatus)\nasync def get_optimization_status(optimization_id: str):\n    \"\"\"Get optimization status\"\"\"\n    if optimization_id not in optimization_jobs:\n        raise HTTPException(status_code=404, detail=\"Optimization not found\")\n\n    return optimization_jobs[optimization_id]\n</code></pre>"},{"location":"roadmap/implementation-roadmap.html#task-12-implement-shap-analysis-endpoints","title":"Task 1.2: Implement SHAP Analysis Endpoints","text":"<p>File: <code>/backend/app/api/v1/analysis.py</code></p> <pre><code># Add real SHAP implementation\n\n@router.post(\"/shap\")\nasync def generate_shap_analysis(\n    optimization_id: str,\n    plot_types: List[str] = [\"bar\", \"beeswarm\"]\n):\n    \"\"\"Generate SHAP analysis from optimization results\"\"\"\n    if optimization_id not in optimization_jobs:\n        raise HTTPException(status_code=404, detail=\"Optimization not found\")\n\n    opt_job = optimization_jobs[optimization_id]\n    if opt_job['status'] != 'completed':\n        raise HTTPException(status_code=400, detail=\"Optimization not completed\")\n\n    # Build SHAP workflow\n    opt_result = opt_job['result']\n\n    workflow = {\n        \"id\": f\"shap_{uuid.uuid4().hex[:8]}\",\n        \"name\": \"SHAP Analysis\",\n        \"nodes\": [{\n            \"id\": \"shap\",\n            \"data\": {\n                \"type\": \"shap-analysis\",\n                \"config\": {\"plot_types\": plot_types}\n            }\n        }],\n        \"edges\": []\n    }\n\n    # Execute SHAP\n    executor = WorkflowExecutor(workflow)\n    # Inject optimization result as input\n    executor.results['input'] = opt_result\n\n    shap_result = executor.execute_node('shap')\n\n    return {\n        \"feature_importance\": extract_feature_importance(shap_result),\n        \"plots\": shap_result.get('plots', {})\n    }\n</code></pre>"},{"location":"roadmap/implementation-roadmap.html#phase-2-update-frontend-to-use-real-apis","title":"Phase 2: Update Frontend to Use Real APIs","text":""},{"location":"roadmap/implementation-roadmap.html#task-21-create-optimizer-api-client","title":"Task 2.1: Create Optimizer API Client","text":"<p>File: <code>/frontend/src/lib/api.ts</code></p> <pre><code>export interface OptimizationRequest {\n  dataset_id: string;\n  dataset_source: 'uci' | 'upload';\n  selected_features: string[];\n  target_column: string;\n  study_name: string;\n  database_name: string;\n  num_trials: number;\n  model_name?: string;\n}\n\nexport interface OptimizationStatus {\n  id: string;\n  status: 'pending' | 'running' | 'completed' | 'failed';\n  current_trial: number;\n  total_trials: number;\n  best_value: number | null;\n  best_params: Record&lt;string, any&gt; | null;\n  started_at: string;\n  completed_at: string | null;\n  error: string | null;\n}\n\nexport async function startOptimization(\n  request: OptimizationRequest\n): Promise&lt;{ id: string }&gt; {\n  const response = await fetch(`${API_BASE_URL}/api/v1/optimize`, {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify(request),\n  });\n\n  if (!response.ok) {\n    const error = await response.json();\n    throw new Error(error.detail || 'Failed to start optimization');\n  }\n\n  return response.json();\n}\n\nexport async function getOptimizationStatus(\n  optimizationId: string\n): Promise&lt;OptimizationStatus&gt; {\n  const response = await fetch(\n    `${API_BASE_URL}/api/v1/optimize/${optimizationId}`\n  );\n\n  if (!response.ok) {\n    throw new Error('Failed to get optimization status');\n  }\n\n  return response.json();\n}\n\nexport async function pollOptimization(\n  optimizationId: string,\n  onUpdate: (status: OptimizationStatus) =&gt; void,\n  intervalMs: number = 2000\n): Promise&lt;OptimizationStatus&gt; {\n  return new Promise((resolve, reject) =&gt; {\n    const poll = async () =&gt; {\n      try {\n        const status = await getOptimizationStatus(optimizationId);\n        onUpdate(status);\n\n        if (status.status === 'completed' || status.status === 'failed') {\n          resolve(status);\n        } else {\n          setTimeout(poll, intervalMs);\n        }\n      } catch (error) {\n        reject(error);\n      }\n    };\n\n    poll();\n  });\n}\n\nexport async function generateSHAP(\n  optimizationId: string,\n  plotTypes: string[] = ['bar', 'beeswarm']\n): Promise&lt;any&gt; {\n  const response = await fetch(`${API_BASE_URL}/api/v1/analysis/shap`, {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({ optimization_id: optimizationId, plot_types: plotTypes }),\n  });\n\n  if (!response.ok) {\n    throw new Error('Failed to generate SHAP analysis');\n  }\n\n  return response.json();\n}\n</code></pre>"},{"location":"roadmap/implementation-roadmap.html#task-22-update-optimizestep-to-use-real-api","title":"Task 2.2: Update OptimizeStep to Use Real API","text":"<p>File: <code>/frontend/src/pages/Optimizer.tsx</code></p> <p>Replace the simulated <code>startOptimization()</code> function:</p> <pre><code>function OptimizeStep({ onNext, onBack, workflowData, setWorkflowData }: StepProps) {\n  const [isRunning, setIsRunning] = useState(false);\n  const [progress, setProgress] = useState(0);\n  const [currentTrial, setCurrentTrial] = useState(0);\n  const [optimizationId, setOptimizationId] = useState&lt;string | null&gt;(null);\n\n  const startOptimization = async () =&gt; {\n    if (!workflowData.dataset) return;\n\n    setIsRunning(true);\n    setProgress(0);\n    setCurrentTrial(0);\n\n    try {\n      // Start real optimization\n      const { id } = await startOptimization({\n        dataset_id: workflowData.dataset.id,\n        dataset_source: workflowData.dataset.source,\n        selected_features: workflowData.features.selectedFeatures,\n        target_column: workflowData.features.targetColumn!,\n        study_name: workflowData.configuration.studyName,\n        database_name: workflowData.configuration.databaseName,\n        num_trials: workflowData.configuration.numTrials,\n      });\n\n      setOptimizationId(id);\n\n      // Poll for status updates\n      const finalStatus = await pollOptimization(id, (status) =&gt; {\n        setCurrentTrial(status.current_trial);\n        setProgress((status.current_trial / status.total_trials) * 100);\n      });\n\n      // Update workflow data with real results\n      setWorkflowData((prev) =&gt; ({\n        ...prev,\n        optimization: {\n          executionId: id,\n          status: 'completed',\n          results: {\n            bestValue: finalStatus.best_value,\n            bestParams: finalStatus.best_params,\n            trials: [], // Could fetch trial history separately\n          },\n        },\n      }));\n\n      setIsRunning(false);\n    } catch (error) {\n      setError(error instanceof Error ? error.message : 'Optimization failed');\n      setIsRunning(false);\n    }\n  };\n\n  // Rest of component...\n}\n</code></pre>"},{"location":"roadmap/implementation-roadmap.html#task-23-update-analyzestep-to-use-real-api","title":"Task 2.3: Update AnalyzeStep to Use Real API","text":"<pre><code>const generateSHAP = async () =&gt; {\n  if (!workflowData.optimization.executionId) return;\n\n  setIsGenerating(true);\n\n  try {\n    // Call real SHAP API\n    const shapResult = await generateSHAP(\n      workflowData.optimization.executionId,\n      ['bar', 'beeswarm', 'waterfall']\n    );\n\n    setWorkflowData((prev) =&gt; ({\n      ...prev,\n      analysis: {\n        shapData: {\n          featureImportance: shapResult.feature_importance,\n          plots: shapResult.plots,\n          generated: true,\n        },\n      },\n    }));\n\n    setIsGenerating(false);\n  } catch (error) {\n    setError(error instanceof Error ? error.message : 'SHAP generation failed');\n    setIsGenerating(false);\n  }\n};\n</code></pre>"},{"location":"roadmap/implementation-roadmap.html#phase-3-add-progress-tracking-with-websockets-optional-but-recommended","title":"Phase 3: Add Progress Tracking with WebSockets (Optional but Recommended)","text":""},{"location":"roadmap/implementation-roadmap.html#why-websockets","title":"Why WebSockets?","text":"<p>Long-running optimizations (10+ minutes) benefit from real-time updates instead of polling.</p>"},{"location":"roadmap/implementation-roadmap.html#implementation","title":"Implementation","text":"<p>Backend: <code>backend/app/api/v1/optimize.py</code></p> <pre><code>from fastapi import WebSocket\n\n@router.websocket(\"/ws/{optimization_id}\")\nasync def optimization_websocket(websocket: WebSocket, optimization_id: str):\n    await websocket.accept()\n\n    while True:\n        if optimization_id in optimization_jobs:\n            status = optimization_jobs[optimization_id]\n            await websocket.send_json(status)\n\n            if status['status'] in ['completed', 'failed']:\n                break\n\n        await asyncio.sleep(1)\n\n    await websocket.close()\n</code></pre> <p>Frontend: Use WebSocket for live updates</p> <pre><code>const ws = new WebSocket(`ws://localhost:8000/api/v1/optimize/ws/${id}`);\n\nws.onmessage = (event) =&gt; {\n  const status = JSON.parse(event.data);\n  setCurrentTrial(status.current_trial);\n  setProgress((status.current_trial / status.total_trials) * 100);\n\n  if (status.status === 'completed') {\n    // Handle completion\n  }\n};\n</code></pre>"},{"location":"roadmap/implementation-roadmap.html#phase-4-testing-checklist","title":"Phase 4: Testing Checklist","text":""},{"location":"roadmap/implementation-roadmap.html#backend-tests","title":"Backend Tests","text":"<ul> <li>[ ] <code>/api/v1/optimize</code> starts optimization correctly</li> <li>[ ] <code>/api/v1/optimize/{id}</code> returns accurate status</li> <li>[ ] Optimization completes with real Optuna results</li> <li>[ ] <code>/api/v1/analysis/shap</code> generates real SHAP plots</li> <li>[ ] Error handling for invalid inputs</li> <li>[ ] Database persistence works</li> </ul>"},{"location":"roadmap/implementation-roadmap.html#frontend-tests","title":"Frontend Tests","text":"<ul> <li>[ ] UI calls <code>/api/v1/optimize</code> instead of mock</li> <li>[ ] Progress bar shows real trial progress</li> <li>[ ] Results match backend response</li> <li>[ ] SHAP analysis displays real feature importance</li> <li>[ ] Error states are handled properly</li> <li>[ ] Loading states work correctly</li> </ul>"},{"location":"roadmap/implementation-roadmap.html#integration-tests","title":"Integration Tests","text":"<ul> <li>[ ] End-to-end: Select dataset \u2192 Optimize \u2192 SHAP \u2192 Report</li> <li>[ ] Test with different datasets (UCI, uploaded)</li> <li>[ ] Test with different trial counts (10, 100, 1000)</li> <li>[ ] Test error scenarios (invalid features, missing data)</li> <li>[ ] Test cancellation of running optimization</li> </ul>"},{"location":"roadmap/implementation-roadmap.html#phase-5-performance-optimization","title":"Phase 5: Performance Optimization","text":""},{"location":"roadmap/implementation-roadmap.html#caching","title":"Caching","text":"<pre><code>from functools import lru_cache\n\n@lru_cache(maxsize=100)\ndef get_uci_dataset(dataset_id: int):\n    return fetch_ucirepo(id=dataset_id)\n</code></pre>"},{"location":"roadmap/implementation-roadmap.html#async-processing","title":"Async Processing","text":"<ul> <li>Use Celery or RQ for background task management</li> <li>Store results in Redis for fast access</li> <li>Implement job queue for multiple concurrent optimizations</li> </ul>"},{"location":"roadmap/implementation-roadmap.html#database","title":"Database","text":"<p>Replace in-memory <code>optimization_jobs</code> dict with: - PostgreSQL for production - Redis for fast access to status - MongoDB for storing large result objects</p>"},{"location":"roadmap/implementation-roadmap.html#timeline-estimates","title":"Timeline Estimates","text":"Phase Estimated Time Priority Phase 1.1: Optimize API 4-6 hours High Phase 1.2: SHAP API 2-3 hours High Phase 2.1: API Client 1-2 hours High Phase 2.2: Update OptimizeStep 2-3 hours High Phase 2.3: Update AnalyzeStep 1-2 hours High Phase 3: WebSockets 3-4 hours Medium Phase 4: Testing 4-6 hours High Phase 5: Optimization 3-4 hours Low <p>Total: ~20-30 hours for complete implementation</p>"},{"location":"roadmap/implementation-roadmap.html#current-vs-future-flow","title":"Current vs Future Flow","text":""},{"location":"roadmap/implementation-roadmap.html#current-simulated","title":"Current (Simulated)","text":"<pre><code>User clicks \"Start\" \u2192 setTimeout loop \u2192 Show mock results\n</code></pre>"},{"location":"roadmap/implementation-roadmap.html#future-real","title":"Future (Real)","text":"<pre><code>User clicks \"Start\"\n    \u2193\nPOST /api/v1/optimize (start background job)\n    \u2193\nPoll GET /api/v1/optimize/{id} every 2 seconds\n    \u2193\nUpdate progress bar with real trial count\n    \u2193\nWhen status === 'completed'\n    \u2193\nDisplay real optimization results\n    \u2193\nPOST /api/v1/analysis/shap\n    \u2193\nDisplay real SHAP analysis\n</code></pre> <p>Next Action: Start with Phase 1.1 (Implement Optimize API)</p>"}]}
var __index = {"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"Home","text":""},{"location":"index.html#quoptuna-documentation","title":"QuOptuna Documentation","text":"<p>Welcome to QuOptuna, a quantum-enhanced hyperparameter optimization framework that combines quantum computing with Optuna for advanced model tuning and explainable AI.</p>"},{"location":"index.html#overview","title":"Overview","text":"<p>QuOptuna provides a comprehensive platform for: - \ud83c\udfaf Automated hyperparameter optimization for quantum and classical ML models - \ud83d\udd0d SHAP-based explainable AI with rich visualizations - \ud83d\udcca UCI ML Repository integration for easy dataset access - \ud83d\udcdd AI-powered report generation for model analysis - \ud83d\udda5\ufe0f Interactive Streamlit interface for the complete workflow</p>"},{"location":"index.html#quick-start","title":"Quick Start","text":""},{"location":"index.html#installation","title":"Installation","text":"<p>Install QuOptuna using UV (recommended) or pip:</p> <pre><code># Using UV (recommended)\nuv pip install quoptuna\n\n# Using pip\npip install quoptuna\n</code></pre>"},{"location":"index.html#launch-the-application","title":"Launch the Application","text":"<p>Start the interactive Streamlit interface:</p> <pre><code>quoptuna --start\n</code></pre> <p>Or run directly with Python:</p> <pre><code>python -m quoptuna.frontend.app run\n</code></pre>"},{"location":"index.html#basic-python-usage","title":"Basic Python Usage","text":"<pre><code>from quoptuna import DataPreparation, Optimizer\nfrom quoptuna.backend.utils.data_utils.data import mock_csv_data\nimport pandas as pd\n\n# Load and prepare data\ndf = pd.read_csv(\"your_data.csv\")\ndf[\"target\"] = df[\"target\"].replace({0: -1, 1: 1})\n\n# Save data\nfile_path = mock_csv_data(df, tmp_path=\"data\", file_name=\"my_data\")\n\n# Prepare for training\ndata_prep = DataPreparation(\n    file_path=file_path,\n    x_cols=[col for col in df.columns if col != \"target\"],\n    y_col=\"target\"\n)\ndata_dict = data_prep.get_data(output_type=\"2\")\n\n# Run optimization\noptimizer = Optimizer(db_name=\"experiment\", study_name=\"trial_1\", data=data_dict)\nstudy, best_trials = optimizer.optimize(n_trials=100)\n\nprint(f\"Best F1 Score: {best_trials[0].value:.4f}\")\nprint(f\"Best Model: {best_trials[0].params['model_type']}\")\n</code></pre>"},{"location":"index.html#key-features","title":"Key Features","text":""},{"location":"index.html#hyperparameter-optimization","title":"\ud83c\udfaf Hyperparameter Optimization","text":"<p>Automated optimization using Optuna with support for: - Multiple quantum models (Data Reuploading, Circuit-Centric, Quantum Kitchen Sinks, etc.) - Classical baselines (SVC, MLP, Perceptron) - Multi-objective optimization - Parallel trial execution - Persistent storage with SQLite</p>"},{"location":"index.html#explainable-ai","title":"\ud83d\udd0d Explainable AI","text":"<p>Comprehensive SHAP analysis with multiple visualization types: - Bar Plot: Feature importance ranking - Beeswarm Plot: Feature value impact distribution - Violin Plot: SHAP value distributions - Heatmap: Instance-level feature contributions - Waterfall Plot: Individual prediction explanations - Confusion Matrix: Model performance visualization</p>"},{"location":"index.html#dataset-management","title":"\ud83d\udcca Dataset Management","text":"<ul> <li>UCI ML Repository: Direct access to 100+ datasets</li> <li>Custom Upload: Support for CSV files</li> <li>Automatic Preprocessing: Handle missing values, feature scaling</li> <li>Target Transformation: Binary classification support (-1/+1 encoding)</li> </ul>"},{"location":"index.html#ai-powered-reports","title":"\ud83d\udcdd AI-Powered Reports","text":"<p>Generate comprehensive analysis reports using: - Google Gemini - OpenAI GPT - Anthropic Claude</p> <p>Reports include performance metrics, SHAP interpretations, and governance recommendations.</p>"},{"location":"index.html#supported-models","title":"Supported Models","text":""},{"location":"index.html#quantum-models","title":"Quantum Models","text":"<ul> <li>Data Reuploading Classifier: Quantum circuit with data re-uploading</li> <li>Circuit-Centric Classifier: Parameterized quantum circuits</li> <li>Quantum Kitchen Sinks: Quantum feature maps</li> <li>Quantum Metric Learner: Metric learning with quantum circuits</li> <li>Dressed Quantum Circuit Classifier: Hybrid quantum-classical</li> </ul>"},{"location":"index.html#classical-models","title":"Classical Models","text":"<ul> <li>Support Vector Classifier (SVC): With multiple kernels</li> <li>Multi-Layer Perceptron (MLP): Neural network classifier</li> <li>Perceptron: Simple linear classifier</li> </ul>"},{"location":"index.html#documentation","title":"Documentation","text":"<ul> <li>User Guide - Complete walkthrough of the Streamlit interface</li> <li>API Reference - Detailed API documentation for Python usage</li> <li>Examples - Code examples for common use cases</li> <li>Changelog - Version history and updates</li> </ul>"},{"location":"index.html#workflow","title":"Workflow","text":"<p>QuOptuna provides a structured workflow:</p> <ol> <li>Dataset Selection</li> <li>Load from UCI ML Repository or upload CSV</li> <li>Configure features and target</li> <li> <p>Apply preprocessing</p> </li> <li> <p>Optimization</p> </li> <li>Prepare train/test splits</li> <li>Run hyperparameter optimization</li> <li> <p>Review best performing models</p> </li> <li> <p>Model Training</p> </li> <li>Select best trial</li> <li>Train model with optimized parameters</li> <li> <p>Evaluate performance</p> </li> <li> <p>SHAP Analysis</p> </li> <li>Calculate SHAP values</li> <li>Generate multiple visualization types</li> <li> <p>Understand feature importance</p> </li> <li> <p>Report Generation</p> </li> <li>Create AI-powered analysis</li> <li>Export results</li> <li>Share insights</li> </ol>"},{"location":"index.html#system-requirements","title":"System Requirements","text":"<ul> <li>Python 3.8+</li> <li>4GB+ RAM (8GB recommended for quantum models)</li> <li>Internet connection (for UCI datasets and LLM reports)</li> </ul>"},{"location":"index.html#dependencies","title":"Dependencies","text":"<p>Core dependencies: - <code>optuna</code> - Hyperparameter optimization - <code>streamlit</code> - Web interface - <code>shap</code> - Explainable AI - <code>pennylane</code> - Quantum computing - <code>scikit-learn</code> - Classical ML models - <code>pandas</code>, <code>numpy</code> - Data processing</p>"},{"location":"index.html#development","title":"Development","text":""},{"location":"index.html#setup","title":"Setup","text":"<pre><code># Clone repository\ngit clone https://github.com/Qentora/quoptuna.git\ncd quoptuna\n\n# Install development dependencies\nuv pip install -e \".[dev]\"\n\n# Run tests\nuv run pytest\n\n# Run linting\nuv run ruff check .\nuv run mypy .\n</code></pre>"},{"location":"index.html#project-structure","title":"Project Structure","text":"<pre><code>quoptuna/\n\u251c\u2500\u2500 src/quoptuna/\n\u2502   \u251c\u2500\u2500 backend/         # Core optimization and model code\n\u2502   \u2502   \u251c\u2500\u2500 models/      # Model implementations\n\u2502   \u2502   \u251c\u2500\u2500 tuners/      # Optuna integration\n\u2502   \u2502   \u251c\u2500\u2500 xai/         # SHAP analysis\n\u2502   \u2502   \u2514\u2500\u2500 utils/       # Utilities\n\u2502   \u2514\u2500\u2500 frontend/        # Streamlit interface\n\u2502       \u251c\u2500\u2500 pages/       # Multi-page app\n\u2502       \u251c\u2500\u2500 app.py       # Main application\n\u2502       \u2514\u2500\u2500 support.py   # Helper functions\n\u251c\u2500\u2500 docs/                # Documentation\n\u251c\u2500\u2500 experiments/         # Example notebooks\n\u2514\u2500\u2500 tests/              # Unit tests\n</code></pre>"},{"location":"index.html#contributing","title":"Contributing","text":"<p>We welcome contributions! Please see our Contributing Guidelines.</p>"},{"location":"index.html#ways-to-contribute","title":"Ways to Contribute","text":"<ul> <li>\ud83d\udc1b Report bugs and issues</li> <li>\ud83d\udca1 Suggest new features</li> <li>\ud83d\udcdd Improve documentation</li> <li>\ud83d\udd27 Submit pull requests</li> <li>\u2b50 Star the repository</li> </ul>"},{"location":"index.html#support-community","title":"Support &amp; Community","text":"<ul> <li>GitHub Issues: Report bugs or request features</li> <li>Discussions: Join the community</li> <li>Documentation: Full docs</li> </ul>"},{"location":"index.html#license","title":"License","text":"<p>QuOptuna is released under the MIT License. See LICENSE for details.</p>"},{"location":"index.html#citation","title":"Citation","text":"<p>If you use QuOptuna in your research, please cite:</p> <pre><code>@software{quoptuna,\n  title = {QuOptuna: Quantum-Enhanced Machine Learning Optimization},\n  author = {QuOptuna Team},\n  year = {2024},\n  url = {https://github.com/Qentora/quoptuna}\n}\n</code></pre>"},{"location":"index.html#acknowledgments","title":"Acknowledgments","text":"<p>Built with: - Optuna - Hyperparameter optimization framework - PennyLane - Quantum machine learning - SHAP - Explainable AI - Streamlit - Web framework</p> <p>Ready to get started? Check out the User Guide or launch the app with <code>quoptuna --start</code>!</p>"},{"location":"IMPLEMENTATION_ROADMAP.html","title":"Implementation Roadmap: Connect Optimizer UI to Real Backend","text":""},{"location":"IMPLEMENTATION_ROADMAP.html#implementation-roadmap-connect-optimizer-ui-to-real-backend","title":"Implementation Roadmap: Connect Optimizer UI to Real Backend","text":"<p>This document outlines the steps needed to replace the simulated optimization/SHAP in the UI with real backend services.</p>"},{"location":"IMPLEMENTATION_ROADMAP.html#phase-1-implement-backend-api-endpoints","title":"Phase 1: Implement Backend API Endpoints","text":""},{"location":"IMPLEMENTATION_ROADMAP.html#task-11-implement-optimization-endpoints","title":"Task 1.1: Implement Optimization Endpoints","text":"<p>File: <code>/backend/app/api/v1/optimize.py</code></p> <pre><code># Current: All TODOs\n# Needed: Real implementation\n\nfrom fastapi import APIRouter, HTTPException, BackgroundTasks\nfrom pydantic import BaseModel\nfrom typing import Dict, Any, List\nimport asyncio\nfrom pathlib import Path\n\nfrom app.services.workflow_service import WorkflowExecutor\n\nrouter = APIRouter()\n\n# In-memory storage (replace with Redis/database in production)\noptimization_jobs = {}\n\nclass OptimizationRequest(BaseModel):\n    dataset_id: str\n    dataset_source: str  # 'uci' or 'upload'\n    selected_features: List[str]\n    target_column: str\n    study_name: str\n    database_name: str\n    num_trials: int\n    model_name: str = \"DataReuploading\"\n\nclass OptimizationStatus(BaseModel):\n    id: str\n    status: str  # 'pending', 'running', 'completed', 'failed'\n    current_trial: int\n    total_trials: int\n    best_value: float | None\n    best_params: Dict[str, Any] | None\n    started_at: str\n    completed_at: str | None\n    error: str | None\n\ndef run_optimization_background(job_id: str, request: OptimizationRequest):\n    \"\"\"Background task to run optimization\"\"\"\n    try:\n        optimization_jobs[job_id]['status'] = 'running'\n\n        # Build workflow\n        workflow = {\n            \"id\": job_id,\n            \"name\": request.study_name,\n            \"nodes\": [\n                {\n                    \"id\": \"data\",\n                    \"data\": {\n                        \"type\": \"data-uci\" if request.dataset_source == \"uci\" else \"data-upload\",\n                        \"config\": {\"dataset_id\": request.dataset_id}\n                    }\n                },\n                {\n                    \"id\": \"features\",\n                    \"data\": {\n                        \"type\": \"feature-selection\",\n                        \"config\": {\n                            \"x_columns\": request.selected_features,\n                            \"y_column\": request.target_column\n                        }\n                    }\n                },\n                {\n                    \"id\": \"split\",\n                    \"data\": {\"type\": \"train-test-split\", \"config\": {}}\n                },\n                {\n                    \"id\": \"model\",\n                    \"data\": {\n                        \"type\": \"quantum-model\",\n                        \"config\": {\"model_name\": request.model_name}\n                    }\n                },\n                {\n                    \"id\": \"optuna\",\n                    \"data\": {\n                        \"type\": \"optuna-config\",\n                        \"config\": {\n                            \"study_name\": request.study_name,\n                            \"n_trials\": request.num_trials,\n                            \"db_name\": request.database_name\n                        }\n                    }\n                },\n                {\n                    \"id\": \"optimize\",\n                    \"data\": {\"type\": \"optimization\", \"config\": {}}\n                }\n            ],\n            \"edges\": [\n                {\"source\": \"data\", \"target\": \"features\"},\n                {\"source\": \"features\", \"target\": \"split\"},\n                {\"source\": \"split\", \"target\": \"model\"},\n                {\"source\": \"model\", \"target\": \"optuna\"},\n                {\"source\": \"optuna\", \"target\": \"optimize\"}\n            ]\n        }\n\n        # Execute workflow\n        executor = WorkflowExecutor(workflow)\n        result = executor.execute()\n\n        # Extract optimization results\n        opt_result = result['node_results']['optimize']\n\n        optimization_jobs[job_id].update({\n            'status': 'completed',\n            'current_trial': request.num_trials,\n            'best_value': opt_result['best_value'],\n            'best_params': opt_result['best_params'],\n            'completed_at': datetime.now().isoformat(),\n            'result': opt_result\n        })\n\n    except Exception as e:\n        optimization_jobs[job_id].update({\n            'status': 'failed',\n            'error': str(e),\n            'completed_at': datetime.now().isoformat()\n        })\n\n@router.post(\"\", response_model=dict)\nasync def start_optimization(\n    request: OptimizationRequest,\n    background_tasks: BackgroundTasks\n):\n    \"\"\"Start a new optimization study\"\"\"\n    job_id = f\"opt_{uuid.uuid4().hex[:8]}\"\n\n    optimization_jobs[job_id] = {\n        'id': job_id,\n        'status': 'pending',\n        'current_trial': 0,\n        'total_trials': request.num_trials,\n        'best_value': None,\n        'best_params': None,\n        'started_at': datetime.now().isoformat(),\n        'completed_at': None,\n        'error': None\n    }\n\n    background_tasks.add_task(run_optimization_background, job_id, request)\n\n    return {'id': job_id, 'status': 'pending'}\n\n@router.get(\"/{optimization_id}\", response_model=OptimizationStatus)\nasync def get_optimization_status(optimization_id: str):\n    \"\"\"Get optimization status\"\"\"\n    if optimization_id not in optimization_jobs:\n        raise HTTPException(status_code=404, detail=\"Optimization not found\")\n\n    return optimization_jobs[optimization_id]\n</code></pre>"},{"location":"IMPLEMENTATION_ROADMAP.html#task-12-implement-shap-analysis-endpoints","title":"Task 1.2: Implement SHAP Analysis Endpoints","text":"<p>File: <code>/backend/app/api/v1/analysis.py</code></p> <pre><code># Add real SHAP implementation\n\n@router.post(\"/shap\")\nasync def generate_shap_analysis(\n    optimization_id: str,\n    plot_types: List[str] = [\"bar\", \"beeswarm\"]\n):\n    \"\"\"Generate SHAP analysis from optimization results\"\"\"\n    if optimization_id not in optimization_jobs:\n        raise HTTPException(status_code=404, detail=\"Optimization not found\")\n\n    opt_job = optimization_jobs[optimization_id]\n    if opt_job['status'] != 'completed':\n        raise HTTPException(status_code=400, detail=\"Optimization not completed\")\n\n    # Build SHAP workflow\n    opt_result = opt_job['result']\n\n    workflow = {\n        \"id\": f\"shap_{uuid.uuid4().hex[:8]}\",\n        \"name\": \"SHAP Analysis\",\n        \"nodes\": [{\n            \"id\": \"shap\",\n            \"data\": {\n                \"type\": \"shap-analysis\",\n                \"config\": {\"plot_types\": plot_types}\n            }\n        }],\n        \"edges\": []\n    }\n\n    # Execute SHAP\n    executor = WorkflowExecutor(workflow)\n    # Inject optimization result as input\n    executor.results['input'] = opt_result\n\n    shap_result = executor.execute_node('shap')\n\n    return {\n        \"feature_importance\": extract_feature_importance(shap_result),\n        \"plots\": shap_result.get('plots', {})\n    }\n</code></pre>"},{"location":"IMPLEMENTATION_ROADMAP.html#phase-2-update-frontend-to-use-real-apis","title":"Phase 2: Update Frontend to Use Real APIs","text":""},{"location":"IMPLEMENTATION_ROADMAP.html#task-21-create-optimizer-api-client","title":"Task 2.1: Create Optimizer API Client","text":"<p>File: <code>/frontend/src/lib/api.ts</code></p> <pre><code>export interface OptimizationRequest {\n  dataset_id: string;\n  dataset_source: 'uci' | 'upload';\n  selected_features: string[];\n  target_column: string;\n  study_name: string;\n  database_name: string;\n  num_trials: number;\n  model_name?: string;\n}\n\nexport interface OptimizationStatus {\n  id: string;\n  status: 'pending' | 'running' | 'completed' | 'failed';\n  current_trial: number;\n  total_trials: number;\n  best_value: number | null;\n  best_params: Record&lt;string, any&gt; | null;\n  started_at: string;\n  completed_at: string | null;\n  error: string | null;\n}\n\nexport async function startOptimization(\n  request: OptimizationRequest\n): Promise&lt;{ id: string }&gt; {\n  const response = await fetch(`${API_BASE_URL}/api/v1/optimize`, {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify(request),\n  });\n\n  if (!response.ok) {\n    const error = await response.json();\n    throw new Error(error.detail || 'Failed to start optimization');\n  }\n\n  return response.json();\n}\n\nexport async function getOptimizationStatus(\n  optimizationId: string\n): Promise&lt;OptimizationStatus&gt; {\n  const response = await fetch(\n    `${API_BASE_URL}/api/v1/optimize/${optimizationId}`\n  );\n\n  if (!response.ok) {\n    throw new Error('Failed to get optimization status');\n  }\n\n  return response.json();\n}\n\nexport async function pollOptimization(\n  optimizationId: string,\n  onUpdate: (status: OptimizationStatus) =&gt; void,\n  intervalMs: number = 2000\n): Promise&lt;OptimizationStatus&gt; {\n  return new Promise((resolve, reject) =&gt; {\n    const poll = async () =&gt; {\n      try {\n        const status = await getOptimizationStatus(optimizationId);\n        onUpdate(status);\n\n        if (status.status === 'completed' || status.status === 'failed') {\n          resolve(status);\n        } else {\n          setTimeout(poll, intervalMs);\n        }\n      } catch (error) {\n        reject(error);\n      }\n    };\n\n    poll();\n  });\n}\n\nexport async function generateSHAP(\n  optimizationId: string,\n  plotTypes: string[] = ['bar', 'beeswarm']\n): Promise&lt;any&gt; {\n  const response = await fetch(`${API_BASE_URL}/api/v1/analysis/shap`, {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({ optimization_id: optimizationId, plot_types: plotTypes }),\n  });\n\n  if (!response.ok) {\n    throw new Error('Failed to generate SHAP analysis');\n  }\n\n  return response.json();\n}\n</code></pre>"},{"location":"IMPLEMENTATION_ROADMAP.html#task-22-update-optimizestep-to-use-real-api","title":"Task 2.2: Update OptimizeStep to Use Real API","text":"<p>File: <code>/frontend/src/pages/Optimizer.tsx</code></p> <p>Replace the simulated <code>startOptimization()</code> function:</p> <pre><code>function OptimizeStep({ onNext, onBack, workflowData, setWorkflowData }: StepProps) {\n  const [isRunning, setIsRunning] = useState(false);\n  const [progress, setProgress] = useState(0);\n  const [currentTrial, setCurrentTrial] = useState(0);\n  const [optimizationId, setOptimizationId] = useState&lt;string | null&gt;(null);\n\n  const startOptimization = async () =&gt; {\n    if (!workflowData.dataset) return;\n\n    setIsRunning(true);\n    setProgress(0);\n    setCurrentTrial(0);\n\n    try {\n      // Start real optimization\n      const { id } = await startOptimization({\n        dataset_id: workflowData.dataset.id,\n        dataset_source: workflowData.dataset.source,\n        selected_features: workflowData.features.selectedFeatures,\n        target_column: workflowData.features.targetColumn!,\n        study_name: workflowData.configuration.studyName,\n        database_name: workflowData.configuration.databaseName,\n        num_trials: workflowData.configuration.numTrials,\n      });\n\n      setOptimizationId(id);\n\n      // Poll for status updates\n      const finalStatus = await pollOptimization(id, (status) =&gt; {\n        setCurrentTrial(status.current_trial);\n        setProgress((status.current_trial / status.total_trials) * 100);\n      });\n\n      // Update workflow data with real results\n      setWorkflowData((prev) =&gt; ({\n        ...prev,\n        optimization: {\n          executionId: id,\n          status: 'completed',\n          results: {\n            bestValue: finalStatus.best_value,\n            bestParams: finalStatus.best_params,\n            trials: [], // Could fetch trial history separately\n          },\n        },\n      }));\n\n      setIsRunning(false);\n    } catch (error) {\n      setError(error instanceof Error ? error.message : 'Optimization failed');\n      setIsRunning(false);\n    }\n  };\n\n  // Rest of component...\n}\n</code></pre>"},{"location":"IMPLEMENTATION_ROADMAP.html#task-23-update-analyzestep-to-use-real-api","title":"Task 2.3: Update AnalyzeStep to Use Real API","text":"<pre><code>const generateSHAP = async () =&gt; {\n  if (!workflowData.optimization.executionId) return;\n\n  setIsGenerating(true);\n\n  try {\n    // Call real SHAP API\n    const shapResult = await generateSHAP(\n      workflowData.optimization.executionId,\n      ['bar', 'beeswarm', 'waterfall']\n    );\n\n    setWorkflowData((prev) =&gt; ({\n      ...prev,\n      analysis: {\n        shapData: {\n          featureImportance: shapResult.feature_importance,\n          plots: shapResult.plots,\n          generated: true,\n        },\n      },\n    }));\n\n    setIsGenerating(false);\n  } catch (error) {\n    setError(error instanceof Error ? error.message : 'SHAP generation failed');\n    setIsGenerating(false);\n  }\n};\n</code></pre>"},{"location":"IMPLEMENTATION_ROADMAP.html#phase-3-add-progress-tracking-with-websockets-optional-but-recommended","title":"Phase 3: Add Progress Tracking with WebSockets (Optional but Recommended)","text":""},{"location":"IMPLEMENTATION_ROADMAP.html#why-websockets","title":"Why WebSockets?","text":"<p>Long-running optimizations (10+ minutes) benefit from real-time updates instead of polling.</p>"},{"location":"IMPLEMENTATION_ROADMAP.html#implementation","title":"Implementation","text":"<p>Backend: <code>backend/app/api/v1/optimize.py</code></p> <pre><code>from fastapi import WebSocket\n\n@router.websocket(\"/ws/{optimization_id}\")\nasync def optimization_websocket(websocket: WebSocket, optimization_id: str):\n    await websocket.accept()\n\n    while True:\n        if optimization_id in optimization_jobs:\n            status = optimization_jobs[optimization_id]\n            await websocket.send_json(status)\n\n            if status['status'] in ['completed', 'failed']:\n                break\n\n        await asyncio.sleep(1)\n\n    await websocket.close()\n</code></pre> <p>Frontend: Use WebSocket for live updates</p> <pre><code>const ws = new WebSocket(`ws://localhost:8000/api/v1/optimize/ws/${id}`);\n\nws.onmessage = (event) =&gt; {\n  const status = JSON.parse(event.data);\n  setCurrentTrial(status.current_trial);\n  setProgress((status.current_trial / status.total_trials) * 100);\n\n  if (status.status === 'completed') {\n    // Handle completion\n  }\n};\n</code></pre>"},{"location":"IMPLEMENTATION_ROADMAP.html#phase-4-testing-checklist","title":"Phase 4: Testing Checklist","text":""},{"location":"IMPLEMENTATION_ROADMAP.html#backend-tests","title":"Backend Tests","text":"<ul> <li>[ ] <code>/api/v1/optimize</code> starts optimization correctly</li> <li>[ ] <code>/api/v1/optimize/{id}</code> returns accurate status</li> <li>[ ] Optimization completes with real Optuna results</li> <li>[ ] <code>/api/v1/analysis/shap</code> generates real SHAP plots</li> <li>[ ] Error handling for invalid inputs</li> <li>[ ] Database persistence works</li> </ul>"},{"location":"IMPLEMENTATION_ROADMAP.html#frontend-tests","title":"Frontend Tests","text":"<ul> <li>[ ] UI calls <code>/api/v1/optimize</code> instead of mock</li> <li>[ ] Progress bar shows real trial progress</li> <li>[ ] Results match backend response</li> <li>[ ] SHAP analysis displays real feature importance</li> <li>[ ] Error states are handled properly</li> <li>[ ] Loading states work correctly</li> </ul>"},{"location":"IMPLEMENTATION_ROADMAP.html#integration-tests","title":"Integration Tests","text":"<ul> <li>[ ] End-to-end: Select dataset \u2192 Optimize \u2192 SHAP \u2192 Report</li> <li>[ ] Test with different datasets (UCI, uploaded)</li> <li>[ ] Test with different trial counts (10, 100, 1000)</li> <li>[ ] Test error scenarios (invalid features, missing data)</li> <li>[ ] Test cancellation of running optimization</li> </ul>"},{"location":"IMPLEMENTATION_ROADMAP.html#phase-5-performance-optimization","title":"Phase 5: Performance Optimization","text":""},{"location":"IMPLEMENTATION_ROADMAP.html#caching","title":"Caching","text":"<pre><code>from functools import lru_cache\n\n@lru_cache(maxsize=100)\ndef get_uci_dataset(dataset_id: int):\n    return fetch_ucirepo(id=dataset_id)\n</code></pre>"},{"location":"IMPLEMENTATION_ROADMAP.html#async-processing","title":"Async Processing","text":"<ul> <li>Use Celery or RQ for background task management</li> <li>Store results in Redis for fast access</li> <li>Implement job queue for multiple concurrent optimizations</li> </ul>"},{"location":"IMPLEMENTATION_ROADMAP.html#database","title":"Database","text":"<p>Replace in-memory <code>optimization_jobs</code> dict with: - PostgreSQL for production - Redis for fast access to status - MongoDB for storing large result objects</p>"},{"location":"IMPLEMENTATION_ROADMAP.html#timeline-estimates","title":"Timeline Estimates","text":"Phase Estimated Time Priority Phase 1.1: Optimize API 4-6 hours High Phase 1.2: SHAP API 2-3 hours High Phase 2.1: API Client 1-2 hours High Phase 2.2: Update OptimizeStep 2-3 hours High Phase 2.3: Update AnalyzeStep 1-2 hours High Phase 3: WebSockets 3-4 hours Medium Phase 4: Testing 4-6 hours High Phase 5: Optimization 3-4 hours Low <p>Total: ~20-30 hours for complete implementation</p>"},{"location":"IMPLEMENTATION_ROADMAP.html#current-vs-future-flow","title":"Current vs Future Flow","text":""},{"location":"IMPLEMENTATION_ROADMAP.html#current-simulated","title":"Current (Simulated)","text":"<pre><code>User clicks \"Start\" \u2192 setTimeout loop \u2192 Show mock results\n</code></pre>"},{"location":"IMPLEMENTATION_ROADMAP.html#future-real","title":"Future (Real)","text":"<pre><code>User clicks \"Start\"\n    \u2193\nPOST /api/v1/optimize (start background job)\n    \u2193\nPoll GET /api/v1/optimize/{id} every 2 seconds\n    \u2193\nUpdate progress bar with real trial count\n    \u2193\nWhen status === 'completed'\n    \u2193\nDisplay real optimization results\n    \u2193\nPOST /api/v1/analysis/shap\n    \u2193\nDisplay real SHAP analysis\n</code></pre> <p>Next Action: Start with Phase 1.1 (Implement Optimize API)</p>"},{"location":"OPTIMIZER_ARCHITECTURE.html","title":"QuOptuna Optimizer Architecture Documentation","text":""},{"location":"OPTIMIZER_ARCHITECTURE.html#quoptuna-optimizer-architecture-documentation","title":"QuOptuna Optimizer Architecture Documentation","text":""},{"location":"OPTIMIZER_ARCHITECTURE.html#current-state-simulated-vs-real-implementation","title":"Current State: Simulated vs Real Implementation","text":""},{"location":"OPTIMIZER_ARCHITECTURE.html#important-the-optimizer-page-is-currently-simulated","title":"\u26a0\ufe0f Important: The Optimizer Page is Currently Simulated","text":"<p>Yes, you're absolutely right! The optimizer and SHAP analysis are currently simulated - they're not actually running real computations. This is why the process feels too fast.</p>"},{"location":"OPTIMIZER_ARCHITECTURE.html#whats-real-vs-simulated","title":"What's Real vs Simulated","text":""},{"location":"OPTIMIZER_ARCHITECTURE.html#real-backend-services-exist-and-work","title":"\u2705 Real Backend Services (Exist and Work)","text":"<p>Located in <code>/backend/app/services/workflow_service.py</code>:</p> <ol> <li>Real Optuna Optimization (lines 270-311)</li> <li>Uses <code>quoptuna.Optimizer</code> class</li> <li>Runs actual hyperparameter optimization</li> <li>Saves results to SQLite database</li> <li> <p>Returns best trial, parameters, and study info</p> </li> <li> <p>Real SHAP Analysis (lines 313-344)</p> </li> <li>Uses <code>quoptuna.XAI</code> class</li> <li>Generates actual SHAP plots (bar, beeswarm, violin, waterfall)</li> <li>Calculates real feature importance</li> <li> <p>Creates visual plots</p> </li> <li> <p>UCI Dataset Fetching (lines 131-147)</p> </li> <li>\u2705 THIS IS REAL NOW - Uses <code>ucimlrepo</code> library</li> <li>Fetches actual datasets from UCI repository</li> <li> <p>Returns real column names and data</p> </li> <li> <p>Data Preparation (lines 195-219)</p> </li> <li>Real train/test split</li> <li>Real data scaling</li> <li>Real label encoding</li> </ol>"},{"location":"OPTIMIZER_ARCHITECTURE.html#simulated-frontend-code","title":"\u274c Simulated Frontend Code","text":"<p>Located in <code>/frontend/src/pages/Optimizer.tsx</code>:</p> <ol> <li> <p>Simulated Optimization (lines 259-301)    </p><pre><code>// This is FAKE - just a setTimeout loop\nfor (let i = 1; i &lt;= totalTrials; i++) {\n  await new Promise((resolve) =&gt; setTimeout(resolve, 50));\n  setCurrentTrial(i);\n  setProgress((i / totalTrials) * 100);\n}\n\n// Mock results - not real\nconst mockResults = {\n  bestValue: 0.9234,  // Hardcoded!\n  bestParams: { ... } // Hardcoded!\n};\n</code></pre><p></p> </li> <li> <p>Simulated SHAP Analysis (lines 455-477)    </p><pre><code>// This is FAKE - just random numbers\nconst mockSHAPData = {\n  featureImportance: features.map(feature =&gt; ({\n    feature,\n    importance: Math.random() * 0.5 + 0.1  // Random!\n  }))\n};\n</code></pre><p></p> </li> <li> <p>Simulated Report Generation (lines 623-686)    </p><pre><code>// This is a template string - not AI generated\nconst report = `# Optimization Analysis Report...`;\n</code></pre><p></p> </li> </ol>"},{"location":"OPTIMIZER_ARCHITECTURE.html#api-endpoints-are-stubs","title":"\ud83d\udea7 API Endpoints are Stubs","text":"<p>Located in <code>/backend/app/api/v1/</code>:</p> <ul> <li><code>/api/v1/optimize</code> - All TODOs, not implemented</li> <li><code>/api/v1/analysis/shap</code> - All TODOs, not implemented</li> <li><code>/api/v1/analysis/report</code> - All TODOs, not implemented</li> </ul>"},{"location":"OPTIMIZER_ARCHITECTURE.html#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        FRONTEND                                  \u2502\n\u2502  /frontend/src/pages/Optimizer.tsx                              \u2502\n\u2502                                                                   \u2502\n\u2502  Step 1: Dataset Selection                                       \u2502\n\u2502    \u251c\u2500 Upload CSV         \u2500\u2500\u2500\u2500\u2510                                  \u2502\n\u2502    \u2514\u2500 Select UCI Dataset \u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500 \u2705 REAL: Calls backend API   \u2502\n\u2502                               \u2502                                  \u2502\n\u2502  Step 2: Features Selection   \u2502                                  \u2502\n\u2502    \u2514\u2500 Select columns      \u2500\u2500\u2500\u2500\u2524 \u274c STORED IN UI STATE ONLY     \u2502\n\u2502                               \u2502                                  \u2502\n\u2502  Step 3: Configuration        \u2502                                  \u2502\n\u2502    \u2514\u2500 Study name, trials  \u2500\u2500\u2500\u2500\u2524 \u274c STORED IN UI STATE ONLY     \u2502\n\u2502                               \u2502                                  \u2502\n\u2502  Step 4: Optimization         \u2502                                  \u2502\n\u2502    \u2514\u2500 Start Optimization  \u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500 \u274c SIMULATED - setTimeout   \u2502\n\u2502                               \u2502                                  \u2502\n\u2502  Step 5: SHAP Analysis        \u2502                                  \u2502\n\u2502    \u2514\u2500 Generate SHAP       \u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500 \u274c SIMULATED - Random data  \u2502\n\u2502                               \u2502                                  \u2502\n\u2502  Step 6: Generate Report      \u2502                                  \u2502\n\u2502    \u2514\u2500 AI Report           \u2500\u2500\u2500\u2500\u2518\u2500\u2500\u2500 \u274c SIMULATED - Template     \u2502\n\u2502                                                                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u2193 HTTP\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     BACKEND API LAYER                            \u2502\n\u2502  /backend/app/api/v1/                                            \u2502\n\u2502                                                                   \u2502\n\u2502  \u2705 /data/uci/{id}        \u2192 fetch_uci_dataset()                 \u2502\n\u2502  \u2705 /data/upload          \u2192 upload_dataset()                    \u2502\n\u2502  \ud83d\udea7 /optimize             \u2192 start_optimization() [TODO]         \u2502\n\u2502  \ud83d\udea7 /optimize/{id}        \u2192 get_optimization_status() [TODO]    \u2502\n\u2502  \ud83d\udea7 /analysis/shap        \u2192 generate_shap_analysis() [TODO]     \u2502\n\u2502  \ud83d\udea7 /analysis/report      \u2192 generate_ai_report() [TODO]         \u2502\n\u2502                                                                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    BACKEND SERVICES LAYER                        \u2502\n\u2502  /backend/app/services/workflow_service.py                      \u2502\n\u2502                                                                   \u2502\n\u2502  \u2705 REAL IMPLEMENTATION EXISTS:                                 \u2502\n\u2502                                                                   \u2502\n\u2502  WorkflowExecutor                                                \u2502\n\u2502    \u251c\u2500 _execute_data_uci()           [REAL: Uses ucimlrepo]     \u2502\n\u2502    \u251c\u2500 _execute_optimization()       [REAL: Uses quoptuna.Optimizer]\n\u2502    \u251c\u2500 _execute_shap_analysis()      [REAL: Uses quoptuna.XAI]  \u2502\n\u2502    \u251c\u2500 _execute_train_test_split()   [REAL: sklearn]            \u2502\n\u2502    \u2514\u2500 _execute_generate_report()    [TODO: Needs LLM]          \u2502\n\u2502                                                                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    CORE QUOPTUNA LIBRARY                         \u2502\n\u2502  (Installed as dependency)                                       \u2502\n\u2502                                                                   \u2502\n\u2502  \u2705 quoptuna.Optimizer                                           \u2502\n\u2502     \u2514\u2500 optimize() \u2192 Runs Optuna study with quantum models       \u2502\n\u2502                                                                   \u2502\n\u2502  \u2705 quoptuna.XAI                                                 \u2502\n\u2502     \u2514\u2500 SHAP analysis, plots (bar, beeswarm, violin, waterfall)  \u2502\n\u2502                                                                   \u2502\n\u2502  \u2705 quoptuna.DataPreparation                                     \u2502\n\u2502     \u2514\u2500 Train/test split, scaling, encoding                      \u2502\n\u2502                                                                   \u2502\n\u2502  \u2705 quoptuna.create_model()                                      \u2502\n\u2502     \u2514\u2500 Quantum and classical models                             \u2502\n\u2502                                                                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"OPTIMIZER_ARCHITECTURE.html#how-ui-triggers-backend-services","title":"How UI Triggers Backend Services","text":""},{"location":"OPTIMIZER_ARCHITECTURE.html#current-flow-what-actually-happens","title":"Current Flow (What Actually Happens)","text":""},{"location":"OPTIMIZER_ARCHITECTURE.html#dataset-selection-working","title":"\u2705 Dataset Selection (Working)","text":"<pre><code>User clicks \"Iris\" in modal\n    \u2193\nfrontend/src/pages/Optimizer.tsx:handleUCISelect(53)\n    \u2193\nfetchUCIDataset(53) in frontend/src/lib/api.ts\n    \u2193\nHTTP POST to /api/v1/data/uci/53\n    \u2193\nbackend/app/api/v1/data.py:fetch_uci_dataset(53)\n    \u2193\nUses ucimlrepo.fetch_ucirepo(id=53)\n    \u2193\nReturns: { dataset_id: \"53\", name: \"Iris\", rows: 150, columns: [...] }\n    \u2193\nStored in workflowData.dataset state\n</code></pre>"},{"location":"OPTIMIZER_ARCHITECTURE.html#optimization-currently-simulated","title":"\u274c Optimization (Currently Simulated)","text":"<pre><code>User clicks \"Start Optimization\"\n    \u2193\nfrontend/src/pages/Optimizer.tsx:startOptimization()\n    \u2193\n[CURRENTLY: setTimeout loop creating fake progress]\n    \u2193\n[SHOULD BE: HTTP POST to /api/v1/optimize]\n    \u2193\n[SHOULD BE: Backend runs real Optuna optimization]\n</code></pre>"},{"location":"OPTIMIZER_ARCHITECTURE.html#shap-analysis-currently-simulated","title":"\u274c SHAP Analysis (Currently Simulated)","text":"<pre><code>User clicks \"Generate SHAP Analysis\"\n    \u2193\nfrontend/src/pages/Optimizer.tsx:generateSHAP()\n    \u2193\n[CURRENTLY: Random numbers for feature importance]\n    \u2193\n[SHOULD BE: HTTP POST to /api/v1/analysis/shap]\n    \u2193\n[SHOULD BE: Backend generates real SHAP plots]\n</code></pre>"},{"location":"OPTIMIZER_ARCHITECTURE.html#what-services-backend-uses","title":"What Services Backend Uses","text":""},{"location":"OPTIMIZER_ARCHITECTURE.html#1-optuna-hyperparameter-optimization","title":"1. Optuna (Hyperparameter Optimization)","text":"<ul> <li>Library: <code>optuna&gt;=4.0.0</code></li> <li>Purpose: Bayesian optimization framework</li> <li>Used in: <code>workflow_service.py:_execute_optimization()</code></li> <li>Features:</li> <li>Pruning (early stopping of bad trials)</li> <li>Visualization of optimization history</li> <li>SQLite database storage</li> <li>Multiple samplers (TPE, CMA-ES, etc.)</li> </ul>"},{"location":"OPTIMIZER_ARCHITECTURE.html#2-shap-explainable-ai","title":"2. SHAP (Explainable AI)","text":"<ul> <li>Library: <code>shap&gt;=0.46.0</code></li> <li>Purpose: Explain model predictions</li> <li>Used in: <code>workflow_service.py:_execute_shap_analysis()</code></li> <li>Features:</li> <li>Feature importance calculation</li> <li>Multiple plot types (bar, beeswarm, violin, waterfall)</li> <li>Works with any ML model</li> </ul>"},{"location":"OPTIMIZER_ARCHITECTURE.html#3-pennylane-quantum-machine-learning","title":"3. PennyLane (Quantum Machine Learning)","text":"<ul> <li>Library: <code>pennylane&gt;=0.39.0</code></li> <li>Purpose: Quantum computing and quantum ML</li> <li>Used in: <code>quoptuna.create_model()</code></li> <li>Features:</li> <li>Variational Quantum Circuits (VQC)</li> <li>Data Reuploading</li> <li>Quantum kernels</li> <li>Hybrid quantum-classical models</li> </ul>"},{"location":"OPTIMIZER_ARCHITECTURE.html#4-scikit-learn-classical-ml","title":"4. Scikit-learn (Classical ML)","text":"<ul> <li>Library: <code>scikit-learn&gt;=1.5.0</code></li> <li>Purpose: Classical machine learning</li> <li>Used in: <code>quoptuna.DataPreparation</code>, models</li> <li>Features:</li> <li>Train/test split</li> <li>Data scaling (StandardScaler)</li> <li>Label encoding</li> <li>Classical models (SVM, RandomForest, etc.)</li> </ul>"},{"location":"OPTIMIZER_ARCHITECTURE.html#5-uci-ml-repository","title":"5. UCI ML Repository","text":"<ul> <li>Library: <code>ucimlrepo&gt;=0.0.3</code></li> <li>Purpose: Access to 600+ datasets</li> <li>Used in: <code>data.py:fetch_uci_dataset()</code>, <code>workflow_service.py</code></li> <li>Features:</li> <li>Fetch datasets by ID</li> <li>Automatic feature/target separation</li> <li>Metadata included</li> </ul>"},{"location":"OPTIMIZER_ARCHITECTURE.html#6-pandas-numpy","title":"6. Pandas &amp; NumPy","text":"<ul> <li>Libraries: <code>pandas&gt;=2.2.0</code>, <code>numpy&gt;=1.24.0</code></li> <li>Purpose: Data manipulation</li> <li>Used in: All data processing steps</li> <li>Features:</li> <li>DataFrame operations</li> <li>Data cleaning</li> <li>Statistical analysis</li> </ul>"},{"location":"OPTIMIZER_ARCHITECTURE.html#implementation-status-summary","title":"Implementation Status Summary","text":"Component Status Location Notes Dataset Selection \u2705 Complete <code>frontend/Optimizer.tsx</code>, <code>backend/data.py</code> Fully working Features Selection \u2705 UI Only <code>frontend/Optimizer.tsx</code> Works but not persisted to backend Configuration \u2705 UI Only <code>frontend/Optimizer.tsx</code> Works but not persisted to backend Optimization \u26a0\ufe0f Backend Only <code>backend/workflow_service.py</code> Real code exists but UI uses mock SHAP Analysis \u26a0\ufe0f Backend Only <code>backend/workflow_service.py</code> Real code exists but UI uses mock Report Generation \u274c Partial <code>frontend/Optimizer.tsx</code> UI has template, backend needs LLM Optimize API \ud83d\udea7 TODO <code>backend/optimize.py</code> Endpoints exist but not implemented Analysis API \ud83d\udea7 TODO <code>backend/analysis.py</code> Endpoints exist but not implemented"},{"location":"OPTIMIZER_ARCHITECTURE.html#why-its-fast-simulated","title":"Why It's Fast (Simulated)","text":"<p>The optimizer completes in ~5 seconds because:</p> <ol> <li>Optimization: Just a <code>setTimeout(50ms)</code> per \"trial\"</li> <li>Real Optuna: 1-10 minutes for 100 trials</li> <li> <p>Simulated: 5 seconds (50ms \u00d7 100 trials)</p> </li> <li> <p>SHAP Analysis: Just <code>Math.random()</code></p> </li> <li>Real SHAP: 30 seconds to 5 minutes</li> <li> <p>Simulated: Instant</p> </li> <li> <p>No Model Training: No actual ML models are trained</p> </li> <li>Real training: Minutes to hours</li> <li>Simulated: 0 seconds</li> </ol>"},{"location":"OPTIMIZER_ARCHITECTURE.html#next-steps-to-connect-ui-to-real-backend","title":"Next Steps to Connect UI to Real Backend","text":"<p>See <code>IMPLEMENTATION_ROADMAP.md</code> for detailed steps to: 1. Implement <code>/api/v1/optimize</code> endpoints 2. Implement <code>/api/v1/analysis</code> endpoints 3. Update frontend to call real APIs 4. Add progress tracking with WebSockets 5. Implement AI report generation with LLM</p> <p>Generated: 2025-11-16 Status: Current implementation uses simulated data in UI, but real services exist in backend</p>"},{"location":"WORKFLOW_TESTING.html","title":"Workflow Testing Guide","text":""},{"location":"WORKFLOW_TESTING.html#workflow-testing-guide","title":"Workflow Testing Guide","text":""},{"location":"WORKFLOW_TESTING.html#overview","title":"Overview","text":"<p>This document describes the current state of the optimizer workflow, known issues, and how to test it properly.</p>"},{"location":"WORKFLOW_TESTING.html#recent-fixes","title":"Recent Fixes","text":""},{"location":"WORKFLOW_TESTING.html#1-data-format-handling-2025-11-16","title":"1. Data Format Handling (2025-11-16)","text":"<p>Fixed critical data format mismatches between optimization and SHAP analysis:</p> <ol> <li>Optimizer expects numpy arrays for <code>train_x</code>, <code>train_y</code>, <code>test_x</code>, <code>test_y</code></li> <li>XAI (SHAP) expects pandas DataFrames for the same data</li> <li>Model.fit() requires numpy arrays</li> </ol> <p>Solution: The workflow now: - Stores DataFrames from DataPreparation - Converts to numpy arrays when passing to Optimizer - Retains DataFrames in optimization results - Converts to numpy arrays only for model.fit() in SHAP analysis - Passes DataFrames to XAI constructor</p>"},{"location":"WORKFLOW_TESTING.html#2-pennylane-quantum-device-deprecation-2025-11-16-fixed","title":"2. PennyLane Quantum Device Deprecation (2025-11-16) \u2705 FIXED","text":"<p>Previous Error: <code>pennylane.exceptions.DeviceError: Device default.qubit.jax does not exist</code></p> <p>Fix: Updated all 14 quantum model classes to use <code>default.qubit</code> instead of deprecated <code>default.qubit.jax</code>: - DataReuploadingClassifier &amp; DataReuploadingClassifierSeparable - DressedQuantumCircuitClassifier &amp; DressedQuantumCircuitClassifierSeparable - CircuitCentricClassifier - ProjectedQuantumKernel - QuantumKitchenSinks - QuantumMetricLearner - IQPVariationalClassifier &amp; IQPKernelClassifier - TreeTensorClassifier - SeparableVariationalClassifier &amp; SeparableKernelClassifier - QuanvolutionalNeuralNetwork, VanillaQNN, WEINet</p> <p>Result: All quantum models now work correctly during optimization! \ud83c\udf89</p>"},{"location":"WORKFLOW_TESTING.html#3-binary-label-encoding-2025-11-16-fixed","title":"3. Binary Label Encoding (2025-11-16) \u2705 FIXED","text":"<p>Previous Error: <code>IndexError: boolean index did not match indexed array</code></p> <p>Cause: Quantum models expect binary classification labels as -1 and 1, but UCI datasets often have labels as 0/1 or other values (e.g., \"g\"/\"b\" for Ionosphere).</p> <p>Fix: Added automatic label encoding step in workflow: - Detects binary classification (2 unique classes) - Automatically maps first class \u2192 -1, second class \u2192 1 - Logs the mapping for transparency - Works with any binary dataset (0/1, g/b, yes/no, etc.)</p> <p>Example: Breast Cancer dataset with labels [0, 1] is now automatically mapped to [-1, 1]</p> <p>Result: All binary classification datasets now work with quantum models! \ud83c\udf89</p>"},{"location":"WORKFLOW_TESTING.html#known-issues","title":"Known Issues","text":""},{"location":"WORKFLOW_TESTING.html#1-sklearn-parameter-validation","title":"1. sklearn Parameter Validation","text":"<p>Error: <code>The 'learning_rate' parameter of MLPClassifier must be a str among {'constant', 'adaptive', 'invscaling'}</code></p> <p>Cause: Optuna search space suggests numeric learning rates, but MLPClassifier expects string values</p> <p>Impact: Some MLPClassifier trials fail with 0.0 value</p> <p>Workaround: Ignored - Optuna will try other parameters</p>"},{"location":"WORKFLOW_TESTING.html#testing-the-workflow","title":"Testing the Workflow","text":""},{"location":"WORKFLOW_TESTING.html#recommended-test-process","title":"Recommended Test Process","text":"<ol> <li>Select UCI Dataset:</li> <li>Choose a simple dataset (Wine, Iris recommended)</li> <li>Use small number of features (&lt; 15)</li> <li> <p>Binary classification preferred</p> </li> <li> <p>Configure Optimization:</p> </li> <li>Study name: <code>test_wine</code> or similar</li> <li>Database name: <code>test_wine</code> or similar</li> <li>Num trials: 10-100 (quantum models now work!)</li> <li>Start with 10-20 trials for quick testing</li> <li> <p>Use 50-100 trials for better results</p> </li> <li> <p>Run Optimization:</p> </li> <li>Both quantum and classical models now work</li> <li>Some MLPClassifier trials may fail (parameter validation)</li> <li>Should complete successfully with mixed model trials</li> <li> <p>Expect 1-5 minutes for 10-20 trials</p> </li> <li> <p>Generate SHAP Analysis:</p> </li> <li>Works with both quantum and classical models</li> <li>Generates bar, beeswarm, and waterfall plots</li> <li>Returns feature importance rankings</li> </ol>"},{"location":"WORKFLOW_TESTING.html#example-working-configuration","title":"Example Working Configuration","text":"<pre><code>{\n  \"dataset\": {\n    \"id\": \"109\",\n    \"name\": \"Wine\",\n    \"source\": \"uci\"\n  },\n  \"features\": {\n    \"selectedFeatures\": [\"Alcohol\", \"Malic_acid\", \"Ash\", \"Alcalinity_of_ash\"],\n    \"targetColumn\": \"class\"\n  },\n  \"configuration\": {\n    \"studyName\": \"wine_test\",\n    \"databaseName\": \"wine_test\",\n    \"numTrials\": 10\n  }\n}\n</code></pre>"},{"location":"WORKFLOW_TESTING.html#reference-notebooks","title":"Reference Notebooks","text":"<p>The workflow implementation is based on these working examples:</p> <ol> <li><code>experiments/basic_dataset_test/test_shap.ipynb</code></li> <li>Shows complete workflow: load study \u2192 create model \u2192 fit \u2192 XAI \u2192 plots</li> <li>Demonstrates correct data format handling</li> <li> <p>Example with Statlog dataset</p> </li> <li> <p><code>experiments/basic_dataset_test/test_new_data_test.ipynb</code></p> </li> <li>Shows UCI dataset fetching and preparation</li> <li>Demonstrates DataPreparation usage</li> <li>Shows conversion to numpy arrays for Optimizer</li> </ol>"},{"location":"WORKFLOW_TESTING.html#workflow-steps-technical","title":"Workflow Steps (Technical)","text":""},{"location":"WORKFLOW_TESTING.html#1-dataset-selection","title":"1. Dataset Selection","text":"<pre><code>Frontend \u2192 POST /api/v1/data/uci/{dataset_id}\nBackend: fetch_ucirepo(id) \u2192 return columns\n</code></pre>"},{"location":"WORKFLOW_TESTING.html#2-feature-selection","title":"2. Feature Selection","text":"<pre><code>Frontend: User selects features + target\nState stored in workflowData.features\n</code></pre>"},{"location":"WORKFLOW_TESTING.html#3-optimization","title":"3. Optimization","text":"<pre><code>Frontend \u2192 POST /api/v1/optimize\nBackend:\n  1. Create workflow nodes (data, features, split, model, optuna, optimize)\n  2. Execute workflow in topological order:\n     - data-uci: Fetch dataset\n     - feature-selection: Select specified features\n     - train-test-split: Use DataPreparation \u2192 returns DataFrames\n     - optimize: Convert to numpy \u2192 create Optimizer \u2192 run trials\n  3. Return optimization results with DataFrames intact\n</code></pre>"},{"location":"WORKFLOW_TESTING.html#4-shap-analysis","title":"4. SHAP Analysis","text":"<pre><code>Frontend \u2192 POST /api/v1/analysis/shap\nBackend:\n  1. Load best trial from Optuna study\n  2. Recreate model with best parameters\n  3. Convert DataFrames to numpy \u2192 fit model\n  4. Create XAI with model + DataFrames\n  5. Generate SHAP plots\n  6. Calculate feature importance\n  7. Return plots + importance\n</code></pre>"},{"location":"WORKFLOW_TESTING.html#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Frontend UI   \u2502\n\u2502  (React/TS)     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u2193 API Calls\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  FastAPI        \u2502\n\u2502  /api/v1/       \u2502\n\u2502  - optimize     \u2502\n\u2502  - analysis     \u2502\n\u2502  - data         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 WorkflowService \u2502\n\u2502  Executes nodes \u2502\n\u2502  in order       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2193         \u2193         \u2193          \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Optuna  \u2502 \u2502 XAI \u2502 \u2502 Data \u2502 \u2502 Models \u2502\n\u2502 Optimizer\u2502 \u2502SHAP \u2502 \u2502 Prep \u2502 \u2502        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"WORKFLOW_TESTING.html#success-criteria","title":"Success Criteria","text":"<p>\u2705 Optimization completes with best_value &gt; 0 \u2705 Multiple trials succeed (both quantum and classical models) \u2705 SHAP analysis generates without 500 error \u2705 Feature importance is non-empty array \u2705 Plots are base64-encoded images</p>"},{"location":"WORKFLOW_TESTING.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"WORKFLOW_TESTING.html#all-trials-returning-00","title":"\"All trials returning 0.0\"","text":"<ul> <li>Check if data has NaN values</li> <li>Verify target column has correct labels</li> <li>Try different dataset</li> <li>Check backend logs for specific model errors</li> </ul>"},{"location":"WORKFLOW_TESTING.html#shap-500-error","title":"\"SHAP 500 error\"","text":"<ul> <li>\u2705 Should be fixed now (data format issue resolved)</li> <li>If still occurring, check backend logs for specific error</li> <li>Verify optimization completed successfully</li> <li>Confirm database file exists in <code>backend/db/</code></li> </ul>"},{"location":"WORKFLOW_TESTING.html#no-plots-generated","title":"\"No plots generated\"","text":"<ul> <li>Check if model has predict_proba method</li> <li>Verify XAI config subset_size is reasonable</li> <li>Check backend logs for plot generation errors</li> </ul>"},{"location":"WORKFLOW_TESTING.html#quantum-models-still-failing","title":"\"Quantum models still failing\"","text":"<ul> <li>\u2705 Should be fixed now (PennyLane device updated)</li> <li>If still occurring, verify PennyLane version is up to date</li> <li>Check that backend has correct dependencies installed</li> </ul>"},{"location":"WORKFLOW_TESTING.html#summary","title":"Summary","text":"<p>The optimizer workflow is now fully functional with both quantum and classical models! \ud83c\udf89</p> <p>What works: - \u2705 UCI dataset selection and loading - \u2705 Feature selection - \u2705 Optuna hyperparameter optimization with 15+ model types - \u2705 Both quantum models (DataReuploading, DressedQuantumCircuit, etc.) - \u2705 Classical models (SVC, MLP, Perceptron) - \u2705 SHAP explainability analysis - \u2705 Feature importance visualization - \u2705 Complete end-to-end workflow</p> <p>Minor issues remaining: - Some MLPClassifier trials may fail due to parameter validation (doesn't affect workflow)</p> <p>The workflow successfully demonstrates quantum machine learning optimization and explainable AI!</p>"},{"location":"api_docs.html","title":"API Documentation","text":""},{"location":"api_docs.html#api-documentation","title":"API Documentation","text":""},{"location":"api_docs.html#main-api","title":"Main API","text":""},{"location":"api_docs.html#quoptuna.XAI","title":"XAI","text":"<pre><code>XAI(model: BaseEstimator, data: DataSet, config: XAIConfig | None = None)\n</code></pre> METHOD DESCRIPTION <code>__str__</code> <code>_generate_final_report</code> <code>_generate_plot</code> <code>_generate_report_images</code> <code>_get_explainer</code> <code>_get_plot_function</code> <code>_get_plot_values</code> <code>_get_shap_values</code> <code>_get_shap_values_each_class</code> <code>_handle_plot_error</code> <p>Handle plot generation errors.</p> <code>_initialize_chat</code> <code>_save_plot_to_base64</code> <code>_save_plot_to_file</code> <code>_set_shap_values_classes</code> <code>_validate_and_get_data</code> <code>_validate_shap_values_class</code> <p>Validate and get SHAP values for class-specific case.</p> <code>generate_report_with_langchain</code> <p>Generate comprehensive report using LangChain and multimodal LLM.</p> <code>get_average_precision_score</code> <p>Get the average precision score of the model.</p> <code>get_bar_plot</code> <code>get_beeswarm_plot</code> <code>get_classes</code> <p>Get model classes.</p> <code>get_classification_report</code> <p>Get the classification report of the model.</p> <code>get_cohens_kappa</code> <p>Get the cohens kappa of the model.</p> <code>get_confusion_matrix</code> <p>Get the confusion matrix of the model.</p> <code>get_f1_score</code> <p>Get the f1 score of the model.</p> <code>get_heatmap_plot</code> <code>get_log_loss</code> <p>Get the log loss of the model.</p> <code>get_mcc</code> <p>Get the mcc of the model.</p> <code>get_plot</code> <p>Generate plot with given configuration.</p> <code>get_precision</code> <p>Get the precision of the model.</p> <code>get_precision_recall_curve</code> <p>Get the precision recall curve of the model.</p> <code>get_recall</code> <p>Get the recall of the model.</p> <code>get_report</code> <p>Get the report of the model.</p> <code>get_roc_auc_score</code> <p>Get the roc auc score of the model.</p> <code>get_roc_curve</code> <p>Get the roc curve of the model.</p> <code>get_violin_plot</code> <code>get_waterfall_plot</code> <code>load_state</code> <p>Loads the state of the class from a pkl file.</p> <code>plot_confusion_matrix</code> <p>Plot confusion matrix with given configuration.</p> <code>save_state</code> <p>Saves the state of the class and its variables in a pkl file.</p> <code>validate_predict_proba</code> ATTRIBUTE DESCRIPTION <code>_classes</code> <p> </p> <code>_explainer</code> <p> TYPE: <code>Explainer | None</code> </p> <code>_predictions</code> <p> TYPE: <code>Series | None</code> </p> <code>_predictions_proba</code> <p> TYPE: <code>DataFrame | None</code> </p> <code>_shap_values</code> <p> TYPE: <code>Explanation | None</code> </p> <code>_shap_values_each_class</code> <p> TYPE: <code>dict[str, Explanation] | None</code> </p> <code>_x_test</code> <p> TYPE: <code>DataFrame | None</code> </p> <code>_y_test</code> <p> TYPE: <code>Series | None</code> </p> <code>config</code> <p> </p> <code>data</code> <p> </p> <code>data_key</code> <p> TYPE: <code>str</code> </p> <code>explainer</code> <p> TYPE: <code>Explainer</code> </p> <code>feature_names</code> <p> TYPE: <code>list[str] | None</code> </p> <code>max_display</code> <p> TYPE: <code>int</code> </p> <code>model</code> <p> </p> <code>onsubset</code> <p> TYPE: <code>bool</code> </p> <code>predictions</code> <p> TYPE: <code>Series</code> </p> <code>predictions_proba</code> <p> TYPE: <code>DataFrame</code> </p> <code>shap_values</code> <p> TYPE: <code>Explanation</code> </p> <code>shap_values_each_class</code> <p> TYPE: <code>dict[str, Explanation] | None</code> </p> <code>subset_size</code> <p> TYPE: <code>int</code> </p> <code>use_proba</code> <p> TYPE: <code>bool</code> </p> <code>x_test</code> <p> TYPE: <code>DataFrame</code> </p> <code>x_test_key</code> <p> TYPE: <code>str</code> </p> <code>y_test</code> <p> TYPE: <code>Series</code> </p> <code>y_test_key</code> <p> TYPE: <code>str</code> </p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def __init__(\n    self,\n    model: BaseEstimator,\n    data: DataSet,\n    config: XAIConfig | None = None,\n) -&gt; None:\n    if model is None:\n        msg = \"Model cannot be None\"\n        raise TypeError(msg)\n\n    self.config = config or XAIConfig()\n    self.model = model\n    self.data = data\n\n    # Explicitly declare instance attributes\n    self.use_proba: bool = self.config.use_proba\n    self.onsubset: bool = self.config.onsubset\n    self.feature_names: list[str] | None = self.config.feature_names\n    self.subset_size: int = self.config.subset_size\n    self.max_display: int = self.config.max_display\n    self.data_key: str = self.config.data_key\n    self.x_test_key: str = self.config.x_test_key\n    self.y_test_key: str = self.config.y_test_key\n\n    self._classes = self.get_classes\n    data_frame = self.data.get(self.data_key)\n    if self.feature_names is None and isinstance(data_frame, pd.DataFrame):\n        self.feature_names = list(data_frame.columns)\n\n    if self.use_proba:\n        self.validate_predict_proba()\n\n    # Initialize these as None, they'll be computed on demand\n    self._explainer: Explainer | None = None\n    self._shap_values: shap.Explanation | None = None\n    self._shap_values_each_class: dict[str, shap.Explanation] | None = None\n    self._x_test: pd.DataFrame | None = None\n    self._y_test: pd.Series | None = None\n    self._predictions: pd.Series | None = None\n    self._predictions_proba: pd.DataFrame | None = None\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI._classes","title":"_classes  <code>instance-attribute</code>","text":"<pre><code>_classes = get_classes\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI._explainer","title":"_explainer  <code>instance-attribute</code>","text":"<pre><code>_explainer: Explainer | None = None\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI._predictions","title":"_predictions  <code>instance-attribute</code>","text":"<pre><code>_predictions: Series | None = None\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI._predictions_proba","title":"_predictions_proba  <code>instance-attribute</code>","text":"<pre><code>_predictions_proba: DataFrame | None = None\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI._shap_values","title":"_shap_values  <code>instance-attribute</code>","text":"<pre><code>_shap_values: Explanation | None = None\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI._shap_values_each_class","title":"_shap_values_each_class  <code>instance-attribute</code>","text":"<pre><code>_shap_values_each_class: dict[str, Explanation] | None = None\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI._x_test","title":"_x_test  <code>instance-attribute</code>","text":"<pre><code>_x_test: DataFrame | None = None\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI._y_test","title":"_y_test  <code>instance-attribute</code>","text":"<pre><code>_y_test: Series | None = None\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config = config or XAIConfig()\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data = data\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.data_key","title":"data_key  <code>instance-attribute</code>","text":"<pre><code>data_key: str = data_key\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.explainer","title":"explainer  <code>property</code>","text":"<pre><code>explainer: Explainer\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.feature_names","title":"feature_names  <code>instance-attribute</code>","text":"<pre><code>feature_names: list[str] | None = feature_names\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.max_display","title":"max_display  <code>instance-attribute</code>","text":"<pre><code>max_display: int = max_display\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = model\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.onsubset","title":"onsubset  <code>instance-attribute</code>","text":"<pre><code>onsubset: bool = onsubset\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.predictions","title":"predictions  <code>property</code>","text":"<pre><code>predictions: Series\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.predictions_proba","title":"predictions_proba  <code>property</code>","text":"<pre><code>predictions_proba: DataFrame\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.shap_values","title":"shap_values  <code>property</code>","text":"<pre><code>shap_values: Explanation\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.shap_values_each_class","title":"shap_values_each_class  <code>property</code>","text":"<pre><code>shap_values_each_class: dict[str, Explanation] | None\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.subset_size","title":"subset_size  <code>instance-attribute</code>","text":"<pre><code>subset_size: int = subset_size\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.use_proba","title":"use_proba  <code>instance-attribute</code>","text":"<pre><code>use_proba: bool = use_proba\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.x_test","title":"x_test  <code>property</code>","text":"<pre><code>x_test: DataFrame\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.x_test_key","title":"x_test_key  <code>instance-attribute</code>","text":"<pre><code>x_test_key: str = x_test_key\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.y_test","title":"y_test  <code>property</code>","text":"<pre><code>y_test: Series\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.y_test_key","title":"y_test_key  <code>instance-attribute</code>","text":"<pre><code>y_test_key: str = y_test_key\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def __str__(self):\n    return str(self.get_report())\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI._generate_final_report","title":"_generate_final_report","text":"<pre><code>_generate_final_report(chat, report, images, prompt2)\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def _generate_final_report(self, chat, report, images, prompt2):\n    prompt_template = ChatPromptTemplate(\n        messages=[\n            SystemMessage(content=prompt2),\n            HumanMessage(content=\"Model Evaluation Report:\\n```\\n{report}\\n```\"),\n            MessagesPlaceholder(variable_name=\"images\"),\n        ]\n    )\n    image_messages = []\n    for plot_type, image_url in images.items():\n        image_messages.append(\n            HumanMessage(\n                content=[\n                    {\"type\": \"text\", \"text\": f\"Here is a {plot_type.replace('_', ' ')} plot:\"},\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": image_url, \"detail\": \"auto\"}},\n                ]\n            )\n        )\n\n    final_prompt = prompt_template.format_messages(report=str(report), images=image_messages)\n\n    response = chat(final_prompt)\n    return response.content\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI._generate_plot","title":"_generate_plot","text":"<pre><code>_generate_plot(plot_type: PlotType, values, max_display: int, index: int, save_config: dict | None) -&gt; str\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def _generate_plot(\n    self, plot_type: PlotType, values, max_display: int, index: int, save_config: dict | None\n) -&gt; str:\n    plt.figure()\n    if plot_type != \"waterfall\":\n        plot_func = self._get_plot_function(plot_type)\n        plot_func(values, max_display=max_display, show=False)\n    else:\n        shap.plots.waterfall(values[index], show=False)\n\n    base64_code = self._save_plot_to_base64()\n\n    if save_config is not None:  # Check if save_config exists\n        self._save_plot_to_file(save_config)\n\n    plt.close()\n    return base64_code\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI._generate_report_images","title":"_generate_report_images","text":"<pre><code>_generate_report_images(num_waterfall_plots: int)\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def _generate_report_images(self, num_waterfall_plots: int):\n    images: dict[str, str] = {}  # Change type hint to allow string keys\n    plot_types: list[PlotType] = [\"bar\", \"beeswarm\", \"violin\", \"heatmap\"]\n\n    try:\n        for plot_type in plot_types:\n            images[plot_type] = self.get_plot(plot_type)\n\n        if self.onsubset:\n            num_waterfall_plots = min(num_waterfall_plots, self.subset_size)\n        else:\n            num_waterfall_plots = min(num_waterfall_plots, len(self.x_test))\n\n        indices = sorted(random.sample(range(num_waterfall_plots), num_waterfall_plots))\n        for i in indices:\n            waterfall_plot_type: PlotType = \"waterfall\"\n            images[f\"{waterfall_plot_type}_{i}\"] = self.get_plot(waterfall_plot_type, index=i)\n\n        fig = self.plot_confusion_matrix()\n        img_buf = io.BytesIO()\n        fig.savefig(img_buf, format=\"png\")\n        img_buf.seek(0)\n        img_base64 = base64.b64encode(img_buf.getvalue()).decode(\"utf-8\")\n        images[\"confusion_matrix\"] = f\"data:image/png;base64,{img_base64}\"\n        plt.close(fig)\n\n    except Exception as e:\n        msg = f\"Error generating plots: {e}\"\n        raise ValueError(msg) from e\n\n    return images\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI._get_explainer","title":"_get_explainer","text":"<pre><code>_get_explainer() -&gt; Explainer\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def _get_explainer(self) -&gt; Explainer:\n    predict_method = self.model.predict_proba if self.use_proba else self.model.predict\n    data = self._validate_and_get_data()\n    if self.onsubset:\n        data = data.iloc[: self.subset_size]\n    return Explainer(model=predict_method, masker=data, feature_names=self.feature_names)\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI._get_plot_function","title":"_get_plot_function","text":"<pre><code>_get_plot_function(plot_type: PlotType)\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def _get_plot_function(self, plot_type: PlotType):\n    return {\n        \"bar\": shap.plots.bar,\n        \"beeswarm\": shap.plots.beeswarm,\n        \"heatmap\": shap.plots.heatmap,\n        \"violin\": shap.plots.violin,\n    }[plot_type]\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI._get_plot_values","title":"_get_plot_values","text":"<pre><code>_get_plot_values(class_index: int) -&gt; Explanation\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def _get_plot_values(self, class_index: int) -&gt; shap.Explanation:\n    if class_index == -1:\n        return self.shap_values\n    if self.shap_values.values.ndim &gt; EXPECTED_SHAP_VALUES_DIM:  # noqa: PD011\n        if self.shap_values_each_class is None:\n            msg = \"No class-specific SHAP values available\"\n            raise ValueError(msg)\n        return self.shap_values_each_class[str(class_index)]\n    return self.shap_values\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI._get_shap_values","title":"_get_shap_values","text":"<pre><code>_get_shap_values() -&gt; Explanation\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def _get_shap_values(self) -&gt; shap.Explanation:\n    data = self._validate_and_get_data()\n    if self.onsubset:\n        data = data.iloc[: self.subset_size]\n    return self.explainer(data)\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI._get_shap_values_each_class","title":"_get_shap_values_each_class","text":"<pre><code>_get_shap_values_each_class(shap_values: Explanation) -&gt; dict[str, Explanation]\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def _get_shap_values_each_class(\n    self, shap_values: shap.Explanation\n) -&gt; dict[str, shap.Explanation]:\n    if shap_values.values.ndim &lt; EXPECTED_SHAP_VALUES_DIM:  # noqa: PD011\n        msg = \"shap_values has less than 2 dimensions\"\n        raise TypeError(msg)\n    return {str(i): shap_values[:, :, i] for i in self.get_classes()}\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI._handle_plot_error","title":"_handle_plot_error","text":"<pre><code>_handle_plot_error(plot_type: PlotType, error: Exception) -&gt; None\n</code></pre> <p>Handle plot generation errors.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def _handle_plot_error(self, plot_type: PlotType, error: Exception) -&gt; None:\n    \"\"\"Handle plot generation errors.\"\"\"\n    if isinstance(error, (ValueError, TypeError, KeyError)):\n        raise error\n    msg = f\"Error generating {plot_type} plot: {error!s}\"\n    raise RuntimeError(msg) from error\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI._initialize_chat","title":"_initialize_chat","text":"<pre><code>_initialize_chat(api_key: str, model_name: str, provider: str)\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def _initialize_chat(self, api_key: str, model_name: str, provider: str):\n    if provider == \"google\":\n        return ChatGoogleGenerativeAI(google_api_key=api_key, model=model_name)\n    if provider == \"openai\":\n        return ChatOpenAI(openai_api_key=api_key, model_name=model_name)\n    msg = \"Invalid provider\"\n    raise ValueError(msg)\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI._save_plot_to_base64","title":"_save_plot_to_base64","text":"<pre><code>_save_plot_to_base64() -&gt; str\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def _save_plot_to_base64(self) -&gt; str:\n    img_buf = io.BytesIO()\n    plt.savefig(img_buf, format=\"png\")\n    img_buf.seek(0)\n    img_base64 = base64.b64encode(img_buf.getvalue()).decode(\"utf-8\")\n    return f\"data:image/png;base64,{img_base64}\"\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI._save_plot_to_file","title":"_save_plot_to_file","text":"<pre><code>_save_plot_to_file(save_config: dict) -&gt; None\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def _save_plot_to_file(self, save_config: dict) -&gt; None:\n    save_path = save_config.get(\"save_path\")\n    save_name = save_config.get(\"save_name\")\n    if not save_path or not save_name:\n        return\n\n    plt.savefig(\n        Path(save_path) / save_name,\n        format=save_config.get(\"save_format\", \"png\"),\n        dpi=save_config.get(\"save_dpi\", 300),\n        bbox_inches=\"tight\",\n    )\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI._set_shap_values_classes","title":"_set_shap_values_classes","text":"<pre><code>_set_shap_values_classes() -&gt; dict[str, Explanation] | None\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def _set_shap_values_classes(self) -&gt; dict[str, shap.Explanation] | None:\n    if not (self.shap_values and self.shap_values.values.ndim &gt; EXPECTED_SHAP_VALUES_DIM):  # noqa: PD011\n        return None\n    return self._get_shap_values_each_class(self.shap_values)\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI._validate_and_get_data","title":"_validate_and_get_data","text":"<pre><code>_validate_and_get_data() -&gt; DataFrame\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def _validate_and_get_data(self) -&gt; pd.DataFrame:\n    data = self.data.get(DATA_KEY)\n    if not isinstance(data, pd.DataFrame):\n        msg = f\"Expected {DATA_KEY} to be a pandas DataFrame\"\n        raise TypeError(msg)\n    return data\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI._validate_shap_values_class","title":"_validate_shap_values_class","text":"<pre><code>_validate_shap_values_class() -&gt; Explanation\n</code></pre> <p>Validate and get SHAP values for class-specific case.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def _validate_shap_values_class(self) -&gt; shap.Explanation:\n    \"\"\"Validate and get SHAP values for class-specific case.\"\"\"\n    if self.shap_values_each_class is None:\n        msg = \"No class-specific SHAP values available\"\n        raise ValueError(msg)\n    first_class = next(iter(self.get_classes()))\n    return self.shap_values_each_class[str(first_class)]\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.generate_report_with_langchain","title":"generate_report_with_langchain","text":"<pre><code>generate_report_with_langchain(api_key: str, model_name: str = 'gpt-4o', provider: str = 'google', num_waterfall_plots: int = 5)\n</code></pre> <p>Generate comprehensive report using LangChain and multimodal LLM.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def generate_report_with_langchain(\n    self,\n    api_key: str,\n    model_name: str = \"gpt-4o\",\n    provider: str = \"google\",\n    num_waterfall_plots: int = 5,\n):\n    \"\"\"Generate comprehensive report using LangChain and multimodal LLM.\"\"\"\n    chat = self._initialize_chat(api_key, model_name, provider)\n\n    report = self.get_report()\n    images = self._generate_report_images(num_waterfall_plots)\n\n    prompt2 = Path(\"prompt.txt\").read_text()\n    return self._generate_final_report(chat, report, images, prompt2)\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.get_average_precision_score","title":"get_average_precision_score","text":"<pre><code>get_average_precision_score()\n</code></pre> <p>Get the average precision score of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_average_precision_score(self):\n    \"\"\"Get the average precision score of the model.\"\"\"\n    return average_precision_score(self.y_test, self.predictions_proba)\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.get_bar_plot","title":"get_bar_plot","text":"<pre><code>get_bar_plot(max_display: int = DEFAULT_MAX_DISPLAY, class_index: int = -1)\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_bar_plot(self, max_display: int = DEFAULT_MAX_DISPLAY, class_index: int = -1):\n    return self.get_plot(\"bar\", max_display, class_index=class_index)\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.get_beeswarm_plot","title":"get_beeswarm_plot","text":"<pre><code>get_beeswarm_plot(max_display: int = DEFAULT_MAX_DISPLAY, class_index: int = -1)\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_beeswarm_plot(self, max_display: int = DEFAULT_MAX_DISPLAY, class_index: int = -1):\n    return self.get_plot(\"beeswarm\", max_display, class_index=class_index)\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.get_classes","title":"get_classes","text":"<pre><code>get_classes() -&gt; dict[int, str]\n</code></pre> <p>Get model classes.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_classes(self) -&gt; dict[int, str]:\n    \"\"\"Get model classes.\"\"\"\n    if not hasattr(self.model, \"classes_\"):\n        msg = \"Model does not have a classes_ attribute\"\n        raise TypeError(msg)\n    return self.model.classes_\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.get_classification_report","title":"get_classification_report","text":"<pre><code>get_classification_report()\n</code></pre> <p>Get the classification report of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_classification_report(self):\n    \"\"\"Get the classification report of the model.\"\"\"\n    return classification_report(self.y_test, self.predictions)\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.get_cohens_kappa","title":"get_cohens_kappa","text":"<pre><code>get_cohens_kappa()\n</code></pre> <p>Get the cohens kappa of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_cohens_kappa(self):\n    \"\"\"Get the cohens kappa of the model.\"\"\"\n    return cohen_kappa_score(self.y_test, self.predictions)\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.get_confusion_matrix","title":"get_confusion_matrix","text":"<pre><code>get_confusion_matrix()\n</code></pre> <p>Get the confusion matrix of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_confusion_matrix(self):\n    \"\"\"Get the confusion matrix of the model.\"\"\"\n    return confusion_matrix(self.y_test, self.predictions)\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.get_f1_score","title":"get_f1_score","text":"<pre><code>get_f1_score()\n</code></pre> <p>Get the f1 score of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_f1_score(self):\n    \"\"\"Get the f1 score of the model.\"\"\"\n    return f1_score(self.y_test, self.predictions)\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.get_heatmap_plot","title":"get_heatmap_plot","text":"<pre><code>get_heatmap_plot(max_display: int = DEFAULT_MAX_DISPLAY, class_index: int = -1)\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_heatmap_plot(self, max_display: int = DEFAULT_MAX_DISPLAY, class_index: int = -1):\n    return self.get_plot(\"heatmap\", max_display, class_index=class_index)\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.get_log_loss","title":"get_log_loss","text":"<pre><code>get_log_loss()\n</code></pre> <p>Get the log loss of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_log_loss(self):\n    \"\"\"Get the log loss of the model.\"\"\"\n    return log_loss(self.y_test, self.predictions_proba)\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.get_mcc","title":"get_mcc","text":"<pre><code>get_mcc()\n</code></pre> <p>Get the mcc of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_mcc(self):\n    \"\"\"Get the mcc of the model.\"\"\"\n    return matthews_corrcoef(self.y_test, self.predictions)\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.get_plot","title":"get_plot","text":"<pre><code>get_plot(plot_type: PlotType, max_display: int = DEFAULT_MAX_DISPLAY, class_index: int = -1, index: int = 0, save_config: dict | None = None)\n</code></pre> <p>Generate plot with given configuration.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_plot(\n    self,\n    plot_type: PlotType,\n    max_display: int = DEFAULT_MAX_DISPLAY,\n    class_index: int = -1,\n    index: int = 0,\n    save_config: dict | None = None,\n):\n    \"\"\"Generate plot with given configuration.\"\"\"\n    try:\n        values = self._get_plot_values(class_index)\n        return self._generate_plot(plot_type, values, max_display, index, save_config)\n    except (ValueError, TypeError, KeyError, RuntimeError) as e:\n        self._handle_plot_error(plot_type, e)\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.get_precision","title":"get_precision","text":"<pre><code>get_precision()\n</code></pre> <p>Get the precision of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_precision(self):\n    \"\"\"Get the precision of the model.\"\"\"\n    return precision_score(self.y_test, self.predictions)\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.get_precision_recall_curve","title":"get_precision_recall_curve","text":"<pre><code>get_precision_recall_curve()\n</code></pre> <p>Get the precision recall curve of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_precision_recall_curve(self):\n    \"\"\"Get the precision recall curve of the model.\"\"\"\n    return precision_recall_curve(self.y_test, self.predictions_proba)\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.get_recall","title":"get_recall","text":"<pre><code>get_recall()\n</code></pre> <p>Get the recall of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_recall(self):\n    \"\"\"Get the recall of the model.\"\"\"\n    return recall_score(self.y_test, self.predictions)\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.get_report","title":"get_report","text":"<pre><code>get_report()\n</code></pre> <p>Get the report of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_report(self):\n    \"\"\"Get the report of the model.\"\"\"\n    report = {}\n    metrics = {\n        \"confusion_matrix\": self.get_confusion_matrix,\n        \"classification_report\": self.get_classification_report,\n        \"roc_curve\": self.get_roc_curve,\n        \"roc_auc_score\": self.get_roc_auc_score,\n        \"precision_recall_curve\": self.get_precision_recall_curve,\n        \"average_precision_score\": self.get_average_precision_score,\n        \"f1_score\": self.get_f1_score,\n        \"mcc\": self.get_mcc,\n        \"log_loss\": self.get_log_loss,\n        \"cohens_kappa\": self.get_cohens_kappa,\n        \"precision\": self.get_precision,\n        \"recall\": self.get_recall,\n    }\n\n    try:\n        for key, func in metrics.items():\n            report[key] = func()\n    except (ValueError, TypeError) as e:\n        report[key] = str(e)\n    return report\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.get_roc_auc_score","title":"get_roc_auc_score","text":"<pre><code>get_roc_auc_score()\n</code></pre> <p>Get the roc auc score of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_roc_auc_score(self):\n    \"\"\"Get the roc auc score of the model.\"\"\"\n    return roc_auc_score(self.y_test, self.predictions_proba)\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.get_roc_curve","title":"get_roc_curve","text":"<pre><code>get_roc_curve()\n</code></pre> <p>Get the roc curve of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_roc_curve(self):\n    \"\"\"Get the roc curve of the model.\"\"\"\n    return roc_curve(self.y_test, self.predictions_proba)\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.get_violin_plot","title":"get_violin_plot","text":"<pre><code>get_violin_plot(max_display: int = DEFAULT_MAX_DISPLAY, class_index: int = -1)\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_violin_plot(self, max_display: int = DEFAULT_MAX_DISPLAY, class_index: int = -1):\n    return self.get_plot(\"violin\", max_display, class_index=class_index)\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.get_waterfall_plot","title":"get_waterfall_plot","text":"<pre><code>get_waterfall_plot(max_display: int = DEFAULT_MAX_DISPLAY, index: int = 0, class_index: int = -1)\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_waterfall_plot(\n    self, max_display: int = DEFAULT_MAX_DISPLAY, index: int = 0, class_index: int = -1\n):\n    return self.get_plot(\"waterfall\", max_display, index=index, class_index=class_index)\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.load_state","title":"load_state  <code>classmethod</code>","text":"<pre><code>load_state(file_path: str)\n</code></pre> <p>Loads the state of the class from a pkl file. Warning: Only use this method with trusted data sources as pickle can be unsafe.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>@classmethod\ndef load_state(cls, file_path: str):\n    \"\"\"Loads the state of the class from a pkl file.\n    Warning: Only use this method with trusted data sources as pickle can be unsafe.\n    \"\"\"\n    with Path(file_path).open(\"rb\") as f:\n        return pickle.load(f)  # noqa: S301\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.plot_confusion_matrix","title":"plot_confusion_matrix","text":"<pre><code>plot_confusion_matrix(plot_config: dict | None = None)\n</code></pre> <p>Plot confusion matrix with given configuration.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def plot_confusion_matrix(self, plot_config: dict | None = None):\n    \"\"\"Plot confusion matrix with given configuration.\"\"\"\n    from sklearn.metrics import ConfusionMatrixDisplay\n\n    config = {\n        \"include_values\": True,\n        \"cmap\": \"viridis\",\n        \"xticks_rotation\": \"horizontal\",\n        \"values_format\": None,\n        \"ax\": None,\n        \"colorbar\": True,\n        \"im_kw\": None,\n        \"text_kw\": None,\n        **(plot_config or {}),\n    }\n\n    cm = self.get_confusion_matrix()\n    ConfusionMatrixDisplay(cm).plot(**config)\n\n    if plot_config and plot_config.get(\"save_path\"):\n        plt.savefig(\n            Path(plot_config[\"save_path\"]) / plot_config[\"save_name\"],\n            format=plot_config.get(\"save_format\", \"png\"),\n            dpi=plot_config.get(\"save_dpi\", 300),\n            bbox_inches=\"tight\",\n        )\n\n    return plt.gcf()\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.save_state","title":"save_state","text":"<pre><code>save_state(file_path: str)\n</code></pre> <p>Saves the state of the class and its variables in a pkl file.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def save_state(self, file_path: str):\n    \"\"\"Saves the state of the class and its variables in a pkl file.\"\"\"\n    with Path(file_path).open(\"wb\") as f:\n        pickle.dump(self, f)\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.validate_predict_proba","title":"validate_predict_proba","text":"<pre><code>validate_predict_proba() -&gt; bool\n</code></pre> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def validate_predict_proba(self) -&gt; bool:\n    if not hasattr(self.model, \"predict_proba\"):\n        msg = \"Model does not have a predict_proba method\"\n        raise TypeError(msg)\n    return True\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAIConfig","title":"XAIConfig  <code>dataclass</code>","text":"<pre><code>XAIConfig(use_proba: bool = True, onsubset: bool = True, feature_names: list[str] | None = None, subset_size: int = DEFAULT_SUBSET_SIZE, max_display: int = DEFAULT_MAX_DISPLAY, data_key: str = DATA_KEY, x_test_key: str = 'x_test', y_test_key: str = 'y_test')\n</code></pre> ATTRIBUTE DESCRIPTION <code>data_key</code> <p> TYPE: <code>str</code> </p> <code>feature_names</code> <p> TYPE: <code>list[str] | None</code> </p> <code>max_display</code> <p> TYPE: <code>int</code> </p> <code>onsubset</code> <p> TYPE: <code>bool</code> </p> <code>subset_size</code> <p> TYPE: <code>int</code> </p> <code>use_proba</code> <p> TYPE: <code>bool</code> </p> <code>x_test_key</code> <p> TYPE: <code>str</code> </p> <code>y_test_key</code> <p> TYPE: <code>str</code> </p>"},{"location":"api_docs.html#quoptuna.XAIConfig.data_key","title":"data_key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>data_key: str = DATA_KEY\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAIConfig.feature_names","title":"feature_names  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>feature_names: list[str] | None = None\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAIConfig.max_display","title":"max_display  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_display: int = DEFAULT_MAX_DISPLAY\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAIConfig.onsubset","title":"onsubset  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>onsubset: bool = True\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAIConfig.subset_size","title":"subset_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>subset_size: int = DEFAULT_SUBSET_SIZE\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAIConfig.use_proba","title":"use_proba  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>use_proba: bool = True\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAIConfig.x_test_key","title":"x_test_key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>x_test_key: str = 'x_test'\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAIConfig.y_test_key","title":"y_test_key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>y_test_key: str = 'y_test'\n</code></pre>"},{"location":"api_docs.html#quoptuna.Optimizer","title":"Optimizer","text":"<pre><code>Optimizer(db_name: str, dataset_name: str = '', data: Optional[dict] = None, study_name: str = '')\n</code></pre> <p>Initialize the Optimizer class.</p> PARAMETER DESCRIPTION <p>The name of the database to be used for storing optimization results.</p> <p> TYPE: <code>str</code> </p> <p>The name of the dataset. If provided, the data will be loaded from a CSV file located in the 'notebook' directory. Defaults to an empty string.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <p>A dictionary containing training and testing data. If not provided, an empty dictionary will be used. Expected keys are 'train_x', 'test_x', 'train_y', and 'test_y'.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> <p>The name of the study for Optuna. Defaults to an empty string.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> ATTRIBUTE DESCRIPTION <code>db_name</code> <p>The name of the database.</p> <p> </p> <code>dataset_name</code> <p>The name of the dataset.</p> <p> </p> <code>data_path</code> <p>The path to the dataset CSV file or an empty string if no dataset name is provided.</p> <p> </p> <code>data</code> <p>The data dictionary containing training and testing data.</p> <p> </p> <code>train_x</code> <p>The training features.</p> <p> </p> <code>test_x</code> <p>The testing features.</p> <p> </p> <code>train_y</code> <p>The training labels.</p> <p> </p> <code>test_y</code> <p>The testing labels.</p> <p> </p> <code>storage_location</code> <p>The storage location for the Optuna study.</p> <p> </p> <code>study_name</code> <p>The name of the Optuna study.</p> <p> </p> <code>study</code> <p>The Optuna study object.</p> <p> </p> METHOD DESCRIPTION <code>load_and_preprocess_data</code> <code>load_study</code> <code>log_user_attributes</code> <code>objective</code> <code>optimize</code> ATTRIBUTE DESCRIPTION <code>data</code> <p> </p> <code>data_path</code> <p> </p> <code>dataset_name</code> <p> </p> <code>db_name</code> <p> </p> <code>storage_location</code> <p> </p> <code>study</code> <p> </p> <code>study_name</code> <p> </p> <code>test_x</code> <p> </p> <code>test_y</code> <p> </p> <code>train_x</code> <p> </p> <code>train_y</code> <p> </p> Source code in <code>src/quoptuna/backend/tuners/optimizer.py</code> <pre><code>def __init__(\n    self,\n    db_name: str,\n    dataset_name: str = \"\",\n    data: Optional[dict] = None,  # noqa: FA100\n    study_name: str = \"\",\n):\n    \"\"\"Initialize the Optimizer class.\n\n    Args:\n        db_name: The name of the database to be used for storing optimization results.\n        dataset_name: The name of the dataset. If provided, the data will be loaded from a\n            CSV file located in the 'notebook' directory. Defaults to an empty string.\n        data: A dictionary containing training and testing data. If not provided, an empty\n            dictionary will be used. Expected keys are 'train_x', 'test_x', 'train_y', and 'test_y'.\n        study_name: The name of the study for Optuna. Defaults to an empty string.\n\n    Attributes:\n        db_name: The name of the database.\n        dataset_name: The name of the dataset.\n        data_path: The path to the dataset CSV file or an empty string if no dataset name is provided.\n        data: The data dictionary containing training and testing data.\n        train_x: The training features.\n        test_x: The testing features.\n        train_y: The training labels.\n        test_y: The testing labels.\n        storage_location: The storage location for the Optuna study.\n        study_name: The name of the Optuna study.\n        study: The Optuna study object.\n    \"\"\"\n    self.db_name = db_name\n    self.dataset_name = dataset_name\n    if len(self.dataset_name) &gt; 0:\n        self.data_path = f\"notebook/{self.dataset_name}.csv\"\n    else:\n        self.data_path = \"\"\n    self.data = data or {}  # Use an empty dictionary if no data is provided\n    self.train_x = self.data.get(\"train_x\")\n    self.test_x = self.data.get(\"test_x\")\n    self.train_y = self.data.get(\"train_y\")\n    self.test_y = self.data.get(\"test_y\")\n    self.data_path = f\"db/{self.db_name}.db\"\n    if not os.path.exists(\"db\"):  # noqa: PTH110\n        os.makedirs(\"db\")  # noqa: PTH103\n    self.storage_location = f\"sqlite:///{self.data_path}\"\n    self.study_name = study_name\n    self.study = None\n</code></pre>"},{"location":"api_docs.html#quoptuna.Optimizer(db_name)","title":"<code>db_name</code>","text":""},{"location":"api_docs.html#quoptuna.Optimizer(dataset_name)","title":"<code>dataset_name</code>","text":""},{"location":"api_docs.html#quoptuna.Optimizer(data)","title":"<code>data</code>","text":""},{"location":"api_docs.html#quoptuna.Optimizer(study_name)","title":"<code>study_name</code>","text":""},{"location":"api_docs.html#quoptuna.Optimizer.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data = data or {}\n</code></pre>"},{"location":"api_docs.html#quoptuna.Optimizer.data_path","title":"data_path  <code>instance-attribute</code>","text":"<pre><code>data_path = f'db/{db_name}.db'\n</code></pre>"},{"location":"api_docs.html#quoptuna.Optimizer.dataset_name","title":"dataset_name  <code>instance-attribute</code>","text":"<pre><code>dataset_name = dataset_name\n</code></pre>"},{"location":"api_docs.html#quoptuna.Optimizer.db_name","title":"db_name  <code>instance-attribute</code>","text":"<pre><code>db_name = db_name\n</code></pre>"},{"location":"api_docs.html#quoptuna.Optimizer.storage_location","title":"storage_location  <code>instance-attribute</code>","text":"<pre><code>storage_location = f'sqlite:///{data_path}'\n</code></pre>"},{"location":"api_docs.html#quoptuna.Optimizer.study","title":"study  <code>instance-attribute</code>","text":"<pre><code>study = None\n</code></pre>"},{"location":"api_docs.html#quoptuna.Optimizer.study_name","title":"study_name  <code>instance-attribute</code>","text":"<pre><code>study_name = study_name\n</code></pre>"},{"location":"api_docs.html#quoptuna.Optimizer.test_x","title":"test_x  <code>instance-attribute</code>","text":"<pre><code>test_x = get('test_x')\n</code></pre>"},{"location":"api_docs.html#quoptuna.Optimizer.test_y","title":"test_y  <code>instance-attribute</code>","text":"<pre><code>test_y = get('test_y')\n</code></pre>"},{"location":"api_docs.html#quoptuna.Optimizer.train_x","title":"train_x  <code>instance-attribute</code>","text":"<pre><code>train_x = get('train_x')\n</code></pre>"},{"location":"api_docs.html#quoptuna.Optimizer.train_y","title":"train_y  <code>instance-attribute</code>","text":"<pre><code>train_y = get('train_y')\n</code></pre>"},{"location":"api_docs.html#quoptuna.Optimizer.load_and_preprocess_data","title":"load_and_preprocess_data","text":"<pre><code>load_and_preprocess_data()\n</code></pre> Source code in <code>src/quoptuna/backend/tuners/optimizer.py</code> <pre><code>def load_and_preprocess_data(self):\n    self.X, self.y = load_data(self.data_path)\n    self.train_x, self.test_x, self.train_y, self.test_y = preprocess_data(self.X, self.y)\n</code></pre>"},{"location":"api_docs.html#quoptuna.Optimizer.load_study","title":"load_study","text":"<pre><code>load_study()\n</code></pre> Source code in <code>src/quoptuna/backend/tuners/optimizer.py</code> <pre><code>def load_study(self):\n    # load the study from the database\n    self.study = load_study(\n        storage=self.storage_location,\n        study_name=self.study_name,\n    )\n</code></pre>"},{"location":"api_docs.html#quoptuna.Optimizer.log_user_attributes","title":"log_user_attributes","text":"<pre><code>log_user_attributes(model_type, eval_scores, trial)\n</code></pre> Source code in <code>src/quoptuna/backend/tuners/optimizer.py</code> <pre><code>def log_user_attributes(self, model_type, eval_scores, trial):\n    if model_type in [\"SVC\", \"SVClinear\", \"MLPClassifier\", \"Perceptron\"]:\n        trial.set_user_attr(\"Classical_accuracy\", eval_scores[\"accuracy\"])\n        trial.set_user_attr(\"Classical_f1_score\", eval_scores[\"f1_score\"])\n        trial.set_user_attr(\"Classical_score\", eval_scores[\"score\"])\n        trial.set_user_attr(\"Quantum_accuracy\", 0)\n        trial.set_user_attr(\"Quantum_f1_score\", 0)\n        trial.set_user_attr(\"Quantum_score\", 0)\n    else:\n        trial.set_user_attr(\"Quantum_accuracy\", eval_scores[\"accuracy\"])\n        trial.set_user_attr(\"Quantum_f1_score\", eval_scores[\"f1_score\"])\n        trial.set_user_attr(\"Quantum_score\", eval_scores[\"score\"])\n        trial.set_user_attr(\"Classical_accuracy\", 0)\n        trial.set_user_attr(\"Classical_f1_score\", 0)\n        trial.set_user_attr(\"Classical_score\", 0)\n</code></pre>"},{"location":"api_docs.html#quoptuna.Optimizer.objective","title":"objective","text":"<pre><code>objective(trial: Trial)\n</code></pre> Source code in <code>src/quoptuna/backend/tuners/optimizer.py</code> <pre><code>def objective(self, trial: Trial):\n    try:\n        # Define the hyperparameter search space\n        params = {\n            \"max_vmap\": trial.suggest_categorical(\"max_vmap\", [1]),\n            \"batch_size\": trial.suggest_categorical(\"batch_size\", [32]),\n            \"learning_rate\": trial.suggest_categorical(\"learning_rate\", [0.001, 0.01, 0.1]),\n            \"n_input_copies\": trial.suggest_categorical(\"n_input_copies\", [1, 2, 3]),\n            \"n_layers\": trial.suggest_categorical(\"n_layers\", [1, 5, 10]),\n            \"observable_type\": trial.suggest_categorical(\n                \"observable_type\", [\"single\", \"half\", \"full\"]\n            ),\n            \"repeats\": trial.suggest_categorical(\"repeats\", [1, 5, 10]),\n            \"C\": trial.suggest_categorical(\"C\", [0.1, 1, 10, 100]),\n            \"gamma_factor\": trial.suggest_categorical(\"gamma_factor\", [0.1, 1, 10]),\n            \"trotter_steps\": trial.suggest_categorical(\"trotter_steps\", [1, 3, 5]),\n            \"t\": trial.suggest_categorical(\"t\", [0.01, 0.1, 1.0]),\n            \"n_qfeatures\": trial.suggest_categorical(\"n_qfeatures\", [\"full\", \"half\"]),\n            \"n_episodes\": trial.suggest_categorical(\"n_episodes\", [10, 100, 500, 2000]),\n            \"visible_qubits\": trial.suggest_categorical(\n                \"visible_qubits\", [\"single\", \"half\", \"full\"]\n            ),\n            \"temperature\": trial.suggest_categorical(\"temperature\", [1, 10, 100]),\n            \"encoding_layers\": trial.suggest_categorical(\"encoding_layers\", [1, 3, 5, 10]),\n            \"degree\": trial.suggest_categorical(\"degree\", [2, 3, 4]),\n            \"n_qchannels\": trial.suggest_categorical(\"n_qchannels\", [1, 5, 10]),\n            \"qkernel_shape\": trial.suggest_categorical(\"qkernel_shape\", [2, 3]),\n            \"kernel_shape\": trial.suggest_categorical(\"kernel_shape\", [2, 3, 5]),\n            \"filter_name\": trial.suggest_categorical(\n                \"filter_name\", [\"edge_detect\", \"smooth\", \"sharpen\"]\n            ),\n            \"gamma\": trial.suggest_categorical(\"gamma\", [0.001, 0.01, 0.1, 1]),\n            \"alpha\": trial.suggest_categorical(\"alpha\", [0.01, 0.001, 0.0001]),\n            \"hidden_layer_sizes\": trial.suggest_categorical(\n                \"hidden_layer_sizes\", [\"[100,)\", \"(10, 10, 10, 10)\", \"(50, 10, 5)\"]\n            ),\n            \"eta0\": trial.suggest_categorical(\"eta0\", [0.1, 1, 10]),\n        }\n\n        model_type = trial.suggest_categorical(\n            \"model_type\",\n            [\n                \"CircuitCentricClassifier\",\n                \"DataReuploadingClassifier\",\n                \"DataReuploadingClassifierSeparable\",\n                \"DressedQuantumCircuitClassifier\",\n                \"DressedQuantumCircuitClassifierSeparable\",\n                \"ProjectedQuantumKernel\",\n                \"QuantumKitchenSinks\",\n                \"QuantumMetricLearner\",\n                \"TreeTensorClassifier\",\n                \"SeparableVariationalClassifier\",\n                \"SeparableKernelClassifier\",\n                \"SVC\",\n                \"SVClinear\",\n                \"MLPClassifier\",\n                \"Perceptron\",\n            ],\n        )\n\n        model = create_model(model_type, **params)\n\n        model.fit(self.train_x, self.train_y)\n        score = model.score(self.test_x, self.test_y)\n\n        f_score_ = f1_score(self.test_y, model.predict(self.test_x))\n        acc_ = accuracy_score(self.test_y, model.predict(self.test_x))\n\n        self.log_user_attributes(\n            model_type,\n            {\"accuracy\": acc_, \"f1_score\": f_score_, \"score\": score},\n            trial,\n        )\n\n        return f_score_  # noqa: TRY300\n    except Exception:\n        import logging\n\n        logging.exception(\"An error occurred\")  # Use logging instead of print\n        return 0\n</code></pre>"},{"location":"api_docs.html#quoptuna.Optimizer.optimize","title":"optimize","text":"<pre><code>optimize(n_trials=100)\n</code></pre> Source code in <code>src/quoptuna/backend/tuners/optimizer.py</code> <pre><code>def optimize(self, n_trials=100):\n    if (\n        self.train_x is None\n        or self.test_x is None\n        or self.train_y is None\n        or self.test_y is None\n    ):\n        self.load_and_preprocess_data()\n    # database  stored in a db folder \"db\"\n\n    # sqllite database\n\n    study = create_study(\n        storage=self.storage_location,\n        sampler=TPESampler(),\n        directions=[\"maximize\"],\n        study_name=self.study_name,\n    )\n    self.study = study\n    study.optimize(self.objective, n_trials=n_trials)\n    return study, study.best_trials\n</code></pre>"},{"location":"api_docs.html#quoptuna.DataPreparation","title":"DataPreparation","text":"<pre><code>DataPreparation(dataset: DataSet | None = None, file_path: str | None = None, x_cols: list[str] | None = None, y_col: str | None = None, scaler=None)\n</code></pre> METHOD DESCRIPTION <code>create_dataset</code> <p>Creates a dataset from raw data.</p> <code>get_data</code> <code>prepare_data</code> <p>Selects columns and preprocesses the data.</p> <code>prepare_data_dict</code> <code>preprocess</code> <p>Preprocess the features and target.</p> <code>read_csv</code> <p>Reads a CSV file and returns a raw dataset.</p> <code>select_columns</code> <p>Selects specified columns and splits the dataset into features and target.</p> <code>set_x_cols</code> <code>set_y_col</code> <code>update_column_names</code> <p>Update column names in x_cols if they are single length after conversion to string.</p> ATTRIBUTE DESCRIPTION <code>dataset</code> <p> </p> <code>scaler</code> <p> </p> <code>x_cols</code> <p> </p> <code>y_col</code> <p> </p> Source code in <code>src/quoptuna/backend/utils/data_utils/prepare.py</code> <pre><code>def __init__(\n    self,\n    dataset: DataSet | None = None,\n    file_path: str | None = None,\n    x_cols: list[str] | None = None,\n    y_col: str | None = None,\n    scaler=None,\n):\n    self.x_cols = x_cols\n    self.y_col = y_col\n    self.scaler = scaler or StandardScaler()\n    if dataset is not None:\n        x = self.update_column_names(dataset.get(\"x\"))\n        self.set_x_cols(x.columns)\n        self.dataset = {\"x\": x, \"y\": dataset.get(\"y\")}\n    elif file_path is not None:\n        if x_cols is None or y_col is None:\n            msg = \"x_cols and y_col must be provided when file_path is used\"\n            raise ValueError(msg)\n        self.dataset = self.create_dataset(self.read_csv(file_path), x_cols, y_col)\n    else:\n        msg = \"Either dataset or file_path must be provided\"\n        raise ValueError(msg)\n    self.x_train, self.x_test, self.y_train, self.y_test = self.prepare_data()\n</code></pre>"},{"location":"api_docs.html#quoptuna.DataPreparation.dataset","title":"dataset  <code>instance-attribute</code>","text":"<pre><code>dataset = {'x': x, 'y': get('y')}\n</code></pre>"},{"location":"api_docs.html#quoptuna.DataPreparation.scaler","title":"scaler  <code>instance-attribute</code>","text":"<pre><code>scaler = scaler or StandardScaler()\n</code></pre>"},{"location":"api_docs.html#quoptuna.DataPreparation.x_cols","title":"x_cols  <code>instance-attribute</code>","text":"<pre><code>x_cols = x_cols\n</code></pre>"},{"location":"api_docs.html#quoptuna.DataPreparation.y_col","title":"y_col  <code>instance-attribute</code>","text":"<pre><code>y_col = y_col\n</code></pre>"},{"location":"api_docs.html#quoptuna.DataPreparation.create_dataset","title":"create_dataset","text":"<pre><code>create_dataset(raw_data: DataFrame, x_cols: list[str], y_col: str) -&gt; DataSet\n</code></pre> <p>Creates a dataset from raw data.</p> Source code in <code>src/quoptuna/backend/utils/data_utils/prepare.py</code> <pre><code>def create_dataset(self, raw_data: pd.DataFrame, x_cols: list[str], y_col: str) -&gt; DataSet:\n    \"\"\"Creates a dataset from raw data.\"\"\"\n    x = raw_data[x_cols]\n    y = raw_data[y_col]\n    x = self.update_column_names(x)\n    self.set_x_cols(x.columns)\n    return {\"x\": x, \"y\": y}\n</code></pre>"},{"location":"api_docs.html#quoptuna.DataPreparation.get_data","title":"get_data","text":"<pre><code>get_data(output_type: Literal['1', '2'] = '1')\n</code></pre> Source code in <code>src/quoptuna/backend/utils/data_utils/prepare.py</code> <pre><code>def get_data(self, output_type: Literal[\"1\", \"2\"] = \"1\"):\n    if output_type == \"1\":\n        return {\n            \"x_train\": self.x_train,\n            \"x_test\": self.x_test,\n            \"y_train\": self.y_train,\n            \"y_test\": self.y_test,\n        }\n    if output_type == \"2\":\n        return {\n            \"train_x\": self.x_train,\n            \"train_y\": self.y_train,\n            \"test_x\": self.x_test,\n            \"test_y\": self.y_test,\n        }\n    return None\n</code></pre>"},{"location":"api_docs.html#quoptuna.DataPreparation.prepare_data","title":"prepare_data","text":"<pre><code>prepare_data()\n</code></pre> <p>Selects columns and preprocesses the data.</p> Source code in <code>src/quoptuna/backend/utils/data_utils/prepare.py</code> <pre><code>def prepare_data(self):\n    \"\"\"Selects columns and preprocesses the data.\"\"\"\n    if self.x_cols is None or self.y_col is None:\n        msg = \"x_cols and y_col must be provided\"\n        raise ValueError(msg)\n    x, y = self.select_columns()\n    return self.preprocess(x, y)\n</code></pre>"},{"location":"api_docs.html#quoptuna.DataPreparation.prepare_data_dict","title":"prepare_data_dict","text":"<pre><code>prepare_data_dict(x_train, y_train, x_test, y_test)\n</code></pre> Source code in <code>src/quoptuna/backend/utils/data_utils/prepare.py</code> <pre><code>def prepare_data_dict(self, x_train, y_train, x_test, y_test):\n    return {\"x_train\": x_train, \"x_test\": x_test, \"y_train\": y_train, \"y_test\": y_test}\n</code></pre>"},{"location":"api_docs.html#quoptuna.DataPreparation.preprocess","title":"preprocess","text":"<pre><code>preprocess(x: DataFrame, y: Series, train_size: float = 0.75)\n</code></pre> <p>Preprocess the features and target.</p> Source code in <code>src/quoptuna/backend/utils/data_utils/prepare.py</code> <pre><code>def preprocess(self, x: pd.DataFrame, y: pd.Series, train_size: float = 0.75):\n    \"\"\"Preprocess the features and target.\"\"\"\n    x = pd.DataFrame(self.scaler.fit_transform(x), columns=x.columns)\n    classes = np.unique(y)\n    y = pd.DataFrame(\n        np.where(y == classes[0], 1, -1),\n        columns=[self.y_col] if not isinstance(self.y_col, list) else self.y_col,\n    )\n    return train_test_split(x, y, train_size=train_size, random_state=42)\n</code></pre>"},{"location":"api_docs.html#quoptuna.DataPreparation.read_csv","title":"read_csv  <code>staticmethod</code>","text":"<pre><code>read_csv(file_path: str) -&gt; DataFrame\n</code></pre> <p>Reads a CSV file and returns a raw dataset.</p> Source code in <code>src/quoptuna/backend/utils/data_utils/prepare.py</code> <pre><code>@staticmethod\ndef read_csv(file_path: str) -&gt; pd.DataFrame:\n    \"\"\"Reads a CSV file and returns a raw dataset.\"\"\"\n    return pd.read_csv(file_path)\n</code></pre>"},{"location":"api_docs.html#quoptuna.DataPreparation.select_columns","title":"select_columns","text":"<pre><code>select_columns()\n</code></pre> <p>Selects specified columns and splits the dataset into features and target.</p> Source code in <code>src/quoptuna/backend/utils/data_utils/prepare.py</code> <pre><code>def select_columns(self):\n    \"\"\"Selects specified columns and splits the dataset into features and target.\"\"\"\n    if self.x_cols is None or self.y_col is None:\n        msg = \"x_cols and y_col must be provided\"\n        raise ValueError(msg)\n    x = self.dataset.get(\"x\")\n    y = self.dataset.get(\"y\")\n    return x, y\n</code></pre>"},{"location":"api_docs.html#quoptuna.DataPreparation.set_x_cols","title":"set_x_cols","text":"<pre><code>set_x_cols(x_cols: list[str])\n</code></pre> Source code in <code>src/quoptuna/backend/utils/data_utils/prepare.py</code> <pre><code>def set_x_cols(self, x_cols: list[str]):\n    self.x_cols = x_cols\n</code></pre>"},{"location":"api_docs.html#quoptuna.DataPreparation.set_y_col","title":"set_y_col","text":"<pre><code>set_y_col(y_col: str)\n</code></pre> Source code in <code>src/quoptuna/backend/utils/data_utils/prepare.py</code> <pre><code>def set_y_col(self, y_col: str):\n    self.y_col = y_col\n</code></pre>"},{"location":"api_docs.html#quoptuna.DataPreparation.update_column_names","title":"update_column_names","text":"<pre><code>update_column_names(dataframe: DataFrame | None = None)\n</code></pre> <p>Update column names in x_cols if they are single length after conversion to string. Also updates the corresponding DataFrame if provided.</p> Source code in <code>src/quoptuna/backend/utils/data_utils/prepare.py</code> <pre><code>def update_column_names(self, dataframe: pd.DataFrame | None = None):\n    \"\"\"Update column names in x_cols if they are single length after conversion to string.\n    Also updates the corresponding DataFrame if provided.\n    \"\"\"\n    if self.x_cols is not None:\n        for i, col in enumerate(self.x_cols):\n            if len(str(col)) == 1:\n                self.x_cols[i] = f\"feat: {col}\"\n                if dataframe is not None and col in dataframe.columns:\n                    dataframe = dataframe.rename(columns={col: f\"feat: {col}\"})\n    return dataframe\n</code></pre>"},{"location":"api_docs.html#quoptuna.create_model","title":"create_model","text":"<pre><code>create_model(model_type, **kwargs)\n</code></pre> Source code in <code>src/quoptuna/backend/models.py</code> <pre><code>def create_model(model_type, **kwargs):\n    model_constructors = {\n        \"CircuitCentricClassifier\": (\n            CircuitCentricClassifier,\n            [\"max_vmap\", \"batch_size\", \"learning_rate\", \"n_input_copies\", \"n_layers\"],\n        ),\n        \"DataReuploadingClassifier\": (\n            DataReuploadingClassifier,\n            [\"max_vmap\", \"batch_size\", \"learning_rate\", \"n_layers\", \"observable_type\"],\n        ),\n        \"DataReuploadingClassifierSeparable\": (\n            DataReuploadingClassifierSeparable,\n            [\"max_vmap\", \"batch_size\", \"learning_rate\", \"n_layers\", \"observable_type\"],\n        ),\n        \"DressedQuantumCircuitClassifier\": (\n            DressedQuantumCircuitClassifier,\n            [\"max_vmap\", \"batch_size\", \"learning_rate\", \"n_layers\"],\n        ),\n        \"DressedQuantumCircuitClassifierSeparable\": (\n            DressedQuantumCircuitClassifierSeparable,\n            [\"max_vmap\", \"batch_size\", \"learning_rate\", \"n_layers\"],\n        ),\n        \"IQPKernelClassifier\": (IQPKernelClassifier, [\"max_vmap\", \"repeats\", \"C\"]),\n        \"ProjectedQuantumKernel\": (\n            ProjectedQuantumKernel,\n            [\"max_vmap\", \"gamma_factor\", \"C\", \"trotter_steps\", \"t\"],\n        ),\n        \"QuantumKitchenSinks\": (\n            QuantumKitchenSinks,\n            [\"max_vmap\", \"n_qfeatures\", \"n_episodes\"],\n        ),\n        \"QuantumMetricLearner\": (\n            QuantumMetricLearner,\n            [\"max_vmap\", \"batch_size\", \"learning_rate\", \"n_layers\"],\n        ),\n        \"QuantumBoltzmannMachine\": (\n            QuantumBoltzmannMachine,\n            [\n                \"max_vmap\",\n                \"batch_size\",\n                \"learning_rate\",\n                \"visible_qubits\",\n                \"temperature\",\n            ],\n        ),\n        \"QuantumBoltzmannMachineSeparable\": (\n            QuantumBoltzmannMachineSeparable,\n            [\n                \"max_vmap\",\n                \"batch_size\",\n                \"learning_rate\",\n                \"visible_qubits\",\n                \"temperature\",\n            ],\n        ),\n        \"TreeTensorClassifier\": (\n            TreeTensorClassifier,\n            [\"max_vmap\", \"batch_size\", \"learning_rate\"],\n        ),\n        \"QuanvolutionalNeuralNetwork\": (\n            QuanvolutionalNeuralNetwork,\n            [\n                \"max_vmap\",\n                \"batch_size\",\n                \"learning_rate\",\n                \"n_qchannels\",\n                \"qkernel_shape\",\n                \"kernel_shape\",\n            ],\n        ),\n        \"WeiNet\": (WeiNet, [\"max_vmap\", \"batch_size\", \"learning_rate\", \"filter_name\"]),\n        \"SeparableVariationalClassifier\": (\n            SeparableVariationalClassifier,\n            [\"batch_size\", \"learning_rate\", \"encoding_layers\"],\n        ),\n        \"SeparableKernelClassifier\": (\n            SeparableKernelClassifier,\n            [\"C\", \"encoding_layers\"],\n        ),\n        \"ConvolutionalNeuralNetwork\": (\n            ConvolutionalNeuralNetwork,\n            [\"batch_size\", \"learning_rate\", \"kernel_shape\"],\n        ),\n        \"SVC\": (SVC, [\"gamma\", \"C\"]),\n        \"SVClinear\": (LinearSVC, [\"C\"]),\n        \"MLPClassifier\": (\n            MLPClassifier,\n            [\"batch_size\", \"learning_rate\", \"hidden_layer_sizes\", \"alpha\"],\n        ),\n        \"Perceptron\": (Perceptron, [\"eta0\"]),\n    }\n\n    if model_type not in model_constructors:\n        raise UnknownModelTypeError(model_type)\n\n    model_class, param_keys = model_constructors[model_type]\n    params = {key: kwargs.get(key) for key in param_keys}\n\n    if model_type == \"MLPClassifier\":\n        params[\"hidden_layer_sizes\"] = ast.literal_eval(params[\"hidden_layer_sizes\"])\n\n    return model_class(**params)\n</code></pre>"},{"location":"api_reference.html","title":"API Reference","text":""},{"location":"api_reference.html#api-reference","title":"API Reference","text":""},{"location":"api_reference.html#overview","title":"Overview","text":"<p>QuOptuna provides a comprehensive Python API for quantum-enhanced machine learning optimization. This reference covers the main classes and functions available for programmatic use.</p>"},{"location":"api_reference.html#core-classes","title":"Core Classes","text":""},{"location":"api_reference.html#datapreparation","title":"DataPreparation","text":"<p>Handles data loading, preprocessing, and splitting.</p> <pre><code>from quoptuna import DataPreparation\n\ndata_prep = DataPreparation(\n    file_path=\"path/to/data.csv\",\n    x_cols=[\"feature1\", \"feature2\", \"feature3\"],\n    y_col=\"target\"\n)\n\n# Get preprocessed data\ndata_dict = data_prep.get_data(output_type=\"2\")\n</code></pre> <p>Parameters: - <code>file_path</code> (str): Path to the CSV data file - <code>x_cols</code> (list): List of feature column names - <code>y_col</code> (str): Target column name - <code>test_size</code> (float, optional): Proportion of data for testing (default: 0.25) - <code>random_state</code> (int, optional): Random seed for reproducibility</p> <p>Methods:</p>"},{"location":"api_reference.html#get_dataoutput_type2","title":"<code>get_data(output_type=\"2\")</code>","text":"<p>Returns preprocessed data dictionary.</p> <p>Parameters: - <code>output_type</code> (str): Format of output   - <code>\"1\"</code>: Returns pandas DataFrames   - <code>\"2\"</code>: Returns numpy arrays (recommended for optimization)</p> <p>Returns: - dict: Dictionary with keys <code>train_x</code>, <code>test_x</code>, <code>train_y</code>, <code>test_y</code></p>"},{"location":"api_reference.html#optimizer","title":"Optimizer","text":"<p>Manages hyperparameter optimization using Optuna.</p> <pre><code>from quoptuna import Optimizer\n\noptimizer = Optimizer(\n    db_name=\"my_experiment\",\n    study_name=\"trial_001\",\n    data=data_dict\n)\n\n# Run optimization\nstudy, best_trials = optimizer.optimize(n_trials=100)\n</code></pre> <p>Parameters: - <code>db_name</code> (str): Database name for storing results - <code>study_name</code> (str): Unique study identifier - <code>data</code> (dict): Data dictionary from DataPreparation - <code>dataset_name</code> (str, optional): Human-readable dataset name</p> <p>Attributes: - <code>storage_location</code> (str): SQLite database URI - <code>study</code> (optuna.Study): Optuna study object - <code>best_trials</code> (list): List of best performing trials</p> <p>Methods:</p>"},{"location":"api_reference.html#optimizen_trials100-timeoutnone","title":"<code>optimize(n_trials=100, timeout=None)</code>","text":"<p>Run hyperparameter optimization.</p> <p>Parameters: - <code>n_trials</code> (int): Number of optimization trials - <code>timeout</code> (int, optional): Maximum optimization time in seconds</p> <p>Returns: - <code>study</code> (optuna.Study): Completed study - <code>best_trials</code> (list): List of Pareto-optimal trials</p>"},{"location":"api_reference.html#load_study","title":"<code>load_study()</code>","text":"<p>Load previously saved study from database.</p> <p>Returns: - <code>study</code> (optuna.Study): Loaded study object</p>"},{"location":"api_reference.html#model-creation","title":"Model Creation","text":"<p>Create models with optimized hyperparameters.</p> <pre><code>from quoptuna.backend.models import create_model\n\n# From trial parameters\nmodel = create_model(**trial.params)\n\n# Or specify directly\nmodel = create_model(\n    model_type=\"DataReuploadingClassifier\",\n    n_layers=10,\n    learning_rate=0.1,\n    batch_size=32\n)\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Make predictions\npredictions = model.predict(X_test)\n</code></pre> <p>Supported Models:</p>"},{"location":"api_reference.html#quantum-models","title":"Quantum Models","text":"<p>DataReuploadingClassifier - <code>n_layers</code> (int): Number of quantum layers - <code>learning_rate</code> (float): Learning rate for optimization - <code>batch_size</code> (int): Batch size for training - <code>n_input_copies</code> (int): Number of data reuploads - <code>observable_type</code> (str): Type of observable (\"all\" or \"half\")</p> <p>CircuitCentricClassifier - <code>n_layers</code> (int): Circuit depth - <code>learning_rate</code> (float): Learning rate - <code>n_qubits</code> (int): Number of qubits</p> <p>QuantumKitchenSinks - <code>n_episodes</code> (int): Number of training episodes - <code>learning_rate</code> (float): Learning rate - <code>gamma</code> (float): Kernel parameter</p>"},{"location":"api_reference.html#classical-models","title":"Classical Models","text":"<p>SVC (Support Vector Classifier) - <code>C</code> (float): Regularization parameter - <code>gamma</code> (str or float): Kernel coefficient - <code>kernel</code> (str): Kernel type</p> <p>MLPClassifier - <code>hidden_layer_sizes</code> (tuple): Hidden layer configuration - <code>learning_rate</code> (str): Learning rate schedule - <code>alpha</code> (float): L2 regularization parameter</p>"},{"location":"api_reference.html#xai-explainable-ai","title":"XAI (Explainable AI)","text":"<p>Generate SHAP explanations and visualizations.</p> <pre><code>from quoptuna import XAI\nfrom quoptuna.backend.xai.xai import XAIConfig\n\n# Configure XAI\nconfig = XAIConfig(\n    use_proba=True,\n    onsubset=True,\n    subset_size=50\n)\n\n# Create XAI instance\nxai = XAI(\n    model=trained_model,\n    data=data_dict,\n    config=config\n)\n\n# Get evaluation report\nreport = xai.get_report()\n</code></pre> <p>XAIConfig Parameters: - <code>use_proba</code> (bool): Use probability predictions - <code>onsubset</code> (bool): Use subset of data - <code>subset_size</code> (int): Size of subset</p> <p>XAI Methods:</p>"},{"location":"api_reference.html#get_report","title":"<code>get_report()</code>","text":"<p>Generate classification report.</p> <p>Returns: - dict: Contains confusion matrix, classification report, and ROC curve data</p>"},{"location":"api_reference.html#get_plotplot_type-kwargs","title":"<code>get_plot(plot_type, **kwargs)</code>","text":"<p>Generate SHAP visualization.</p> <p>Parameters: - <code>plot_type</code> (str): Type of plot   - <code>\"bar\"</code>: Feature importance bar plot   - <code>\"beeswarm\"</code>: SHAP value distribution   - <code>\"violin\"</code>: Violin plot of SHAP values   - <code>\"heatmap\"</code>: Instance-level SHAP heatmap   - <code>\"waterfall\"</code>: Individual prediction explanation - <code>max_display</code> (int): Maximum features to display - <code>class_index</code> (int): Class to explain (for binary: 0 or 1) - <code>index</code> (int): Sample index (for waterfall plot) - <code>save_config</code> (dict, optional): Configuration for saving plot</p> <p>Returns: - str: Base64 encoded image</p>"},{"location":"api_reference.html#plot_confusion_matrix","title":"<code>plot_confusion_matrix()</code>","text":"<p>Generate confusion matrix plot.</p> <p>Returns: - matplotlib.figure.Figure: Confusion matrix figure</p>"},{"location":"api_reference.html#generate_report_with_langchainprovider-api_key-model_name-dataset_infonone","title":"<code>generate_report_with_langchain(provider, api_key, model_name, dataset_info=None)</code>","text":"<p>Generate AI-powered analysis report.</p> <p>Parameters: - <code>provider</code> (str): LLM provider (\"google\", \"openai\", \"anthropic\") - <code>api_key</code> (str): API key for the provider - <code>model_name</code> (str): Model identifier - <code>dataset_info</code> (dict, optional): Dataset metadata</p> <p>Returns: - str: Markdown formatted report</p>"},{"location":"api_reference.html#utility-functions","title":"Utility Functions","text":""},{"location":"api_reference.html#mock_csv_data","title":"mock_csv_data","text":"<p>Save DataFrame to CSV file.</p> <pre><code>from quoptuna.backend.utils.data_utils.data import mock_csv_data\n\nfile_path = mock_csv_data(\n    dataframe,\n    tmp_path=\"data\",\n    file_name=\"my_dataset\"\n)\n</code></pre> <p>Parameters: - <code>dataframe</code> (pd.DataFrame): Data to save - <code>tmp_path</code> (str): Directory path - <code>file_name</code> (str): File name (without .csv extension)</p> <p>Returns: - str: Full path to saved file</p>"},{"location":"api_reference.html#complete-example","title":"Complete Example","text":"<p>Here's a complete example workflow:</p> <pre><code>import pandas as pd\nfrom ucimlrepo import fetch_ucirepo\nfrom quoptuna import DataPreparation, Optimizer, XAI\nfrom quoptuna.backend.models import create_model\nfrom quoptuna.backend.xai.xai import XAIConfig\nfrom quoptuna.backend.utils.data_utils.data import mock_csv_data\n\n# 1. Load dataset\ndataset = fetch_ucirepo(id=143)  # Statlog dataset\nX = dataset.data.features\ny = dataset.data.targets\ndf = pd.concat([X, y], axis=1)\n\n# 2. Prepare data\ndf[\"target\"] = df[\"A15\"].replace({0: -1, 1: 1})\ndf = df.drop(columns=[\"A15\"])\ndf = df.dropna()\n\n# 3. Save to file\nfile_path = mock_csv_data(df, tmp_path=\"data\", file_name=\"Statlog\")\n\n# 4. Prepare for training\ndata_prep = DataPreparation(\n    file_path=file_path,\n    x_cols=list(df.columns.difference([\"target\"])),\n    y_col=\"target\"\n)\ndata_dict = data_prep.get_data(output_type=\"2\")\n\n# Convert to numpy arrays\ndata_dict[\"train_x\"] = data_dict[\"train_x\"].values\ndata_dict[\"test_x\"] = data_dict[\"test_x\"].values\ndata_dict[\"train_y\"] = data_dict[\"train_y\"].values\ndata_dict[\"test_y\"] = data_dict[\"test_y\"].values\n\n# 5. Run optimization\noptimizer = Optimizer(\n    db_name=\"Statlog\",\n    study_name=\"Statlog\",\n    data=data_dict\n)\nstudy, best_trials = optimizer.optimize(n_trials=100)\n\n# 6. Train best model\nbest_trial = best_trials[0]\nmodel = create_model(**best_trial.params)\nmodel.fit(data_dict[\"train_x\"], data_dict[\"train_y\"])\n\n# 7. SHAP analysis\nconfig = XAIConfig(use_proba=True, onsubset=True, subset_size=50)\nxai = XAI(model=model, data=data_dict, config=config)\n\n# 8. Generate visualizations\nbar_plot = xai.get_plot(\"bar\", max_display=10, class_index=1)\nbeeswarm_plot = xai.get_plot(\"beeswarm\", max_display=10, class_index=1)\n\n# 9. Generate report\nreport = xai.generate_report_with_langchain(\n    provider=\"google\",\n    api_key=\"your-api-key\",\n    model_name=\"models/gemini-2.0-flash-exp\",\n    dataset_info={\n        \"Name\": \"Statlog Credit Approval\",\n        \"URL\": \"https://archive.ics.uci.edu/dataset/143\",\n        \"Description\": \"Credit card application dataset\"\n    }\n)\n\nprint(report)\n</code></pre>"},{"location":"api_reference.html#data-format-requirements","title":"Data Format Requirements","text":""},{"location":"api_reference.html#input-data","title":"Input Data","text":"<p>CSV Format: - Must have header row with column names - Target column should contain binary values - Features can be numeric or categorical - Missing values will be removed</p> <p>Target Encoding: - Binary classification: Must use <code>-1</code> and <code>1</code> - QuOptuna does not currently support multi-class classification</p>"},{"location":"api_reference.html#data-dictionary-format","title":"Data Dictionary Format","text":"<p>After preprocessing, data should be in this format:</p> <pre><code>data_dict = {\n    \"train_x\": np.ndarray,  # Shape: (n_train_samples, n_features)\n    \"test_x\": np.ndarray,   # Shape: (n_test_samples, n_features)\n    \"train_y\": np.ndarray,  # Shape: (n_train_samples,)\n    \"test_y\": np.ndarray    # Shape: (n_test_samples,)\n}\n</code></pre>"},{"location":"api_reference.html#advanced-usage","title":"Advanced Usage","text":""},{"location":"api_reference.html#custom-optimization-objectives","title":"Custom Optimization Objectives","text":"<p>You can customize the optimization objective:</p> <pre><code>import optuna\n\ndef custom_objective(trial):\n    # Define your custom objective\n    params = {\n        \"model_type\": trial.suggest_categorical(\"model_type\", [\"SVC\", \"MLPClassifier\"]),\n        \"C\": trial.suggest_float(\"C\", 0.1, 10.0)\n    }\n\n    model = create_model(**params)\n    model.fit(train_x, train_y)\n\n    # Return custom metric\n    return custom_metric(model, test_x, test_y)\n\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(custom_objective, n_trials=100)\n</code></pre>"},{"location":"api_reference.html#parallel-optimization","title":"Parallel Optimization","text":"<p>Run multiple trials in parallel:</p> <pre><code>optimizer = Optimizer(db_name=\"my_db\", study_name=\"my_study\", data=data_dict)\n\n# Use n_jobs for parallel execution\nstudy, best_trials = optimizer.optimize(\n    n_trials=100,\n    n_jobs=4  # Run 4 trials in parallel\n)\n</code></pre>"},{"location":"api_reference.html#saving-and-loading-models","title":"Saving and Loading Models","text":"<pre><code>import joblib\n\n# Save model\njoblib.dump(model, \"model.pkl\")\n\n# Load model\nloaded_model = joblib.load(\"model.pkl\")\n</code></pre>"},{"location":"api_reference.html#error-handling","title":"Error Handling","text":"<p>Common errors and solutions:</p> <pre><code>try:\n    optimizer.optimize(n_trials=100)\nexcept ValueError as e:\n    # Handle data validation errors\n    print(f\"Data error: {e}\")\nexcept RuntimeError as e:\n    # Handle optimization errors\n    print(f\"Optimization error: {e}\")\nexcept Exception as e:\n    # Handle unexpected errors\n    print(f\"Unexpected error: {e}\")\n</code></pre>"},{"location":"api_reference.html#performance-tips","title":"Performance Tips","text":"<ol> <li>Use Subsets for SHAP: Analyze 50-100 samples for faster computation</li> <li>Increase Trials Gradually: Start with 50 trials, increase as needed</li> <li>Use Caching: Reuse loaded studies when possible</li> <li>Monitor Memory: Large datasets may require subset analysis</li> <li>Parallel Processing: Use <code>n_jobs</code> parameter for faster optimization</li> </ol>"},{"location":"api_reference.html#see-also","title":"See Also","text":"<ul> <li>User Guide - Step-by-step usage instructions</li> <li>Examples - Common use cases and tutorials</li> <li>GitHub Repository - Source code and issues</li> </ul>"},{"location":"changelog.html","title":"Changelog","text":""},{"location":"changelog.html#changelog","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog.html#unreleased","title":"Unreleased","text":""},{"location":"examples.html","title":"Examples","text":""},{"location":"examples.html#examples","title":"Examples","text":""},{"location":"examples.html#introduction","title":"Introduction","text":"<p>This page provides practical examples for common QuOptuna use cases. Each example includes complete, runnable code.</p>"},{"location":"examples.html#table-of-contents","title":"Table of Contents","text":"<ol> <li>Basic Workflow</li> <li>UCI Dataset Analysis</li> <li>Custom Dataset</li> <li>SHAP Analysis</li> <li>Comparing Models</li> <li>Report Generation</li> <li>Batch Processing</li> </ol>"},{"location":"examples.html#basic-workflow","title":"Basic Workflow","text":"<p>Complete workflow from data loading to SHAP analysis:</p> <pre><code>from quoptuna import DataPreparation, Optimizer, XAI\nfrom quoptuna.backend.models import create_model\nfrom quoptuna.backend.xai.xai import XAIConfig\nfrom quoptuna.backend.utils.data_utils.data import mock_csv_data\nimport pandas as pd\n\n# Load your data\ndf = pd.read_csv(\"your_data.csv\")\n\n# Ensure target is -1 and 1\ndf[\"target\"] = df[\"target\"].replace({0: -1, 1: 1})\n\n# Save to file\nfile_path = mock_csv_data(df, tmp_path=\"data\", file_name=\"my_data\")\n\n# Prepare data\ndata_prep = DataPreparation(\n    file_path=file_path,\n    x_cols=[col for col in df.columns if col != \"target\"],\n    y_col=\"target\"\n)\ndata_dict = data_prep.get_data(output_type=\"2\")\n\n# Convert to numpy\nfor key in [\"train_x\", \"test_x\", \"train_y\", \"test_y\"]:\n    data_dict[key] = data_dict[key].values\n\n# Optimize\noptimizer = Optimizer(db_name=\"my_experiment\", study_name=\"run_1\", data=data_dict)\nstudy, best_trials = optimizer.optimize(n_trials=50)\n\n# Train best model\nbest_model = create_model(**best_trials[0].params)\nbest_model.fit(data_dict[\"train_x\"], data_dict[\"train_y\"])\n\n# SHAP analysis\nxai = XAI(\n    model=best_model,\n    data=data_dict,\n    config=XAIConfig(use_proba=True, onsubset=True, subset_size=50)\n)\n\n# Generate plots\nbar_plot = xai.get_plot(\"bar\", max_display=10, class_index=1)\nprint(\"Analysis complete!\")\n</code></pre>"},{"location":"examples.html#uci-dataset-analysis","title":"UCI Dataset Analysis","text":"<p>Working with UCI ML Repository datasets:</p> <pre><code>from ucimlrepo import fetch_ucirepo\nfrom quoptuna import DataPreparation, Optimizer\nfrom quoptuna.backend.utils.data_utils.data import mock_csv_data\nimport pandas as pd\n\n# Fetch dataset from UCI\ndataset = fetch_ucirepo(id=143)  # Statlog Credit Approval\n\n# Get metadata\nprint(\"Dataset:\", dataset.metadata[\"name\"])\nprint(\"Instances:\", dataset.metadata[\"num_instances\"])\nprint(\"Features:\", dataset.metadata[\"num_features\"])\n\n# Prepare data\nX = dataset.data.features\ny = dataset.data.targets\ndf = pd.concat([X, y], axis=1)\n\n# Transform target\ntarget_col = dataset.metadata[\"target_col\"][0]\ndf[\"target\"] = df[target_col].replace({0: -1, 1: 1})\ndf = df.drop(columns=[target_col])\ndf = df.dropna()\n\n# Save and prepare\nfile_path = mock_csv_data(df, tmp_path=\"data\", file_name=\"Statlog\")\n\ndata_prep = DataPreparation(\n    file_path=file_path,\n    x_cols=list(df.columns.difference([\"target\"])),\n    y_col=\"target\"\n)\ndata_dict = data_prep.get_data(output_type=\"2\")\n\n# Convert to numpy\nfor key in data_dict.keys():\n    data_dict[key] = data_dict[key].values\n\n# Run optimization\noptimizer = Optimizer(db_name=\"Statlog\", study_name=\"Statlog\", data=data_dict)\nstudy, best_trials = optimizer.optimize(n_trials=100)\n\n# Show results\nfor i, trial in enumerate(best_trials[:3]):\n    print(f\"\\n=== Best Trial {i+1} ===\")\n    print(f\"Model: {trial.params['model_type']}\")\n    print(f\"Quantum F1: {trial.user_attrs.get('Quantum_f1_score', 0):.4f}\")\n    print(f\"Classical F1: {trial.user_attrs.get('Classical_f1_score', 0):.4f}\")\n</code></pre>"},{"location":"examples.html#custom-dataset","title":"Custom Dataset","text":"<p>Loading and processing a custom CSV file:</p> <pre><code>import pandas as pd\nfrom quoptuna import DataPreparation, Optimizer\nfrom quoptuna.backend.utils.data_utils.data import mock_csv_data\n\n# Load custom dataset\ndf = pd.read_csv(\"my_dataset.csv\")\n\n# Explore data\nprint(\"Shape:\", df.shape)\nprint(\"Columns:\", df.columns.tolist())\nprint(\"Missing values:\", df.isnull().sum().sum())\n\n# Handle missing values\ndf = df.dropna()\n\n# Transform target to -1 and 1\n# Example: If target is 'Yes'/'No'\ndf[\"target\"] = df[\"outcome\"].map({\"Yes\": 1, \"No\": -1})\n\n# Drop original target column\ndf = df.drop(columns=[\"outcome\"])\n\n# Select features\nfeature_cols = [\"age\", \"income\", \"credit_score\", \"debt_ratio\"]\n\n# Keep only selected columns\ndf = df[feature_cols + [\"target\"]]\n\n# Save processed data\nfile_path = mock_csv_data(df, tmp_path=\"data\", file_name=\"custom_dataset\")\n\n# Prepare for training\ndata_prep = DataPreparation(\n    file_path=file_path,\n    x_cols=feature_cols,\n    y_col=\"target\"\n)\ndata_dict = data_prep.get_data(output_type=\"2\")\n\n# Convert to numpy\nfor key in data_dict.keys():\n    data_dict[key] = data_dict[key].values\n\n# Optimize\noptimizer = Optimizer(\n    db_name=\"custom_experiment\",\n    study_name=\"trial_001\",\n    data=data_dict\n)\nstudy, best_trials = optimizer.optimize(n_trials=100)\n\nprint(f\"\\nFound {len(best_trials)} best trials\")\n</code></pre>"},{"location":"examples.html#shap-analysis","title":"SHAP Analysis","text":"<p>Comprehensive SHAP analysis with all plot types:</p> <pre><code>from quoptuna import XAI\nfrom quoptuna.backend.models import create_model\nfrom quoptuna.backend.xai.xai import XAIConfig\nimport os\n\n# Assuming you have optimized model and data_dict from previous steps\n# Load best trial parameters\nbest_params = best_trials[0].params\n\n# Train model\nmodel = create_model(**best_params)\nmodel.fit(data_dict[\"train_x\"], data_dict[\"train_y\"])\n\n# Configure XAI\nconfig = XAIConfig(\n    use_proba=True,\n    onsubset=True,\n    subset_size=100\n)\n\n# Create XAI instance\nxai = XAI(model=model, data=data_dict, config=config)\n\n# Create output directory\nos.makedirs(\"outputs/shap_plots\", exist_ok=True)\n\n# Generate and save all plot types\nplot_types = [\"bar\", \"beeswarm\", \"violin\", \"heatmap\"]\n\nfor plot_type in plot_types:\n    print(f\"Generating {plot_type} plot...\")\n\n    plot = xai.get_plot(\n        plot_type,\n        max_display=10,\n        class_index=1,\n        save_config={\n            \"save_path\": \"outputs/shap_plots\",\n            \"save_name\": f\"{plot_type}_plot\",\n            \"save_format\": \"png\",\n            \"save_dpi\": 300\n        }\n    )\n\n    print(f\"Saved {plot_type} plot\")\n\n# Generate waterfall plots for first 5 samples\nfor i in range(5):\n    waterfall = xai.get_plot(\n        \"waterfall\",\n        index=i,\n        class_index=1,\n        save_config={\n            \"save_path\": \"outputs/shap_plots\",\n            \"save_name\": f\"waterfall_sample_{i}\",\n            \"save_format\": \"png\",\n            \"save_dpi\": 300\n        }\n    )\n\n    print(f\"Saved waterfall plot for sample {i}\")\n\n# Get classification report\nreport = xai.get_report()\n\nprint(\"\\n=== Classification Report ===\")\nprint(report[\"classification_report\"])\n\n# Plot confusion matrix\nimport matplotlib.pyplot as plt\n\nfig = xai.plot_confusion_matrix()\nplt.savefig(\"outputs/shap_plots/confusion_matrix.png\", dpi=300, bbox_inches=\"tight\")\nplt.close()\n\nprint(\"\\nAll SHAP plots saved to outputs/shap_plots/\")\n</code></pre>"},{"location":"examples.html#comparing-models","title":"Comparing Models","text":"<p>Compare quantum vs classical models:</p> <pre><code>from quoptuna import Optimizer\nfrom quoptuna.backend.models import create_model\nimport pandas as pd\n\n# Run optimization (assumes data_dict is prepared)\noptimizer = Optimizer(db_name=\"comparison\", study_name=\"quantum_vs_classical\", data=data_dict)\nstudy, best_trials = optimizer.optimize(n_trials=100)\n\n# Separate quantum and classical trials\nquantum_trials = []\nclassical_trials = []\n\nfor trial in study.get_trials():\n    model_type = trial.params.get(\"model_type\", \"\")\n\n    # Determine if quantum or classical\n    if \"Classifier\" in model_type and any(\n        q in model_type\n        for q in [\"Reuploading\", \"Circuit\", \"Quantum\", \"Kitchen\", \"Dressed\"]\n    ):\n        quantum_trials.append(trial)\n    else:\n        classical_trials.append(trial)\n\n# Compare performance\ndef get_f1_score(trial):\n    q_f1 = trial.user_attrs.get(\"Quantum_f1_score\", 0)\n    c_f1 = trial.user_attrs.get(\"Classical_f1_score\", 0)\n    return max(q_f1, c_f1)\n\n# Get best from each category\nbest_quantum = max(quantum_trials, key=get_f1_score) if quantum_trials else None\nbest_classical = max(classical_trials, key=get_f1_score) if classical_trials else None\n\nprint(\"=== Model Comparison ===\\n\")\n\nif best_quantum:\n    print(\"Best Quantum Model:\")\n    print(f\"  Type: {best_quantum.params['model_type']}\")\n    print(f\"  F1 Score: {get_f1_score(best_quantum):.4f}\")\n    print(f\"  Trial: {best_quantum.number}\")\n\nif best_classical:\n    print(\"\\nBest Classical Model:\")\n    print(f\"  Type: {best_classical.params['model_type']}\")\n    print(f\"  F1 Score: {get_f1_score(best_classical):.4f}\")\n    print(f\"  Trial: {best_classical.number}\")\n\n# Create comparison DataFrame\ncomparison_data = []\n\nfor trial in quantum_trials + classical_trials:\n    comparison_data.append({\n        \"Trial\": trial.number,\n        \"Model Type\": trial.params[\"model_type\"],\n        \"Category\": \"Quantum\" if trial in quantum_trials else \"Classical\",\n        \"F1 Score\": get_f1_score(trial),\n        \"State\": trial.state.name\n    })\n\ndf_comparison = pd.DataFrame(comparison_data)\ndf_comparison = df_comparison.sort_values(\"F1 Score\", ascending=False)\n\nprint(\"\\n=== Top 10 Models ===\")\nprint(df_comparison.head(10))\n\n# Save results\ndf_comparison.to_csv(\"outputs/model_comparison.csv\", index=False)\n</code></pre>"},{"location":"examples.html#report-generation","title":"Report Generation","text":"<p>Generate comprehensive AI reports:</p> <pre><code>from quoptuna import XAI\nfrom quoptuna.backend.xai.xai import XAIConfig\nimport os\n\n# Train model (from previous steps)\nmodel = create_model(**best_trials[0].params)\nmodel.fit(data_dict[\"train_x\"], data_dict[\"train_y\"])\n\n# Create XAI instance\nxai = XAI(\n    model=model,\n    data=data_dict,\n    config=XAIConfig(use_proba=True, onsubset=True, subset_size=50)\n)\n\n# Dataset information for better reports\ndataset_info = {\n    \"Name\": \"Credit Card Approval\",\n    \"URL\": \"https://archive.ics.uci.edu/dataset/143\",\n    \"Description\": \"\"\"\n        This dataset concerns credit card applications.\n        It contains a mix of continuous and categorical features\n        for predicting credit approval decisions.\n    \"\"\",\n    \"Features\": [\"Age\", \"Income\", \"Credit Score\", \"Employment Status\"],\n    \"Target\": \"Approval Decision\",\n    \"Instances\": 690,\n    \"Task\": \"Binary Classification\"\n}\n\n# Generate report with Google Gemini\nreport = xai.generate_report_with_langchain(\n    provider=\"google\",\n    api_key=os.getenv(\"GOOGLE_API_KEY\"),\n    model_name=\"models/gemini-2.0-flash-exp\",\n    dataset_info=dataset_info\n)\n\n# Save report\nwith open(\"outputs/analysis_report.md\", \"w\") as f:\n    f.write(report)\n\nprint(\"Report saved to outputs/analysis_report.md\")\n\n# Generate with OpenAI GPT\nreport_gpt = xai.generate_report_with_langchain(\n    provider=\"openai\",\n    api_key=os.getenv(\"OPENAI_API_KEY\"),\n    model_name=\"gpt-4\",\n    dataset_info=dataset_info\n)\n\nwith open(\"outputs/analysis_report_gpt4.md\", \"w\") as f:\n    f.write(report_gpt)\n\nprint(\"GPT-4 report saved to outputs/analysis_report_gpt4.md\")\n</code></pre>"},{"location":"examples.html#batch-processing","title":"Batch Processing","text":"<p>Process multiple datasets:</p> <pre><code>from quoptuna import DataPreparation, Optimizer\nfrom quoptuna.backend.utils.data_utils.data import mock_csv_data\nimport pandas as pd\nimport os\n\n# List of datasets to process\ndatasets = [\n    {\"id\": 143, \"name\": \"Statlog\"},\n    {\"id\": 176, \"name\": \"Blood\"},\n    {\"id\": 267, \"name\": \"Banknote\"},\n]\n\nresults = []\n\nfor dataset_info in datasets:\n    print(f\"\\n{'='*50}\")\n    print(f\"Processing: {dataset_info['name']}\")\n    print('='*50)\n\n    try:\n        # Fetch dataset\n        from ucimlrepo import fetch_ucirepo\n        dataset = fetch_ucirepo(id=dataset_info[\"id\"])\n\n        # Prepare data\n        X = dataset.data.features\n        y = dataset.data.targets\n        df = pd.concat([X, y], axis=1)\n\n        # Get target column name\n        target_col = dataset.metadata[\"target_col\"][0]\n        df[\"target\"] = df[target_col].replace({0: -1, 1: 1})\n        df = df.drop(columns=[target_col])\n        df = df.dropna()\n\n        # Save\n        file_path = mock_csv_data(\n            df,\n            tmp_path=\"data/batch\",\n            file_name=dataset_info[\"name\"]\n        )\n\n        # Prepare\n        data_prep = DataPreparation(\n            file_path=file_path,\n            x_cols=list(df.columns.difference([\"target\"])),\n            y_col=\"target\"\n        )\n        data_dict = data_prep.get_data(output_type=\"2\")\n\n        # Convert to numpy\n        for key in data_dict.keys():\n            data_dict[key] = data_dict[key].values\n\n        # Optimize\n        optimizer = Optimizer(\n            db_name=f\"batch_{dataset_info['name']}\",\n            study_name=dataset_info[\"name\"],\n            data=data_dict\n        )\n        study, best_trials = optimizer.optimize(n_trials=50)\n\n        # Record results\n        best_f1 = max(\n            best_trials[0].user_attrs.get(\"Quantum_f1_score\", 0),\n            best_trials[0].user_attrs.get(\"Classical_f1_score\", 0)\n        )\n\n        results.append({\n            \"Dataset\": dataset_info[\"name\"],\n            \"Best Model\": best_trials[0].params[\"model_type\"],\n            \"Best F1\": best_f1,\n            \"Trials\": len(study.trials),\n            \"Status\": \"Success\"\n        })\n\n        print(f\"\u2713 Completed: {dataset_info['name']}\")\n        print(f\"  Best F1: {best_f1:.4f}\")\n        print(f\"  Model: {best_trials[0].params['model_type']}\")\n\n    except Exception as e:\n        print(f\"\u2717 Failed: {dataset_info['name']}\")\n        print(f\"  Error: {e}\")\n\n        results.append({\n            \"Dataset\": dataset_info[\"name\"],\n            \"Best Model\": None,\n            \"Best F1\": None,\n            \"Trials\": 0,\n            \"Status\": f\"Failed: {str(e)}\"\n        })\n\n# Save summary\ndf_results = pd.DataFrame(results)\ndf_results.to_csv(\"outputs/batch_processing_results.csv\", index=False)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"BATCH PROCESSING COMPLETE\")\nprint(\"=\"*50)\nprint(df_results)\n</code></pre>"},{"location":"examples.html#advanced-custom-objective-function","title":"Advanced: Custom Objective Function","text":"<p>Define custom optimization objectives:</p> <pre><code>import optuna\nfrom quoptuna.backend.models import create_model\nfrom sklearn.metrics import f1_score, precision_score, recall_score\n\ndef custom_objective(trial, data_dict):\n    \"\"\"Custom objective balancing F1 score and model complexity.\"\"\"\n\n    # Suggest model type\n    model_type = trial.suggest_categorical(\n        \"model_type\",\n        [\"SVC\", \"MLPClassifier\", \"DataReuploadingClassifier\"]\n    )\n\n    # Suggest hyperparameters based on model type\n    if model_type == \"SVC\":\n        params = {\n            \"model_type\": model_type,\n            \"C\": trial.suggest_float(\"C\", 0.1, 10.0),\n            \"gamma\": trial.suggest_categorical(\"gamma\", [\"scale\", \"auto\"])\n        }\n    elif model_type == \"MLPClassifier\":\n        params = {\n            \"model_type\": model_type,\n            \"hidden_layer_sizes\": trial.suggest_categorical(\n                \"hidden_layer_sizes\",\n                [\"(10,)\", \"(50,)\", \"(10, 10)\"]\n            ),\n            \"alpha\": trial.suggest_float(\"alpha\", 0.0001, 0.1, log=True)\n        }\n    else:  # DataReuploadingClassifier\n        params = {\n            \"model_type\": model_type,\n            \"n_layers\": trial.suggest_int(\"n_layers\", 2, 10),\n            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.5)\n        }\n\n    # Create and train model\n    model = create_model(**params)\n    model.fit(data_dict[\"train_x\"], data_dict[\"train_y\"])\n\n    # Evaluate\n    y_pred = model.predict(data_dict[\"test_x\"])\n    y_true = data_dict[\"test_y\"]\n\n    # Calculate metrics\n    f1 = f1_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred)\n    recall = recall_score(y_true, y_pred)\n\n    # Store metrics as user attributes\n    trial.set_user_attr(\"precision\", precision)\n    trial.set_user_attr(\"recall\", recall)\n\n    # Return weighted score\n    # Prefer higher F1 but penalize complex models\n    complexity_penalty = 0.01 if model_type == \"DataReuploadingClassifier\" else 0\n    return f1 - complexity_penalty\n\n# Create study\nstudy = optuna.create_study(direction=\"maximize\")\n\n# Optimize\nstudy.optimize(\n    lambda trial: custom_objective(trial, data_dict),\n    n_trials=100\n)\n\n# Show results\nprint(f\"Best F1 Score: {study.best_value:.4f}\")\nprint(f\"Best Parameters: {study.best_params}\")\nprint(f\"Precision: {study.best_trial.user_attrs['precision']:.4f}\")\nprint(f\"Recall: {study.best_trial.user_attrs['recall']:.4f}\")\n</code></pre>"},{"location":"examples.html#next-steps","title":"Next Steps","text":"<ul> <li>Review the API Reference for detailed class documentation</li> <li>Check the User Guide for the Streamlit interface</li> <li>Visit GitHub for more examples</li> </ul>"},{"location":"user_guide.html","title":"User Guide","text":""},{"location":"user_guide.html#user-guide","title":"User Guide","text":""},{"location":"user_guide.html#introduction","title":"Introduction","text":"<p>QuOptuna is a comprehensive platform for quantum-enhanced machine learning optimization. This guide will walk you through the complete workflow from dataset selection to model analysis and report generation.</p>"},{"location":"user_guide.html#workflow-overview","title":"Workflow Overview","text":"<p>The QuOptuna workflow consists of four main stages:</p> <ol> <li>Dataset Selection - Load and prepare your data</li> <li>Optimization - Find the best hyperparameters</li> <li>Model Training - Train models with optimized parameters</li> <li>SHAP Analysis - Understand and explain model behavior</li> </ol>"},{"location":"user_guide.html#getting-started","title":"Getting Started","text":""},{"location":"user_guide.html#installation","title":"Installation","text":"<p>Install QuOptuna using UV (recommended) or pip:</p> <pre><code># Using UV (recommended)\nuv pip install quoptuna\n\n# Using pip\npip install quoptuna\n</code></pre>"},{"location":"user_guide.html#launching-the-application","title":"Launching the Application","text":"<p>Start the Streamlit interface:</p> <pre><code>quoptuna --start\n</code></pre> <p>Or using Python:</p> <pre><code>python -m quoptuna.frontend.app run\n</code></pre>"},{"location":"user_guide.html#dataset-selection","title":"Dataset Selection","text":""},{"location":"user_guide.html#uci-ml-repository","title":"UCI ML Repository","text":"<p>QuOptuna provides easy access to datasets from the UCI Machine Learning Repository:</p> <ol> <li>Navigate to the Dataset Selection page</li> <li>Select UCI ML Repository tab</li> <li>Choose from popular datasets or enter a custom UCI ID</li> <li>Click Load UCI Dataset</li> </ol> <p>Popular Datasets: - Statlog (Australian Credit Approval) - ID: 143 - Blood Transfusion Service Center - ID: 176 - Banknote Authentication - ID: 267 - Heart Disease - ID: 45 - Ionosphere - ID: 225</p>"},{"location":"user_guide.html#custom-dataset-upload","title":"Custom Dataset Upload","text":"<p>To use your own dataset:</p> <ol> <li>Navigate to the Upload Custom Dataset tab</li> <li>Upload a CSV file</li> <li>Configure target and feature columns</li> <li>Apply target transformation if needed</li> </ol>"},{"location":"user_guide.html#data-configuration","title":"Data Configuration","text":"<p>Important: QuOptuna requires binary classification targets to be encoded as <code>-1</code> and <code>1</code>.</p> <ol> <li>Select Target Column: Choose the column you want to predict</li> <li>Select Features: Choose the features to use for prediction</li> <li>Target Transformation: Map your target values to -1 and 1</li> <li>Handle Missing Values: QuOptuna will automatically remove rows with missing values</li> </ol> <p>Click Save Configuration to proceed to the next step.</p>"},{"location":"user_guide.html#data-preparation-optimization","title":"Data Preparation &amp; Optimization","text":""},{"location":"user_guide.html#data-preparation","title":"Data Preparation","text":"<p>Once your dataset is configured:</p> <ol> <li>Review the dataset summary (rows, columns, target distribution)</li> <li>Click Prepare Data for Training</li> <li>QuOptuna will automatically:</li> <li>Split data into training and test sets</li> <li>Scale features</li> <li>Convert to the format required by models</li> </ol>"},{"location":"user_guide.html#hyperparameter-optimization","title":"Hyperparameter Optimization","text":"<p>Configure and run optimization:</p> <ol> <li>Database Name: Name for storing optimization results</li> <li>Study Name: Unique identifier for this optimization study</li> <li>Number of Trials: How many hyperparameter combinations to try (recommended: 50-200)</li> </ol> <p>Click Start Optimization to begin. This will: - Test multiple model types (both quantum and classical) - Try different hyperparameter combinations - Track the best performing configurations</p> <p>Model Types Tested: - Data Reuploading Classifier (Quantum) - Circuit-Centric Classifier (Quantum) - Quantum Kitchen Sinks (Quantum) - Support Vector Classifier (Classical) - Multi-Layer Perceptron (Classical) - And more...</p>"},{"location":"user_guide.html#understanding-results","title":"Understanding Results","text":"<p>After optimization completes, you'll see: - Best Trials: Top performing configurations - F1 Scores: Performance metrics for quantum and classical approaches - Hyperparameters: The configuration for each trial</p>"},{"location":"user_guide.html#shap-analysis-reporting","title":"SHAP Analysis &amp; Reporting","text":""},{"location":"user_guide.html#trial-selection","title":"Trial Selection","text":"<ol> <li>Navigate to the SHAP Analysis page</li> <li>Select a trial from the dropdown (sorted by performance)</li> <li>Review the trial details and parameters</li> </ol>"},{"location":"user_guide.html#model-training","title":"Model Training","text":"<ol> <li>Click Train Model to train the selected model</li> <li>The model will be trained on your data with the optimized hyperparameters</li> </ol>"},{"location":"user_guide.html#shap-analysis","title":"SHAP Analysis","text":"<p>Configure SHAP analysis:</p> <ul> <li>Use Probability Predictions: Use probability outputs instead of class predictions</li> <li>Use Subset of Data: Analyze a subset for faster computation</li> <li>Subset Size: Number of samples to analyze (recommended: 50-100)</li> </ul> <p>Click Run SHAP Analysis to calculate SHAP values.</p>"},{"location":"user_guide.html#shap-visualizations","title":"SHAP Visualizations","text":"<p>QuOptuna provides multiple visualization types:</p>"},{"location":"user_guide.html#bar-plot","title":"Bar Plot","text":"<p>Shows the mean absolute SHAP value for each feature, indicating overall importance.</p> <p>Use Case: Quick overview of feature importance</p>"},{"location":"user_guide.html#beeswarm-plot","title":"Beeswarm Plot","text":"<p>Shows the distribution of SHAP values, with color indicating feature value (red = high, blue = low).</p> <p>Use Case: Understanding how feature values affect predictions</p>"},{"location":"user_guide.html#violin-plot","title":"Violin Plot","text":"<p>Shows the distribution of SHAP values for each feature.</p> <p>Use Case: Understanding the variability in feature impact</p>"},{"location":"user_guide.html#heatmap","title":"Heatmap","text":"<p>Shows SHAP values for individual instances.</p> <p>Use Case: Instance-level analysis, finding patterns in predictions</p>"},{"location":"user_guide.html#waterfall-plot","title":"Waterfall Plot","text":"<p>Explains how features contribute to a single prediction.</p> <p>Use Case: Understanding individual predictions in detail</p>"},{"location":"user_guide.html#confusion-matrix","title":"Confusion Matrix","text":"<p>Shows classification performance.</p> <p>Use Case: Evaluating overall model accuracy</p>"},{"location":"user_guide.html#report-generation","title":"Report Generation","text":"<p>Generate comprehensive AI-powered reports:</p> <ol> <li>Select LLM Provider: Google (Gemini), OpenAI (GPT), or Anthropic (Claude)</li> <li>Enter API Key: Your API key for the selected provider</li> <li>Model Name: Specific model to use (e.g., \"models/gemini-2.0-flash-exp\")</li> <li>Dataset Information (optional): Add context about your dataset</li> </ol> <p>Click Generate Report to create a detailed analysis report.</p> <p>Report Includes: - Performance metrics analysis - SHAP value interpretation - Feature importance ranking - Risk and fairness assessment - Governance recommendations</p>"},{"location":"user_guide.html#best-practices","title":"Best Practices","text":""},{"location":"user_guide.html#optimization","title":"Optimization","text":"<ul> <li>Start Small: Begin with 50-100 trials to get quick results</li> <li>Increase Gradually: Use 100-200 trials for production models</li> <li>Monitor Performance: Check both quantum and classical model scores</li> <li>Save Studies: Use descriptive names for databases and studies</li> </ul>"},{"location":"user_guide.html#shap-analysis_1","title":"SHAP Analysis","text":"<ul> <li>Use Subsets: Analyze 50-100 samples for faster computation</li> <li>Multiple Plots: Generate several plot types for comprehensive understanding</li> <li>Document Findings: Save plots and reports for future reference</li> <li>Understand Context: Consider domain knowledge when interpreting SHAP values</li> </ul>"},{"location":"user_guide.html#report-generation_1","title":"Report Generation","text":"<ul> <li>Provide Context: Add dataset URL and description for better AI insights</li> <li>Choose Appropriate Models:</li> <li>Fast models (Gemini Flash): Quick exploratory reports</li> <li>Advanced models (GPT-4, Gemini Pro): Detailed production reports</li> <li>Review Carefully: AI-generated reports should be reviewed by domain experts</li> </ul>"},{"location":"user_guide.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"user_guide.html#common-issues","title":"Common Issues","text":"<p>Dataset Loading Fails - Check UCI dataset ID is correct - Ensure CSV file is properly formatted - Verify file encoding (UTF-8 recommended)</p> <p>Optimization Errors - Ensure data has no missing values - Check target column has exactly 2 unique values - Verify sufficient samples for train/test split</p> <p>SHAP Analysis Slow - Reduce subset size - Use simpler model types - Check available memory</p> <p>Report Generation Fails - Verify API key is valid - Check internet connection - Ensure model name is correct - Try a different LLM provider</p>"},{"location":"user_guide.html#advanced-features","title":"Advanced Features","text":""},{"location":"user_guide.html#loading-previous-studies","title":"Loading Previous Studies","text":"<p>You can load and analyze previously run optimizations:</p> <ol> <li>Go to the Optimization page</li> <li>Enter the database name and study name</li> <li>Click Load Optimizer</li> <li>Results will be available for analysis</li> </ol>"},{"location":"user_guide.html#batch-processing","title":"Batch Processing","text":"<p>For multiple datasets, you can: 1. Use the Python API directly (see API documentation) 2. Script the workflow using QuOptuna classes 3. Save results to different databases</p>"},{"location":"user_guide.html#custom-models","title":"Custom Models","text":"<p>Advanced users can integrate custom models by: 1. Following the model interface in <code>quoptuna.backend.models</code> 2. Adding model configurations to the optimizer 3. See API documentation for details</p>"},{"location":"user_guide.html#next-steps","title":"Next Steps","text":"<ul> <li>Explore the API Documentation for programmatic usage</li> <li>Check out Examples for common use cases</li> <li>Contribute on GitHub</li> </ul>"},{"location":"user_guide.html#support","title":"Support","text":"<ul> <li>GitHub Issues: Report bugs or request features</li> <li>Documentation: Full documentation</li> <li>Community: Join our discussions on GitHub</li> </ul>"}]}
var __index = {"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":""},{"location":"index.html#quoptuna-documentation","title":"QuOptuna Documentation","text":"<p>Welcome to QuOptuna, a quantum-enhanced hyperparameter optimization framework that combines quantum computing with Optuna for advanced model tuning and explainable AI.</p>"},{"location":"index.html#overview","title":"Overview","text":"<p>QuOptuna provides a comprehensive platform for: - \ud83c\udfaf Automated hyperparameter optimization for quantum and classical ML models - \ud83d\udd0d SHAP-based explainable AI with rich visualizations - \ud83d\udcca UCI ML Repository integration for easy dataset access - \ud83d\udcdd AI-powered report generation for model analysis - \ud83d\udda5\ufe0f Interactive Streamlit interface for the complete workflow</p>"},{"location":"index.html#quick-start","title":"Quick Start","text":""},{"location":"index.html#installation","title":"Installation","text":"<p>Install QuOptuna using UV (recommended) or pip:</p> <pre><code># Using UV (recommended)\nuv pip install quoptuna\n\n# Using pip\npip install quoptuna\n</code></pre>"},{"location":"index.html#launch-the-application","title":"Launch the Application","text":"<p>Start the interactive Streamlit interface:</p> <pre><code>quoptuna --start\n</code></pre> <p>Or run directly with Python:</p> <pre><code>python -m quoptuna.frontend.app run\n</code></pre>"},{"location":"index.html#basic-python-usage","title":"Basic Python Usage","text":"<pre><code>from quoptuna import DataPreparation, Optimizer\nfrom quoptuna.backend.utils.data_utils.data import mock_csv_data\nimport pandas as pd\n\n# Load and prepare data\ndf = pd.read_csv(\"your_data.csv\")\ndf[\"target\"] = df[\"target\"].replace({0: -1, 1: 1})\n\n# Save data\nfile_path = mock_csv_data(df, tmp_path=\"data\", file_name=\"my_data\")\n\n# Prepare for training\ndata_prep = DataPreparation(\n    file_path=file_path,\n    x_cols=[col for col in df.columns if col != \"target\"],\n    y_col=\"target\"\n)\ndata_dict = data_prep.get_data(output_type=\"2\")\n\n# Run optimization\noptimizer = Optimizer(db_name=\"experiment\", study_name=\"trial_1\", data=data_dict)\nstudy, best_trials = optimizer.optimize(n_trials=100)\n\nprint(f\"Best F1 Score: {best_trials[0].value:.4f}\")\nprint(f\"Best Model: {best_trials[0].params['model_type']}\")\n</code></pre>"},{"location":"index.html#key-features","title":"Key Features","text":""},{"location":"index.html#hyperparameter-optimization","title":"\ud83c\udfaf Hyperparameter Optimization","text":"<p>Automated optimization using Optuna with support for: - Multiple quantum models (Data Reuploading, Circuit-Centric, Quantum Kitchen Sinks, etc.) - Classical baselines (SVC, MLP, Perceptron) - Multi-objective optimization - Parallel trial execution - Persistent storage with SQLite</p>"},{"location":"index.html#explainable-ai","title":"\ud83d\udd0d Explainable AI","text":"<p>Comprehensive SHAP analysis with multiple visualization types: - Bar Plot: Feature importance ranking - Beeswarm Plot: Feature value impact distribution - Violin Plot: SHAP value distributions - Heatmap: Instance-level feature contributions - Waterfall Plot: Individual prediction explanations - Confusion Matrix: Model performance visualization</p>"},{"location":"index.html#dataset-management","title":"\ud83d\udcca Dataset Management","text":"<ul> <li>UCI ML Repository: Direct access to 100+ datasets</li> <li>Custom Upload: Support for CSV files</li> <li>Automatic Preprocessing: Handle missing values, feature scaling</li> <li>Target Transformation: Binary classification support (-1/+1 encoding)</li> </ul>"},{"location":"index.html#ai-powered-reports","title":"\ud83d\udcdd AI-Powered Reports","text":"<p>Generate comprehensive analysis reports using: - Google Gemini - OpenAI GPT - Anthropic Claude</p> <p>Reports include performance metrics, SHAP interpretations, and governance recommendations.</p>"},{"location":"index.html#supported-models","title":"Supported Models","text":""},{"location":"index.html#quantum-models","title":"Quantum Models","text":"<ul> <li>Data Reuploading Classifier: Quantum circuit with data re-uploading</li> <li>Circuit-Centric Classifier: Parameterized quantum circuits</li> <li>Quantum Kitchen Sinks: Quantum feature maps</li> <li>Quantum Metric Learner: Metric learning with quantum circuits</li> <li>Dressed Quantum Circuit Classifier: Hybrid quantum-classical</li> </ul>"},{"location":"index.html#classical-models","title":"Classical Models","text":"<ul> <li>Support Vector Classifier (SVC): With multiple kernels</li> <li>Multi-Layer Perceptron (MLP): Neural network classifier</li> <li>Perceptron: Simple linear classifier</li> </ul>"},{"location":"index.html#documentation","title":"Documentation","text":"<ul> <li>User Guide - Complete walkthrough of the Streamlit interface</li> <li>API Reference - Detailed API documentation for Python usage</li> <li>Examples - Code examples for common use cases</li> <li>Changelog - Version history and updates</li> </ul>"},{"location":"index.html#workflow","title":"Workflow","text":"<p>QuOptuna provides a structured workflow:</p> <ol> <li>Dataset Selection</li> <li>Load from UCI ML Repository or upload CSV</li> <li>Configure features and target</li> <li> <p>Apply preprocessing</p> </li> <li> <p>Optimization</p> </li> <li>Prepare train/test splits</li> <li>Run hyperparameter optimization</li> <li> <p>Review best performing models</p> </li> <li> <p>Model Training</p> </li> <li>Select best trial</li> <li>Train model with optimized parameters</li> <li> <p>Evaluate performance</p> </li> <li> <p>SHAP Analysis</p> </li> <li>Calculate SHAP values</li> <li>Generate multiple visualization types</li> <li> <p>Understand feature importance</p> </li> <li> <p>Report Generation</p> </li> <li>Create AI-powered analysis</li> <li>Export results</li> <li>Share insights</li> </ol>"},{"location":"index.html#system-requirements","title":"System Requirements","text":"<ul> <li>Python 3.8+</li> <li>4GB+ RAM (8GB recommended for quantum models)</li> <li>Internet connection (for UCI datasets and LLM reports)</li> </ul>"},{"location":"index.html#dependencies","title":"Dependencies","text":"<p>Core dependencies: - <code>optuna</code> - Hyperparameter optimization - <code>streamlit</code> - Web interface - <code>shap</code> - Explainable AI - <code>pennylane</code> - Quantum computing - <code>scikit-learn</code> - Classical ML models - <code>pandas</code>, <code>numpy</code> - Data processing</p>"},{"location":"index.html#development","title":"Development","text":""},{"location":"index.html#setup","title":"Setup","text":"<pre><code># Clone repository\ngit clone https://github.com/Qentora/quoptuna.git\ncd quoptuna\n\n# Install development dependencies\nuv pip install -e \".[dev]\"\n\n# Run tests\nuv run pytest\n\n# Run linting\nuv run ruff check .\nuv run mypy .\n</code></pre>"},{"location":"index.html#project-structure","title":"Project Structure","text":"<pre><code>quoptuna/\n\u251c\u2500\u2500 src/quoptuna/\n\u2502   \u251c\u2500\u2500 backend/         # Core optimization and model code\n\u2502   \u2502   \u251c\u2500\u2500 models/      # Model implementations\n\u2502   \u2502   \u251c\u2500\u2500 tuners/      # Optuna integration\n\u2502   \u2502   \u251c\u2500\u2500 xai/         # SHAP analysis\n\u2502   \u2502   \u2514\u2500\u2500 utils/       # Utilities\n\u2502   \u2514\u2500\u2500 frontend/        # Streamlit interface\n\u2502       \u251c\u2500\u2500 pages/       # Multi-page app\n\u2502       \u251c\u2500\u2500 app.py       # Main application\n\u2502       \u2514\u2500\u2500 support.py   # Helper functions\n\u251c\u2500\u2500 docs/                # Documentation\n\u251c\u2500\u2500 experiments/         # Example notebooks\n\u2514\u2500\u2500 tests/              # Unit tests\n</code></pre>"},{"location":"index.html#contributing","title":"Contributing","text":"<p>We welcome contributions! Please see our Contributing Guidelines.</p>"},{"location":"index.html#ways-to-contribute","title":"Ways to Contribute","text":"<ul> <li>\ud83d\udc1b Report bugs and issues</li> <li>\ud83d\udca1 Suggest new features</li> <li>\ud83d\udcdd Improve documentation</li> <li>\ud83d\udd27 Submit pull requests</li> <li>\u2b50 Star the repository</li> </ul>"},{"location":"index.html#support-community","title":"Support &amp; Community","text":"<ul> <li>GitHub Issues: Report bugs or request features</li> <li>Discussions: Join the community</li> <li>Documentation: Full docs</li> </ul>"},{"location":"index.html#license","title":"License","text":"<p>QuOptuna is released under the MIT License. See LICENSE for details.</p>"},{"location":"index.html#citation","title":"Citation","text":"<p>If you use QuOptuna in your research, please cite:</p> <pre><code>@software{quoptuna,\n  title = {QuOptuna: Quantum-Enhanced Machine Learning Optimization},\n  author = {QuOptuna Team},\n  year = {2024},\n  url = {https://github.com/Qentora/quoptuna}\n}\n</code></pre>"},{"location":"index.html#acknowledgments","title":"Acknowledgments","text":"<p>Built with: - Optuna - Hyperparameter optimization framework - PennyLane - Quantum machine learning - SHAP - Explainable AI - Streamlit - Web framework</p> <p>Ready to get started? Check out the User Guide or launch the app with <code>quoptuna --start</code>!</p>"},{"location":"api_docs.html","title":"API documentation","text":""},{"location":"api_docs.html#api-documentation","title":"API documentation","text":""},{"location":"api_docs.html#quoptuna","title":"quoptuna","text":"MODULE DESCRIPTION <code>backend</code> <code>frontend</code> CLASS DESCRIPTION <code>DataPreparation</code> <code>Optimizer</code> <code>XAI</code>"},{"location":"api_docs.html#quoptuna.DataPreparation","title":"DataPreparation","text":"<pre><code>DataPreparation(dataset: DataSet | None = None, file_path: str | None = None, x_cols: list[str] | None = None, y_col: str | None = None, scaler=None)\n</code></pre> METHOD DESCRIPTION <code>create_dataset</code> <p>Creates a dataset from raw data.</p> <code>prepare_data</code> <p>Selects columns and preprocesses the data.</p> <code>preprocess</code> <p>Preprocess the features and target.</p> <code>read_csv</code> <p>Reads a CSV file and returns a raw dataset.</p> <code>select_columns</code> <p>Selects specified columns and splits the dataset into features and target.</p> <code>update_column_names</code> <p>Update column names in x_cols if they are single length after conversion to string.</p> Source code in <code>src/quoptuna/backend/utils/data_utils/prepare.py</code> <pre><code>def __init__(\n    self,\n    dataset: DataSet | None = None,\n    file_path: str | None = None,\n    x_cols: list[str] | None = None,\n    y_col: str | None = None,\n    scaler=None,\n):\n    self.x_cols = x_cols\n    self.y_col = y_col\n    self.scaler = scaler or StandardScaler()\n    if dataset is not None:\n        x = self.update_column_names(dataset.get(\"x\"))\n        self.set_x_cols(x.columns)\n        self.dataset = {\"x\": x, \"y\": dataset.get(\"y\")}\n    elif file_path is not None:\n        if x_cols is None or y_col is None:\n            msg = \"x_cols and y_col must be provided when file_path is used\"\n            raise ValueError(msg)\n        self.dataset = self.create_dataset(self.read_csv(file_path), x_cols, y_col)\n    else:\n        msg = \"Either dataset or file_path must be provided\"\n        raise ValueError(msg)\n    self.x_train, self.x_test, self.y_train, self.y_test = self.prepare_data()\n</code></pre>"},{"location":"api_docs.html#quoptuna.DataPreparation.create_dataset","title":"create_dataset","text":"<pre><code>create_dataset(raw_data: DataFrame, x_cols: list[str], y_col: str) -&gt; DataSet\n</code></pre> <p>Creates a dataset from raw data.</p> Source code in <code>src/quoptuna/backend/utils/data_utils/prepare.py</code> <pre><code>def create_dataset(self, raw_data: pd.DataFrame, x_cols: list[str], y_col: str) -&gt; DataSet:\n    \"\"\"Creates a dataset from raw data.\"\"\"\n    x = raw_data[x_cols]\n    y = raw_data[y_col]\n    x = self.update_column_names(x)\n    self.set_x_cols(x.columns)\n    return {\"x\": x, \"y\": y}\n</code></pre>"},{"location":"api_docs.html#quoptuna.DataPreparation.prepare_data","title":"prepare_data","text":"<pre><code>prepare_data()\n</code></pre> <p>Selects columns and preprocesses the data.</p> Source code in <code>src/quoptuna/backend/utils/data_utils/prepare.py</code> <pre><code>def prepare_data(self):\n    \"\"\"Selects columns and preprocesses the data.\"\"\"\n    if self.x_cols is None or self.y_col is None:\n        msg = \"x_cols and y_col must be provided\"\n        raise ValueError(msg)\n    x, y = self.select_columns()\n    return self.preprocess(x, y)\n</code></pre>"},{"location":"api_docs.html#quoptuna.DataPreparation.preprocess","title":"preprocess","text":"<pre><code>preprocess(x: DataFrame, y: Series, train_size: float = 0.75)\n</code></pre> <p>Preprocess the features and target.</p> Source code in <code>src/quoptuna/backend/utils/data_utils/prepare.py</code> <pre><code>def preprocess(self, x: pd.DataFrame, y: pd.Series, train_size: float = 0.75):\n    \"\"\"Preprocess the features and target.\"\"\"\n    x = pd.DataFrame(self.scaler.fit_transform(x), columns=x.columns)\n    classes = np.unique(y)\n    y = pd.DataFrame(\n        np.where(y == classes[0], 1, -1),\n        columns=[self.y_col] if not isinstance(self.y_col, list) else self.y_col,\n    )\n    return train_test_split(x, y, train_size=train_size, random_state=42)\n</code></pre>"},{"location":"api_docs.html#quoptuna.DataPreparation.read_csv","title":"read_csv  <code>staticmethod</code>","text":"<pre><code>read_csv(file_path: str) -&gt; DataFrame\n</code></pre> <p>Reads a CSV file and returns a raw dataset.</p> Source code in <code>src/quoptuna/backend/utils/data_utils/prepare.py</code> <pre><code>@staticmethod\ndef read_csv(file_path: str) -&gt; pd.DataFrame:\n    \"\"\"Reads a CSV file and returns a raw dataset.\"\"\"\n    return pd.read_csv(file_path)\n</code></pre>"},{"location":"api_docs.html#quoptuna.DataPreparation.select_columns","title":"select_columns","text":"<pre><code>select_columns()\n</code></pre> <p>Selects specified columns and splits the dataset into features and target.</p> Source code in <code>src/quoptuna/backend/utils/data_utils/prepare.py</code> <pre><code>def select_columns(self):\n    \"\"\"Selects specified columns and splits the dataset into features and target.\"\"\"\n    if self.x_cols is None or self.y_col is None:\n        msg = \"x_cols and y_col must be provided\"\n        raise ValueError(msg)\n    x = self.dataset.get(\"x\")\n    y = self.dataset.get(\"y\")\n    return x, y\n</code></pre>"},{"location":"api_docs.html#quoptuna.DataPreparation.update_column_names","title":"update_column_names","text":"<pre><code>update_column_names(dataframe: DataFrame | None = None)\n</code></pre> <p>Update column names in x_cols if they are single length after conversion to string. Also updates the corresponding DataFrame if provided.</p> Source code in <code>src/quoptuna/backend/utils/data_utils/prepare.py</code> <pre><code>def update_column_names(self, dataframe: pd.DataFrame | None = None):\n    \"\"\"Update column names in x_cols if they are single length after conversion to string.\n    Also updates the corresponding DataFrame if provided.\n    \"\"\"\n    if self.x_cols is not None:\n        for i, col in enumerate(self.x_cols):\n            if len(str(col)) == 1:\n                self.x_cols[i] = f\"feat: {col}\"\n                if dataframe is not None and col in dataframe.columns:\n                    dataframe = dataframe.rename(columns={col: f\"feat: {col}\"})\n    return dataframe\n</code></pre>"},{"location":"api_docs.html#quoptuna.Optimizer","title":"Optimizer","text":"<pre><code>Optimizer(db_name: str, dataset_name: str = '', data: Optional[dict] = None, study_name: str = '')\n</code></pre> <p>Initialize the Optimizer class.</p> PARAMETER DESCRIPTION <p>The name of the database to be used for storing optimization results.</p> <p> TYPE: <code>str</code> </p> <p>The name of the dataset.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <p>A dictionary containing training and testing data.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> <p>The name of the study for Optuna.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> ATTRIBUTE DESCRIPTION <code>db_name</code> <p>The name of the database.</p> <p> TYPE: <code>str</code> </p> <code>dataset_name</code> <p>The name of the dataset.</p> <p> TYPE: <code>str</code> </p> <code>data_path</code> <p>The path to the dataset CSV file or an empty string</p> <p> TYPE: <code>str</code> </p> <code>data</code> <p>The data dictionary containing training and testing data.</p> <p> TYPE: <code>dict</code> </p> <code>train_x</code> <p>The training features.</p> <p> TYPE: <code>Optional[ndarray]</code> </p> <code>test_x</code> <p>The testing features.</p> <p> TYPE: <code>Optional[ndarray]</code> </p> <code>train_y</code> <p>The training labels.</p> <p> TYPE: <code>Optional[ndarray]</code> </p> <code>test_y</code> <p>The testing labels.</p> <p> TYPE: <code>Optional[ndarray]</code> </p> <code>storage_location</code> <p>The storage location for the Optuna study.</p> <p> TYPE: <code>str</code> </p> <code>study_name</code> <p>The name of the Optuna study.</p> <p> TYPE: <code>str</code> </p> <code>study</code> <p>The Optuna study object.</p> <p> TYPE: <code>Optional[Study]</code> </p> Source code in <code>src/quoptuna/backend/tuners/optimizer.py</code> <pre><code>def __init__(\n    self,\n    db_name: str,\n    dataset_name: str = \"\",\n    data: Optional[dict] = None,  # noqa: FA100\n    study_name: str = \"\",\n):\n    \"\"\"\n\n    Initialize the Optimizer class.\n\n    Args:\n        db_name (str): The name of the database to be used for storing optimization results.\n        dataset_name (str, optional): The name of the dataset.\n        If provided, the data will be loaded from a CSV file located in the 'notebook'\n        directory.Defaults to an empty string.\n        data (Optional[dict], optional): A dictionary containing training and testing data.\n        If not provided, an empty dictionary will be used.\n        Expected keys are 'train_x', 'test_x', 'train_y', and 'test_y'.\n        study_name (str, optional): The name of the study for Optuna.\n        Defaults to an empty string.\n\n    Attributes:\n        db_name (str): The name of the database.\n        dataset_name (str): The name of the dataset.\n        data_path (str): The path to the dataset CSV file or an empty string\n        if no dataset name is provided.\n        data (dict): The data dictionary containing training and testing data.\n        train_x (Optional[np.ndarray]): The training features.\n        test_x (Optional[np.ndarray]): The testing features.\n        train_y (Optional[np.ndarray]): The training labels.\n        test_y (Optional[np.ndarray]): The testing labels.\n        storage_location (str): The storage location for the Optuna study.\n        study_name (str): The name of the Optuna study.\n        study (Optional[Study]): The Optuna study object.\n    \"\"\"\n    self.db_name = db_name\n    self.dataset_name = dataset_name\n    if len(self.dataset_name) &gt; 0:\n        self.data_path = f\"notebook/{self.dataset_name}.csv\"\n    else:\n        self.data_path = \"\"\n    self.data = data or {}  # Use an empty dictionary if no data is provided\n    self.train_x = self.data.get(\"train_x\")\n    self.test_x = self.data.get(\"test_x\")\n    self.train_y = self.data.get(\"train_y\")\n    self.test_y = self.data.get(\"test_y\")\n    self.data_path = f\"db/{self.db_name}.db\"\n    if not os.path.exists(\"db\"):  # noqa: PTH110\n        os.makedirs(\"db\")  # noqa: PTH103\n    self.storage_location = f\"sqlite:///{self.data_path}\"\n    self.study_name = study_name\n    self.study = None\n</code></pre>"},{"location":"api_docs.html#quoptuna.Optimizer(db_name)","title":"<code>db_name</code>","text":""},{"location":"api_docs.html#quoptuna.Optimizer(dataset_name)","title":"<code>dataset_name</code>","text":""},{"location":"api_docs.html#quoptuna.Optimizer(data)","title":"<code>data</code>","text":""},{"location":"api_docs.html#quoptuna.Optimizer(study_name)","title":"<code>study_name</code>","text":""},{"location":"api_docs.html#quoptuna.XAI","title":"XAI","text":"<pre><code>XAI(model: BaseEstimator, data: DataSet, config: XAIConfig | None = None)\n</code></pre> METHOD DESCRIPTION <code>generate_report_with_langchain</code> <p>Generate comprehensive report using LangChain and multimodal LLM.</p> <code>get_average_precision_score</code> <p>Get the average precision score of the model.</p> <code>get_classes</code> <p>Get model classes.</p> <code>get_classification_report</code> <p>Get the classification report of the model.</p> <code>get_cohens_kappa</code> <p>Get the cohens kappa of the model.</p> <code>get_confusion_matrix</code> <p>Get the confusion matrix of the model.</p> <code>get_f1_score</code> <p>Get the f1 score of the model.</p> <code>get_log_loss</code> <p>Get the log loss of the model.</p> <code>get_mcc</code> <p>Get the mcc of the model.</p> <code>get_plot</code> <p>Generate plot with given configuration.</p> <code>get_precision</code> <p>Get the precision of the model.</p> <code>get_precision_recall_curve</code> <p>Get the precision recall curve of the model.</p> <code>get_recall</code> <p>Get the recall of the model.</p> <code>get_report</code> <p>Get the report of the model.</p> <code>get_roc_auc_score</code> <p>Get the roc auc score of the model.</p> <code>get_roc_curve</code> <p>Get the roc curve of the model.</p> <code>load_state</code> <p>Loads the state of the class from a pkl file.</p> <code>plot_confusion_matrix</code> <p>Plot confusion matrix with given configuration.</p> <code>save_state</code> <p>Saves the state of the class and its variables in a pkl file.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def __init__(\n    self,\n    model: BaseEstimator,\n    data: DataSet,\n    config: XAIConfig | None = None,\n) -&gt; None:\n    if model is None:\n        msg = \"Model cannot be None\"\n        raise TypeError(msg)\n\n    self.config = config or XAIConfig()\n    self.model = model\n    self.data = data\n\n    # Explicitly declare instance attributes\n    self.use_proba: bool = self.config.use_proba\n    self.onsubset: bool = self.config.onsubset\n    self.feature_names: list[str] | None = self.config.feature_names\n    self.subset_size: int = self.config.subset_size\n    self.max_display: int = self.config.max_display\n    self.data_key: str = self.config.data_key\n    self.x_test_key: str = self.config.x_test_key\n    self.y_test_key: str = self.config.y_test_key\n\n    self._classes = self.get_classes\n    data_frame = self.data.get(self.data_key)\n    if self.feature_names is None and isinstance(data_frame, pd.DataFrame):\n        self.feature_names = list(data_frame.columns)\n\n    if self.use_proba:\n        self.validate_predict_proba()\n\n    # Initialize these as None, they'll be computed on demand\n    self._explainer: Explainer | None = None\n    self._shap_values: shap.Explanation | None = None\n    self._shap_values_each_class: dict[str, shap.Explanation] | None = None\n    self._x_test: pd.DataFrame | None = None\n    self._y_test: pd.Series | None = None\n    self._predictions: pd.Series | None = None\n    self._predictions_proba: pd.DataFrame | None = None\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.generate_report_with_langchain","title":"generate_report_with_langchain","text":"<pre><code>generate_report_with_langchain(api_key: str, model_name: str = 'gpt-4o', provider: str = 'google', num_waterfall_plots: int = 5)\n</code></pre> <p>Generate comprehensive report using LangChain and multimodal LLM.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def generate_report_with_langchain(\n    self,\n    api_key: str,\n    model_name: str = \"gpt-4o\",\n    provider: str = \"google\",\n    num_waterfall_plots: int = 5,\n):\n    \"\"\"Generate comprehensive report using LangChain and multimodal LLM.\"\"\"\n    chat = self._initialize_chat(api_key, model_name, provider)\n\n    report = self.get_report()\n    images = self._generate_report_images(num_waterfall_plots)\n\n    prompt2 = Path(\"prompt.txt\").read_text()\n    return self._generate_final_report(chat, report, images, prompt2)\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.get_average_precision_score","title":"get_average_precision_score","text":"<pre><code>get_average_precision_score()\n</code></pre> <p>Get the average precision score of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_average_precision_score(self):\n    \"\"\"Get the average precision score of the model.\"\"\"\n    return average_precision_score(self.y_test, self.predictions_proba)\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.get_classes","title":"get_classes","text":"<pre><code>get_classes() -&gt; dict[int, str]\n</code></pre> <p>Get model classes.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_classes(self) -&gt; dict[int, str]:\n    \"\"\"Get model classes.\"\"\"\n    if not hasattr(self.model, \"classes_\"):\n        msg = \"Model does not have a classes_ attribute\"\n        raise TypeError(msg)\n    return self.model.classes_\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.get_classification_report","title":"get_classification_report","text":"<pre><code>get_classification_report()\n</code></pre> <p>Get the classification report of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_classification_report(self):\n    \"\"\"Get the classification report of the model.\"\"\"\n    return classification_report(self.y_test, self.predictions)\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.get_cohens_kappa","title":"get_cohens_kappa","text":"<pre><code>get_cohens_kappa()\n</code></pre> <p>Get the cohens kappa of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_cohens_kappa(self):\n    \"\"\"Get the cohens kappa of the model.\"\"\"\n    return cohen_kappa_score(self.y_test, self.predictions)\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.get_confusion_matrix","title":"get_confusion_matrix","text":"<pre><code>get_confusion_matrix()\n</code></pre> <p>Get the confusion matrix of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_confusion_matrix(self):\n    \"\"\"Get the confusion matrix of the model.\"\"\"\n    return confusion_matrix(self.y_test, self.predictions)\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.get_f1_score","title":"get_f1_score","text":"<pre><code>get_f1_score()\n</code></pre> <p>Get the f1 score of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_f1_score(self):\n    \"\"\"Get the f1 score of the model.\"\"\"\n    return f1_score(self.y_test, self.predictions)\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.get_log_loss","title":"get_log_loss","text":"<pre><code>get_log_loss()\n</code></pre> <p>Get the log loss of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_log_loss(self):\n    \"\"\"Get the log loss of the model.\"\"\"\n    return log_loss(self.y_test, self.predictions_proba)\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.get_mcc","title":"get_mcc","text":"<pre><code>get_mcc()\n</code></pre> <p>Get the mcc of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_mcc(self):\n    \"\"\"Get the mcc of the model.\"\"\"\n    return matthews_corrcoef(self.y_test, self.predictions)\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.get_plot","title":"get_plot","text":"<pre><code>get_plot(plot_type: PlotType, max_display: int = DEFAULT_MAX_DISPLAY, class_index: int = -1, index: int = 0, save_config: dict | None = None)\n</code></pre> <p>Generate plot with given configuration.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_plot(\n    self,\n    plot_type: PlotType,\n    max_display: int = DEFAULT_MAX_DISPLAY,\n    class_index: int = -1,\n    index: int = 0,\n    save_config: dict | None = None,\n):\n    \"\"\"Generate plot with given configuration.\"\"\"\n    try:\n        values = self._get_plot_values(class_index)\n        return self._generate_plot(plot_type, values, max_display, index, save_config)\n    except (ValueError, TypeError, KeyError, RuntimeError) as e:\n        self._handle_plot_error(plot_type, e)\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.get_precision","title":"get_precision","text":"<pre><code>get_precision()\n</code></pre> <p>Get the precision of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_precision(self):\n    \"\"\"Get the precision of the model.\"\"\"\n    return precision_score(self.y_test, self.predictions)\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.get_precision_recall_curve","title":"get_precision_recall_curve","text":"<pre><code>get_precision_recall_curve()\n</code></pre> <p>Get the precision recall curve of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_precision_recall_curve(self):\n    \"\"\"Get the precision recall curve of the model.\"\"\"\n    return precision_recall_curve(self.y_test, self.predictions_proba)\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.get_recall","title":"get_recall","text":"<pre><code>get_recall()\n</code></pre> <p>Get the recall of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_recall(self):\n    \"\"\"Get the recall of the model.\"\"\"\n    return recall_score(self.y_test, self.predictions)\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.get_report","title":"get_report","text":"<pre><code>get_report()\n</code></pre> <p>Get the report of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_report(self):\n    \"\"\"Get the report of the model.\"\"\"\n    report = {}\n    metrics = {\n        \"confusion_matrix\": self.get_confusion_matrix,\n        \"classification_report\": self.get_classification_report,\n        \"roc_curve\": self.get_roc_curve,\n        \"roc_auc_score\": self.get_roc_auc_score,\n        \"precision_recall_curve\": self.get_precision_recall_curve,\n        \"average_precision_score\": self.get_average_precision_score,\n        \"f1_score\": self.get_f1_score,\n        \"mcc\": self.get_mcc,\n        \"log_loss\": self.get_log_loss,\n        \"cohens_kappa\": self.get_cohens_kappa,\n        \"precision\": self.get_precision,\n        \"recall\": self.get_recall,\n    }\n\n    try:\n        for key, func in metrics.items():\n            report[key] = func()\n    except (ValueError, TypeError) as e:\n        report[key] = str(e)\n    return report\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.get_roc_auc_score","title":"get_roc_auc_score","text":"<pre><code>get_roc_auc_score()\n</code></pre> <p>Get the roc auc score of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_roc_auc_score(self):\n    \"\"\"Get the roc auc score of the model.\"\"\"\n    return roc_auc_score(self.y_test, self.predictions_proba)\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.get_roc_curve","title":"get_roc_curve","text":"<pre><code>get_roc_curve()\n</code></pre> <p>Get the roc curve of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_roc_curve(self):\n    \"\"\"Get the roc curve of the model.\"\"\"\n    return roc_curve(self.y_test, self.predictions_proba)\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.load_state","title":"load_state  <code>classmethod</code>","text":"<pre><code>load_state(file_path: str)\n</code></pre> <p>Loads the state of the class from a pkl file. Warning: Only use this method with trusted data sources as pickle can be unsafe.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>@classmethod\ndef load_state(cls, file_path: str):\n    \"\"\"Loads the state of the class from a pkl file.\n    Warning: Only use this method with trusted data sources as pickle can be unsafe.\n    \"\"\"\n    with Path(file_path).open(\"rb\") as f:\n        return pickle.load(f)  # noqa: S301\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.plot_confusion_matrix","title":"plot_confusion_matrix","text":"<pre><code>plot_confusion_matrix(plot_config: dict | None = None)\n</code></pre> <p>Plot confusion matrix with given configuration.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def plot_confusion_matrix(self, plot_config: dict | None = None):\n    \"\"\"Plot confusion matrix with given configuration.\"\"\"\n    from sklearn.metrics import ConfusionMatrixDisplay\n\n    config = {\n        \"include_values\": True,\n        \"cmap\": \"viridis\",\n        \"xticks_rotation\": \"horizontal\",\n        \"values_format\": None,\n        \"ax\": None,\n        \"colorbar\": True,\n        \"im_kw\": None,\n        \"text_kw\": None,\n        **(plot_config or {}),\n    }\n\n    cm = self.get_confusion_matrix()\n    ConfusionMatrixDisplay(cm).plot(**config)\n\n    if plot_config and plot_config.get(\"save_path\"):\n        plt.savefig(\n            Path(plot_config[\"save_path\"]) / plot_config[\"save_name\"],\n            format=plot_config.get(\"save_format\", \"png\"),\n            dpi=plot_config.get(\"save_dpi\", 300),\n            bbox_inches=\"tight\",\n        )\n\n    return plt.gcf()\n</code></pre>"},{"location":"api_docs.html#quoptuna.XAI.save_state","title":"save_state","text":"<pre><code>save_state(file_path: str)\n</code></pre> <p>Saves the state of the class and its variables in a pkl file.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def save_state(self, file_path: str):\n    \"\"\"Saves the state of the class and its variables in a pkl file.\"\"\"\n    with Path(file_path).open(\"wb\") as f:\n        pickle.dump(self, f)\n</code></pre>"},{"location":"api_docs.html#quoptuna.backend","title":"backend","text":"MODULE DESCRIPTION <code>base</code> <code>tuners</code> <code>typing</code> <code>utils</code> <code>xai</code>"},{"location":"api_docs.html#quoptuna.backend.base","title":"base","text":"MODULE DESCRIPTION <code>pennylane_models</code>"},{"location":"api_docs.html#quoptuna.backend.base.pennylane_models","title":"pennylane_models","text":"MODULE DESCRIPTION <code>qml_benchmarks</code> CLASS DESCRIPTION <code>CircuitCentricClassifier</code> <code>ConvolutionalNeuralNetwork</code> <code>DataReuploadingClassifier</code> <code>DataReuploadingClassifierNoCost</code> <code>DataReuploadingClassifierNoScaling</code> <code>DataReuploadingClassifierNoTrainableEmbedding</code> <code>DataReuploadingClassifierSeparable</code> <code>DressedQuantumCircuitClassifier</code> <code>IQPKernelClassifier</code> <code>IQPVariationalClassifier</code> <code>ProjectedQuantumKernel</code> <code>QuantumBoltzmannMachine</code> <code>QuantumKitchenSinks</code> <code>QuantumMetricLearner</code> <code>QuanvolutionalNeuralNetwork</code> <code>SeparableKernelClassifier</code> <code>SeparableVariationalClassifier</code> <code>TreeTensorClassifier</code> <code>VanillaQNN</code> <code>WeiNet</code>"},{"location":"api_docs.html#quoptuna.backend.base.pennylane_models.CircuitCentricClassifier","title":"CircuitCentricClassifier","text":"<pre><code>CircuitCentricClassifier(n_input_copies=2, n_layers=4, convergence_interval=200, max_steps=10000, learning_rate=0.001, batch_size=32, max_vmap=None, jit=True, scaling=1.0, random_state=42, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax-jit'})\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>A variational quantum model based on https://arxiv.org/pdf/1804.03680v2.pdf.</p> <p>Uses amplitude encoding as the data embedding, but implements several copies of the initial state. The data vector :math:<code>x</code> of size :math:<code>N</code> gets padded to the next power of 2 by values :math:<code>1/len(\\tilde{x})</code> (so that the new data vector :math:<code>\\tilde{x}</code> has length :math:<code>2^n &gt; N</code>). The padded vector gets normalised to :math:<code>\\bar{\\tilde{x}}</code>, and embedded into the amplitudes of several copies of :math:<code>n</code>-qubit quantum states. Altogether, the variational circuit acts on a quantum state of amplitudes :math:<code>\\bar{\\tilde{x}} \\otimes \\bar{\\tilde{x}} \\otimes ...</code>.</p> <p>The total number of qubits :math:<code>d\\lceil \\log(n_features) \\rceil</code> of this model depends on the number of features :math:<code>N</code> and the number of copies :math:<code>d</code>.</p> <p>The variational part of the circuit uses general single qubit rotations as trainable gates. Each layer in the ansatz first applies a rotation to each qubit, and then controlled rotations connecting each qubit :math:<code>i</code> to qubit :math:<code>i+r \\text{mod } n</code>, where :math:<code>r</code> repeatedly runs through the range :math:<code>[0,...,n-1]</code>. The number of layers in the ansatz is a hyperparameter of the model.</p> <p>The result of the model is the sigma-z expectation of the first qubit, plus a trainable scalar bias. Training is via the square loss.</p> PARAMETER DESCRIPTION <code>n_input_copies</code> <p>Number of copies of the amplitude embedded state to produce.</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>n_layers</code> <p>Number of layers in the variational ansatz.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>convergence_interval</code> <p>The number of loss values to consider to decide convergence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>max_steps</code> <p>Maximum number of training steps. A warning will be raised if training did not converge.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> <code>learning_rate</code> <p>Initial learning rate for gradient descent.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> <code>batch_size</code> <p>Size of batches used for computing parameter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dev_type</code> <p>Pennylane device type; e.g. 'default.qubit.jax'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>Keyword arguments for the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax-jit'}</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>generate_key</code> <p>Generates a random key used in sampling batches.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for batch of data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>The feature vectors padded to the next power of 2 and then normalised.</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/circuit_centric.py</code> <pre><code>def __init__(\n    self,\n    n_input_copies=2,\n    n_layers=4,\n    convergence_interval=200,\n    max_steps=10000,\n    learning_rate=0.001,\n    batch_size=32,\n    max_vmap=None,\n    jit=True,\n    scaling=1.0,\n    random_state=42,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax-jit\"},\n):\n    r\"\"\"\n    A variational quantum model based on https://arxiv.org/pdf/1804.03680v2.pdf.\n\n    Uses amplitude encoding as the data embedding, but implements several\n    copies of the initial state. The data vector :math:`x` of size :math:`N`\n    gets padded to the next power of 2 by values :math:`1/len(\\tilde{x})`\n    (so that the new data vector :math:`\\tilde{x}` has length :math:`2^n &gt; N`).\n    The padded vector gets normalised to :math:`\\bar{\\tilde{x}}`, and\n    embedded into the amplitudes of several copies of :math:`n`-qubit\n    quantum states. Altogether, the variational circuit acts on a quantum\n    state of amplitudes :math:`\\bar{\\tilde{x}} \\otimes \\bar{\\tilde{x}} \\otimes ...`.\n\n    The total number of qubits :math:`d\\lceil \\log(n_features) \\rceil` of\n    this model depends on the number of features :math:`N` and the number\n    of copies :math:`d`.\n\n    The variational part of the circuit uses general single qubit rotations\n    as trainable gates. Each layer in the ansatz first applies a rotation\n    to each qubit, and then controlled rotations connecting each qubit\n    :math:`i` to qubit :math:`i+r \\text{mod } n`, where :math:`r` repeatedly runs\n    through the range :math:`[0,...,n-1]`. The number of layers in the\n    ansatz is a hyperparameter of the model.\n\n    The result of the model is the sigma-z expectation of the first qubit,\n    plus a trainable scalar bias. Training is via the square loss.\n\n    Args:\n        n_input_copies (int): Number of copies of the amplitude embedded\n            state to produce.\n        n_layers (int): Number of layers in the variational ansatz.\n        convergence_interval (int): The number of loss values to consider to decide convergence.\n        max_steps (int): Maximum number of training steps. A warning will\n            be raised if training did not converge.\n        learning_rate (float): Initial learning rate for gradient descent.\n        batch_size (int): Size of batches used for computing parameter updates.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        dev_type (str): Pennylane device type; e.g. 'default.qubit.jax'.\n        qnode_kwargs (str): Keyword arguments for the circuit qnode.\n        scaling (float): Factor by which to scale the input data.\n        random_state (int): Seed used for pseudorandom number generation.\n    \"\"\"\n\n    # attributes that do not depend on data\n    self.n_input_copies = n_input_copies\n    self.n_layers = n_layers\n    self.convergence_interval = convergence_interval\n    self.max_steps = max_steps\n    self.learning_rate = learning_rate\n    self.batch_size = batch_size\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.jit = jit\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # Dictionary containing the trainable parameters\n    self.n_qubits_ = None\n    self.scaler = None\n    self.circuit = None\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/circuit_centric.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n\n    self.initialize(n_features=X.shape[1], classes=np.unique(y))\n\n    X = self.transform(X)\n\n    optimizer = optax.adam\n\n    def loss_fn(params, X, y):\n        pred = self.forward(params, X)  # jnp.stack([self.forward(params, x) for x in X])\n        return jnp.mean(optax.l2_loss(pred, y))\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        X,\n        y,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    return self\n</code></pre> <code></code> generate_key <pre><code>generate_key() -&gt; Array\n</code></pre> <p>Generates a random key used in sampling batches.</p> RETURNS DESCRIPTION <code>Array</code> <p>jax.Array: description</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/circuit_centric.py</code> <pre><code>def generate_key(self) -&gt; jax.Array:\n    \"\"\"\n    Generates a random key used in sampling batches.\n\n    Returns:\n        jax.Array: _description_\n    \"\"\"\n    return jax.random.PRNGKey(self.rng.integers(1000000))\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels. Args:     n_features (int): Number of features that the classifier expects     classes (array-like): class labels</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/circuit_centric.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    n_qubits_per_copy = int(np.ceil(np.log2(n_features)))\n    self.n_qubits_ = self.n_input_copies * n_qubits_per_copy\n\n    self.construct_model()\n    self.initialize_params()\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for batch of data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/circuit_centric.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for batch of data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/circuit_centric.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    predictions = self.chunked_forward(self.params_, X)\n    predictions_2d = np.c_[(1 - predictions) / 2, (1 + predictions) / 2]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=False)\n</code></pre> <p>The feature vectors padded to the next power of 2 and then normalised.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/circuit_centric.py</code> <pre><code>def transform(self, X, preprocess=False):\n    \"\"\"\n    The feature vectors padded to the next power of 2 and then normalised.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    n_features = X.shape[1]\n    X = X * self.scaling\n\n    n_qubits_per_copy = int(np.ceil(np.log2(n_features)))\n    max_n_features = 2**n_qubits_per_copy\n    n_padding = max_n_features - n_features\n    padding = np.ones(shape=(len(X), n_padding)) / max_n_features\n\n    X_padded = np.c_[X, padding]\n    X_normalised = np.divide(X_padded, np.expand_dims(np.linalg.norm(X_padded, axis=1), axis=1))\n    return X_normalised\n</code></pre>"},{"location":"api_docs.html#quoptuna.backend.base.pennylane_models.ConvolutionalNeuralNetwork","title":"ConvolutionalNeuralNetwork","text":"<pre><code>ConvolutionalNeuralNetwork(kernel_shape=3, output_channels=[32, 64], learning_rate=0.001, convergence_interval=200, max_steps=10000, batch_size=32, max_vmap=None, jit=True, random_state=42, scaling=1.0)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>This implements a vanilla convolutional neural network (CNN) two-class classifier with JAX and flax (https://github.com/google/flax).</p> <p>The structure of the neural network is as follows:</p> <ul> <li>a 2D convolutional layer with self.output_channels[0] output channels</li> <li>a max pool layer</li> <li>a 2D convolutional layer with self.output_channels[1] output channels</li> <li>a max pool layer</li> <li>a two layer fully connected feedforward neural network with 2*self.output_channels[1] hidden neurons     and one output neuron</li> </ul> <p>The probability of class 1 is given by :math:<code>P(+1\\vert \\vec{w},\\vec{x}) = \\sigma(f(\\vec{w}),\\vec{x})</code> where :math:<code>\\vec{w}</code> are the weights of the network and :math:<code>\\sigma</code> is the logistic function and :math:<code>f</code> gives the value of the neuron in the final later. These probabilities are fed to binary cross entropy loss for training.</p> <p>The 2d input data should be flattened to have shape (n_samples, height*width), where height and width are the dimensions of the 2d data. The model works for square data only, i.e. height=width.</p> PARAMETER DESCRIPTION <code>kernel_shape</code> <p>the shape of the kernel used in the CNN. e.g. kernel_shape=3 uses a 3x3 filter</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>output_channels</code> <p>Two integers specifying the output sizes of the convolutional layers in the CNN. Defaults to [32, 62].</p> <p> TYPE: <code>list[int]</code> DEFAULT: <code>[32, 64]</code> </p> <code>learning_rate</code> <p>Initial learning rate for training.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> <code>max_steps</code> <p>Maximum number of training steps. A warning will be raised if training did not converge.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> <code>convergence_interval</code> <p>The number of loss values to consider to decide convergence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>batch_size</code> <p>Size of batches used for computing parameter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> METHOD DESCRIPTION <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>If scaler is initialized, transform the inputs.</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/convolutional_neural_network.py</code> <pre><code>def __init__(\n    self,\n    kernel_shape=3,\n    output_channels=[32, 64],\n    learning_rate=0.001,\n    convergence_interval=200,\n    max_steps=10000,\n    batch_size=32,\n    max_vmap=None,\n    jit=True,\n    random_state=42,\n    scaling=1.0,\n):\n    r\"\"\"\n    This implements a vanilla convolutional neural network (CNN) two-class classifier with JAX and flax\n    (https://github.com/google/flax).\n\n    The structure of the neural network is as follows:\n\n    - a 2D convolutional layer with self.output_channels[0] output channels\n    - a max pool layer\n    - a 2D convolutional layer with self.output_channels[1] output channels\n    - a max pool layer\n    - a two layer fully connected feedforward neural network with 2*self.output_channels[1] hidden neurons\n        and one output neuron\n\n\n    The probability of class 1 is given by :math:`P(+1\\vert \\vec{w},\\vec{x}) = \\sigma(f(\\vec{w}),\\vec{x})`\n    where :math:`\\vec{w}` are the weights of the network and :math:`\\sigma` is the logistic function and\n    :math:`f` gives the value of the neuron in the final later. These probabilities are fed to binary cross entropy\n    loss for training.\n\n    The 2d input data should be flattened to have shape (n_samples, height*width), where height and width are the\n    dimensions of the 2d data. The model works for square data only, i.e. height=width.\n\n    Args:\n        kernel_shape (int): the shape of the kernel used in the CNN. e.g. kernel_shape=3 uses a 3x3 filter\n        output_channels (list[int]): Two integers specifying the output sizes of the convolutional layers\n            in the CNN. Defaults to [32, 62].\n        learning_rate (float): Initial learning rate for training.\n        max_steps (int): Maximum number of training steps. A warning will be raised if training did not converge.\n        convergence_interval (int): The number of loss values to consider to decide convergence.\n        batch_size (int): Size of batches used for computing parameter updates.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        random_state (int): Seed used for pseudorandom number generation.\n        scaling (float): Factor by which to scale the input data.\n    \"\"\"\n\n    # attributes that do not depend on data\n    self.learning_rate = learning_rate\n    self.max_steps = max_steps\n    self.scaling = scaling\n    self.jit = jit\n    self.kernel_shape = kernel_shape\n    self.output_channels = output_channels\n    self.convergence_interval = convergence_interval\n    self.batch_size = batch_size\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # Dictionary containing the trainable parameters\n    self.scaler = None  # data scaler will be fitted on training data\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/convolutional_neural_network.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n\n    self.initialize(X.shape[1], classes=np.unique(y))\n\n    y = jnp.array(y, dtype=int)\n\n    # scale input data\n    self.scaler = StandardScaler()\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    def loss_fn(params, X, y):\n        y = jax.nn.relu(y)  # convert to 0,1 labels\n        vals = self.forward.apply(params, X)[:, 0]\n        loss = jnp.mean(optax.sigmoid_binary_cross_entropy(vals, y))\n        return loss\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    optimizer = optax.adam\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        X,\n        y,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels. Args:     classes (array-like): class labels that the classifier expects</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/convolutional_neural_network.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n    Args:\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    # initialise the model\n    self.cnn = construct_cnn(self.output_channels, self.kernel_shape)\n    self.forward = self.cnn\n\n    # create dummy data input to initialise the cnn\n    height = int(jnp.sqrt(n_features))\n    X0 = jnp.ones(shape=(1, height, height, 1))\n    self.initialize_params(X0)\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred</code> <p>Predicted labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/convolutional_neural_network.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred (np.ndarray): Predicted labels of shape (n_samples,)\n    \"\"\"\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/convolutional_neural_network.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    # get probabilities of y=1\n    p1 = jax.nn.sigmoid(self.forward.apply(self.params_, X)[:, 0])\n    predictions_2d = jnp.c_[1 - p1, p1]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X)\n</code></pre> <p>If scaler is initialized, transform the inputs.</p> <p>Put into NCHW format. This assumes square images. Args:     X (np.ndarray): Data of shape (n_samples, n_features)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/convolutional_neural_network.py</code> <pre><code>def transform(self, X):\n    \"\"\"\n    If scaler is initialized, transform the inputs.\n\n    Put into NCHW format. This assumes square images.\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if self.scaler is None:\n        # if the model is unfitted, initialise the scaler here\n        self.scaler = StandardScaler()\n        self.scaler.fit(X)\n\n    X = self.scaler.transform(X) * self.scaling\n\n    # reshape data to square array\n    X = jnp.array(X)\n    height = int(jnp.sqrt(X.shape[1]))\n    X = jnp.reshape(X, (X.shape[0], height, height, 1))\n\n    return X\n</code></pre>"},{"location":"api_docs.html#quoptuna.backend.base.pennylane_models.DataReuploadingClassifier","title":"DataReuploadingClassifier","text":"<pre><code>DataReuploadingClassifier(n_layers=4, observable_type='single', convergence_interval=200, max_steps=10000, learning_rate=0.05, batch_size=32, max_vmap=None, jit=True, scaling=1.0, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax-jit'}, random_state=42)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Data reuploading classifier from https://arxiv.org/abs/1907.02085. Here we code the 'multi-qubit classifier'. The model consists of layers of trainable data encoding unitaries. Each subsequent sequence of 3 elements of an input vector x are encoded into a SU(2) rotation via pennylane.Rot. The model therefore uses ceil(d/3) qubits.</p> <p>The cost function is given by eqn (23) in the plots, and includes trainable parameters for scaling input data (omegas), the class weights (alphas), and the circuit parameters (thetas). The cost function is a weighted sum of the squared differences between the ideal fidelity to the corresponding class state (here, either |0&gt; or |1&gt;) and the output state of the circuit, for a number of qubits specified by observable_weight. More specifically</p> <p>.. math::</p> <pre><code>\\ell(\\vec{\\theta},\\vec{\\omega},\\vec{\\alpha},\\vec{x}_i) =\n\\sum_{j=1}^{n_{\\text{max}}}(\\alpha_j^0F^0_j-(1-y_i))^2 + (\\alpha_j^1F^1_j-y_i)^2\n</code></pre> <p>where :math:<code>n_max</code> is given by observable_weight and :math:<code>F^0_j</code> is the fidelity of the jth output quibt to the state |0&gt;.</p> <p>When observable_weight !=1 it is not clear from the plots how to predict labels. Here we look at the average fidelities to |0&gt; and |1&gt; of the qubits up to index self.observable_weight and take the label with the largest average fidelity. For self.observable_weight=1 this reverts to their strategy in the plots.</p> <p>Where possible, we have followed the numerical examples found in the repo which accompanies the plots:</p> <p>https://github.com/AdrianPerezSalinas/universal_qlassifier/tree/354d70f940ea737192de4063ff72f859c77e5760</p> PARAMETER DESCRIPTION <code>n_layers</code> <p>Number of blocks used in the trainable embedding. The data is uploaded n_layers+1 times.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>observable_type</code> <p>Defines the number of qubits used to evaluate the weighed cost fucntion, either 'single', 'half' or 'full'.  max_steps (int): Maximum number of training steps. A warning will be raised if training did not     converge.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'single'</code> </p> <code>learning_rate</code> <p>Initial learning rate for training.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.05</code> </p> <code>convergence_interval</code> <p>The number of loss values to consider to decide convergence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>batch_size</code> <p>Size of batches used for computing paraeeter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the keyword arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax-jit'}</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>construct_model</code> <p>Construct the quantum circuit used in the model.</p> <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for batch of data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def __init__(\n    self,\n    n_layers=4,\n    observable_type=\"single\",\n    convergence_interval=200,\n    max_steps=10000,\n    learning_rate=0.05,\n    batch_size=32,\n    max_vmap=None,\n    jit=True,\n    scaling=1.0,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax-jit\"},\n    random_state=42,\n):\n    r\"\"\"\n    Data reuploading classifier from https://arxiv.org/abs/1907.02085. Here we code the 'multi-qubit classifier'.\n    The model consists of layers of trainable data encoding unitaries. Each subsequent sequence of 3 elements of\n    an input vector x are encoded into a SU(2) rotation via pennylane.Rot. The model therefore uses ceil(d/3)\n    qubits.\n\n    The cost function is given by eqn (23) in the plots, and includes trainable parameters for scaling input data\n    (omegas), the class weights (alphas), and the circuit parameters (thetas). The cost\n    function is a weighted sum of the squared differences between the ideal fidelity to the corresponding class state\n    (here, either |0&gt; or |1&gt;) and the output state of the circuit, for a number of qubits specified by\n    observable_weight. More specifically\n\n    .. math::\n\n        \\ell(\\vec{\\theta},\\vec{\\omega},\\vec{\\alpha},\\vec{x}_i) =\n        \\sum_{j=1}^{n_{\\text{max}}}(\\alpha_j^0F^0_j-(1-y_i))^2 + (\\alpha_j^1F^1_j-y_i)^2\n\n    where :math:`n_max` is given by observable_weight and :math:`F^0_j` is the fidelity of the jth output\n    quibt to the state |0&gt;.\n\n    When observable_weight !=1 it is not clear from the plots how to predict labels. Here we look at the average\n    fidelities to |0&gt; and |1&gt; of the qubits up to index self.observable_weight and take the label with the largest\n    average fidelity. For self.observable_weight=1 this reverts to their strategy in the plots.\n\n    Where possible, we have followed the numerical examples found in the repo which accompanies the plots:\n\n    https://github.com/AdrianPerezSalinas/universal_qlassifier/tree/354d70f940ea737192de4063ff72f859c77e5760\n\n    Args:\n        n_layers (int): Number of blocks used in the trainable embedding. The data is uploaded n_layers+1 times.\n        observable_type (str): Defines the number of qubits used to evaluate the weighed cost fucntion,\n            either 'single', 'half' or 'full'.\n             max_steps (int): Maximum number of training steps. A warning will be raised if training did not\n                converge.\n        learning_rate (float): Initial learning rate for training.\n        convergence_interval (int): The number of loss values to consider to decide convergence.\n        batch_size (int): Size of batches used for computing paraeeter updates.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'.\n        qnode_kwargs (str): the keyword arguments passed to the circuit qnode.\n        scaling (float): Factor by which to scale the input data.\n        random_state (int): Seed used for pseudorandom number generation.\n    \"\"\"\n\n    # attributes that do not depend on data\n    self.n_layers = n_layers\n    self.observable_type = observable_type\n    self.convergence_interval = convergence_interval\n    self.max_steps = max_steps\n    self.learning_rate = learning_rate\n    self.batch_size = batch_size\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.jit = jit\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # dictionary containing the trainable parameters\n    self.n_qubits_ = None\n    self.scaler = None  # data preprocessing\n    self.circuit = None\n</code></pre> <code></code> construct_model <pre><code>construct_model()\n</code></pre> <p>Construct the quantum circuit used in the model.</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def construct_model(self):\n    \"\"\"Construct the quantum circuit used in the model.\"\"\"\n\n    dev = qml.device(self.dev_type, wires=self.n_qubits_)\n\n    @qml.qnode(dev, **self.qnode_kwargs)\n    def circuit(params, x):\n        \"\"\"A variational quantum circuit with data reuploading.\n\n        Args:\n            params (array[float]): dictionary of parameters\n            x (array[float]): input vector\n\n        Returns:\n            list: Expectation values of Pauli Z on each qubit\n        \"\"\"\n        for layer in range(self.n_layers):\n            x_idx = 0  # to keep track of the data index\n            for i in range(self.n_qubits_):\n                # scaled inputs\n                angles = x[x_idx : x_idx + 3] * params[\"omegas\"][layer, x_idx : x_idx + 3]\n                qml.Rot(*angles, wires=i)\n\n                # variational\n                angles = params[\"thetas\"][i, layer, :]\n                qml.Rot(*angles, wires=i)\n\n                x_idx += 3\n            if layer % 2 == 0:\n                qml.broadcast(qml.CZ, range(self.n_qubits_), pattern=\"double\")\n            else:\n                qml.broadcast(qml.CZ, range(self.n_qubits_), pattern=\"double_odd\")\n\n        # final reupload without CZs\n        x_idx = 0\n        for i in range(self.n_qubits_):\n            angles = (\n                x[x_idx : x_idx + 3] * params[\"omegas\"][self.n_layers, x_idx : x_idx + 3]\n                + params[\"thetas\"][i, self.n_layers, :]\n            )\n            qml.Rot(*angles, wires=i)\n            x_idx += 3\n\n        return [qml.expval(qml.PauliZ(wires=[i])) for i in range(self.n_qubits_)]\n\n    self.circuit = circuit\n\n    def circuit_as_array(params, x):\n        # pennylane returns a list in the circuit above, so we need this\n        return jnp.array(circuit(params, x))\n\n    if self.jit:\n        circuit_as_array = jax.jit(circuit_as_array)\n    self.forward = jax.vmap(circuit_as_array, in_axes=(None, 0))\n    self.chunked_forward = chunk_vmapped_fn(self.forward, 1, self.max_vmap)\n\n    return self.forward\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n    self.initialize(X.shape[1], classes=np.unique(y))\n\n    self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    def loss_fn(params, x, y):\n        y_mat = jnp.vstack(\n            [y for __ in range(self.observable_weight)]\n        ).T  # repeated columns of labels\n        y_mat0 = (1 - y_mat) / 2\n        y_mat1 = (1 + y_mat) / 2\n        alpha_mat0 = jnp.vstack(\n            [params[\"alphas\"][0, :] for __ in range(len(x))]\n        )  # repeated rows of alpha parameters\n        alpha_mat1 = jnp.vstack([params[\"alphas\"][1, :] for __ in range(len(x))])\n\n        expvals = self.forward(params, x)\n        probs0 = (1 - expvals[:, : self.observable_weight]) / 2  # fidelity with |0&gt;\n        probs1 = (1 + expvals[:, : self.observable_weight]) / 2  # fidelity with |1&gt;\n        loss = (\n            1\n            / 2\n            * (\n                jnp.sum(\n                    (alpha_mat0 * probs0 - y_mat0) ** 2 + (alpha_mat1 * probs1 - y_mat1) ** 2\n                )\n            )\n        )  # eqn 23 in plots\n        return loss / len(y)\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    self.construct_model()\n    self.initialize_params()\n    optimizer = optax.adam\n\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        X,\n        y,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels. Args:     n_features (int): Number of features that the classifier expects     classes (array-like): class labels that the classifier expects</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_features = n_features\n    self.n_qubits_ = ceil(self.n_features / 3)\n\n    if self.observable_type == \"single\":\n        self.observable_weight = 1\n    elif self.observable_type == \"half\":\n        if self.n_qubits_ == 1:\n            self.observable_weight = 1\n        else:\n            self.observable_weight = self.n_qubits_ // 2\n    elif self.observable_type == \"full\":\n        self.observable_weight = self.n_qubits_\n\n    self.initialize_params()\n    self.construct_model()\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for batch of data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for batch of data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    check_is_fitted(self)\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    predictions = self.chunked_forward(self.params_, X)\n    predictions = jnp.mean(\n        predictions[:, : self.observable_weight], axis=1\n    )  # use mean of expvals over relevant qubits\n    predictions_2d = np.c_[(1 - predictions) / 2, (1 + predictions) / 2]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n            self.scaler.fit(X)\n\n        X = self.scaler.transform(X)\n\n    X = X * self.scaling\n    X = jnp.pad(X, ((0, 0), (0, (3 - X.shape[1]) % 3)), \"constant\")\n    return X\n</code></pre>"},{"location":"api_docs.html#quoptuna.backend.base.pennylane_models.DataReuploadingClassifierNoCost","title":"DataReuploadingClassifierNoCost","text":"<pre><code>DataReuploadingClassifierNoCost(n_layers=4, observable_type='single', convergence_interval=200, max_steps=10000, learning_rate=0.05, batch_size=32, max_vmap=None, jit=True, scaling=1.0, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax-jit'}, random_state=42)\n</code></pre> <p>               Bases: <code>DataReuploadingClassifier</code></p> <p>Data reuploading classifier from https://arxiv.org/abs/1907.02085. Here we code the 'multi-qubit classifier'. The model consists of layers of trainable data encoding unitaries. Each subsequent sequence of 3 elements of an input vector x are encoded into a SU(2) rotation via pennylane.Rot. The model therefore uses ceil(d/3) qubits.</p> <p>The cost function is given by eqn (23) in the plots, and includes trainable parameters for scaling input data (omegas), the class weights (alphas), and the circuit parameters (thetas). The cost function is a weighted sum of the squared differences between the ideal fidelity to the corresponding class state (here, either |0&gt; or |1&gt;) and the output state of the circuit, for a number of qubits specified by observable_weight. More specifically</p> <p>.. math::</p> <pre><code>\\ell(\\vec{\\theta},\\vec{\\omega},\\vec{\\alpha},\\vec{x}_i) =\n\\sum_{j=1}^{n_{\\text{max}}}(\\alpha_j^0F^0_j-(1-y_i))^2 + (\\alpha_j^1F^1_j-y_i)^2\n</code></pre> <p>where :math:<code>n_max</code> is given by observable_weight and :math:<code>F^0_j</code> is the fidelity of the jth output quibt to the state |0&gt;.</p> <p>When observable_weight !=1 it is not clear from the plots how to predict labels. Here we look at the average fidelities to |0&gt; and |1&gt; of the qubits up to index self.observable_weight and take the label with the largest average fidelity. For self.observable_weight=1 this reverts to their strategy in the plots.</p> <p>Where possible, we have followed the numerical examples found in the repo which accompanies the plots:</p> <p>https://github.com/AdrianPerezSalinas/universal_qlassifier/tree/354d70f940ea737192de4063ff72f859c77e5760</p> PARAMETER DESCRIPTION <code>n_layers</code> <p>Number of blocks used in the trainable embedding. The data is uploaded n_layers+1 times.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>observable_type</code> <p>Defines the number of qubits used to evaluate the weighed cost fucntion, either 'single', 'half' or 'full'.  max_steps (int): Maximum number of training steps. A warning will be raised if training did not     converge.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'single'</code> </p> <code>learning_rate</code> <p>Initial learning rate for training.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.05</code> </p> <code>convergence_interval</code> <p>The number of loss values to consider to decide convergence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>batch_size</code> <p>Size of batches used for computing paraeeter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the keyword arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax-jit'}</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>construct_model</code> <p>Construct the quantum circuit used in the model.</p> <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for batch of data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def __init__(\n    self,\n    n_layers=4,\n    observable_type=\"single\",\n    convergence_interval=200,\n    max_steps=10000,\n    learning_rate=0.05,\n    batch_size=32,\n    max_vmap=None,\n    jit=True,\n    scaling=1.0,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax-jit\"},\n    random_state=42,\n):\n    r\"\"\"\n    Data reuploading classifier from https://arxiv.org/abs/1907.02085. Here we code the 'multi-qubit classifier'.\n    The model consists of layers of trainable data encoding unitaries. Each subsequent sequence of 3 elements of\n    an input vector x are encoded into a SU(2) rotation via pennylane.Rot. The model therefore uses ceil(d/3)\n    qubits.\n\n    The cost function is given by eqn (23) in the plots, and includes trainable parameters for scaling input data\n    (omegas), the class weights (alphas), and the circuit parameters (thetas). The cost\n    function is a weighted sum of the squared differences between the ideal fidelity to the corresponding class state\n    (here, either |0&gt; or |1&gt;) and the output state of the circuit, for a number of qubits specified by\n    observable_weight. More specifically\n\n    .. math::\n\n        \\ell(\\vec{\\theta},\\vec{\\omega},\\vec{\\alpha},\\vec{x}_i) =\n        \\sum_{j=1}^{n_{\\text{max}}}(\\alpha_j^0F^0_j-(1-y_i))^2 + (\\alpha_j^1F^1_j-y_i)^2\n\n    where :math:`n_max` is given by observable_weight and :math:`F^0_j` is the fidelity of the jth output\n    quibt to the state |0&gt;.\n\n    When observable_weight !=1 it is not clear from the plots how to predict labels. Here we look at the average\n    fidelities to |0&gt; and |1&gt; of the qubits up to index self.observable_weight and take the label with the largest\n    average fidelity. For self.observable_weight=1 this reverts to their strategy in the plots.\n\n    Where possible, we have followed the numerical examples found in the repo which accompanies the plots:\n\n    https://github.com/AdrianPerezSalinas/universal_qlassifier/tree/354d70f940ea737192de4063ff72f859c77e5760\n\n    Args:\n        n_layers (int): Number of blocks used in the trainable embedding. The data is uploaded n_layers+1 times.\n        observable_type (str): Defines the number of qubits used to evaluate the weighed cost fucntion,\n            either 'single', 'half' or 'full'.\n             max_steps (int): Maximum number of training steps. A warning will be raised if training did not\n                converge.\n        learning_rate (float): Initial learning rate for training.\n        convergence_interval (int): The number of loss values to consider to decide convergence.\n        batch_size (int): Size of batches used for computing paraeeter updates.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'.\n        qnode_kwargs (str): the keyword arguments passed to the circuit qnode.\n        scaling (float): Factor by which to scale the input data.\n        random_state (int): Seed used for pseudorandom number generation.\n    \"\"\"\n\n    # attributes that do not depend on data\n    self.n_layers = n_layers\n    self.observable_type = observable_type\n    self.convergence_interval = convergence_interval\n    self.max_steps = max_steps\n    self.learning_rate = learning_rate\n    self.batch_size = batch_size\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.jit = jit\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # dictionary containing the trainable parameters\n    self.n_qubits_ = None\n    self.scaler = None  # data preprocessing\n    self.circuit = None\n</code></pre> <code></code> construct_model <pre><code>construct_model()\n</code></pre> <p>Construct the quantum circuit used in the model.</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def construct_model(self):\n    \"\"\"Construct the quantum circuit used in the model.\"\"\"\n\n    dev = qml.device(self.dev_type, wires=self.n_qubits_)\n\n    @qml.qnode(dev, **self.qnode_kwargs)\n    def circuit(params, x):\n        \"\"\"A variational quantum circuit with data reuploading.\n\n        Args:\n            params (array[float]): dictionary of parameters\n            x (array[float]): input vector\n\n        Returns:\n            list: Expectation values of Pauli Z on each qubit\n        \"\"\"\n        for layer in range(self.n_layers):\n            x_idx = 0  # to keep track of the data index\n            for i in range(self.n_qubits_):\n                # scaled inputs\n                angles = x[x_idx : x_idx + 3] * params[\"omegas\"][layer, x_idx : x_idx + 3]\n                qml.Rot(*angles, wires=i)\n\n                # variational\n                angles = params[\"thetas\"][i, layer, :]\n                qml.Rot(*angles, wires=i)\n\n                x_idx += 3\n            if layer % 2 == 0:\n                qml.broadcast(qml.CZ, range(self.n_qubits_), pattern=\"double\")\n            else:\n                qml.broadcast(qml.CZ, range(self.n_qubits_), pattern=\"double_odd\")\n\n        # final reupload without CZs\n        x_idx = 0\n        for i in range(self.n_qubits_):\n            angles = (\n                x[x_idx : x_idx + 3] * params[\"omegas\"][self.n_layers, x_idx : x_idx + 3]\n                + params[\"thetas\"][i, self.n_layers, :]\n            )\n            qml.Rot(*angles, wires=i)\n            x_idx += 3\n\n        return [qml.expval(qml.PauliZ(wires=[i])) for i in range(self.n_qubits_)]\n\n    self.circuit = circuit\n\n    def circuit_as_array(params, x):\n        # pennylane returns a list in the circuit above, so we need this\n        return jnp.array(circuit(params, x))\n\n    if self.jit:\n        circuit_as_array = jax.jit(circuit_as_array)\n    self.forward = jax.vmap(circuit_as_array, in_axes=(None, 0))\n    self.chunked_forward = chunk_vmapped_fn(self.forward, 1, self.max_vmap)\n\n    return self.forward\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n    self.initialize(X.shape[1], classes=np.unique(y))\n\n    self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    def loss_fn(params, X, y):\n        expvals = jnp.sum(self.forward(params, X), axis=1)\n        probs = (1 - expvals * y) / 2  # the probs of incorrect classification\n        return jnp.mean(probs)\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    self.construct_model()\n    self.initialize_params()\n    optimizer = optax.adam\n\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        X,\n        y,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels. Args:     n_features (int): Number of features that the classifier expects     classes (array-like): class labels that the classifier expects</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_features = n_features\n    self.n_qubits_ = ceil(self.n_features / 3)\n\n    if self.observable_type == \"single\":\n        self.observable_weight = 1\n    elif self.observable_type == \"half\":\n        if self.n_qubits_ == 1:\n            self.observable_weight = 1\n        else:\n            self.observable_weight = self.n_qubits_ // 2\n    elif self.observable_type == \"full\":\n        self.observable_weight = self.n_qubits_\n\n    self.initialize_params()\n    self.construct_model()\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for batch of data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for batch of data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    check_is_fitted(self)\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    predictions = self.chunked_forward(self.params_, X)\n    predictions = jnp.mean(\n        predictions[:, : self.observable_weight], axis=1\n    )  # use mean of expvals over relevant qubits\n    predictions_2d = np.c_[(1 - predictions) / 2, (1 + predictions) / 2]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n            self.scaler.fit(X)\n\n        X = self.scaler.transform(X)\n\n    X = X * self.scaling\n    X = jnp.pad(X, ((0, 0), (0, (3 - X.shape[1]) % 3)), \"constant\")\n    return X\n</code></pre>"},{"location":"api_docs.html#quoptuna.backend.base.pennylane_models.DataReuploadingClassifierNoScaling","title":"DataReuploadingClassifierNoScaling","text":"<pre><code>DataReuploadingClassifierNoScaling(n_layers=4, observable_type='single', convergence_interval=200, max_steps=10000, learning_rate=0.05, batch_size=32, max_vmap=None, jit=True, scaling=1.0, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax-jit'}, random_state=42)\n</code></pre> <p>               Bases: <code>DataReuploadingClassifier</code></p> <p>Data reuploading classifier from https://arxiv.org/abs/1907.02085. Here we code the 'multi-qubit classifier'. The model consists of layers of trainable data encoding unitaries. Each subsequent sequence of 3 elements of an input vector x are encoded into a SU(2) rotation via pennylane.Rot. The model therefore uses ceil(d/3) qubits.</p> <p>The cost function is given by eqn (23) in the plots, and includes trainable parameters for scaling input data (omegas), the class weights (alphas), and the circuit parameters (thetas). The cost function is a weighted sum of the squared differences between the ideal fidelity to the corresponding class state (here, either |0&gt; or |1&gt;) and the output state of the circuit, for a number of qubits specified by observable_weight. More specifically</p> <p>.. math::</p> <pre><code>\\ell(\\vec{\\theta},\\vec{\\omega},\\vec{\\alpha},\\vec{x}_i) =\n\\sum_{j=1}^{n_{\\text{max}}}(\\alpha_j^0F^0_j-(1-y_i))^2 + (\\alpha_j^1F^1_j-y_i)^2\n</code></pre> <p>where :math:<code>n_max</code> is given by observable_weight and :math:<code>F^0_j</code> is the fidelity of the jth output quibt to the state |0&gt;.</p> <p>When observable_weight !=1 it is not clear from the plots how to predict labels. Here we look at the average fidelities to |0&gt; and |1&gt; of the qubits up to index self.observable_weight and take the label with the largest average fidelity. For self.observable_weight=1 this reverts to their strategy in the plots.</p> <p>Where possible, we have followed the numerical examples found in the repo which accompanies the plots:</p> <p>https://github.com/AdrianPerezSalinas/universal_qlassifier/tree/354d70f940ea737192de4063ff72f859c77e5760</p> PARAMETER DESCRIPTION <code>n_layers</code> <p>Number of blocks used in the trainable embedding. The data is uploaded n_layers+1 times.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>observable_type</code> <p>Defines the number of qubits used to evaluate the weighed cost fucntion, either 'single', 'half' or 'full'.  max_steps (int): Maximum number of training steps. A warning will be raised if training did not     converge.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'single'</code> </p> <code>learning_rate</code> <p>Initial learning rate for training.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.05</code> </p> <code>convergence_interval</code> <p>The number of loss values to consider to decide convergence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>batch_size</code> <p>Size of batches used for computing paraeeter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the keyword arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax-jit'}</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>construct_model</code> <p>Construct the quantum circuit used in the model.</p> <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for batch of data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def __init__(\n    self,\n    n_layers=4,\n    observable_type=\"single\",\n    convergence_interval=200,\n    max_steps=10000,\n    learning_rate=0.05,\n    batch_size=32,\n    max_vmap=None,\n    jit=True,\n    scaling=1.0,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax-jit\"},\n    random_state=42,\n):\n    r\"\"\"\n    Data reuploading classifier from https://arxiv.org/abs/1907.02085. Here we code the 'multi-qubit classifier'.\n    The model consists of layers of trainable data encoding unitaries. Each subsequent sequence of 3 elements of\n    an input vector x are encoded into a SU(2) rotation via pennylane.Rot. The model therefore uses ceil(d/3)\n    qubits.\n\n    The cost function is given by eqn (23) in the plots, and includes trainable parameters for scaling input data\n    (omegas), the class weights (alphas), and the circuit parameters (thetas). The cost\n    function is a weighted sum of the squared differences between the ideal fidelity to the corresponding class state\n    (here, either |0&gt; or |1&gt;) and the output state of the circuit, for a number of qubits specified by\n    observable_weight. More specifically\n\n    .. math::\n\n        \\ell(\\vec{\\theta},\\vec{\\omega},\\vec{\\alpha},\\vec{x}_i) =\n        \\sum_{j=1}^{n_{\\text{max}}}(\\alpha_j^0F^0_j-(1-y_i))^2 + (\\alpha_j^1F^1_j-y_i)^2\n\n    where :math:`n_max` is given by observable_weight and :math:`F^0_j` is the fidelity of the jth output\n    quibt to the state |0&gt;.\n\n    When observable_weight !=1 it is not clear from the plots how to predict labels. Here we look at the average\n    fidelities to |0&gt; and |1&gt; of the qubits up to index self.observable_weight and take the label with the largest\n    average fidelity. For self.observable_weight=1 this reverts to their strategy in the plots.\n\n    Where possible, we have followed the numerical examples found in the repo which accompanies the plots:\n\n    https://github.com/AdrianPerezSalinas/universal_qlassifier/tree/354d70f940ea737192de4063ff72f859c77e5760\n\n    Args:\n        n_layers (int): Number of blocks used in the trainable embedding. The data is uploaded n_layers+1 times.\n        observable_type (str): Defines the number of qubits used to evaluate the weighed cost fucntion,\n            either 'single', 'half' or 'full'.\n             max_steps (int): Maximum number of training steps. A warning will be raised if training did not\n                converge.\n        learning_rate (float): Initial learning rate for training.\n        convergence_interval (int): The number of loss values to consider to decide convergence.\n        batch_size (int): Size of batches used for computing paraeeter updates.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'.\n        qnode_kwargs (str): the keyword arguments passed to the circuit qnode.\n        scaling (float): Factor by which to scale the input data.\n        random_state (int): Seed used for pseudorandom number generation.\n    \"\"\"\n\n    # attributes that do not depend on data\n    self.n_layers = n_layers\n    self.observable_type = observable_type\n    self.convergence_interval = convergence_interval\n    self.max_steps = max_steps\n    self.learning_rate = learning_rate\n    self.batch_size = batch_size\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.jit = jit\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # dictionary containing the trainable parameters\n    self.n_qubits_ = None\n    self.scaler = None  # data preprocessing\n    self.circuit = None\n</code></pre> <code></code> construct_model <pre><code>construct_model()\n</code></pre> <p>Construct the quantum circuit used in the model.</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def construct_model(self):\n    \"\"\"Construct the quantum circuit used in the model.\"\"\"\n\n    dev = qml.device(self.dev_type, wires=self.n_qubits_)\n\n    @qml.qnode(dev, **self.qnode_kwargs)\n    def circuit(params, x):\n        \"\"\"A variational quantum circuit with data reuploading.\n\n        Args:\n            params (array[float]): dictionary of parameters\n            x (array[float]): input vector\n\n        Returns:\n            list: Expectation values of Pauli Z on each qubit\n        \"\"\"\n        for layer in range(self.n_layers):\n            x_idx = 0  # to keep track of the data index\n            for i in range(self.n_qubits_):\n                angles = x[x_idx : x_idx + 3] + params[\"thetas\"][i, layer, :]\n                qml.Rot(*angles, wires=i)\n                x_idx += 3\n            if layer % 2 == 0:\n                qml.broadcast(qml.CZ, range(self.n_qubits_), pattern=\"double\")\n            else:\n                qml.broadcast(qml.CZ, range(self.n_qubits_), pattern=\"double_odd\")\n\n        # final reupload without CZs\n        x_idx = 0\n        for i in range(self.n_qubits_):\n            angles = x[x_idx : x_idx + 3] + params[\"thetas\"][i, self.n_layers, :]\n            qml.Rot(*angles, wires=i)\n            x_idx += 3\n\n        return [qml.expval(qml.PauliZ(wires=[i])) for i in range(self.n_qubits_)]\n\n    self.circuit = circuit\n\n    def circuit_as_array(params, x):\n        # pennylane returns a list in the circuit above, so we need this\n        return jnp.array(circuit(params, x))\n\n    if self.jit:\n        circuit_as_array = jax.jit(circuit_as_array)\n    self.forward = jax.vmap(circuit_as_array, in_axes=(None, 0))\n    self.chunked_forward = chunk_vmapped_fn(self.forward, 1, self.max_vmap)\n\n    return self.forward\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n    self.initialize(X.shape[1], classes=np.unique(y))\n\n    self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    def loss_fn(params, x, y):\n        y_mat = jnp.vstack(\n            [y for __ in range(self.observable_weight)]\n        ).T  # repeated columns of labels\n        y_mat0 = (1 - y_mat) / 2\n        y_mat1 = (1 + y_mat) / 2\n        alpha_mat0 = jnp.vstack(\n            [params[\"alphas\"][0, :] for __ in range(len(x))]\n        )  # repeated rows of alpha parameters\n        alpha_mat1 = jnp.vstack([params[\"alphas\"][1, :] for __ in range(len(x))])\n\n        expvals = self.forward(params, x)\n        probs0 = (1 - expvals[:, : self.observable_weight]) / 2  # fidelity with |0&gt;\n        probs1 = (1 + expvals[:, : self.observable_weight]) / 2  # fidelity with |1&gt;\n        loss = (\n            1\n            / 2\n            * (\n                jnp.sum(\n                    (alpha_mat0 * probs0 - y_mat0) ** 2 + (alpha_mat1 * probs1 - y_mat1) ** 2\n                )\n            )\n        )  # eqn 23 in plots\n        return loss / len(y)\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    self.construct_model()\n    self.initialize_params()\n    optimizer = optax.adam\n\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        X,\n        y,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels. Args:     n_features (int): Number of features that the classifier expects     classes (array-like): class labels that the classifier expects</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_features = n_features\n    self.n_qubits_ = ceil(self.n_features / 3)\n\n    if self.observable_type == \"single\":\n        self.observable_weight = 1\n    elif self.observable_type == \"half\":\n        if self.n_qubits_ == 1:\n            self.observable_weight = 1\n        else:\n            self.observable_weight = self.n_qubits_ // 2\n    elif self.observable_type == \"full\":\n        self.observable_weight = self.n_qubits_\n\n    self.initialize_params()\n    self.construct_model()\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for batch of data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for batch of data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    check_is_fitted(self)\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    predictions = self.chunked_forward(self.params_, X)\n    predictions = jnp.mean(\n        predictions[:, : self.observable_weight], axis=1\n    )  # use mean of expvals over relevant qubits\n    predictions_2d = np.c_[(1 - predictions) / 2, (1 + predictions) / 2]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n            self.scaler.fit(X)\n\n        X = self.scaler.transform(X)\n\n    X = X * self.scaling\n    X = jnp.pad(X, ((0, 0), (0, (3 - X.shape[1]) % 3)), \"constant\")\n    return X\n</code></pre>"},{"location":"api_docs.html#quoptuna.backend.base.pennylane_models.DataReuploadingClassifierNoTrainableEmbedding","title":"DataReuploadingClassifierNoTrainableEmbedding","text":"<pre><code>DataReuploadingClassifierNoTrainableEmbedding(n_layers=4, observable_type='single', convergence_interval=200, max_steps=10000, learning_rate=0.05, batch_size=32, max_vmap=None, jit=True, scaling=1.0, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax-jit'}, random_state=42)\n</code></pre> <p>               Bases: <code>DataReuploadingClassifier</code></p> <p>Data reuploading classifier from https://arxiv.org/abs/1907.02085. Here we code the 'multi-qubit classifier'. The model consists of layers of trainable data encoding unitaries. Each subsequent sequence of 3 elements of an input vector x are encoded into a SU(2) rotation via pennylane.Rot. The model therefore uses ceil(d/3) qubits.</p> <p>The cost function is given by eqn (23) in the plots, and includes trainable parameters for scaling input data (omegas), the class weights (alphas), and the circuit parameters (thetas). The cost function is a weighted sum of the squared differences between the ideal fidelity to the corresponding class state (here, either |0&gt; or |1&gt;) and the output state of the circuit, for a number of qubits specified by observable_weight. More specifically</p> <p>.. math::</p> <pre><code>\\ell(\\vec{\\theta},\\vec{\\omega},\\vec{\\alpha},\\vec{x}_i) =\n\\sum_{j=1}^{n_{\\text{max}}}(\\alpha_j^0F^0_j-(1-y_i))^2 + (\\alpha_j^1F^1_j-y_i)^2\n</code></pre> <p>where :math:<code>n_max</code> is given by observable_weight and :math:<code>F^0_j</code> is the fidelity of the jth output quibt to the state |0&gt;.</p> <p>When observable_weight !=1 it is not clear from the plots how to predict labels. Here we look at the average fidelities to |0&gt; and |1&gt; of the qubits up to index self.observable_weight and take the label with the largest average fidelity. For self.observable_weight=1 this reverts to their strategy in the plots.</p> <p>Where possible, we have followed the numerical examples found in the repo which accompanies the plots:</p> <p>https://github.com/AdrianPerezSalinas/universal_qlassifier/tree/354d70f940ea737192de4063ff72f859c77e5760</p> PARAMETER DESCRIPTION <code>n_layers</code> <p>Number of blocks used in the trainable embedding. The data is uploaded n_layers+1 times.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>observable_type</code> <p>Defines the number of qubits used to evaluate the weighed cost fucntion, either 'single', 'half' or 'full'.  max_steps (int): Maximum number of training steps. A warning will be raised if training did not     converge.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'single'</code> </p> <code>learning_rate</code> <p>Initial learning rate for training.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.05</code> </p> <code>convergence_interval</code> <p>The number of loss values to consider to decide convergence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>batch_size</code> <p>Size of batches used for computing paraeeter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the keyword arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax-jit'}</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>construct_model</code> <p>Construct the quantum circuit used in the model.</p> <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for batch of data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def __init__(\n    self,\n    n_layers=4,\n    observable_type=\"single\",\n    convergence_interval=200,\n    max_steps=10000,\n    learning_rate=0.05,\n    batch_size=32,\n    max_vmap=None,\n    jit=True,\n    scaling=1.0,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax-jit\"},\n    random_state=42,\n):\n    r\"\"\"\n    Data reuploading classifier from https://arxiv.org/abs/1907.02085. Here we code the 'multi-qubit classifier'.\n    The model consists of layers of trainable data encoding unitaries. Each subsequent sequence of 3 elements of\n    an input vector x are encoded into a SU(2) rotation via pennylane.Rot. The model therefore uses ceil(d/3)\n    qubits.\n\n    The cost function is given by eqn (23) in the plots, and includes trainable parameters for scaling input data\n    (omegas), the class weights (alphas), and the circuit parameters (thetas). The cost\n    function is a weighted sum of the squared differences between the ideal fidelity to the corresponding class state\n    (here, either |0&gt; or |1&gt;) and the output state of the circuit, for a number of qubits specified by\n    observable_weight. More specifically\n\n    .. math::\n\n        \\ell(\\vec{\\theta},\\vec{\\omega},\\vec{\\alpha},\\vec{x}_i) =\n        \\sum_{j=1}^{n_{\\text{max}}}(\\alpha_j^0F^0_j-(1-y_i))^2 + (\\alpha_j^1F^1_j-y_i)^2\n\n    where :math:`n_max` is given by observable_weight and :math:`F^0_j` is the fidelity of the jth output\n    quibt to the state |0&gt;.\n\n    When observable_weight !=1 it is not clear from the plots how to predict labels. Here we look at the average\n    fidelities to |0&gt; and |1&gt; of the qubits up to index self.observable_weight and take the label with the largest\n    average fidelity. For self.observable_weight=1 this reverts to their strategy in the plots.\n\n    Where possible, we have followed the numerical examples found in the repo which accompanies the plots:\n\n    https://github.com/AdrianPerezSalinas/universal_qlassifier/tree/354d70f940ea737192de4063ff72f859c77e5760\n\n    Args:\n        n_layers (int): Number of blocks used in the trainable embedding. The data is uploaded n_layers+1 times.\n        observable_type (str): Defines the number of qubits used to evaluate the weighed cost fucntion,\n            either 'single', 'half' or 'full'.\n             max_steps (int): Maximum number of training steps. A warning will be raised if training did not\n                converge.\n        learning_rate (float): Initial learning rate for training.\n        convergence_interval (int): The number of loss values to consider to decide convergence.\n        batch_size (int): Size of batches used for computing paraeeter updates.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'.\n        qnode_kwargs (str): the keyword arguments passed to the circuit qnode.\n        scaling (float): Factor by which to scale the input data.\n        random_state (int): Seed used for pseudorandom number generation.\n    \"\"\"\n\n    # attributes that do not depend on data\n    self.n_layers = n_layers\n    self.observable_type = observable_type\n    self.convergence_interval = convergence_interval\n    self.max_steps = max_steps\n    self.learning_rate = learning_rate\n    self.batch_size = batch_size\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.jit = jit\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # dictionary containing the trainable parameters\n    self.n_qubits_ = None\n    self.scaler = None  # data preprocessing\n    self.circuit = None\n</code></pre> <code></code> construct_model <pre><code>construct_model()\n</code></pre> <p>Construct the quantum circuit used in the model.</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def construct_model(self):\n    \"\"\"Construct the quantum circuit used in the model.\"\"\"\n\n    dev = qml.device(self.dev_type, wires=self.n_qubits_)\n\n    @qml.qnode(dev, **self.qnode_kwargs)\n    def circuit(params, x):\n        \"\"\"A variational quantum circuit with data reuploading.\n\n        Args:\n            params (array[float]): dictionary of parameters\n            x (array[float]): input vector\n\n        Returns:\n            list: Expectation values of Pauli Z on each qubit\n        \"\"\"\n        for layer in range(self.n_layers):\n            x_idx = 0  # to keep track of the data index\n            for i in range(self.n_qubits_):\n                angles = x[x_idx : x_idx + 3] * params[\"omegas\"][layer, x_idx : x_idx + 3]\n                qml.Rot(*angles, wires=i)\n                x_idx += 3\n\n        # final reupload without CZs\n        x_idx = 0\n        for i in range(self.n_qubits_):\n            angles = x[x_idx : x_idx + 3] * params[\"omegas\"][self.n_layers, x_idx : x_idx + 3]\n            qml.Rot(*angles, wires=i)\n            x_idx += 3\n\n        for layer in range(self.n_layers):\n            x_idx = 0  # to keep track of the data index\n            for i in range(self.n_qubits_):\n                angles = params[\"thetas\"][i, layer, :]\n                qml.Rot(*angles, wires=i)\n                x_idx += 3\n            if layer % 2 == 0:\n                qml.broadcast(qml.CZ, range(self.n_qubits_), pattern=\"double\")\n            else:\n                qml.broadcast(qml.CZ, range(self.n_qubits_), pattern=\"double_odd\")\n\n            # final reupload without CZs\n        x_idx = 0\n        for i in range(self.n_qubits_):\n            angles = params[\"thetas\"][i, self.n_layers, :]\n            qml.Rot(*angles, wires=i)\n            x_idx += 3\n\n        return [qml.expval(qml.PauliZ(wires=[i])) for i in range(self.n_qubits_)]\n\n    self.circuit = circuit\n\n    def circuit_as_array(params, x):\n        # pennylane returns a list in the circuit above, so we need this\n        return jnp.array(circuit(params, x))\n\n    if self.jit:\n        circuit_as_array = jax.jit(circuit_as_array)\n    self.forward = jax.vmap(circuit_as_array, in_axes=(None, 0))\n    self.chunked_forward = chunk_vmapped_fn(self.forward, 1, self.max_vmap)\n\n    return self.forward\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n    self.initialize(X.shape[1], classes=np.unique(y))\n\n    self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    def loss_fn(params, x, y):\n        y_mat = jnp.vstack(\n            [y for __ in range(self.observable_weight)]\n        ).T  # repeated columns of labels\n        y_mat0 = (1 - y_mat) / 2\n        y_mat1 = (1 + y_mat) / 2\n        alpha_mat0 = jnp.vstack(\n            [params[\"alphas\"][0, :] for __ in range(len(x))]\n        )  # repeated rows of alpha parameters\n        alpha_mat1 = jnp.vstack([params[\"alphas\"][1, :] for __ in range(len(x))])\n\n        expvals = self.forward(params, x)\n        probs0 = (1 - expvals[:, : self.observable_weight]) / 2  # fidelity with |0&gt;\n        probs1 = (1 + expvals[:, : self.observable_weight]) / 2  # fidelity with |1&gt;\n        loss = (\n            1\n            / 2\n            * (\n                jnp.sum(\n                    (alpha_mat0 * probs0 - y_mat0) ** 2 + (alpha_mat1 * probs1 - y_mat1) ** 2\n                )\n            )\n        )  # eqn 23 in plots\n        return loss / len(y)\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    self.construct_model()\n    self.initialize_params()\n    optimizer = optax.adam\n\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        X,\n        y,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels. Args:     n_features (int): Number of features that the classifier expects     classes (array-like): class labels that the classifier expects</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_features = n_features\n    self.n_qubits_ = ceil(self.n_features / 3)\n\n    if self.observable_type == \"single\":\n        self.observable_weight = 1\n    elif self.observable_type == \"half\":\n        if self.n_qubits_ == 1:\n            self.observable_weight = 1\n        else:\n            self.observable_weight = self.n_qubits_ // 2\n    elif self.observable_type == \"full\":\n        self.observable_weight = self.n_qubits_\n\n    self.initialize_params()\n    self.construct_model()\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for batch of data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for batch of data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    check_is_fitted(self)\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    predictions = self.chunked_forward(self.params_, X)\n    predictions = jnp.mean(\n        predictions[:, : self.observable_weight], axis=1\n    )  # use mean of expvals over relevant qubits\n    predictions_2d = np.c_[(1 - predictions) / 2, (1 + predictions) / 2]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n            self.scaler.fit(X)\n\n        X = self.scaler.transform(X)\n\n    X = X * self.scaling\n    X = jnp.pad(X, ((0, 0), (0, (3 - X.shape[1]) % 3)), \"constant\")\n    return X\n</code></pre>"},{"location":"api_docs.html#quoptuna.backend.base.pennylane_models.DataReuploadingClassifierSeparable","title":"DataReuploadingClassifierSeparable","text":"<pre><code>DataReuploadingClassifierSeparable(n_layers=4, observable_type='single', convergence_interval=200, max_steps=10000, learning_rate=0.05, batch_size=32, max_vmap=None, jit=True, scaling=1.0, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax-jit'}, random_state=42)\n</code></pre> <p>               Bases: <code>DataReuploadingClassifier</code></p> <p>Data reuploading classifier from https://arxiv.org/abs/1907.02085. Here we code the 'multi-qubit classifier'. The model consists of layers of trainable data encoding unitaries. Each subsequent sequence of 3 elements of an input vector x are encoded into a SU(2) rotation via pennylane.Rot. The model therefore uses ceil(d/3) qubits.</p> <p>The cost function is given by eqn (23) in the plots, and includes trainable parameters for scaling input data (omegas), the class weights (alphas), and the circuit parameters (thetas). The cost function is a weighted sum of the squared differences between the ideal fidelity to the corresponding class state (here, either |0&gt; or |1&gt;) and the output state of the circuit, for a number of qubits specified by observable_weight. More specifically</p> <p>.. math::</p> <pre><code>\\ell(\\vec{\\theta},\\vec{\\omega},\\vec{\\alpha},\\vec{x}_i) =\n\\sum_{j=1}^{n_{\\text{max}}}(\\alpha_j^0F^0_j-(1-y_i))^2 + (\\alpha_j^1F^1_j-y_i)^2\n</code></pre> <p>where :math:<code>n_max</code> is given by observable_weight and :math:<code>F^0_j</code> is the fidelity of the jth output quibt to the state |0&gt;.</p> <p>When observable_weight !=1 it is not clear from the plots how to predict labels. Here we look at the average fidelities to |0&gt; and |1&gt; of the qubits up to index self.observable_weight and take the label with the largest average fidelity. For self.observable_weight=1 this reverts to their strategy in the plots.</p> <p>Where possible, we have followed the numerical examples found in the repo which accompanies the plots:</p> <p>https://github.com/AdrianPerezSalinas/universal_qlassifier/tree/354d70f940ea737192de4063ff72f859c77e5760</p> PARAMETER DESCRIPTION <code>n_layers</code> <p>Number of blocks used in the trainable embedding. The data is uploaded n_layers+1 times.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>observable_type</code> <p>Defines the number of qubits used to evaluate the weighed cost fucntion, either 'single', 'half' or 'full'.  max_steps (int): Maximum number of training steps. A warning will be raised if training did not     converge.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'single'</code> </p> <code>learning_rate</code> <p>Initial learning rate for training.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.05</code> </p> <code>convergence_interval</code> <p>The number of loss values to consider to decide convergence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>batch_size</code> <p>Size of batches used for computing paraeeter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the keyword arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax-jit'}</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>construct_model</code> <p>Construct the quantum circuit used in the model.</p> <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for batch of data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def __init__(\n    self,\n    n_layers=4,\n    observable_type=\"single\",\n    convergence_interval=200,\n    max_steps=10000,\n    learning_rate=0.05,\n    batch_size=32,\n    max_vmap=None,\n    jit=True,\n    scaling=1.0,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax-jit\"},\n    random_state=42,\n):\n    r\"\"\"\n    Data reuploading classifier from https://arxiv.org/abs/1907.02085. Here we code the 'multi-qubit classifier'.\n    The model consists of layers of trainable data encoding unitaries. Each subsequent sequence of 3 elements of\n    an input vector x are encoded into a SU(2) rotation via pennylane.Rot. The model therefore uses ceil(d/3)\n    qubits.\n\n    The cost function is given by eqn (23) in the plots, and includes trainable parameters for scaling input data\n    (omegas), the class weights (alphas), and the circuit parameters (thetas). The cost\n    function is a weighted sum of the squared differences between the ideal fidelity to the corresponding class state\n    (here, either |0&gt; or |1&gt;) and the output state of the circuit, for a number of qubits specified by\n    observable_weight. More specifically\n\n    .. math::\n\n        \\ell(\\vec{\\theta},\\vec{\\omega},\\vec{\\alpha},\\vec{x}_i) =\n        \\sum_{j=1}^{n_{\\text{max}}}(\\alpha_j^0F^0_j-(1-y_i))^2 + (\\alpha_j^1F^1_j-y_i)^2\n\n    where :math:`n_max` is given by observable_weight and :math:`F^0_j` is the fidelity of the jth output\n    quibt to the state |0&gt;.\n\n    When observable_weight !=1 it is not clear from the plots how to predict labels. Here we look at the average\n    fidelities to |0&gt; and |1&gt; of the qubits up to index self.observable_weight and take the label with the largest\n    average fidelity. For self.observable_weight=1 this reverts to their strategy in the plots.\n\n    Where possible, we have followed the numerical examples found in the repo which accompanies the plots:\n\n    https://github.com/AdrianPerezSalinas/universal_qlassifier/tree/354d70f940ea737192de4063ff72f859c77e5760\n\n    Args:\n        n_layers (int): Number of blocks used in the trainable embedding. The data is uploaded n_layers+1 times.\n        observable_type (str): Defines the number of qubits used to evaluate the weighed cost fucntion,\n            either 'single', 'half' or 'full'.\n             max_steps (int): Maximum number of training steps. A warning will be raised if training did not\n                converge.\n        learning_rate (float): Initial learning rate for training.\n        convergence_interval (int): The number of loss values to consider to decide convergence.\n        batch_size (int): Size of batches used for computing paraeeter updates.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'.\n        qnode_kwargs (str): the keyword arguments passed to the circuit qnode.\n        scaling (float): Factor by which to scale the input data.\n        random_state (int): Seed used for pseudorandom number generation.\n    \"\"\"\n\n    # attributes that do not depend on data\n    self.n_layers = n_layers\n    self.observable_type = observable_type\n    self.convergence_interval = convergence_interval\n    self.max_steps = max_steps\n    self.learning_rate = learning_rate\n    self.batch_size = batch_size\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.jit = jit\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # dictionary containing the trainable parameters\n    self.n_qubits_ = None\n    self.scaler = None  # data preprocessing\n    self.circuit = None\n</code></pre> <code></code> construct_model <pre><code>construct_model()\n</code></pre> <p>Construct the quantum circuit used in the model.</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def construct_model(self):\n    \"\"\"Construct the quantum circuit used in the model.\"\"\"\n\n    dev = qml.device(self.dev_type, wires=self.n_qubits_)\n\n    @qml.qnode(dev, **self.qnode_kwargs)\n    def circuit(params, x):\n        \"\"\"A variational quantum circuit with data reuploading.\n\n        Args:\n            params (array[float]): dictionary of parameters\n            x (array[float]): input vector\n\n        Returns:\n            list: Expectation values of Pauli Z on each qubit\n        \"\"\"\n        for layer in range(self.n_layers):\n            x_idx = 0  # to keep track of the data index\n            for i in range(self.n_qubits_):\n                angles = (\n                    x[x_idx : x_idx + 3] * params[\"omegas\"][layer, x_idx : x_idx + 3]\n                    + params[\"thetas\"][i, layer, :]\n                )\n                qml.Rot(*angles, wires=i)\n                x_idx += 3\n\n        # final reupload without CZs\n        x_idx = 0\n        for i in range(self.n_qubits_):\n            angles = (\n                x[x_idx : x_idx + 3] * params[\"omegas\"][self.n_layers, x_idx : x_idx + 3]\n                + params[\"thetas\"][i, self.n_layers, :]\n            )\n            qml.Rot(*angles, wires=i)\n            x_idx += 3\n\n        return [qml.expval(qml.PauliZ(wires=[i])) for i in range(self.n_qubits_)]\n\n    self.circuit = circuit\n\n    def circuit_as_array(params, x):\n        # pennylane returns a list in the circuit above, so we need this\n        return jnp.array(circuit(params, x))\n\n    if self.jit:\n        circuit_as_array = jax.jit(circuit_as_array)\n    self.forward = jax.vmap(circuit_as_array, in_axes=(None, 0))\n    self.chunked_forward = chunk_vmapped_fn(self.forward, 1, self.max_vmap)\n\n    return self.forward\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n    self.initialize(X.shape[1], classes=np.unique(y))\n\n    self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    def loss_fn(params, x, y):\n        y_mat = jnp.vstack(\n            [y for __ in range(self.observable_weight)]\n        ).T  # repeated columns of labels\n        y_mat0 = (1 - y_mat) / 2\n        y_mat1 = (1 + y_mat) / 2\n        alpha_mat0 = jnp.vstack(\n            [params[\"alphas\"][0, :] for __ in range(len(x))]\n        )  # repeated rows of alpha parameters\n        alpha_mat1 = jnp.vstack([params[\"alphas\"][1, :] for __ in range(len(x))])\n\n        expvals = self.forward(params, x)\n        probs0 = (1 - expvals[:, : self.observable_weight]) / 2  # fidelity with |0&gt;\n        probs1 = (1 + expvals[:, : self.observable_weight]) / 2  # fidelity with |1&gt;\n        loss = (\n            1\n            / 2\n            * (\n                jnp.sum(\n                    (alpha_mat0 * probs0 - y_mat0) ** 2 + (alpha_mat1 * probs1 - y_mat1) ** 2\n                )\n            )\n        )  # eqn 23 in plots\n        return loss / len(y)\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    self.construct_model()\n    self.initialize_params()\n    optimizer = optax.adam\n\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        X,\n        y,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels. Args:     n_features (int): Number of features that the classifier expects     classes (array-like): class labels that the classifier expects</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_features = n_features\n    self.n_qubits_ = ceil(self.n_features / 3)\n\n    if self.observable_type == \"single\":\n        self.observable_weight = 1\n    elif self.observable_type == \"half\":\n        if self.n_qubits_ == 1:\n            self.observable_weight = 1\n        else:\n            self.observable_weight = self.n_qubits_ // 2\n    elif self.observable_type == \"full\":\n        self.observable_weight = self.n_qubits_\n\n    self.initialize_params()\n    self.construct_model()\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for batch of data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for batch of data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    check_is_fitted(self)\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    predictions = self.chunked_forward(self.params_, X)\n    predictions = jnp.mean(\n        predictions[:, : self.observable_weight], axis=1\n    )  # use mean of expvals over relevant qubits\n    predictions_2d = np.c_[(1 - predictions) / 2, (1 + predictions) / 2]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n            self.scaler.fit(X)\n\n        X = self.scaler.transform(X)\n\n    X = X * self.scaling\n    X = jnp.pad(X, ((0, 0), (0, (3 - X.shape[1]) % 3)), \"constant\")\n    return X\n</code></pre>"},{"location":"api_docs.html#quoptuna.backend.base.pennylane_models.DressedQuantumCircuitClassifier","title":"DressedQuantumCircuitClassifier","text":"<pre><code>DressedQuantumCircuitClassifier(n_layers=3, learning_rate=0.001, batch_size=32, max_vmap=None, jit=True, max_steps=100000, convergence_interval=200, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax-jit'}, scaling=1.0, random_state=42)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Dressed quantum circuit from https://arxiv.org/abs/1912.08278. The model consists of the following sequence     * a single layer fully connected trainable neural network with tanh activation function     * a parameterised quantum circuit taking the above outputs as input     * a single layer fully connected trainable neural network taking local expectation values of the above       circuit as input</p> <p>The last neural network maps to two neurons that we take the softmax of to get class probabilities. The model is trained via binary cross entropy loss.</p> PARAMETER DESCRIPTION <code>n_layers</code> <p>number of layers in the variational part of the circuit.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>learning_rate</code> <p>initial learning rate for gradient descent.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> <code>max_steps</code> <p>Maximum number of training steps. A warning will be raised if training did not converge.</p> <p> TYPE: <code>int</code> DEFAULT: <code>100000</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>batch_size</code> <p>Size of batches used for computing parameter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>convergence_interval</code> <p>The number of loss values to consider to decide convergence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the keyword arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax-jit'}</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>input_transform</code> <p>The first neural network that we implment as matrix multiplication.</p> <code>output_transform</code> <p>The final neural network</p> <code>predict</code> <p>Predict labels for data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/dressed_quantum_circuit.py</code> <pre><code>def __init__(\n    self,\n    n_layers=3,\n    learning_rate=0.001,\n    batch_size=32,\n    max_vmap=None,\n    jit=True,\n    max_steps=100000,\n    convergence_interval=200,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax-jit\"},\n    scaling=1.0,\n    random_state=42,\n):\n    r\"\"\"\n    Dressed quantum circuit from https://arxiv.org/abs/1912.08278. The model consists of the following sequence\n        * a single layer fully connected trainable neural network with tanh activation function\n        * a parameterised quantum circuit taking the above outputs as input\n        * a single layer fully connected trainable neural network taking local expectation values of the above\n          circuit as input\n\n    The last neural network maps to two neurons that we take the softmax of to get class probabilities.\n    The model is trained via binary cross entropy loss.\n\n    Args:\n        n_layers (int): number of layers in the variational part of the circuit.\n        learning_rate (float): initial learning rate for gradient descent.\n        max_steps (int): Maximum number of training steps. A warning will be raised if training did not converge.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        batch_size (int): Size of batches used for computing parameter updates.\n        convergence_interval (int): The number of loss values to consider to decide convergence.\n        jit (bool): Whether to use just in time compilation.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'.\n        qnode_kwargs (str): the keyword arguments passed to the circuit qnode.\n        scaling (float): Factor by which to scale the input data.\n        random_state (int): Seed used for pseudorandom number generation.\n    \"\"\"\n    # attributes that do not depend on data\n    self.n_layers = n_layers\n    self.learning_rate = learning_rate\n    self.batch_size = batch_size\n    self.max_steps = max_steps\n    self.convergence_interval = convergence_interval\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.jit = jit\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # Dictionary containing the trainable parameters\n    self.n_qubits_ = None\n    self.n_features_ = None\n    self.scaler = None  # data scaler will be fitted on training data\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/dressed_quantum_circuit.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n\n    self.scaler = StandardScaler()\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    self.initialize(X.shape[1], classes=np.unique(y))\n\n    optimizer = optax.adam\n\n    def loss_fn(params, X, y):\n        vals = self.forward(params, X)\n        # convert to 0,1 one hot encoded labels\n        labels = jax.nn.one_hot(jax.nn.relu(y), 2)\n        return jnp.mean(optax.softmax_cross_entropy(vals, labels))\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        X,\n        y,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels. Args:     n_features (int): Number of features that the classifier expects     classes (array-like): class labels that the classifier expects</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/dressed_quantum_circuit.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_features_ = n_features\n    self.n_qubits_ = self.n_features_\n\n    self.initialize_params()\n    self.construct_model()\n</code></pre> <code></code> input_transform <pre><code>input_transform(params, x)\n</code></pre> <p>The first neural network that we implment as matrix multiplication.</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/dressed_quantum_circuit.py</code> <pre><code>def input_transform(self, params, x):\n    \"\"\"\n    The first neural network that we implment as matrix multiplication.\n    \"\"\"\n    x = jnp.matmul(params[\"input_weights\"], x)\n    x = jnp.tanh(x) * jnp.pi / 2\n    return x\n</code></pre> <code></code> output_transform <pre><code>output_transform(params, x)\n</code></pre> <p>The final neural network</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/dressed_quantum_circuit.py</code> <pre><code>def output_transform(self, params, x):\n    \"\"\"\n    The final neural network\n    \"\"\"\n    x = jnp.matmul(params[\"output_weights\"], x)\n    return x\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred</code> <p>Predicted labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/dressed_quantum_circuit.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred (np.ndarray): Predicted labels of shape (n_samples,)\n    \"\"\"\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/dressed_quantum_circuit.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    return jax.nn.softmax(self.chunked_forward(self.params_, X))\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/dressed_quantum_circuit.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = StandardScaler()\n            self.scaler.fit(X)\n\n        X = self.scaler.transform(X)\n    return X * self.scaling\n</code></pre>"},{"location":"api_docs.html#quoptuna.backend.base.pennylane_models.IQPKernelClassifier","title":"IQPKernelClassifier","text":"<pre><code>IQPKernelClassifier(svm=SVC(kernel='precomputed', probability=True), repeats=2, C=1.0, jit=False, random_state=42, scaling=1.0, max_vmap=250, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax-jit', 'diff_method': None})\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Kernel version of the classifier from https://arxiv.org/pdf/1804.11326v2.pdf. The kernel function is given by</p> <p>.. math::     k(x,x')=\\vert\\langle 0 \\vert U^\\dagger(x')U(x)\\vert 0 \\rangle\\vert^2</p> <p>where :math:<code>U(x)</code> is an IQP circuit implemented via Pennylane's <code>IQPEmbedding</code>.</p> <p>We precompute the kernel matrix from the data directly, and pass it to scikit-learn's support vector classifier SVC. This  allows us to benefit from JAX parallelisation when computing the kernel matrices.</p> <p>The model requires evaluating a number of circuits given by the square of the number of data samples, and is therefore only appropriate for relatively small datasets.</p> PARAMETER DESCRIPTION <code>svm</code> <p>scikit-learn SVM class object used to fit the model from the kernel matrix</p> <p> TYPE: <code>SVC</code> DEFAULT: <code>SVC(kernel='precomputed', probability=True)</code> </p> <code>repeats</code> <p>number of times the IQP structure is repeated in the embedding circuit.</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>C</code> <p>regularization parameter for SVC. Lower values imply stronger regularization.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>250</code> </p> <code>qnode_kwargs</code> <p>the key word arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax-jit', 'diff_method': None}</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>random_state</code> <p>seed used for reproducibility.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>fit</code> <p>Fit the model to data X and labels y. Uses sklearn's SVM classifier</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>precompute_kernel</code> <p>compute the kernel matrix relative to data sets X1 and X2</p> <code>predict</code> <p>Predict labels for data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/iqp_kernel.py</code> <pre><code>def __init__(\n    self,\n    svm=SVC(kernel=\"precomputed\", probability=True),\n    repeats=2,\n    C=1.0,\n    jit=False,\n    random_state=42,\n    scaling=1.0,\n    max_vmap=250,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax-jit\", \"diff_method\": None},\n):\n    r\"\"\"\n    Kernel version of the classifier from https://arxiv.org/pdf/1804.11326v2.pdf.\n    The kernel function is given by\n\n    .. math::\n        k(x,x')=\\vert\\langle 0 \\vert U^\\dagger(x')U(x)\\vert 0 \\rangle\\vert^2\n\n    where :math:`U(x)` is an IQP circuit implemented via Pennylane's `IQPEmbedding`.\n\n    We precompute the kernel matrix from the data directly, and pass it to scikit-learn's support vector\n    classifier SVC. This  allows us to benefit from JAX parallelisation when computing the kernel\n    matrices.\n\n    The model requires evaluating a number of circuits given by the square of the number of data\n    samples, and is therefore only appropriate for relatively small datasets.\n\n    Args:\n        svm (sklearn.svm.SVC): scikit-learn SVM class object used to fit the model from the kernel matrix\n        repeats (int): number of times the IQP structure is repeated in the embedding circuit.\n        C (float): regularization parameter for SVC. Lower values imply stronger regularization.\n        jit (bool): Whether to use just in time compilation.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        qnode_kwargs (str): the key word arguments passed to the circuit qnode.\n        scaling (float): Factor by which to scale the input data.\n        random_state (int): seed used for reproducibility.\n    \"\"\"\n    # attributes that do not depend on data\n    self.repeats = repeats\n    self.C = C\n    self.jit = jit\n    self.max_vmap = max_vmap\n    self.svm = svm\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None\n    self.n_qubits_ = None\n    self.scaler = None  # data scaler will be fitted on training data\n    self.circuit = None\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y. Uses sklearn's SVM classifier</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/iqp_kernel.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y. Uses sklearn's SVM classifier\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n\n    self.svm.random_state = int(\n        jax.random.randint(self.generate_key(), shape=(1,), minval=0, maxval=1000000)\n    )\n\n    self.initialize(X.shape[1], np.unique(y))\n\n    self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    self.params_ = {\"x_train\": X}\n    kernel_matrix = self.precompute_kernel(X, X)\n\n    start = time.time()\n    # we are updating this value here, in case it was\n    # changed after initialising the model\n    self.svm.C = self.C\n    self.svm.fit(kernel_matrix, y)\n    end = time.time()\n    self.training_time_ = end - start\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels.</p> PARAMETER DESCRIPTION <code>n_features</code> <p>Number of features that the classifier expects</p> <p> TYPE: <code>int</code> </p> <code>classes</code> <p>class labels that the classifier expects</p> <p> TYPE: <code>array - like</code> DEFAULT: <code>None</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/iqp_kernel.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_qubits_ = n_features\n\n    self.construct_circuit()\n</code></pre> <code></code> precompute_kernel <pre><code>precompute_kernel(X1, X2)\n</code></pre> <p>compute the kernel matrix relative to data sets X1 and X2 Args:     X1 (np.array): first dataset of input vectors     X2 (np.array): second dataset of input vectors Returns:     kernel_matrix (np.array): matrix of size (len(X1),len(X2)) with elements K(x_1,x_2)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/iqp_kernel.py</code> <pre><code>def precompute_kernel(self, X1, X2):\n    \"\"\"\n    compute the kernel matrix relative to data sets X1 and X2\n    Args:\n        X1 (np.array): first dataset of input vectors\n        X2 (np.array): second dataset of input vectors\n    Returns:\n        kernel_matrix (np.array): matrix of size (len(X1),len(X2)) with elements K(x_1,x_2)\n    \"\"\"\n    dim1 = len(X1)\n    dim2 = len(X2)\n\n    # concatenate all pairs of vectors\n    Z = jnp.array([np.concatenate((X1[i], X2[j])) for i in range(dim1) for j in range(dim2)])\n\n    circuit = self.construct_circuit()\n    self.batched_circuit = chunk_vmapped_fn(\n        jax.vmap(circuit, 0), start=0, max_vmap=self.max_vmap\n    )\n    kernel_values = self.batched_circuit(Z)[:, 0]\n\n    # reshape the values into the kernel matrix\n    kernel_matrix = np.reshape(kernel_values, (dim1, dim2))\n\n    return kernel_matrix\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred</code> <p>Predicted labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/iqp_kernel.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred (np.ndarray): Predicted labels of shape (n_samples,)\n    \"\"\"\n    X = self.transform(X)\n    kernel_matrix = self.precompute_kernel(X, self.params_[\"x_train\"])\n    return self.svm.predict(kernel_matrix)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X. note that this may be inconsistent with predict; see the sklearn docummentation for details.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/iqp_kernel.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n    note that this may be inconsistent with predict; see the sklearn docummentation for details.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n\n    if \"x_train\" not in self.params_:\n        raise ValueError(\"Model cannot predict without fitting to data first.\")\n\n    X = self.transform(X)\n    kernel_matrix = self.precompute_kernel(X, self.params_[\"x_train\"])\n    return self.svm.predict_proba(kernel_matrix)\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/iqp_kernel.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n            self.scaler.fit(X)\n        X = self.scaler.transform(X)\n\n    return X * self.scaling\n</code></pre>"},{"location":"api_docs.html#quoptuna.backend.base.pennylane_models.IQPVariationalClassifier","title":"IQPVariationalClassifier","text":"<pre><code>IQPVariationalClassifier(repeats=1, n_layers=10, learning_rate=0.001, batch_size=32, max_vmap=None, jit=True, max_steps=10000, convergence_interval=200, random_state=42, scaling=1.0, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax'})\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Variational verison of the classifier from https://arxiv.org/pdf/1804.11326v2.pdf. The model is a standard variational quantum classifier</p> <p>.. math::</p> <pre><code>f(x)=\\langle 0 \\vert U^\\dagger(x)V^\\dagger(\\theta) H V(\\theta)U(x)\\vert 0 \\rangle\n</code></pre> <p>where the data embedding unitary :math:<code>U(x)</code> is based on an IQP circuit stucture and implemented via pennylane.IQPEmbedding, and the trainable unitay :math:<code>V(\\theta)</code> is implemented via pennylane.StronglyEntanglingLayers.</p> <p>The model is trained using a linear loss function equivalent to the probability of incorrect classification.</p> PARAMETER DESCRIPTION <code>repeats</code> <p>Number of times to repeat the IQP embedding circuit structure.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>n_layers</code> <p>Number of layers in the variational part of the circuit.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>learning_rate</code> <p>Learning rate for gradient descent.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> <code>batch_size</code> <p>Size of batches used for computing paraemeter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>convergence_threshold</code> <p>If loss changes less than this threshold for 10 consecutive steps we stop training.</p> <p> TYPE: <code>float</code> </p> <code>max_steps</code> <p>Maximum number of training steps. A warning will be raised if training did not converge.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the keyword arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax'}</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/iqp_variational.py</code> <pre><code>def __init__(\n    self,\n    repeats=1,\n    n_layers=10,\n    learning_rate=0.001,\n    batch_size=32,\n    max_vmap=None,\n    jit=True,\n    max_steps=10000,\n    convergence_interval=200,\n    random_state=42,\n    scaling=1.0,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax\"},\n):\n    r\"\"\"\n    Variational verison of the classifier from https://arxiv.org/pdf/1804.11326v2.pdf.\n    The model is a standard variational quantum classifier\n\n    .. math::\n\n        f(x)=\\langle 0 \\vert U^\\dagger(x)V^\\dagger(\\theta) H V(\\theta)U(x)\\vert 0 \\rangle\n\n    where the data embedding unitary :math:`U(x)` is based on an IQP circuit stucture and implemented via\n    pennylane.IQPEmbedding, and the trainable unitay :math:`V(\\theta)` is implemented via\n    pennylane.StronglyEntanglingLayers.\n\n    The model is trained using a linear loss function equivalent to the probability of incorrect classification.\n\n    Args:\n        repeats (int): Number of times to repeat the IQP embedding circuit structure.\n        n_layers (int): Number of layers in the variational part of the circuit.\n        learning_rate (float): Learning rate for gradient descent.\n        batch_size (int): Size of batches used for computing paraemeter updates.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        convergence_threshold (float): If loss changes less than this threshold for 10 consecutive steps we stop training.\n        max_steps (int): Maximum number of training steps. A warning will be raised if training did not converge.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'.\n        qnode_kwargs (str): the keyword arguments passed to the circuit qnode.\n        random_state (int): Seed used for pseudorandom number generation.\n    \"\"\"\n\n    # attributes that do not depend on data\n    self.repeats = repeats\n    self.n_layers = n_layers\n    self.learning_rate = learning_rate\n    self.batch_size = batch_size\n    self.max_steps = max_steps\n    self.convergence_interval = convergence_interval\n    self.batch_size = batch_size\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.jit = jit\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # Dictionary containing the trainable parameters\n    self.n_qubits_ = None\n    self.scaler = None  # data scaler will be fitted on training data\n    self.circuit = None\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/iqp_variational.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n\n    self.initialize(n_features=X.shape[1], classes=np.unique(y))\n\n    self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    optimizer = optax.adam\n\n    def loss_fn(params, X, y):\n        expvals = self.forward(params, X)\n        probs = (1 - expvals * y) / 2  # the probs of incorrect classification\n        return jnp.mean(probs)\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        X,\n        y,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels.</p> PARAMETER DESCRIPTION <code>n_features</code> <p>Number of features that the classifier expects</p> <p> TYPE: <code>int</code> </p> <code>classes</code> <p>class labels that the classifier expects</p> <p> TYPE: <code>array - like</code> DEFAULT: <code>None</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/iqp_variational.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_qubits_ = n_features\n    self.initialize_params()\n    self.construct_model()\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred</code> <p>Predicted labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/iqp_variational.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred (np.ndarray): Predicted labels of shape (n_samples,)\n    \"\"\"\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/iqp_variational.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    predictions = self.chunked_forward(self.params_, X)\n    predictions_2d = np.c_[(1 - predictions) / 2, (1 + predictions) / 2]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/iqp_variational.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n            self.scaler.fit(X)\n        X = self.scaler.transform(X)\n    return X * self.scaling\n</code></pre>"},{"location":"api_docs.html#quoptuna.backend.base.pennylane_models.ProjectedQuantumKernel","title":"ProjectedQuantumKernel","text":"<pre><code>ProjectedQuantumKernel(svm=SVC(kernel='precomputed', probability=True), gamma_factor=1.0, C=1.0, embedding='Hamiltonian', t=1.0 / 3, trotter_steps=5, jit=True, max_vmap=None, scaling=1.0, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax-jit', 'diff_method': None}, random_state=42)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Kernel based classifier from https://arxiv.org/pdf/2011.01938v2.pdf.</p> <p>The Kernel function is</p> <p>.. math::     k(x_i, x_j) = \\exp (-\\gamma  \\sum_k \\sum_{P \\in {X, Y, Z}} ( \\text{tr}(P \\rho(x_i)_k)     - \\text{tr}( P \\rho(x_j)_k))^2)</p> <p>where :math:<code>\\rho_k(x_i)</code> is the reduced state of the kth qubit of the data embedded density matrix and :math:<code>\\gamma</code> is a hyperparameter of the model that we scale from the default value given in the plots.</p> <p>For embedding='Hamiltonian' a layer or random single qubit rotations are performed, followed by a Hamiltonian time evolution corresponding to a trotterised evolution of a Heisenberg Hamiltonian:</p> <p>.. math::     \\prod_{j=1}^n \\exp(-i \\frac{t}{L} x_{j} (X_j X_{j+1} + Y_j Y_{j+1} + Z_j Z_{j+1})).</p> <p>where :math:<code>t</code> and :math:<code>L</code> are the evolution time and the number of trotter steps that are controlled by hyperparameters <code>t</code> and <code>trotter_steps</code>.</p> <p>For emedding='IQP' an IQP embedding is used via PennyLanes's IQPEmbedding class.</p> <p>We precompute the kernel matrix from data and pass it to scikit-learn's support vector machine class SVC, which fits a classifier.</p> PARAMETER DESCRIPTION <code>svm</code> <p>scikit-learn SVC class object.</p> <p> TYPE: <code>SVC</code> DEFAULT: <code>SVC(kernel='precomputed', probability=True)</code> </p> <code>gamma_factor</code> <p>the factor that multiplies the default scaling parameter in the kernel.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>C</code> <p>regularization parameter when fitting the kernel model.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>embedding</code> <p>The choice of embedding circuit used to construct the kernel. Either 'IQP' or 'Hamiltonian'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'Hamiltonian'</code> </p> <code>t</code> <p>The evolution time used in the 'Hamiltonian' data embedding. The time is given by n_features*t.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0 / 3</code> </p> <code>trotter_steps</code> <p>the number of trotter steps used in the 'Hamiltonian' embedding circuit.</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the key word arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax-jit', 'diff_method': None}</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>construct_circuit</code> <p>Constructs the circuit to get the expvals of a given qubit and Pauli operator</p> <code>fit</code> <p>Fit the model to data X and labels y. Uses sklearn's SVM classifier</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>precompute_kernel</code> <p>compute the kernel matrix relative to data sets X1 and X2</p> <code>predict</code> <p>Predict labels for data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/projected_quantum_kernel.py</code> <pre><code>def __init__(\n    self,\n    svm=SVC(kernel=\"precomputed\", probability=True),\n    gamma_factor=1.0,\n    C=1.0,\n    embedding=\"Hamiltonian\",\n    t=1.0 / 3,\n    trotter_steps=5,\n    jit=True,\n    max_vmap=None,\n    scaling=1.0,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax-jit\", \"diff_method\": None},\n    random_state=42,\n):\n    r\"\"\"\n    Kernel based classifier from https://arxiv.org/pdf/2011.01938v2.pdf.\n\n    The Kernel function is\n\n    .. math::\n        k(x_i, x_j) = \\exp (-\\gamma  \\sum_k \\sum_{P \\in \\{X, Y, Z\\}} ( \\text{tr}(P \\rho(x_i)_k)\n        - \\text{tr}( P \\rho(x_j)_k))^2)\n\n    where :math:`\\rho_k(x_i)` is the reduced state of the kth qubit of the data embedded density matrix and\n    :math:`\\gamma` is a hyperparameter of the model that we scale from the default value given in the plots.\n\n    For embedding='Hamiltonian' a layer or random single qubit rotations are performed, followed by a Hamiltonian\n    time evolution corresponding to a trotterised evolution of a Heisenberg Hamiltonian:\n\n    .. math::\n        \\prod_{j=1}^n \\exp(-i \\frac{t}{L} x_{j} (X_j X_{j+1} + Y_j Y_{j+1} + Z_j Z_{j+1})).\n\n    where :math:`t` and :math:`L` are the evolution time and the number of trotter steps that are controlled by\n    hyperparameters `t` and `trotter_steps`.\n\n    For emedding='IQP' an IQP embedding is used via PennyLanes's IQPEmbedding class.\n\n    We precompute the kernel matrix from data and pass it to scikit-learn's support vector machine class SVC,\n    which fits a classifier.\n\n    Args:\n        svm (sklearn.svm.SVC): scikit-learn SVC class object.\n        gamma_factor (float): the factor that multiplies the default scaling parameter in the kernel.\n        C (float): regularization parameter when fitting the kernel model.\n        embedding (str): The choice of embedding circuit used to construct the kernel.\n            Either 'IQP' or 'Hamiltonian'.\n        t (float): The evolution time used in the 'Hamiltonian' data embedding. The time is\n            given by n_features*t.\n        trotter_steps (int): the number of trotter steps used in the 'Hamiltonian' embedding circuit.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'\n        qnode_kwargs (str): the key word arguments passed to the circuit qnode.\n        scaling (float): Factor by which to scale the input data.\n        random_state (int): Seed used for pseudorandom number generation.\n    \"\"\"\n    # attributes that do not depend on data\n    self.gamma_factor = gamma_factor\n    self.svm = svm\n    self.C = C\n    self.embedding = embedding\n    self.t = t\n    self.trotter_steps = trotter_steps\n    self.jit = jit\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = 50\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None\n    self.n_qubits_ = None\n    self.n_features_ = None\n    self.rotation_angles_ = None  # for hamiltonian embedding\n    self.scaler = None  # data scaler will be fitted on training data\n    self.circuit = None\n</code></pre> <code></code> construct_circuit <pre><code>construct_circuit()\n</code></pre> <p>Constructs the circuit to get the expvals of a given qubit and Pauli operator We will use JAX to parallelize over these circuits in precompute kernel. Args:     P: a pennylane Pauli X,Y,Z operator on a given qubit</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/projected_quantum_kernel.py</code> <pre><code>def construct_circuit(self):\n    \"\"\"\n    Constructs the circuit to get the expvals of a given qubit and Pauli operator\n    We will use JAX to parallelize over these circuits in precompute kernel.\n    Args:\n        P: a pennylane Pauli X,Y,Z operator on a given qubit\n    \"\"\"\n    if self.embedding == \"IQP\":\n\n        def embedding(x):\n            qml.IQPEmbedding(x, wires=range(self.n_qubits_), n_repeats=2)\n\n    elif self.embedding == \"Hamiltonian\":\n\n        def embedding(x):\n            evol_time = self.t / self.trotter_steps * (self.n_qubits_ - 1)\n            for i in range(self.n_qubits_):\n                qml.Rot(\n                    self.rotation_angles_[i, 0],\n                    self.rotation_angles_[i, 1],\n                    self.rotation_angles_[i, 2],\n                    wires=i,\n                )\n            for __ in range(self.trotter_steps):\n                for j in range(self.n_qubits_ - 1):\n                    qml.IsingXX(x[j] * evol_time, wires=[j, j + 1])\n                    qml.IsingYY(x[j] * evol_time, wires=[j, j + 1])\n                    qml.IsingZZ(x[j] * evol_time, wires=[j, j + 1])\n\n    dev = qml.device(self.dev_type, wires=self.n_qubits_)\n\n    @qml.qnode(dev, **self.qnode_kwargs)\n    def circuit(x):\n        embedding(x)\n        return (\n            [qml.expval(qml.PauliX(wires=i)) for i in range(self.n_qubits_)]\n            + [qml.expval(qml.PauliY(wires=i)) for i in range(self.n_qubits_)]\n            + [qml.expval(qml.PauliZ(wires=i)) for i in range(self.n_qubits_)]\n        )\n\n    self.circuit = circuit\n\n    def circuit_as_array(x):\n        return jnp.array(circuit(x))\n\n    if self.jit:\n        circuit_as_array = jax.jit(circuit_as_array)\n    circuit_as_array = jax.vmap(circuit_as_array, in_axes=(0))\n    circuit_as_array = chunk_vmapped_fn(circuit_as_array, 0, self.max_vmap)\n\n    return circuit_as_array\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y. Uses sklearn's SVM classifier</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/projected_quantum_kernel.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y. Uses sklearn's SVM classifier\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n    self.svm.random_state = self.rng.integers(100000)\n\n    self.initialize(X.shape[1], classes=np.unique(y))\n\n    self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    self.params_ = {\"X_train\": X}\n    kernel_matrix = self.precompute_kernel(X, X)\n\n    start = time.time()\n    self.svm.C = self.C\n    self.svm.fit(kernel_matrix, y)\n    end = time.time()\n    self.training_time_ = end - start\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels.</p> PARAMETER DESCRIPTION <code>n_features</code> <p>Number of features that the classifier expects</p> <p> TYPE: <code>int</code> </p> <code>classes</code> <p>class labels that the classifier expects</p> <p> TYPE: <code>array - like</code> DEFAULT: <code>None</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/projected_quantum_kernel.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_features_ = n_features\n\n    if self.embedding == \"IQP\":\n        self.n_qubits_ = self.n_features_\n    elif self.embedding == \"Hamiltonian\":\n        self.n_qubits_ = self.n_features_ + 1\n        self.rotation_angles_ = jnp.array(\n            self.rng.uniform(size=(self.n_qubits_, 3)) * np.pi * 2\n        )\n    self.construct_circuit()\n</code></pre> <code></code> precompute_kernel <pre><code>precompute_kernel(X1, X2)\n</code></pre> <p>compute the kernel matrix relative to data sets X1 and X2 Args:     X1 (np.array): first dataset of input vectors     X2 (np.array): second dataset of input vectors Returns:     kernel_matrix (np.array): matrix of size (len(X1),len(X2)) with elements K(x_1,x_2)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/projected_quantum_kernel.py</code> <pre><code>def precompute_kernel(self, X1, X2):\n    \"\"\"\n    compute the kernel matrix relative to data sets X1 and X2\n    Args:\n        X1 (np.array): first dataset of input vectors\n        X2 (np.array): second dataset of input vectors\n    Returns:\n        kernel_matrix (np.array): matrix of size (len(X1),len(X2)) with elements K(x_1,x_2)\n    \"\"\"\n    dim1 = len(X1)\n    dim2 = len(X2)\n\n    # get all of the Pauli expvals needed to constrcut the kernel\n    self.circuit = self.construct_circuit()\n\n    valsX1 = np.array(self.circuit(X1))\n    valsX1 = np.reshape(valsX1, (dim1, 3, -1))\n    valsX2 = np.array(self.circuit(X2))\n    valsX2 = np.reshape(valsX2, (dim2, 3, -1))\n\n    valsX_X1 = valsX1[:, 0]\n    valsX_X2 = valsX2[:, 0]\n    valsY_X1 = valsX1[:, 1]\n    valsY_X2 = valsX2[:, 1]\n    valsZ_X1 = valsX1[:, 2]\n    valsZ_X2 = valsX2[:, 2]\n\n    all_vals_X1 = np.reshape(np.concatenate((valsX_X1, valsY_X1, valsZ_X1)), -1)\n    default_gamma = 1 / np.var(all_vals_X1) / self.n_features_\n\n    # construct kernel following plots\n    kernel_matrix = np.zeros([dim1, dim2])\n\n    for i in range(dim1):\n        for j in range(dim2):\n            sumX = sum([(valsX_X1[i, q] - valsX_X2[j, q]) ** 2 for q in range(self.n_qubits_)])\n            sumY = sum([(valsY_X1[i, q] - valsY_X2[j, q]) ** 2 for q in range(self.n_qubits_)])\n            sumZ = sum([(valsZ_X1[i, q] - valsZ_X2[j, q]) ** 2 for q in range(self.n_qubits_)])\n\n            kernel_matrix[i, j] = np.exp(\n                -default_gamma * self.gamma_factor * (sumX + sumY + sumZ)\n            )\n    return kernel_matrix\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred</code> <p>Predicted labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/projected_quantum_kernel.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred (np.ndarray): Predicted labels of shape (n_samples,)\n    \"\"\"\n    X = self.transform(X)\n    kernel_matrix = self.precompute_kernel(X, self.params_[\"X_train\"])\n    return self.svm.predict(kernel_matrix)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X. Note that this may be inconsistent with predict; see the sklearn docummentation for details.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/projected_quantum_kernel.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n    Note that this may be inconsistent with predict; see the sklearn docummentation for details.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n\n    if \"X_train\" not in self.params_:\n        raise ValueError(\"Model cannot predict without fitting to data first.\")\n\n    X = self.transform(X)\n    kernel_matrix = self.precompute_kernel(X, self.params_[\"X_train\"])\n    return self.svm.predict_proba(kernel_matrix)\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/projected_quantum_kernel.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n            self.scaler.fit(X)\n        X = self.scaler.transform(X)\n    return X * self.scaling\n</code></pre>"},{"location":"api_docs.html#quoptuna.backend.base.pennylane_models.QuantumBoltzmannMachine","title":"QuantumBoltzmannMachine","text":"<pre><code>QuantumBoltzmannMachine(visible_qubits='single', observable_type='sum', temperature=1, learning_rate=0.001, batch_size=32, max_vmap=None, jit=True, max_steps=10000, convergence_threshold=1e-06, random_state=42, scaling=1.0)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Variational Quantum Boltzmann Machine from https://arxiv.org/abs/2006.06004</p> <p>The model works as follows 1. One prepares a gibbs state :math:<code>e^{-H(\u000bec{theta},x)/K_b T}/Z</code>, where :math:<code>H(     heta,x)</code> is a parameterised n_qubit     Hamiltonian and :math:<code>Z</code> is the partition function normalisation. Here we take n_qubits equal to the number of features 2. A :math:<code>\\pm1</code> valued observable :math:<code>O</code> is measured on a subset of qubits (called 'visible qubits'). The forward     function of the model is then</p> <p>.. maths::</p> <pre><code>f(x,        heta) = Tr[O e^{-H(\u000bec{theta},x)/k_b T}/Z]\n</code></pre> <p>from this expectation value, the class probabilities are computed and used in a binary cross entropy loss.</p> <p>The specific Hamiltonian we use is a generalisation of the one used in the plots:</p> <p>.. maths::</p> <pre><code>H(x,        heta) = \\sum_i Z_i      heta_i\\cdot x + \\sum_i X_i      heta_{i+n_qubits}\\cdot x + \\sum_{ij} Z_iZ_j     heta_{i+2*n_qubits}\\cdot x\n</code></pre> <p>The observable use can be either a sum or tensor product of Z operators on the visible qubits.</p> <p>In practice, the full algorithm involves parameterising a trial state for the gibbs state and performing variational imaginary time evolution to approximate the true gibbs state in each optimisation step. Since this is quite computationally involved, we here assume (as they do in the plots numerics) that we have access to the perfect Gibbs state. It is thus unclear whether the full algorithm can be expected to perform as well as this implementation.</p> PARAMETER DESCRIPTION <code>visible_qubits</code> <p>The subset of qubits used for prediction. if 'single' a single qubit is used if 'half' half are used and if 'all' all are used.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'single'</code> </p> <code>observable_type</code> <p>If 'sum' a sum of Z operators is used, if 'product' a tensor product is used.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'sum'</code> </p> <code>temperature</code> <p>The temperature of the Gibbs state in units of K_bT. e.g. temperature = 2 is equivalent to K_bT=2.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>learning_rate</code> <p>learning rate for gradient descent.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> <code>batch_size</code> <p>Number of data points to subsample in each training step.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>max_vmap</code> <p>The largest size of vmap used (to control memory).</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>convergence_threshold</code> <p>If loss changes less than this threshold for 10 consecutive steps we stop training.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-06</code> </p> <code>max_steps</code> <p>Maximum number of training steps. A warning will be raised if training did not converge.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> <code>scaling</code> <p>The data is scaled by this factor after standardisation</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> METHOD DESCRIPTION <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_boltzmann_machine.py</code> <pre><code>def __init__(\n    self,\n    visible_qubits=\"single\",\n    observable_type=\"sum\",\n    temperature=1,\n    learning_rate=0.001,\n    batch_size=32,\n    max_vmap=None,\n    jit=True,\n    max_steps=10000,\n    convergence_threshold=1e-6,\n    random_state=42,\n    scaling=1.0,\n):\n    \"\"\"\n    Variational Quantum Boltzmann Machine from https://arxiv.org/abs/2006.06004\n\n    The model works as follows\n    1. One prepares a gibbs state :math:`e^{-H(\\vec{theta},x)/K_b T}/Z`, where :math:`H(\\theta,x)` is a parameterised n_qubit\n        Hamiltonian and :math:`Z` is the partition function normalisation. Here we take n_qubits equal to the number of features\n    2. A :math:`\\\\pm1` valued observable :math:`O` is measured on a subset of qubits (called 'visible qubits'). The forward\n        function of the model is then\n\n    .. maths::\n\n        f(x, \\theta) = Tr[O e^{-H(\\vec{theta},x)/k_b T}/Z]\n\n    from this expectation value, the class probabilities are computed and used in a binary cross entropy loss.\n\n    The specific Hamiltonian we use is a generalisation of the one used in the plots:\n\n    .. maths::\n\n        H(x, \\theta) = \\\\sum_i Z_i \\theta_i\\\\cdot x + \\\\sum_i X_i \\theta_{i+n_qubits}\\\\cdot x + \\\\sum_{ij} Z_iZ_j \\theta_{i+2*n_qubits}\\\\cdot x\n\n    The observable use can be either a sum or tensor product of Z operators on the visible qubits.\n\n    In practice, the full algorithm involves parameterising a trial state for the gibbs state and performing variational imaginary\n    time evolution to approximate the true gibbs state in each optimisation step. Since this is quite computationally involved, we\n    here assume (as they do in the plots numerics) that we have access to the perfect Gibbs state. It is thus unclear whether the full\n    algorithm can be expected to perform as well as this implementation.\n\n    Args:\n        visible_qubits (str): The subset of qubits used for prediction. if 'single' a single qubit is used\n            if 'half' half are used and if 'all' all are used.\n        observable_type (str): If 'sum' a sum of Z operators is used, if 'product' a tensor product is used.\n        temperature (int): The temperature of the Gibbs state in units of K_bT. e.g. temperature = 2 is equivalent to K_bT=2.\n        learning_rate (float): learning rate for gradient descent.\n        batch_size (int): Number of data points to subsample in each training step.\n        max_vmap (int): The largest size of vmap used (to control memory).\n        jit (bool): Whether to use just in time compilation.\n        convergence_threshold (float): If loss changes less than this threshold for 10 consecutive steps we stop training.\n        max_steps (int): Maximum number of training steps. A warning will be raised if training did not converge.\n        random_state (int): Seed used for pseudorandom number generation\n        scaling (float): The data is scaled by this factor after standardisation\n    \"\"\"\n\n    # attributes that do not depend on data\n    self.visible_qubits = visible_qubits\n    self.observable_type = observable_type\n    self.temperature = temperature\n    self.learning_rate = learning_rate\n    self.batch_size = batch_size\n    self.max_vmap = max_vmap\n    self.jit = jit\n    self.max_steps = max_steps\n    self.convergence_threshold = convergence_threshold\n    self.batch_size = batch_size\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # Dictionary containing the trainable parameters\n    self.n_qubits = None\n    self.n_visible = None  # number of visible qubits\n    self.scaler = None  # data scaler will be fitted on training data\n    self.circuit = None\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_boltzmann_machine.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n    self.initialize(X.shape[1], classes=np.unique(y))\n\n    self.scaler = StandardScaler()\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    optimizer = optax.adam\n\n    def loss_fn(params, X, y):\n        # binary cross entropy loss\n        vals = self.forward(params[\"thetas\"], X)\n        probs = (1 + vals) / 2\n        y = jax.nn.relu(y)  # convert to 0,1\n        return jnp.mean(-y * jnp.log(probs) - (1 - y) * jnp.log(1 - probs))\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    self.params_ = train(self, loss_fn, optimizer, X, y, self.generate_key)\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels.</p> PARAMETER DESCRIPTION <code>n_features</code> <p>Number of features that the classifier expects</p> <p> TYPE: <code>int</code> </p> <code>classes</code> <p>class labels that the classifier expects</p> <p> TYPE: <code>array - like</code> DEFAULT: <code>None</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_boltzmann_machine.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_qubits = n_features\n\n    if self.visible_qubits == \"single\":\n        self.n_visible = 1\n    elif self.visible_qubits == \"half\":\n        self.n_visible = self.n_qubits // 2\n    elif self.visible_qubits == \"full\":\n        self.n_visible = self.n_qubits\n\n    self.construct_model()\n    self.initialize_params()\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred</code> <p>Predicted labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_boltzmann_machine.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred (np.ndarray): Predicted labels of shape (n_samples,)\n    \"\"\"\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_boltzmann_machine.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    predictions = self.forward(self.params_[\"thetas\"], X)\n    predictions_2d = np.c_[(1 - predictions) / 2, (1 + predictions) / 2]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_boltzmann_machine.py</code> <pre><code>def transform(self, X):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if self.scaler is None:\n        # if the model is unfitted, initialise the scaler here\n        self.scaler = StandardScaler()\n        self.scaler.fit(X)\n\n    return self.scaler.transform(X) * self.scaling\n</code></pre>"},{"location":"api_docs.html#quoptuna.backend.base.pennylane_models.QuantumKitchenSinks","title":"QuantumKitchenSinks","text":"<pre><code>QuantumKitchenSinks(linear_model=LogisticRegression(penalty=None, solver='lbfgs', tol=0.001), n_episodes=100, n_qfeatures='full', var=1.0, jit=True, max_vmap=None, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax', 'diff_method': None}, scaling=1.0, random_state=42)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Quantum kitchen sinks model classical data classification.</p> <p>Based on: https://arxiv.org/pdf/1806.08321.pdf</p> <p>The quantum computer is used to generate random feature vectors which are fed to a linear classifier that we implement using scikit-learn's LogisticRegression (note logistic regression is a standard method to perform linear classification despite the name)</p> <p>The feature map procedure works as follows:</p> <ol> <li> <p>Linearly transform an input feature vector :math:<code>x</code> of length :math:<code>n</code> via :math:<code>x' = \\omega x + \\beta</code> using random :math:<code>\\omega, \\beta</code>. The output is of shape <code>(n_episodes, n_qfeatures)</code>.</p> </li> <li> <p>Feed each row vector of the matrix :math:<code>x'</code> into a quantum circuit that returns <code>n_qfeatures</code> samples. The samples are concatenated in a feature vector :math:<code>x</code> of length <code>n_episodes*n_qfeatures</code>, which is the input to the linear model.</p> </li> </ol> <p>.. note::</p> <pre><code>It is not stated in the plots how to generalise the circuits to higher qubit numbers;\nthis implementation is a simple generalisation of the circuit in Fig 2(c), which consists\nof encoding the features into single qubits via X rotaiton gates, and perfoming a sequence\nof CNOT gates on nearest neighbour and next-nearest neighbour qubits.\n</code></pre> PARAMETER DESCRIPTION <code>linear_model</code> <p>linear model to use with the transformed features</p> <p> TYPE: <code>sklearn Estimator</code> DEFAULT: <code>LogisticRegression(penalty=None, solver='lbfgs', tol=0.001)</code> </p> <code>n_episodes</code> <p>Number of features fed into the linear model after data transformation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>n_qfeatures</code> <p>Determines the number of features fed into the quantum circuit to transform the data. This is the number of qubits used by the model. If 'full', the number of qubits is equal to the number of input features, if 'half' it is half the number of input features.</p> <p> TYPE: <code>(str, int)</code> DEFAULT: <code>'full'</code> </p> <code>var</code> <p>detemined the variance of the matrix <code>\\omega</code> used to lienarly transform the input features.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the key word arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax', 'diff_method': None}</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>construct_model</code> <p>The circuit used to generate the feature vectors</p> <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for batch of data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Apply the feature map: The inputs go through a random linear transformation</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_kitchen_sinks.py</code> <pre><code>def __init__(\n    self,\n    linear_model=LogisticRegression(penalty=None, solver=\"lbfgs\", tol=10e-4),\n    n_episodes=100,\n    n_qfeatures=\"full\",\n    var=1.0,\n    jit=True,\n    max_vmap=None,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax\", \"diff_method\": None},\n    scaling=1.0,\n    random_state=42,\n):\n    r\"\"\"\n    Quantum kitchen sinks model classical data classification.\n\n    Based on: https://arxiv.org/pdf/1806.08321.pdf\n\n    The quantum computer is used to generate random feature vectors which are\n    fed to a linear classifier that we implement using scikit-learn's LogisticRegression (note logistic\n    regression is a standard method to perform linear classification despite the name)\n\n    The feature map procedure works as follows:\n\n    1. Linearly transform an input feature vector :math:`x` of length :math:`n`\n    via :math:`x' = \\omega x + \\beta` using random :math:`\\omega, \\beta`.\n    The output is of shape `(n_episodes, n_qfeatures)`.\n\n    2. Feed each row vector of the matrix :math:`x'` into a quantum circuit that\n    returns `n_qfeatures` samples. The samples are concatenated in a feature vector :math:`x`\n    of length `n_episodes*n_qfeatures`, which is the input to the linear model.\n\n    .. note::\n\n        It is not stated in the plots how to generalise the circuits to higher qubit numbers;\n        this implementation is a simple generalisation of the circuit in Fig 2(c), which consists\n        of encoding the features into single qubits via X rotaiton gates, and perfoming a sequence\n        of CNOT gates on nearest neighbour and next-nearest neighbour qubits.\n\n    Args:\n        linear_model (sklearn Estimator): linear model to use with the transformed features\n        n_episodes (int): Number of features fed into the linear model after data transformation.\n        n_qfeatures (str, int): Determines the number of features fed into the quantum circuit to transform the\n            data. This is the number of qubits used by the model. If 'full', the number of qubits is equal\n            to the number of input features, if 'half' it is half the number of input features.\n        var (float): detemined the variance of the matrix `\\omega` used to lienarly transform the input\n            features.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'\n        qnode_kwargs (str): the key word arguments passed to the circuit qnode.\n        random_state (int): Seed used for pseudorandom number generation.\n    \"\"\"\n\n    # attributes that do not depend on data\n    self.linear_model = linear_model\n    self.n_episodes = n_episodes\n    self.n_qfeatures = n_qfeatures\n    self.var = var\n    self.jit = jit\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = 100000\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None\n    self.n_qubits_ = None\n    self.scaler = None  # data scaler for inputs\n    self.scaler2 = None  # data scaler for quantum generated features\n    self.circuit = None\n</code></pre> <code></code> construct_model <pre><code>construct_model()\n</code></pre> <p>The circuit used to generate the feature vectors It is not clear from the plots how to generalise this to higher qubit numbers. This is a simple generalisation of the circuit in Fig 2(c).</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_kitchen_sinks.py</code> <pre><code>def construct_model(self):\n    \"\"\"\n    The circuit used to generate the feature vectors\n    It is not clear from the plots how to generalise this to higher qubit numbers.\n    This is a simple generalisation of the circuit in Fig 2(c).\n    \"\"\"\n\n    pattern = [[i, i + 2] for i in range(self.n_qubits_ - 2)]\n\n    dev = qml.device(\n        self.dev_type,\n        wires=self.n_qubits_,\n        shots=1,\n        prng_key=self.generate_key(),\n    )\n\n    @qml.qnode(dev, **self.qnode_kwargs)\n    def circuit(Q):\n        for i, q in enumerate(Q):\n            qml.RX(q, wires=i)\n        qml.broadcast(qml.CNOT, wires=range(self.n_qubits_), pattern=\"double\")\n        qml.broadcast(qml.CNOT, wires=range(self.n_qubits_), pattern=pattern)\n\n        return qml.sample(wires=range(self.n_qubits_))\n\n    self.circuit = circuit\n\n    if self.jit:\n        circuit = jax.jit(circuit)\n    circuit = chunk_vmapped_fn(jax.vmap(circuit), 0, self.max_vmap)\n\n    self.forward = circuit\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels 1-, 1 of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_kitchen_sinks.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels 1-, 1 of shape (n_samples,)\n    \"\"\"\n\n    self.linear_model.random_state = self.rng.integers(100000)\n\n    self.initialize(X.shape[1], np.unique(y))\n\n    self.scaler = MinMaxScaler(feature_range=(-1, 1))\n    self.scaler.fit(X)\n    features = self.transform(X)\n\n    start = time.time()\n    self.linear_model.fit(features, y)\n    end = time.time()\n    self.params_[\"weights\"] = self.linear_model.coef_\n    self.training_time_ = end - start\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels.</p> PARAMETER DESCRIPTION <code>n_features</code> <p>Number of features that the classifier expects</p> <p> TYPE: <code>int</code> </p> <code>classes</code> <p>class labels that the classifier expects</p> <p> TYPE: <code>array - like</code> DEFAULT: <code>None</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_kitchen_sinks.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_features_ = n_features\n\n    if self.n_qfeatures == \"full\":\n        self.n_qubits_ = n_features\n    elif self.n_qfeatures == \"half\":\n        self.n_qubits_ = int(np.ceil(n_features / 2))\n    else:\n        self.n_qubits_ = int(self.n_qfeatures)\n\n    self.construct_model()\n    self.initialize_params()\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for batch of data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_kitchen_sinks.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for batch of data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_kitchen_sinks.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    predictions_2d = self.linear_model.predict_proba(X)\n    return softmax(predictions_2d, copy=False)\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> <p>Apply the feature map: The inputs go through a random linear transformation followed by a quantum circuit.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_kitchen_sinks.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Apply the feature map: The inputs go through a random linear transformation\n    followed by a quantum circuit.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if self.params_[\"betas\"] is None or self.params_[\"omegas\"] is None or self.circuit is None:\n        raise ValueError(\"Model cannot predict without fitting to data first.\")\n\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = MinMaxScaler(feature_range=(-1, 1))\n            self.scaler.fit(X)\n        X = self.scaler.transform(X)\n    X = X * self.scaling\n\n    n_data = X.shape[0]\n    input_features = np.zeros([self.n_episodes, n_data, self.n_qubits_])\n    for e in range(self.n_episodes):\n        stacked_beta = np.stack([self.params_[\"betas\"][e] for __ in range(n_data)])\n        input_features[e] = (self.params_[\"omegas\"][e] @ X.T + stacked_beta.T).T\n    input_features = np.reshape(input_features, (n_data * self.n_episodes, -1))\n\n    features = self.forward(input_features)\n    features = np.reshape(features, (self.n_episodes, n_data, -1))\n    features = np.array([features[:, i, :] for i in range(n_data)])\n    features = np.reshape(features, (n_data, -1))\n\n    if self.scaler2 is None:\n        self.scaler2 = StandardScaler().fit(features)\n\n    return features\n</code></pre>"},{"location":"api_docs.html#quoptuna.backend.base.pennylane_models.QuantumMetricLearner","title":"QuantumMetricLearner","text":"<pre><code>QuantumMetricLearner(n_layers=3, n_examples_predict=32, convergence_interval=200, max_steps=50000, learning_rate=0.01, batch_size=32, max_vmap=4, jit=True, random_state=42, scaling=1.0, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax-jit'})\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Following https://arxiv.org/abs/2001.03622.</p> <p>This classifier uses a trainable embedding to encode inputs into quantum states. Training and prediction relies on comparing these states with each other using the fidelity/state overlap:</p> <pre><code>* During training, the embedding is optimised to place data from the same class close together and data\n  from different classes far apart from each other.\n* Prediction compares a new embedded input with memorised training samples from each class and predicts\n  the class whose samples are closest.\n</code></pre> <p>Since pairwise comparison between data points are expensive, training and classification only uses samples from the data.</p> <p>The trainable embedding uses PennyLane's <code>QAOAEmbedding</code>.</p> <p>The classifier uses <code>batch_size*3</code> circuits for an evaluation of the loss function, and <code>n_examples_predict*2</code> circuits for prediction.</p> PARAMETER DESCRIPTION <code>n_examples_predict</code> <p>Number of examples from each class of the training set used for prediction.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>n_layers</code> <p>Number of layers used in the trainable embedding.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>convergence_interval</code> <p>The number of loss values to consider to decide convergence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>max_steps</code> <p>Maximum number of training steps. A warning will be raised if training did not converge.</p> <p> TYPE: <code>int</code> DEFAULT: <code>50000</code> </p> <code>batch_size</code> <p>Size of batches used for computing parameter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>learning_rate</code> <p>Initial learning rate for training.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.01</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>4</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dev_type</code> <p>Wtring specifying the pennylane device type; e.g. 'default.qubit'</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>The keyword arguments passed to the circuit qnode</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax-jit'}</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> METHOD DESCRIPTION <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for batch of data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X using a batch of training examples.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_metric_learning.py</code> <pre><code>def __init__(\n    self,\n    n_layers=3,\n    n_examples_predict=32,\n    convergence_interval=200,\n    max_steps=50000,\n    learning_rate=0.01,\n    batch_size=32,\n    max_vmap=4,\n    jit=True,\n    random_state=42,\n    scaling=1.0,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax-jit\"},\n):\n    \"\"\"\n    Following https://arxiv.org/abs/2001.03622.\n\n    This classifier uses a trainable embedding to encode inputs into quantum states. Training and prediction\n    relies on comparing these states with each other using the fidelity/state overlap:\n\n        * During training, the embedding is optimised to place data from the same class close together and data\n          from different classes far apart from each other.\n        * Prediction compares a new embedded input with memorised training samples from each class and predicts\n          the class whose samples are closest.\n\n    Since pairwise comparison between data points are expensive, training and classification only\n    uses samples from the data.\n\n    The trainable embedding uses PennyLane's `QAOAEmbedding`.\n\n    The classifier uses `batch_size*3` circuits for an evaluation of the loss function, and `n_examples_predict*2`\n    circuits for prediction.\n\n    Args:\n        n_examples_predict (int): Number of examples from each class of the training set used for prediction.\n        n_layers (int): Number of layers used in the trainable embedding.\n        convergence_interval (int): The number of loss values to consider to decide convergence.\n        max_steps (int): Maximum number of training steps. A warning will be raised if training did not converge.\n        batch_size (int): Size of batches used for computing parameter updates.\n        learning_rate (float): Initial learning rate for training.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        dev_type (str): Wtring specifying the pennylane device type; e.g. 'default.qubit'\n        qnode_kwargs (str): The keyword arguments passed to the circuit qnode\n        scaling (float): Factor by which to scale the input data.\n\n    \"\"\"\n\n    # attributes that do not depend on data\n    self.n_examples_predict = n_examples_predict\n    self.n_layers = n_layers\n    self.convergence_interval = convergence_interval\n    self.max_steps = max_steps\n    self.learning_rate = learning_rate\n    self.batch_size = batch_size\n    self.jit = jit\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # Dictionary containing the trainable parameters\n    self.n_qubits_ = None\n    self.n_features_ = None\n    self.scaler = None  # data scaler will be fitted on training data\n    self.circuit = None\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_metric_learning.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n\n    self.initialize(X.shape[1], classes=np.unique(y))\n\n    self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    # split data\n    A = jnp.array(X[y == -1])\n    B = jnp.array(X[y == 1])\n\n    if self.batch_size &gt; min(len(A), len(B)):\n        warnings.warn(\"batch size too large, setting to \" + str(min(len(A), len(B))))\n        self.batch_size = min(len(A), len(B))\n\n    def loss_fn(params, A=None, B=None):\n        aa = self.forward(params, X1=A, X2=A)\n        bb = self.forward(params, X1=B, X2=B)\n        ab = self.forward(params, X1=A, X2=B)\n\n        d_hs = -ab + 0.5 * (aa + bb)\n        return 1 - d_hs\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n\n    optimizer = optax.adam\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        A,\n        B,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    self.params_[\"examples_-1\"] = A\n    self.params_[\"examples_+1\"] = B\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels.</p> PARAMETER DESCRIPTION <code>n_features</code> <p>Number of features that the classifier expects</p> <p> TYPE: <code>int</code> </p> <code>classes</code> <p>class labels that the classifier expects</p> <p> TYPE: <code>array - like</code> DEFAULT: <code>None</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_metric_learning.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_features_ = n_features\n    self.n_qubits_ = (\n        self.n_features_ + 1\n    )  # +1 to add constant features as described in the plots\n    self.initialize_params()\n    self.construct_model()\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for batch of data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_metric_learning.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for batch of data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X using a batch of training examples. The examples are stored in the parameter dictionary.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_metric_learning.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X using a batch of training examples.\n    The examples are stored in the parameter dictionary.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    if \"examples_-1\" not in self.params_:\n        raise ValueError(\"Model cannot predict without fitting to data first.\")\n\n    X = self.transform(X)\n\n    max_examples = min(len(self.params_[\"examples_-1\"]), len(self.params_[\"examples_+1\"]))\n    if self.n_examples_predict &gt; max_examples:\n        warnings.warn(\"n_examples_predict too large, setting to \" + str(max_examples))\n        self.n_examples_predict = max_examples\n\n    A_examples, B_examples = get_batch(\n        self.n_examples_predict,\n        self.params_[\"examples_-1\"],\n        self.params_[\"examples_+1\"],\n        [self.generate_key(), self.generate_key()],\n    )\n\n    predictions = []\n    for x in X:\n        # create list [x, x, x, ...] to get overlaps with A_examples = [a1, a2, a3...] and B_examples\n        x_tiled = jnp.tile(x, (self.n_examples_predict, 1))\n\n        pred_a = jnp.mean(self.chunked_forward(self.params_, A_examples, x_tiled))\n        pred_b = jnp.mean(self.chunked_forward(self.params_, B_examples, x_tiled))\n\n        # normalise to [0,1]\n        predictions.append([pred_a / (pred_a + pred_b), pred_b / (pred_a + pred_b)])\n\n    return jnp.array(predictions)\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_metric_learning.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n            self.scaler.fit(X)\n        X = self.scaler.transform(X)\n    return X * self.scaling\n</code></pre>"},{"location":"api_docs.html#quoptuna.backend.base.pennylane_models.QuanvolutionalNeuralNetwork","title":"QuanvolutionalNeuralNetwork","text":"<pre><code>QuanvolutionalNeuralNetwork(qkernel_shape=2, n_qchannels=1, rand_depth=10, rand_rot=20, threshold=0.0, kernel_shape=3, output_channels=[32, 64], max_vmap=None, jit=True, learning_rate=0.001, max_steps=10000, convergence_interval=0.001, batch_size=32, random_state=42, scaling=1.0, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax-jit'})\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Quanvolutional Neural network described in https://arxiv.org/pdf/1904.04767v1.pdf.</p> <p>The model is an adaptation of the ConvolutionalNeuralNetwork model. The only difference is that the data is preprocessed by a quantum convolutional layer before being fed to the classical CNN. This quantum preprocessing is a fixed, non-trainiable feature map.</p> <p>The feature map consists of the following:</p> <pre><code>1. A step function that converts the input to 0,1 valued. If the value is below `threshold` it is 0,\notherwise it is 1.\n\n2. A 2d convolution layer is a applied to the binarised data. The filter is given by a random quantum\ncircuit acting on `qkernel_shape*qkernel_shape` qubits (a square grid), and there are `n_qchannels` output\nchannels. The scalar output of the filter is given by the number of ones appearing in the output bitstring\nof the circuit with the highest probability to be sampled. We implement the random quantum circuit\nvia PennyLane's `RandomLayers`.\n\n3. The transformed data is fed into a classical CNN of the same form as ConvolutionalNeuralNetwork and\nthe model is equivalent from that point onwards.\n</code></pre> <p>The input data X should have shape (dataset_size,height*width) and will be reshaped to (dataset_size, height, width, 1) in the model. We assume height=width.</p> PARAMETER DESCRIPTION <code>qkernel_shape</code> <p>The size of the quantum filter: a circuit with <code>qkernel_shape*qkernel_shape</code> qubits will be used.</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>n_qchannels</code> <p>The number of output channels in the quanvolutional layer.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>rand_depth</code> <p>The depth of the random circuit in pennylane.RandomLayers</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>rand_rot</code> <p>The number of random rotations in pennylane.RandomLayers</p> <p> TYPE: <code>int</code> DEFAULT: <code>20</code> </p> <code>threshold</code> <p>The threshold that determines the binarisation of the input data. Since we use a StadardScaler this is set to 0.0 as default.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>kernel_shape</code> <p>the size of the kernel used in the CNN. e.g. kernel_shape=3 uses a 3x3 filter</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>output_channels</code> <p>a list of integers specifying the output size of the convolutional layers in the CNN.</p> <p> TYPE: <code>list[int]</code> DEFAULT: <code>[32, 64]</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>convergence_interval</code> <p>The number of loss values to consider to decide convergence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0.001</code> </p> <code>learning_rate</code> <p>Initial learning rate for training.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> <code>max_steps</code> <p>Maximum number of training steps. A warning will be raised if training did not converge.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> <code>batch_size</code> <p>Size of batches used for computing parameter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the key word arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax-jit'}</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>construct_quanvolutional_layer</code> <p>construct the quantum feature map.</p> <code>construct_random_circuit</code> <p>construct a random circuit to be used as a filter in the quanvolutional layer</p> <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quanvolutional_neural_network.py</code> <pre><code>def __init__(\n    self,\n    qkernel_shape=2,\n    n_qchannels=1,\n    rand_depth=10,\n    rand_rot=20,\n    threshold=0.0,\n    kernel_shape=3,\n    output_channels=[32, 64],\n    max_vmap=None,\n    jit=True,\n    learning_rate=0.001,\n    max_steps=10000,\n    convergence_interval=10e-4,\n    batch_size=32,\n    random_state=42,\n    scaling=1.0,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax-jit\"},\n):\n    r\"\"\"\n    Quanvolutional Neural network described in https://arxiv.org/pdf/1904.04767v1.pdf.\n\n    The model is an adaptation of the ConvolutionalNeuralNetwork model. The only difference is that the data is\n    preprocessed by a quantum convolutional layer before being fed to the classical CNN. This quantum preprocessing\n    is a fixed, non-trainiable feature map.\n\n    The feature map consists of the following:\n\n        1. A step function that converts the input to 0,1 valued. If the value is below `threshold` it is 0,\n        otherwise it is 1.\n\n        2. A 2d convolution layer is a applied to the binarised data. The filter is given by a random quantum\n        circuit acting on `qkernel_shape*qkernel_shape` qubits (a square grid), and there are `n_qchannels` output\n        channels. The scalar output of the filter is given by the number of ones appearing in the output bitstring\n        of the circuit with the highest probability to be sampled. We implement the random quantum circuit\n        via PennyLane's `RandomLayers`.\n\n        3. The transformed data is fed into a classical CNN of the same form as ConvolutionalNeuralNetwork and\n        the model is equivalent from that point onwards.\n\n    The input data X should have shape (dataset_size,height*width) and will be reshaped to\n    (dataset_size, height, width, 1) in the model. We assume height=width.\n\n    Args:\n        qkernel_shape (int): The size of the quantum filter: a circuit with `qkernel_shape*qkernel_shape`\n            qubits will be used.\n        n_qchannels (int): The number of output channels in the quanvolutional layer.\n        rand_depth (int): The depth of the random circuit in pennylane.RandomLayers\n        rand_rot (int): The number of random rotations in pennylane.RandomLayers\n        threshold (float): The threshold that determines the binarisation of the input data. Since we use a\n            StadardScaler this is set to 0.0 as default.\n        kernel_shape (int): the size of the kernel used in the CNN. e.g. kernel_shape=3 uses a 3x3 filter\n        output_channels (list[int]): a list of integers specifying the output size of the convolutional layers\n            in the CNN.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        convergence_interval (int): The number of loss values to consider to decide convergence.\n        learning_rate (float): Initial learning rate for training.\n        max_steps (int): Maximum number of training steps. A warning will be raised if training did not converge.\n        batch_size (int): Size of batches used for computing parameter updates.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'\n        qnode_kwargs (str): the key word arguments passed to the circuit qnode.\n        random_state (int): Seed used for pseudorandom number generation.\n    \"\"\"\n    # attributes that do not depend on data\n    self.learning_rate = learning_rate\n    self.max_steps = max_steps\n    self.convergence_interval = convergence_interval\n    self.n_qchannels = n_qchannels\n    self.rand_depth = rand_depth\n    self.rand_rot = rand_rot\n    self.threshold = threshold\n    self.kernel_shape = kernel_shape\n    self.qkernel_shape = qkernel_shape\n    self.output_channels = output_channels\n    self.batch_size = batch_size\n    self.jit = jit\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.batch_size = batch_size\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # Dictionary containing the trainable parameters\n    self.scaler = None  # data scaler will be fitted on training data\n    self.circuit = None\n</code></pre> <code></code> construct_quanvolutional_layer <pre><code>construct_quanvolutional_layer()\n</code></pre> <p>construct the quantum feature map.</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quanvolutional_neural_network.py</code> <pre><code>def construct_quanvolutional_layer(self):\n    \"\"\"\n    construct the quantum feature map.\n    \"\"\"\n    random_circuits = [self.construct_random_circuit() for __ in range(self.n_qchannels)]\n\n    # construct an array that specifies the indices of the 'windows' of the image used for the convolution.\n    idx_mat = jnp.array([[(i, j) for j in range(self.width)] for i in range(self.height)])\n    idxs = jnp.array(\n        [\n            idx_mat[j : j + self.qkernel_shape, k : k + self.qkernel_shape]\n            for k in range(self.height - self.qkernel_shape + 1)\n            for j in range(self.height - self.qkernel_shape + 1)\n        ]\n    )\n    idxs = idxs.reshape(len(idxs), -1, 2)\n    zerovec = jnp.zeros(len(idxs[0, :, 0]))  # needed for last axis of NHWC format\n    idxs = jnp.array(\n        [[idxs[i, :, 0], idxs[i, :, 1], zerovec] for i in range(len(idxs))],\n        dtype=int,\n    )\n\n    def quanv_layer(x):\n        \"\"\"\n        A convolutional layer where the filter is given by a random quantum circuit. The layer has a stride of 1.\n        Args:\n            x (jnp.array): input data of shape (n_data, height, width, 1)\n        \"\"\"\n\n        # the windows from the image to be fed into the quantum circuits\n        x_windows = x[idxs[:, 0, :], idxs[:, 1, :], idxs[:, 2, :]]\n\n        layer_out = []\n        for channel in range(self.n_qchannels):\n            out = []\n            # find most likely outputs\n            probs = random_circuits[channel](x_windows)\n            # convert to scalars based on the number of ones in the state vec\n            max_idxs = jnp.argmax(probs, axis=1)\n            state_vecs = [\n                jnp.unravel_index(idx, [2 for __ in range(self.qkernel_shape**2)])\n                for idx in max_idxs\n            ]\n            out = jnp.sum(jnp.array(state_vecs), axis=1)\n            out = jnp.array(out, dtype=\"float64\")\n            # put back to correct shape\n            out = jnp.reshape(\n                out,\n                (\n                    self.height - self.qkernel_shape + 1,\n                    self.width - self.qkernel_shape + 1,\n                ),\n            )\n            layer_out.append(out)\n\n        layer_out = jnp.array(layer_out)\n        layer_out = jnp.moveaxis(\n            layer_out, 0, -1\n        )  # the above is in CHW format, so we switch to WHC\n        return layer_out\n\n    return quanv_layer\n</code></pre> <code></code> construct_random_circuit <pre><code>construct_random_circuit()\n</code></pre> <p>construct a random circuit to be used as a filter in the quanvolutional layer</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quanvolutional_neural_network.py</code> <pre><code>def construct_random_circuit(self):\n    \"\"\"\n    construct a random circuit to be used as a filter in the quanvolutional layer\n    \"\"\"\n    wires = range(self.qkernel_shape**2)\n    dev = qml.device(self.dev_type, wires=wires)\n    weights = (\n        jnp.pi\n        * 2\n        * jnp.array(\n            jax.random.uniform(self.generate_key(), shape=(self.rand_depth, self.rand_rot))\n        )\n    )\n\n    @qml.qnode(dev, **self.qnode_kwargs)\n    def circuit(x):\n        \"\"\"\n        Apply a random circuit and return the probabilities of the output strings.\n        Here we use Pennylane's RandomLayers which deviates slightly from the desciption in the plots,\n        but we expect similar behaviour since they are both random circuit generators.\n        \"\"\"\n        for i in wires:\n            qml.RY(x[i] * jnp.pi, wires=i)\n        qml.RandomLayers(weights, wires=wires)\n        return qml.probs(wires=wires)\n\n    self.circuit = circuit\n\n    if self.jit:\n        circuit = jax.jit(circuit)\n    circuit = chunk_vmapped_fn(jax.vmap(circuit, in_axes=(0)), 0, self.max_vmap)\n    return circuit\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quanvolutional_neural_network.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n\n    self.initialize(X.shape[1], classes=np.unique(y))\n\n    y = jnp.array(y, dtype=int)\n    self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    # quantum feature map the entire dataset for training. Assuming we use more than one epoch of training,\n    # it is more efficient to do this first\n    X = self.batched_quanv_layer(X)\n\n    def loss_fn(params, X, y):\n        \"\"\"\n        this takes the quantum feature mapped data as input and returns the sigmoid binary cross entropy\n        \"\"\"\n        y = jax.nn.relu(y)  # convert to 0,1 labels\n        vals = self.cnn.apply(params, X)[:, 0]\n        loss = jnp.mean(optax.sigmoid_binary_cross_entropy(vals, y))\n        return loss\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n\n    optimizer = optax.adam\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        X,\n        y,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels. Args:     classes (array-like): class labels that the classifier expects</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quanvolutional_neural_network.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n    Args:\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.height = int(jnp.sqrt(n_features))\n    self.width = self.height\n\n    # initialise the model\n    self.quanv_layer = self.construct_quanvolutional_layer()\n    self.batched_quanv_layer = chunk_vmapped_fn(\n        jax.vmap(self.quanv_layer, in_axes=(0)), 0, self.max_vmap\n    )\n    self.cnn = construct_cnn(self.output_channels, self.kernel_shape)\n\n    # create dummy data input to initialise the cnn\n    X0 = jnp.ones(shape=(1, self.height, self.height, 1))\n    X0 = self.batched_quanv_layer(X0)\n    self.initialize_params(X0)\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred</code> <p>Predicted labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quanvolutional_neural_network.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred (np.ndarray): Predicted labels of shape (n_samples,)\n    \"\"\"\n    predictions = self.predict_proba(X)\n    mapped_predictions = jnp.argmax(predictions, axis=1)\n    return jnp.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quanvolutional_neural_network.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    p1 = jax.nn.sigmoid(self.forward(self.params_, X)[:, 0])\n    predictions_2d = jnp.c_[1 - p1, p1]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quanvolutional_neural_network.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n            self.scaler.fit(X)\n        X = self.scaler.transform(X)\n    X = X * self.scaling\n    X = jnp.array(X)\n\n    # put in NHWC format. We assume square images\n    self.height = int(jnp.sqrt(X.shape[1]))\n    self.width = self.height\n    X = jnp.reshape(X, (X.shape[0], self.height, self.width, 1))\n    X = jnp.heaviside(X - self.threshold, 0.0)  # binarise input\n\n    return X\n</code></pre>"},{"location":"api_docs.html#quoptuna.backend.base.pennylane_models.SeparableKernelClassifier","title":"SeparableKernelClassifier","text":"<pre><code>SeparableKernelClassifier(encoding_layers=1, svm=SVC(kernel='precomputed', probability=True), C=1.0, jit=True, random_state=42, scaling=1.0, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax', 'diff_method': None})\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>A kernel model that uses a separable embedding. The embedding consists of layers of fixed single qubit X rotations by an angle :math:<code>pi/4</code> on the Bloch sphere and single qubit Y rotation data encoding gates.</p> <p>The kernel is given by</p> <p>.. math::     k(x,x')=\\vert\\langle 0 \\vert U^\\dagger(x')U(x)\\vert 0 \\rangle\\vert^2</p> PARAMETER DESCRIPTION <code>encoding_layers</code> <p>number of layers in the data encoding circuit.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>svm</code> <p>scikit-learn SVC class object used to fit the model from the kernel matrix.</p> <p> TYPE: <code>SVC</code> DEFAULT: <code>SVC(kernel='precomputed', probability=True)</code> </p> <code>C</code> <p>regularization parameter for the SVC. Lower values imply stronger regularization.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the keyword arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax', 'diff_method': None}</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> METHOD DESCRIPTION <code>fit</code> <p>Fit the model to data X and labels y. Uses sklearn's SVM classifier</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>precompute_kernel</code> <p>compute the kernel matrix relative to data sets X1 and X2</p> <code>predict</code> <p>Predict labels for data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/separable.py</code> <pre><code>def __init__(\n    self,\n    encoding_layers=1,\n    svm=SVC(kernel=\"precomputed\", probability=True),\n    C=1.0,\n    jit=True,\n    random_state=42,\n    scaling=1.0,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax\", \"diff_method\": None},\n):\n    r\"\"\"\n    A kernel model that uses a separable embedding. The embedding consists of layers of fixed single qubit X\n    rotations by an angle :math:`pi/4` on the Bloch sphere and single qubit Y rotation data encoding gates.\n\n    The kernel is given by\n\n    .. math::\n        k(x,x')=\\vert\\langle 0 \\vert U^\\dagger(x')U(x)\\vert 0 \\rangle\\vert^2\n\n\n    Args:\n        encoding_layers (int): number of layers in the data encoding circuit.\n        svm (sklearn.svm.SVC): scikit-learn SVC class object used to fit the model from the kernel matrix.\n        C (float): regularization parameter for the SVC. Lower values imply stronger regularization.\n        jit (bool): Whether to use just in time compilation.\n        random_state (int): Seed used for pseudorandom number generation.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'.\n        qnode_kwargs (str): the keyword arguments passed to the circuit qnode.\n        scaling (float): Factor by which to scale the input data.\n    \"\"\"\n    # attributes that do not depend on data\n    self.encoding_layers = encoding_layers\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.jit = jit\n    self.svm = svm\n    self.C = C\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # Dictionary containing the trainable parameters\n    self.n_qubits_ = None\n    self.scaler = None  # data scaler will be fitted on training data\n    self.circuit = None\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y. Uses sklearn's SVM classifier</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/separable.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y. Uses sklearn's SVM classifier\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n\n    self.svm.random_state = int(\n        jax.random.randint(self.generate_key(), shape=(1,), minval=0, maxval=1000000)\n    )\n\n    self.initialize(X.shape[1], np.unique(y))\n\n    self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    self.params_ = {\"x_train\": X}\n    kernel_matrix = self.precompute_kernel(X, X)\n\n    start = time.time()\n    # we are updating this value here, in case it was\n    # changed after initialising the model\n    self.svm.C = self.C\n    self.svm.fit(kernel_matrix, y)\n    end = time.time()\n    self.training_time_ = end - start\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels.</p> PARAMETER DESCRIPTION <code>n_features</code> <p>Number of features that the classifier expects</p> <p> TYPE: <code>int</code> </p> <code>classes</code> <p>class labels that the classifier expects</p> <p> TYPE: <code>array - like</code> DEFAULT: <code>None</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/separable.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_qubits_ = n_features\n    self.params_ = {}\n    self.construct_circuit()\n</code></pre> <code></code> precompute_kernel <pre><code>precompute_kernel(X1, X2)\n</code></pre> <p>compute the kernel matrix relative to data sets X1 and X2 Args:     X1 (np.array): first dataset of input vectors     X2 (np.array): second dataset of input vectors Returns:     kernel_matrix (np.array): matrix of size (len(X1),len(X2)) with elements K(x_1,x_2)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/separable.py</code> <pre><code>def precompute_kernel(self, X1, X2):\n    \"\"\"\n    compute the kernel matrix relative to data sets X1 and X2\n    Args:\n        X1 (np.array): first dataset of input vectors\n        X2 (np.array): second dataset of input vectors\n    Returns:\n        kernel_matrix (np.array): matrix of size (len(X1),len(X2)) with elements K(x_1,x_2)\n    \"\"\"\n    dim1 = len(X1)\n    dim2 = len(X2)\n\n    # concatenate all pairs of vectors\n    Z = np.array([np.concatenate((X1[i], X2[j])) for i in range(dim1) for j in range(dim2)])\n    self.construct_circuit()\n    kernel_values = [self.forward(z) for z in Z]\n    # reshape the values into the kernel matrix\n    kernel_matrix = np.reshape(kernel_values, (dim1, dim2))\n\n    return kernel_matrix\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred</code> <p>Predicted labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/separable.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred (np.ndarray): Predicted labels of shape (n_samples,)\n    \"\"\"\n    X = self.transform(X)\n    kernel_matrix = self.precompute_kernel(X, self.params_[\"x_train\"])\n    return self.svm.predict(kernel_matrix)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X. note that this may be inconsistent with predict; see the sklearn docummentation for details.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/separable.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n    note that this may be inconsistent with predict; see the sklearn docummentation for details.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    if \"x_train\" not in self.params_:\n        raise ValueError(\"Model cannot predict without fitting to data first.\")\n\n    X = self.transform(X)\n    kernel_matrix = self.precompute_kernel(X, self.params_[\"x_train\"])\n    return self.svm.predict_proba(kernel_matrix)\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/separable.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n            self.scaler.fit(X)\n        X = self.scaler.transform(X)\n    return X * self.scaling\n</code></pre>"},{"location":"api_docs.html#quoptuna.backend.base.pennylane_models.SeparableVariationalClassifier","title":"SeparableVariationalClassifier","text":"<pre><code>SeparableVariationalClassifier(encoding_layers=1, learning_rate=0.001, batch_size=32, max_vmap=None, jit=True, max_steps=10000, random_state=42, scaling=1.0, convergence_interval=200, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax'})\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Variational model that uses only separable operations (i.e. there is no entanglement in the model). The circuit consists of layers of encoding gates and parameterised unitaries followed by measurement of an observable.</p> <p>Each encoding layer consists of a trainiable arbitrary qubit rotation on each qubit followed by a product angle embedding of the input data, using RY gates. A final layer of trainable qubit rotations is applied at the end of the circuit.</p> <p>The obserable O is the mean value of Pauli Z observables on each of the output qubits. The value of this observable is used to predict the probability for class 1 as :math:<code>P(+1)=\\sigma(6\\langle O \\rangle)</code> where :math<code>\\sigma</code> is the logistic funciton. The model is then fit using the cross entropy loss.</p> PARAMETER DESCRIPTION <code>encoding_layers</code> <p>number of layers in the data encoding circuit.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>learning_rate</code> <p>learning rate for gradient descent.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> <code>batch_size</code> <p>Size of batches used for computing parameter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>convergence_interval</code> <p>The number of loss values to consider to decide convergence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>max_steps</code> <p>Maximum number of training steps. A warning will be raised if training did not converge.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the keyword arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax'}</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/separable.py</code> <pre><code>def __init__(\n    self,\n    encoding_layers=1,\n    learning_rate=0.001,\n    batch_size=32,\n    max_vmap=None,\n    jit=True,\n    max_steps=10000,\n    random_state=42,\n    scaling=1.0,\n    convergence_interval=200,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax\"},\n):\n    r\"\"\"\n    Variational model that uses only separable operations (i.e. there is no entanglement in the model). The circuit\n    consists of layers of encoding gates and parameterised unitaries followed by measurement of an observable.\n\n    Each encoding layer consists of a trainiable arbitrary qubit rotation on each qubit followed by\n    a product angle embedding of the input data, using RY gates. A final layer of trainable qubit rotations is\n    applied at the end of the circuit.\n\n    The obserable O is the mean value of Pauli Z observables on each of the output qubits. The value of this\n    observable is used to predict the probability for class 1 as :math:`P(+1)=\\sigma(6\\langle O \\rangle)`\n    where :math`\\sigma` is the logistic funciton. The model is then fit using the cross entropy loss.\n\n    Args:\n        encoding_layers (int): number of layers in the data encoding circuit.\n        learning_rate (float): learning rate for gradient descent.\n        batch_size (int): Size of batches used for computing parameter updates.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        convergence_interval (int): The number of loss values to consider to decide convergence.\n        max_steps (int): Maximum number of training steps. A warning will be raised if training did not converge.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'.\n        qnode_kwargs (str): the keyword arguments passed to the circuit qnode.\n        random_state (int): Seed used for pseudorandom number generation\n    \"\"\"\n    # attributes that do not depend on data\n    self.encoding_layers = encoding_layers\n    self.learning_rate = learning_rate\n    self.batch_size = batch_size\n    self.max_steps = max_steps\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.jit = jit\n    self.convergence_interval = convergence_interval\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # Dictionary containing the trainable parameters\n    self.n_qubits_ = None\n    self.scaler = None  # data scaler will be fitted on training data\n    self.circuit = None\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/separable.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n    self.initialize(X.shape[1], np.unique(y))\n\n    self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    optimizer = optax.adam\n\n    def loss_fn(params, X, y):\n        # we multiply by 6 because a relevant domain of the sigmoid function is [-6,6]\n        vals = self.forward(params, X) * 6\n        y = jax.nn.relu(y)  # convert to 0,1\n        return jnp.mean(optax.sigmoid_binary_cross_entropy(vals, y))\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        X,\n        y,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels.</p> PARAMETER DESCRIPTION <code>n_features</code> <p>Number of features that the classifier expects</p> <p> TYPE: <code>int</code> </p> <code>classes</code> <p>class labels that the classifier expects</p> <p> TYPE: <code>array - like</code> DEFAULT: <code>None</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/separable.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_qubits_ = n_features\n    self.initialize_params()\n    self.construct_model()\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred</code> <p>Predicted labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/separable.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred (np.ndarray): Predicted labels of shape (n_samples,)\n    \"\"\"\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/separable.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    predictions = self.chunked_forward(self.params_, X)\n    predictions_2d = np.c_[(1 - predictions) / 2, (1 + predictions) / 2]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/separable.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n            self.scaler.fit(X)\n        X = self.scaler.transform(X)\n    return X * self.scaling\n</code></pre>"},{"location":"api_docs.html#quoptuna.backend.base.pennylane_models.TreeTensorClassifier","title":"TreeTensorClassifier","text":"<pre><code>TreeTensorClassifier(learning_rate=0.01, batch_size=32, max_steps=10000, convergence_interval=200, random_state=42, max_vmap=None, jit=True, scaling=1.0, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax-jit'})\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Tree tensor network classifier from https://arxiv.org/abs/2011.06258v2 (see figure 1)</p> <p>This is a variational model where data is amplitude embedded and a trainiable circuit with a tree like structure is used for prediction. Due to the tree structure, the number of qubits must be a power of 2.</p> <p>In the plots, the data encoding state for a given input x is approximated  using a variational circuit. In practice, this means one has to train an encoding circuit for every training and test input. Since this is very expensive, we just use the exact amplitude encoded state. If the input data dimension is smaller than the state vector dimension, we pad with a value :math:<code>1/2^{n_qubits}</code>.</p> <p>The classification is performed via a Z measurement on the first qubit plus a trainable bias. Training is via the square loss.</p> PARAMETER DESCRIPTION <code>learning_rate</code> <p>Initial learning rate for gradient descent.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.01</code> </p> <code>batch_size</code> <p>Size of batches used for computing parameter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>convergence_interval</code> <p>The number of loss values to consider to decide convergence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>max_steps</code> <p>Maximum number of training steps. A warning will be raised if training did not converge.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the key word arguments passed to the circuit qnode</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax-jit'}</code> </p> METHOD DESCRIPTION <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>The feature vectors padded to the next power of 2 and then normalised.</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/tree_tensor.py</code> <pre><code>def __init__(\n    self,\n    learning_rate=0.01,\n    batch_size=32,\n    max_steps=10000,\n    convergence_interval=200,\n    random_state=42,\n    max_vmap=None,\n    jit=True,\n    scaling=1.0,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax-jit\"},\n):\n    r\"\"\"\n    Tree tensor network classifier from https://arxiv.org/abs/2011.06258v2 (see figure 1)\n\n    This is a variational model where data is amplitude embedded and a trainiable circuit with a tree like\n    structure is used for prediction. Due to the tree structure, the number of qubits must be a power of 2.\n\n    In the plots, the data encoding state for a given input x is approximated  using a variational circuit.\n    In practice, this means one has to train an encoding circuit for every training and test\n    input. Since this is very expensive, we just use the exact amplitude encoded state. If the input data\n    dimension is smaller than the state vector dimension, we pad with a value :math:`1/2^{n_qubits}`.\n\n    The classification is performed via a Z measurement on the first qubit plus a trainable bias. Training\n    is via the square loss.\n\n    Args:\n        learning_rate (float): Initial learning rate for gradient descent.\n        batch_size (int): Size of batches used for computing parameter updates.\n        convergence_interval (int): The number of loss values to consider to decide convergence.\n        max_steps (int): Maximum number of training steps. A warning will be raised if training did not converge.\n        random_state (int): Seed used for pseudorandom number generation\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        scaling (float): Factor by which to scale the input data.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'\n        qnode_kwargs (str): the key word arguments passed to the circuit qnode\n    \"\"\"\n\n    # attributes that do not depend on data\n    self.learning_rate = learning_rate\n    self.max_steps = max_steps\n    self.convergence_interval = convergence_interval\n    self.batch_size = batch_size\n    self.dev_type = dev_type\n    self.jit = jit\n    self.qnode_kwargs = qnode_kwargs\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # Dictionary containing the trainable parameters\n    self.circuit = None\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/tree_tensor.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n    self.initialize(X.shape[1], classes=np.unique(y))\n\n    X = self.transform(X)\n\n    def loss_fn(params, X, y):\n        # square loss\n        predictions = self.forward(params, X)\n        return jnp.mean((predictions - y) ** 2)\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    optimizer = optax.adam\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        X,\n        y,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels.</p> PARAMETER DESCRIPTION <code>n_features</code> <p>Number of features that the classifier expects</p> <p> TYPE: <code>int</code> </p> <code>classes</code> <p>class labels that the classifier expects</p> <p> TYPE: <code>array - like</code> DEFAULT: <code>None</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/tree_tensor.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    if n_features == 1:\n        self.n_qubits = 1\n    else:\n        n_qubits_ae = int(\n            np.ceil(np.log2(n_features))\n        )  # the num qubits needed to amplitude encode\n        n_qubits = 2 ** int(\n            np.ceil(np.log2(n_qubits_ae))\n        )  # the model needs 2**m qubits, for some m\n        self.n_qubits = n_qubits\n\n    self.initialize_params()\n    self.construct_model()\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred</code> <p>Predicted labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/tree_tensor.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred (np.ndarray): Predicted labels of shape (n_samples,)\n    \"\"\"\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/tree_tensor.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    predictions = self.chunked_forward(self.params_, X)\n    predictions_2d = np.c_[(1 - predictions) / 2, (1 + predictions) / 2]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> <p>The feature vectors padded to the next power of 2 and then normalised.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/tree_tensor.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    The feature vectors padded to the next power of 2 and then normalised.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    n_features = X.shape[1]\n    X = X * self.scaling\n\n    n_qubits_ae = int(np.ceil(np.log2(n_features)))  # the num qubits needed to amplitude encode\n    n_qubits = 2 ** int(\n        np.ceil(np.log2(n_qubits_ae))\n    )  # the model needs 2**m qubits, for some m\n    max_n_features = 2**n_qubits\n    n_padding = max_n_features - n_features\n    padding = np.ones(shape=(len(X), n_padding)) / max_n_features\n\n    X_padded = np.c_[X, padding]\n    X_normalised = np.divide(X_padded, np.expand_dims(np.linalg.norm(X_padded, axis=1), axis=1))\n    return X_normalised\n</code></pre>"},{"location":"api_docs.html#quoptuna.backend.base.pennylane_models.VanillaQNN","title":"VanillaQNN","text":"<pre><code>VanillaQNN(embedding_layers=2, variational_layers=3, learning_rate=0.01, batch_size=32, max_vmap=None, jit=True, max_steps=10000, convergence_threshold=1e-06, random_state=42, scaling=1.0, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax'})\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>A vanilla implementation of a quantum neural network with layer-wise angle embedding and a layered variational circuit.</p> PARAMETER DESCRIPTION <code>embedding_layers</code> <p>number of times to repeat the embedding circuit structure.</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>variational_layers</code> <p>number of layers in the variational part of the circuit.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>learning_rate</code> <p>learning rate for gradient descent.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.01</code> </p> <code>batch_size</code> <p>Number of data points to subsample.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>max_vmap</code> <p>The largest size of vmap used (to control memory)</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>convergence_threshold</code> <p>If loss changes less than this threshold for 10 consecutive steps we stop training.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-06</code> </p> <code>max_steps</code> <p>Maximum number of training steps. A warning will be raised if training did not converge.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the keyword arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax'}</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/vanilla_qnn.py</code> <pre><code>def __init__(\n    self,\n    embedding_layers=2,\n    variational_layers=3,\n    learning_rate=0.01,\n    batch_size=32,\n    max_vmap=None,\n    jit=True,\n    max_steps=10000,\n    convergence_threshold=1e-6,\n    random_state=42,\n    scaling=1.0,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax\"},\n):\n    \"\"\"\n    A vanilla implementation of a quantum neural network with layer-wise angle embedding and a layered\n    variational circuit.\n\n    Args:\n        embedding_layers (int): number of times to repeat the embedding circuit structure.\n        variational_layers (int): number of layers in the variational part of the circuit.\n        learning_rate (float): learning rate for gradient descent.\n        batch_size (int): Number of data points to subsample.\n        max_vmap (int): The largest size of vmap used (to control memory)\n        jit (bool): Whether to use just in time compilation.\n        convergence_threshold (float): If loss changes less than this threshold for 10 consecutive steps we stop training.\n        max_steps (int): Maximum number of training steps. A warning will be raised if training did not converge.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'.\n        qnode_kwargs (str): the keyword arguments passed to the circuit qnode.\n        random_state (int): Seed used for pseudorandom number generation.\n    \"\"\"\n\n    # attributes that do not depend on data\n    self.embedding_layers = embedding_layers\n    self.variational_layers = variational_layers\n    self.learning_rate = learning_rate\n    self.batch_size = batch_size\n    self.max_steps = max_steps\n    self.convergence_threshold = convergence_threshold\n    self.batch_size = batch_size\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.jit = jit\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # Dictionary containing the trainable parameters\n    self.n_qubits_ = None\n    self.scaler = None  # data scaler will be fitted on training data\n    self.circuit = None\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/vanilla_qnn.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n\n    self.initialize(n_features=X.shape[1], classes=np.unique(y))\n\n    self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    optimizer = optax.adam\n\n    def loss_fn(params, X, y):\n        # we multiply by 6 because a relevant domain of the sigmoid function is [-6,6]\n        vals = self.forward(params, X) * 6\n        y = jax.nn.relu(y)  # convert to 0,1\n        return jnp.mean(optax.sigmoid_binary_cross_entropy(vals, y))\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    self.params_ = train(self, loss_fn, optimizer, X, y, self.generate_key)\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels.</p> PARAMETER DESCRIPTION <code>n_features</code> <p>Number of features that the classifier expects</p> <p> TYPE: <code>int</code> </p> <code>classes</code> <p>class labels that the classifier expects</p> <p> TYPE: <code>array - like</code> DEFAULT: <code>None</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/vanilla_qnn.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_features = n_features\n    self.n_qubits_ = n_features\n    self.initialize_params()\n    self.construct_model()\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred</code> <p>Predicted labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/vanilla_qnn.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred (np.ndarray): Predicted labels of shape (n_samples,)\n    \"\"\"\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/vanilla_qnn.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    predictions = self.chunked_forward(self.params_, X)\n    predictions_2d = np.c_[(1 - predictions) / 2, (1 + predictions) / 2]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/vanilla_qnn.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n            self.scaler.fit(X)\n        X = self.scaler.transform(X)\n    return X * self.scaling\n</code></pre>"},{"location":"api_docs.html#quoptuna.backend.base.pennylane_models.WeiNet","title":"WeiNet","text":"<pre><code>WeiNet(filter_name='edge_detect', learning_rate=0.1, max_steps=10000, convergence_interval=200, random_state=42, max_vmap=None, jit=True, scaling=1.0, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax-jit'}, batch_size=32)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Quantum convolutional neural network from https://arxiv.org/abs/2104.06918v3  (see fig 2 of plots)</p> <p>The model has two registers: the ancilliary register and the work register. The ancilliary register is used to parameterise a 4 qubit state which in turn controls a number of unitaries that act on the work register, where the data is encoded via amplitude encoding.</p> <p>The qubits with index -1 and height-1 are then traced out, which is equivalent to a type of pooling. All single and double correlators  and  are measured, and a linear model on these values is used for classification.</p> <p>The plots does not specify the loss: we use the binary cross entropy.</p> <p>The input data X should have shape (dataset_size,height*width) and will be reshaped to (dataset_size,1, height, width) in the model. We assume height=width.</p> <p>Note that in figure 2 of the plots, the Hadamards on the ancilla register have no effect since we trace this register out. The effect of this register is then to simply perform  a classical mixture of the unitaries Q_i on the work register. For simplicity (and to save qubit numbers), we parameterise this distribution via params_['s'] rather than model the ancilla qubits themselves.</p> PARAMETER DESCRIPTION <code>filter_name</code> <p>The classical filter that defines the unitaries Q_i. either 'edge_detect', 'smooth', or 'sharpen'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'edge_detect'</code> </p> <code>learning_rate</code> <p>Initial learning rate for gradient descent.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>convergence_interval</code> <p>The number of loss values to consider to decide convergence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>max_steps</code> <p>Maximum number of training steps. A warning will be raised if training did not converge.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> <code>batch_size</code> <p>Size of batches used for computing parameter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the key word arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax-jit'}</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> METHOD DESCRIPTION <code>construct_models</code> <p>constructs the 9 circuits used for the convolutional layer (Q_k in the plots).</p> <code>construct_unitaries</code> <p>Construct the unitaries V' defined in the plots</p> <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>forward_fn</code> <p>We have taken some shortcuts here compared to the plots description but the result is the same.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>initialize_params</code> <p>initialise the trainable parameters</p> <code>predict</code> <p>Predict labels for data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/weinet.py</code> <pre><code>def __init__(\n    self,\n    filter_name=\"edge_detect\",\n    learning_rate=0.1,\n    max_steps=10000,\n    convergence_interval=200,\n    random_state=42,\n    max_vmap=None,\n    jit=True,\n    scaling=1.0,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax-jit\"},\n    batch_size=32,\n):\n    \"\"\"\n    Quantum convolutional neural network from https://arxiv.org/abs/2104.06918v3  (see fig 2 of plots)\n\n    The model has two registers: the ancilliary register and the work register. The ancilliary register is used to\n    parameterise a 4 qubit state which in turn controls a number of unitaries that act on the work register, where\n    the data is encoded via amplitude encoding.\n\n    The qubits with index -1 and height-1 are then traced out, which is equivalent to a type of pooling. All single\n    and double correlators &lt;Z&gt; and &lt;ZZ&gt; are measured, and a linear model on these values is used for classification.\n\n    The plots does not specify the loss: we use the binary cross entropy.\n\n    The input data X should have shape (dataset_size,height*width) and will be reshaped to\n    (dataset_size,1, height, width) in the model. We assume height=width.\n\n    Note that in figure 2 of the plots, the Hadamards on the ancilla register have no effect since we trace this\n    register out. The effect of this register is then to simply perform  a classical mixture of the unitaries\n    Q_i on the work register. For simplicity (and to save qubit numbers), we parameterise this distribution\n    via params_['s'] rather than model the ancilla qubits themselves.\n\n    Args:\n        filter_name (str): The classical filter that defines the unitaries Q_i. either 'edge_detect', 'smooth', or\n            'sharpen'.\n        learning_rate (float): Initial learning rate for gradient descent.\n        convergence_interval (int): The number of loss values to consider to decide convergence.\n        max_steps (int): Maximum number of training steps. A warning will be raised if training did not converge.\n        batch_size (int): Size of batches used for computing parameter updates.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'.\n        qnode_kwargs (str): the key word arguments passed to the circuit qnode.\n        random_state (int): Seed used for pseudorandom number generation.\n        scaling (float): Factor by which to scale the input data.\n    \"\"\"\n    # attributes that do not depend on data\n    self.learning_rate = learning_rate\n    self.max_steps = max_steps\n    self.filter_name = filter_name\n    self.convergence_interval = convergence_interval\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.scaling = scaling\n    self.batch_size = batch_size\n    self.jit = jit\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n    self.unitaries = []\n\n    if filter_name == \"edge_detect\":\n        self.filter = jnp.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]])\n    elif filter_name == \"smooth\":\n        self.filter = jnp.array([[1, 1, 1], [1, 5, 1], [1, 1, 1]]) / 13\n    elif filter_name == \"sharpen\":\n        self.filter = jnp.array([[-2, -2, -2], [-2, 32, -2], [-2, -2, -2]]) / 16\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.n_qubits_ = None\n    self.params_ = None  # Dictionary containing the trainable parameters\n    self.height_ = None  # height of image data\n    self.width_ = None  # width of image data\n    self.circuit = None\n</code></pre> <code></code> construct_models <pre><code>construct_models()\n</code></pre> <p>constructs the 9 circuits used for the convolutional layer (Q_k in the plots).</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/weinet.py</code> <pre><code>def construct_models(self):\n    \"\"\"\n    constructs the 9 circuits used for the convolutional layer (Q_k in the plots).\n    \"\"\"\n\n    # get the operators that are used for prediction. We don't include the self.height_ qubit or the last\n    # qubit as per the plots, since there are lost in the pooling layer\n    operators = []\n    for i in range(self.n_qubits_ - 1):\n        if i != int(self.n_qubits_ / 2) - 1:\n            operators.append(qml.PauliZ(wires=i))\n            for j in range(i + 1, self.n_qubits_ - 1):\n                if j != int(self.n_qubits_ / 2) - 1:\n                    operators.append(qml.PauliZ(wires=i) @ qml.PauliZ(wires=j))\n    operators.append(qml.Identity(wires=0))\n\n    wires = range(self.n_qubits_)\n    dev = qml.device(self.dev_type, wires=wires)\n    circuits = []\n    for nu in range(3):\n        for mu in range(3):\n\n            @qml.qnode(dev, **self.qnode_kwargs)\n            def circuit(x):\n                qml.AmplitudeEmbedding(\n                    jnp.reshape(x, -1), wires=wires, normalize=True, pad_with=0.0\n                )\n                qml.QubitUnitary(\n                    jnp.kron(self.unitaries[nu][nu], jnp.array(self.unitaries[nu][mu])),\n                    wires=wires,\n                )\n                return [qml.expval(op) for op in operators]\n\n            self.circuit = circuit  # we use the last one of the circuits here as an example\n\n            if self.jit:\n                circuit = jax.jit(circuit)\n            circuits.append(circuit)\n\n    self.circuits = circuits\n</code></pre> <code></code> construct_unitaries <pre><code>construct_unitaries()\n</code></pre> <p>Construct the unitaries V' defined in the plots</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/weinet.py</code> <pre><code>def construct_unitaries(self):\n    \"\"\"\n    Construct the unitaries V' defined in the plots\n    \"\"\"\n    self.unitaries = [[None for __ in range(3)] for __ in range(3)]\n    for mu in range(3):\n        for nu, k in enumerate([-1, 0, 1]):\n            V = np.zeros([self.height_, self.width_])\n            for i in range(self.height_):\n                V[i, (i + k) % self.height_] = self.filter[nu, mu]\n            self.unitaries[nu][mu] = V / self.filter[nu, mu]\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Image data of shape (n_samples, height**2)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/weinet.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Image data of shape (n_samples, height**2)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n\n    self.initialize(X.shape[1], classes=np.unique(y))\n    y = jnp.array(y, dtype=int)\n    X = self.transform(X)\n\n    # initialise the model\n    self.construct_unitaries()\n    self.construct_models()\n    self.forward = jax.vmap(self.forward_fn, in_axes=(None, 0))\n    self.chunked_forward = chunk_vmapped_fn(self.forward, 1, self.max_vmap)\n\n    def loss_fn(params, X, y):\n        # we use the usual cross entropy\n        y = jax.nn.relu(y)  # convert to 0,1 labels\n        vals = self.forward(params, X)\n        loss = jnp.mean(optax.sigmoid_binary_cross_entropy(vals, y))\n        return loss\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    optimizer = optax.adam\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        X,\n        y,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    return self\n</code></pre> <code></code> forward_fn <pre><code>forward_fn(params, x)\n</code></pre> <p>We have taken some shortcuts here compared to the plots description but the result is the same. Since we trace out the ancilla register, the final hadamards in the circuit diagram have no effect, and the process is equivalent to classically sampling one of the unitaries Q_i, parameterised by params['s'].</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/weinet.py</code> <pre><code>def forward_fn(self, params, x):\n    \"\"\"\n    We have taken some shortcuts here compared to the plots description but the result is the same.\n    Since we trace out the ancilla register, the final hadamards in the circuit diagram have no effect, and the process\n    is equivalent to classically sampling one of the unitaries Q_i, parameterised by params['s'].\n    \"\"\"\n    probs = jax.nn.softmax(params[\"s\"])\n    expvals = jnp.array([probs[i] * jnp.array(self.circuits[i](x)).T for i in range(9)])\n    expvals = jnp.sum(expvals, axis=0)\n    out = jnp.sum(params[\"weights\"] * expvals)\n    # out = jax.nn.sigmoid(out)  # convert to a probability\n    # out = jnp.vstack((out, 1 - out)).T  # convert to 'two neurons'\n    # out = jnp.reshape(out, (2))\n    return out\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels.</p> PARAMETER DESCRIPTION <code>n_features</code> <p>Number of features that the classifier expects</p> <p> TYPE: <code>int</code> </p> <code>classes</code> <p>class labels that the classifier expects</p> <p> TYPE: <code>array - like</code> DEFAULT: <code>None</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/weinet.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    im_height = int(jnp.sqrt(n_features))\n    self.n_qubits_ = 2 * ceil(jnp.log2(im_height))\n    self.height_ = 2 ** (self.n_qubits_ // 2)\n    self.width_ = 2 ** (self.n_qubits_ // 2)\n    self.initialize_params()\n    self.construct_unitaries()\n    self.construct_models()\n</code></pre> <code></code> initialize_params <pre><code>initialize_params()\n</code></pre> <p>initialise the trainable parameters</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/weinet.py</code> <pre><code>def initialize_params(self):\n    \"\"\"\n    initialise the trainable parameters\n    \"\"\"\n    # no of expvals that are combined with weights\n    n_expvals = int(\n        self.n_qubits_ - 1 + factorial(self.n_qubits_ - 2) / 2 / factorial(self.n_qubits_ - 4)\n    )\n\n    self.params_ = {\n        \"s\": jax.random.normal(self.generate_key(), shape=(9,)),\n        \"weights\": jax.random.normal(self.generate_key(), shape=(n_expvals,)) / n_expvals,\n    }\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred</code> <p>Predicted labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/weinet.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred (np.ndarray): Predicted labels of shape (n_samples,)\n    \"\"\"\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/weinet.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    p1 = jax.nn.sigmoid(self.chunked_forward(self.params_, X))\n    predictions_2d = jnp.c_[1 - p1, p1]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/weinet.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n\n    # put in NCHW format. We assume square images\n    im_height = int(jnp.sqrt(X.shape[1]))\n    X = jnp.reshape(X, (X.shape[0], 1, im_height, im_height))\n\n    X = self.scaling * X\n\n    padded_X = np.zeros([X.shape[0], X.shape[1], self.height_, self.width_])\n    padded_X[: X.shape[0], : X.shape[1], : X.shape[2], : X.shape[3]] = X\n    return jnp.array(padded_X)\n</code></pre>"},{"location":"api_docs.html#quoptuna.backend.base.pennylane_models.qml_benchmarks","title":"qml_benchmarks","text":"MODULE DESCRIPTION <code>data</code> <p>Module containing data generating functions for classification tasks.</p> <code>hyperparam_search_utils</code> <p>Utility functions for hyperparameter search</p> <code>hyperparameter_settings</code> <p>Hyperparameter settings for all models</p> <code>model_utils</code> <p>Utility functions shared by models.</p> <code>models</code> <p>Module containing models to be used in benchmarks.</p> <code></code> data <p>Module containing data generating functions for classification tasks.</p> MODULE DESCRIPTION <code>bars_and_stripes</code> <code>hidden_manifold</code> <code>hyperplanes</code> <code>linearly_separable</code> <code>mnist</code> <p>Note: Requires the torch, torchvision and keras packages to be installed to download the original data.</p> <code>two_curves</code> FUNCTION DESCRIPTION <code>generate_bars_and_stripes</code> <p>Data generation procedure for 'bars and stripes'.</p> <code>generate_hidden_manifold_model</code> <p>Data generation procedure for the 'hidden manifold model'.</p> <code>generate_hyperplanes_parity</code> <p>Data generation procedure for 'hyperplanes and parity'.</p> <code>generate_linearly_separable</code> <p>Data generation procedure for 'linearly separable'.</p> <code>generate_two_curves</code> <p>Data generation procedure for 'two curves'.</p> <code></code> generate_bars_and_stripes <pre><code>generate_bars_and_stripes(n_samples, height, width, noise_std)\n</code></pre> <p>Data generation procedure for 'bars and stripes'.</p> PARAMETER DESCRIPTION <code>n_samples</code> <p>number of data samples to produce</p> <p> TYPE: <code>int</code> </p> <code>height</code> <p>number of pixels for image height</p> <p> TYPE: <code>int</code> </p> <code>width</code> <p>number of pixels for image width</p> <p> TYPE: <code>int</code> </p> <code>noise_std</code> <p>standard deviation of Gaussian noise added to the pixels</p> <p> TYPE: <code>float</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/data/bars_and_stripes.py</code> <pre><code>def generate_bars_and_stripes(n_samples, height, width, noise_std):\n    \"\"\"Data generation procedure for 'bars and stripes'.\n\n    Args:\n        n_samples (int): number of data samples to produce\n        height (int): number of pixels for image height\n        width (int): number of pixels for image width\n        noise_std (float): standard deviation of Gaussian noise added to the pixels\n    \"\"\"\n    X = np.ones([n_samples, 1, height, width]) * -1\n    y = np.zeros([n_samples])\n\n    for i in range(len(X)):\n        if np.random.rand() &gt; 0.5:\n            rows = np.where(np.random.rand(width) &gt; 0.5)[0]\n            X[i, 0, rows, :] = 1.0\n            y[i] = -1\n        else:\n            columns = np.where(np.random.rand(height) &gt; 0.5)[0]\n            X[i, 0, :, columns] = 1.0\n            y[i] = +1\n        X[i, 0] = X[i, 0] + np.random.normal(0, noise_std, size=X[i, 0].shape)\n\n    return X, y\n</code></pre> <code></code> generate_hidden_manifold_model <pre><code>generate_hidden_manifold_model(n_samples, n_features, manifold_dimension)\n</code></pre> <p>Data generation procedure for the 'hidden manifold model'.</p> PARAMETER DESCRIPTION <code>n_samples</code> <p>number of samples to generate</p> <p> TYPE: <code>int</code> </p> <code>n_features</code> <p>dimension of the data samples</p> <p> TYPE: <code>int</code> </p> <code>manifold_dimension</code> <p>dimension of hidden maniforls</p> <p> TYPE: <code>int</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/data/hidden_manifold.py</code> <pre><code>def generate_hidden_manifold_model(n_samples, n_features, manifold_dimension):\n    \"\"\"Data generation procedure for the 'hidden manifold model'.\n\n    Args:\n        n_samples (int): number of samples to generate\n        n_features (int): dimension of the data samples\n        manifold_dimension (int): dimension of hidden maniforls\n    \"\"\"\n\n    # feature matrix F controls the embedding of the manifold\n    F = np.random.normal(size=(manifold_dimension, n_features))\n\n    # Gaussian matrix samples original inputs from the lower-dimensional manifold\n    C = np.random.normal(size=(n_samples, manifold_dimension), loc=0, scale=1)\n\n    # embed data, adding an element-wise nonlinearity\n    biases = 2 * np.random.uniform(size=(n_features,)) - 1\n    X = nonlinearity(C @ F / np.sqrt(manifold_dimension), biases)\n\n    # define labels via a neural network\n    W = np.random.normal(size=(manifold_dimension, manifold_dimension))\n    v = np.random.normal(size=(manifold_dimension,))\n    y = np.array([neural_net(c, W, v) for c in C])\n\n    # post-process the labels to get balanced classes\n    y = y - np.median(y)\n    y = np.array([-1 if y_ &lt; 0 else 1 for y_ in y])\n    assert len(X[y == 1]) == n_samples // 2\n    assert len(X[y == -1]) == n_samples // 2\n\n    return X, y\n</code></pre> <code></code> generate_hyperplanes_parity <pre><code>generate_hyperplanes_parity(n_samples, n_features, n_hyperplanes, dim_hyperplanes)\n</code></pre> <p>Data generation procedure for 'hyperplanes and parity'.</p> PARAMETER DESCRIPTION <code>n_samples</code> <p>number of samples to generate</p> <p> TYPE: <code>int</code> </p> <code>n_features</code> <p>dimension of the data samples</p> <p> TYPE: <code>int</code> </p> <code>n_hyperplanes</code> <p>number of hyperplanes to use for prediction</p> <p> TYPE: <code>int</code> </p> <code>dim_hyperplanes</code> <p>dimension of space in which hyperplanes are defined</p> <p> TYPE: <code>int</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/data/hyperplanes.py</code> <pre><code>def generate_hyperplanes_parity(n_samples, n_features, n_hyperplanes, dim_hyperplanes):\n    \"\"\"Data generation procedure for 'hyperplanes and parity'.\n\n    Args:\n        n_samples (int): number of samples to generate\n        n_features (int): dimension of the data samples\n        n_hyperplanes (int): number of hyperplanes to use for prediction\n        dim_hyperplanes (int): dimension of space in which\n            hyperplanes are defined\n    \"\"\"\n\n    # define hyperplanes\n    weights = np.random.uniform(size=(n_hyperplanes, dim_hyperplanes))\n    biases = np.random.uniform(size=(n_hyperplanes,))\n\n    # hack: initially create more data than we need,\n    # and then subselect to get balanced classes\n    X = np.random.normal(size=(4 * n_samples, dim_hyperplanes))\n    y = np.array([predict(x, weights, biases) for x in X])\n    A = X[y == 1]\n    B = X[y == -1]\n    assert len(A) &gt;= n_samples // 2\n    assert len(B) &gt;= n_samples // 2\n    X = np.r_[A[: n_samples // 2], B[: n_samples // 2]]\n    y = np.array([-1] * (n_samples // 2) + [1] * (n_samples // 2))\n\n    # embed into feature space by a linear transform\n    emb_W = np.random.uniform(size=(dim_hyperplanes, n_features))\n    X = X @ emb_W\n\n    s = StandardScaler()\n    X = s.fit_transform(X)\n\n    return X, y\n</code></pre> <code></code> generate_linearly_separable <pre><code>generate_linearly_separable(n_samples, n_features, margin)\n</code></pre> <p>Data generation procedure for 'linearly separable'.</p> PARAMETER DESCRIPTION <code>n_samples</code> <p>number of samples to generate</p> <p> TYPE: <code>int</code> </p> <code>n_features</code> <p>dimension of the data samples</p> <p> TYPE: <code>int</code> </p> <code>margin</code> <p>width between hyperplane and closest samples</p> <p> TYPE: <code>float</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/data/linearly_separable.py</code> <pre><code>def generate_linearly_separable(n_samples, n_features, margin):\n    \"\"\"Data generation procedure for 'linearly separable'.\n\n    Args:\n        n_samples (int): number of samples to generate\n        n_features (int): dimension of the data samples\n        margin (float): width between hyperplane and closest samples\n    \"\"\"\n\n    w_true = np.ones(n_features)\n\n    # hack: sample more data than we need randomly from a hypercube\n    X = 2 * np.random.rand(2 * n_samples, n_features) - 1\n\n    # only retain data outside a margin\n    X = [x for x in X if np.abs(np.dot(x, w_true)) &gt; margin]\n    X = X[:n_samples]\n\n    y = [np.dot(x, w_true) for x in X]\n    y = [-1 if y_ &gt; 0 else 1 for y_ in y]\n    return X, y\n</code></pre> <code></code> generate_two_curves <pre><code>generate_two_curves(n_samples, n_features, degree, offset, noise)\n</code></pre> <p>Data generation procedure for 'two curves'.</p> PARAMETER DESCRIPTION <code>n_samples</code> <p>number of samples to generate</p> <p> TYPE: <code>int</code> </p> <code>n_features</code> <p>dimension of the data samples</p> <p> TYPE: <code>int</code> </p> <code>degree</code> <p>maximum degree of Fourier series</p> <p> TYPE: <code>int</code> </p> <code>offset</code> <p>distance between two curves</p> <p> TYPE: <code>float</code> </p> <code>noise</code> <p>standard deviation of Gaussian noise added to curves</p> <p> TYPE: <code>float</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/data/two_curves.py</code> <pre><code>def generate_two_curves(n_samples, n_features, degree, offset, noise):\n    \"\"\"Data generation procedure for 'two curves'.\n\n    Args:\n        n_samples (int): number of samples to generate\n        n_features (int): dimension of the data samples\n        degree (int): maximum degree of Fourier series\n        offset (float): distance between two curves\n        noise (float): standard deviation of Gaussian noise added to curves\n    \"\"\"\n    fourier_coeffs = np.random.uniform(size=(n_features, degree + 1, 2))\n    fourier_coeffs = fourier_coeffs / np.linalg.norm(fourier_coeffs)\n\n    # first manifold\n    A = np.zeros(shape=(n_samples // 2, n_features))\n    for s in range(n_samples // 2):\n        # sample a point on the curve\n        t = np.random.rand()\n        # embed this point\n        # every component is computed by another Fourier series\n        for i in range(n_features):\n            A[s, i] = fourier_series(t, fourier_coeffs[i], degree=degree, noise=noise)\n\n    # second manifold: use same fourier series, plus offset\n    B = np.zeros(shape=(n_samples // 2, n_features))\n    for s in range(n_samples // 2):\n        t = np.random.rand()\n        for i in range(n_features):\n            B[s, i] = fourier_series(t, fourier_coeffs[i], degree=degree, noise=noise)\n    B = np.add(B, offset)\n\n    X = np.r_[A, B]\n    y = np.array([-1] * (n_samples // 2) + [1] * (n_samples // 2))\n\n    s = StandardScaler()\n    X = s.fit_transform(X)\n\n    return X, y\n</code></pre> <code></code> bars_and_stripes FUNCTION DESCRIPTION <code>generate_bars_and_stripes</code> <p>Data generation procedure for 'bars and stripes'.</p> <code></code> generate_bars_and_stripes <pre><code>generate_bars_and_stripes(n_samples, height, width, noise_std)\n</code></pre> <p>Data generation procedure for 'bars and stripes'.</p> PARAMETER DESCRIPTION <code>n_samples</code> <p>number of data samples to produce</p> <p> TYPE: <code>int</code> </p> <code>height</code> <p>number of pixels for image height</p> <p> TYPE: <code>int</code> </p> <code>width</code> <p>number of pixels for image width</p> <p> TYPE: <code>int</code> </p> <code>noise_std</code> <p>standard deviation of Gaussian noise added to the pixels</p> <p> TYPE: <code>float</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/data/bars_and_stripes.py</code> <pre><code>def generate_bars_and_stripes(n_samples, height, width, noise_std):\n    \"\"\"Data generation procedure for 'bars and stripes'.\n\n    Args:\n        n_samples (int): number of data samples to produce\n        height (int): number of pixels for image height\n        width (int): number of pixels for image width\n        noise_std (float): standard deviation of Gaussian noise added to the pixels\n    \"\"\"\n    X = np.ones([n_samples, 1, height, width]) * -1\n    y = np.zeros([n_samples])\n\n    for i in range(len(X)):\n        if np.random.rand() &gt; 0.5:\n            rows = np.where(np.random.rand(width) &gt; 0.5)[0]\n            X[i, 0, rows, :] = 1.0\n            y[i] = -1\n        else:\n            columns = np.where(np.random.rand(height) &gt; 0.5)[0]\n            X[i, 0, :, columns] = 1.0\n            y[i] = +1\n        X[i, 0] = X[i, 0] + np.random.normal(0, noise_std, size=X[i, 0].shape)\n\n    return X, y\n</code></pre> <code></code> hidden_manifold FUNCTION DESCRIPTION <code>generate_hidden_manifold_model</code> <p>Data generation procedure for the 'hidden manifold model'.</p> <code>neural_net</code> <p>Transforms inputs via a single-layer neural network.</p> <code>nonlinearity</code> <p>Element-wise nonlinearity.</p> <code></code> generate_hidden_manifold_model <pre><code>generate_hidden_manifold_model(n_samples, n_features, manifold_dimension)\n</code></pre> <p>Data generation procedure for the 'hidden manifold model'.</p> PARAMETER DESCRIPTION <code>n_samples</code> <p>number of samples to generate</p> <p> TYPE: <code>int</code> </p> <code>n_features</code> <p>dimension of the data samples</p> <p> TYPE: <code>int</code> </p> <code>manifold_dimension</code> <p>dimension of hidden maniforls</p> <p> TYPE: <code>int</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/data/hidden_manifold.py</code> <pre><code>def generate_hidden_manifold_model(n_samples, n_features, manifold_dimension):\n    \"\"\"Data generation procedure for the 'hidden manifold model'.\n\n    Args:\n        n_samples (int): number of samples to generate\n        n_features (int): dimension of the data samples\n        manifold_dimension (int): dimension of hidden maniforls\n    \"\"\"\n\n    # feature matrix F controls the embedding of the manifold\n    F = np.random.normal(size=(manifold_dimension, n_features))\n\n    # Gaussian matrix samples original inputs from the lower-dimensional manifold\n    C = np.random.normal(size=(n_samples, manifold_dimension), loc=0, scale=1)\n\n    # embed data, adding an element-wise nonlinearity\n    biases = 2 * np.random.uniform(size=(n_features,)) - 1\n    X = nonlinearity(C @ F / np.sqrt(manifold_dimension), biases)\n\n    # define labels via a neural network\n    W = np.random.normal(size=(manifold_dimension, manifold_dimension))\n    v = np.random.normal(size=(manifold_dimension,))\n    y = np.array([neural_net(c, W, v) for c in C])\n\n    # post-process the labels to get balanced classes\n    y = y - np.median(y)\n    y = np.array([-1 if y_ &lt; 0 else 1 for y_ in y])\n    assert len(X[y == 1]) == n_samples // 2\n    assert len(X[y == -1]) == n_samples // 2\n\n    return X, y\n</code></pre> <code></code> neural_net <pre><code>neural_net(x, W, v)\n</code></pre> <p>Transforms inputs via a single-layer neural network. Args:     x (ndarray): input of shape (manifold_dimension,)     W (ndarray): input-to-hidden weight matrix of shape (manifold_dimension, manifold_dimension)     v (ndarray): hidden-to-output weight matrix of shape (manifold_dimension,)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/data/hidden_manifold.py</code> <pre><code>def neural_net(x, W, v):\n    \"\"\"Transforms inputs via a single-layer neural network.\n    Args:\n        x (ndarray): input of shape (manifold_dimension,)\n        W (ndarray): input-to-hidden weight matrix of shape (manifold_dimension, manifold_dimension)\n        v (ndarray): hidden-to-output weight matrix of shape (manifold_dimension,)\n    \"\"\"\n    return np.dot(v, np.tanh(W @ x) / np.sqrt(W.shape[0]))\n</code></pre> <code></code> nonlinearity <pre><code>nonlinearity(X, biases)\n</code></pre> <p>Element-wise nonlinearity.</p> PARAMETER DESCRIPTION <code>X</code> <p>inputs of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>biases</code> <p>biases of shape (n_features,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/data/hidden_manifold.py</code> <pre><code>def nonlinearity(X, biases):\n    \"\"\"Element-wise nonlinearity.\n\n    Args:\n        X (ndarray): inputs of shape (n_samples, n_features)\n        biases (ndarray): biases of shape (n_features,)\n    \"\"\"\n    return np.tanh(X - biases)\n</code></pre> <code></code> hyperplanes FUNCTION DESCRIPTION <code>generate_hyperplanes_parity</code> <p>Data generation procedure for 'hyperplanes and parity'.</p> <code>perceptron</code> <p>Transforms inputs according to a perceptron.</p> <code>predict</code> <p>Implements the parity prediction logic.</p> <code></code> generate_hyperplanes_parity <pre><code>generate_hyperplanes_parity(n_samples, n_features, n_hyperplanes, dim_hyperplanes)\n</code></pre> <p>Data generation procedure for 'hyperplanes and parity'.</p> PARAMETER DESCRIPTION <code>n_samples</code> <p>number of samples to generate</p> <p> TYPE: <code>int</code> </p> <code>n_features</code> <p>dimension of the data samples</p> <p> TYPE: <code>int</code> </p> <code>n_hyperplanes</code> <p>number of hyperplanes to use for prediction</p> <p> TYPE: <code>int</code> </p> <code>dim_hyperplanes</code> <p>dimension of space in which hyperplanes are defined</p> <p> TYPE: <code>int</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/data/hyperplanes.py</code> <pre><code>def generate_hyperplanes_parity(n_samples, n_features, n_hyperplanes, dim_hyperplanes):\n    \"\"\"Data generation procedure for 'hyperplanes and parity'.\n\n    Args:\n        n_samples (int): number of samples to generate\n        n_features (int): dimension of the data samples\n        n_hyperplanes (int): number of hyperplanes to use for prediction\n        dim_hyperplanes (int): dimension of space in which\n            hyperplanes are defined\n    \"\"\"\n\n    # define hyperplanes\n    weights = np.random.uniform(size=(n_hyperplanes, dim_hyperplanes))\n    biases = np.random.uniform(size=(n_hyperplanes,))\n\n    # hack: initially create more data than we need,\n    # and then subselect to get balanced classes\n    X = np.random.normal(size=(4 * n_samples, dim_hyperplanes))\n    y = np.array([predict(x, weights, biases) for x in X])\n    A = X[y == 1]\n    B = X[y == -1]\n    assert len(A) &gt;= n_samples // 2\n    assert len(B) &gt;= n_samples // 2\n    X = np.r_[A[: n_samples // 2], B[: n_samples // 2]]\n    y = np.array([-1] * (n_samples // 2) + [1] * (n_samples // 2))\n\n    # embed into feature space by a linear transform\n    emb_W = np.random.uniform(size=(dim_hyperplanes, n_features))\n    X = X @ emb_W\n\n    s = StandardScaler()\n    X = s.fit_transform(X)\n\n    return X, y\n</code></pre> <code></code> perceptron <pre><code>perceptron(x, w, b)\n</code></pre> <p>Transforms inputs according to a perceptron. Args:     x (ndarray): input of shape (dim_hyperplanes,)     w (ndarray): input-to-hidden weight matrix of shape (dim_hyperplanes,)     b (float): bias</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/data/hyperplanes.py</code> <pre><code>def perceptron(x, w, b):\n    \"\"\"Transforms inputs according to a perceptron.\n    Args:\n        x (ndarray): input of shape (dim_hyperplanes,)\n        w (ndarray): input-to-hidden weight matrix of shape (dim_hyperplanes,)\n        b (float): bias\n    \"\"\"\n    if np.dot(w, x) + b &gt; 0:\n        return 1\n    return 0\n</code></pre> <code></code> predict <pre><code>predict(x, weights, biases)\n</code></pre> <p>Implements the parity prediction logic.</p> PARAMETER DESCRIPTION <code>x</code> <p>input of shape (dim_hyperplanes,)</p> <p> TYPE: <code>ndarray</code> </p> <code>weights</code> <p>array of weight vectors defining the orientation of the hyperplanes</p> <p> TYPE: <code>ndarray</code> </p> <code>biases</code> <p>array of biases defining the offset of the hyperplanes</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/data/hyperplanes.py</code> <pre><code>def predict(x, weights, biases):\n    \"\"\"Implements the parity prediction logic.\n\n    Args:\n        x (ndarray): input of shape (dim_hyperplanes,)\n        weights (ndarray): array of weight vectors defining\n            the orientation of the hyperplanes\n        biases (ndarray): array of biases defining\n            the offset of the hyperplanes\n    \"\"\"\n    preds = [perceptron(x, w, b) for w, b in zip(weights, biases)]\n    n_ones = np.sum(preds)\n    if n_ones % 2 == 0:\n        return 1\n    return -1\n</code></pre> <code></code> linearly_separable FUNCTION DESCRIPTION <code>generate_linearly_separable</code> <p>Data generation procedure for 'linearly separable'.</p> <code></code> generate_linearly_separable <pre><code>generate_linearly_separable(n_samples, n_features, margin)\n</code></pre> <p>Data generation procedure for 'linearly separable'.</p> PARAMETER DESCRIPTION <code>n_samples</code> <p>number of samples to generate</p> <p> TYPE: <code>int</code> </p> <code>n_features</code> <p>dimension of the data samples</p> <p> TYPE: <code>int</code> </p> <code>margin</code> <p>width between hyperplane and closest samples</p> <p> TYPE: <code>float</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/data/linearly_separable.py</code> <pre><code>def generate_linearly_separable(n_samples, n_features, margin):\n    \"\"\"Data generation procedure for 'linearly separable'.\n\n    Args:\n        n_samples (int): number of samples to generate\n        n_features (int): dimension of the data samples\n        margin (float): width between hyperplane and closest samples\n    \"\"\"\n\n    w_true = np.ones(n_features)\n\n    # hack: sample more data than we need randomly from a hypercube\n    X = 2 * np.random.rand(2 * n_samples, n_features) - 1\n\n    # only retain data outside a margin\n    X = [x for x in X if np.abs(np.dot(x, w_true)) &gt; margin]\n    X = X[:n_samples]\n\n    y = [np.dot(x, w_true) for x in X]\n    y = [-1 if y_ &gt; 0 else 1 for y_ in y]\n    return X, y\n</code></pre> <code></code> mnist <p>Note: Requires the torch, torchvision and keras packages to be installed to download the original data. Since these are large they are not listed as dependencies of this repo.</p> <p>The return type differs from other data generators since we want to reproduce the original MNIST train/test split.</p> <code></code> two_curves FUNCTION DESCRIPTION <code>fourier_series</code> <p>Fourier series of input t.</p> <code>generate_two_curves</code> <p>Data generation procedure for 'two curves'.</p> <code></code> fourier_series <pre><code>fourier_series(t, coeffs, degree=5, noise=0.0)\n</code></pre> <p>Fourier series of input t.</p> PARAMETER DESCRIPTION <code>t</code> <p>scalar input</p> <p> TYPE: <code>float</code> </p> <code>coeffs</code> <p>coefficient tensor of dimension ()</p> <p> TYPE: <code>ndarray</code> </p> <code>degree</code> <p>maximum degree of Fourier series</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> <code>noise</code> <p>standard deviation of Gaussian noise added to output</p> <p> TYPE: <code>flaot</code> DEFAULT: <code>0.0</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/data/two_curves.py</code> <pre><code>def fourier_series(t, coeffs, degree=5, noise=0.00):\n    \"\"\"Fourier series of input t.\n\n    Args:\n        t (float): scalar input\n        coeffs (ndarray): coefficient tensor of dimension ()\n        degree (int): maximum degree of Fourier series\n        noise (flaot): standard deviation of Gaussian noise added to output\n    \"\"\"\n    scaling = 0.5 * 2 * np.pi\n    res = coeffs[0, 0] + coeffs[0, 1]\n    for frequency in range(1, degree + 1):\n        res += coeffs[frequency, 0] * np.cos(frequency * scaling * t) + coeffs[\n            frequency, 1\n        ] * np.sin(frequency * scaling * t)\n    return res + np.random.normal(loc=0, scale=noise)\n</code></pre> <code></code> generate_two_curves <pre><code>generate_two_curves(n_samples, n_features, degree, offset, noise)\n</code></pre> <p>Data generation procedure for 'two curves'.</p> PARAMETER DESCRIPTION <code>n_samples</code> <p>number of samples to generate</p> <p> TYPE: <code>int</code> </p> <code>n_features</code> <p>dimension of the data samples</p> <p> TYPE: <code>int</code> </p> <code>degree</code> <p>maximum degree of Fourier series</p> <p> TYPE: <code>int</code> </p> <code>offset</code> <p>distance between two curves</p> <p> TYPE: <code>float</code> </p> <code>noise</code> <p>standard deviation of Gaussian noise added to curves</p> <p> TYPE: <code>float</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/data/two_curves.py</code> <pre><code>def generate_two_curves(n_samples, n_features, degree, offset, noise):\n    \"\"\"Data generation procedure for 'two curves'.\n\n    Args:\n        n_samples (int): number of samples to generate\n        n_features (int): dimension of the data samples\n        degree (int): maximum degree of Fourier series\n        offset (float): distance between two curves\n        noise (float): standard deviation of Gaussian noise added to curves\n    \"\"\"\n    fourier_coeffs = np.random.uniform(size=(n_features, degree + 1, 2))\n    fourier_coeffs = fourier_coeffs / np.linalg.norm(fourier_coeffs)\n\n    # first manifold\n    A = np.zeros(shape=(n_samples // 2, n_features))\n    for s in range(n_samples // 2):\n        # sample a point on the curve\n        t = np.random.rand()\n        # embed this point\n        # every component is computed by another Fourier series\n        for i in range(n_features):\n            A[s, i] = fourier_series(t, fourier_coeffs[i], degree=degree, noise=noise)\n\n    # second manifold: use same fourier series, plus offset\n    B = np.zeros(shape=(n_samples // 2, n_features))\n    for s in range(n_samples // 2):\n        t = np.random.rand()\n        for i in range(n_features):\n            B[s, i] = fourier_series(t, fourier_coeffs[i], degree=degree, noise=noise)\n    B = np.add(B, offset)\n\n    X = np.r_[A, B]\n    y = np.array([-1] * (n_samples // 2) + [1] * (n_samples // 2))\n\n    s = StandardScaler()\n    X = s.fit_transform(X)\n\n    return X, y\n</code></pre> <code></code> hyperparam_search_utils <p>Utility functions for hyperparameter search</p> FUNCTION DESCRIPTION <code>construct_hyperparameter_grid</code> <p>Constructs a grid of hyperparameters from the dictionary of hyperparameter</p> <code>csv_to_dict</code> <p>Read a csv file and interpret the content as a dictionary.</p> <code>read_data</code> <p>Read data from a csv file where each row is a data sample.</p> <code></code> construct_hyperparameter_grid <pre><code>construct_hyperparameter_grid(hyperparameter_settings, classifier_name)\n</code></pre> <p>Constructs a grid of hyperparameters from the dictionary of hyperparameter settings for a given classifier.</p> PARAMETER DESCRIPTION <code>hyperparameter_settings</code> <p>a dictionary of hyperparameter settings</p> <p> TYPE: <code>dict</code> </p> <code>classifier_name</code> <p>classifier name</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>hyperparameter_grid</code> <p>A grid of hyperparameters to search</p> <p> TYPE: <code>dict</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/hyperparam_search_utils.py</code> <pre><code>def construct_hyperparameter_grid(hyperparameter_settings, classifier_name):\n    \"\"\"Constructs a grid of hyperparameters from the dictionary of hyperparameter\n    settings for a given classifier.\n\n    Args:\n        hyperparameter_settings (dict): a dictionary of hyperparameter settings\n        classifier_name (str): classifier name\n\n    Returns:\n        hyperparameter_grid (dict): A grid of hyperparameters to search\n    \"\"\"\n    hyperparams = hyperparameter_settings[classifier_name].keys()\n    hyperparameter_grid = {}\n\n    for hyperparam in hyperparams:\n        if hyperparameter_settings[classifier_name][hyperparam][\"type\"] == \"list\":\n            val = hyperparameter_settings[classifier_name][hyperparam][\"val\"]\n            dtype = hyperparameter_settings[classifier_name][hyperparam][\"dtype\"]\n            if dtype == \"tuple\":\n                hyperparameter_grid[hyperparam] = [eval(v) for v in val]\n            else:\n                hyperparameter_grid[hyperparam] = np.array(val, dtype=dtype)\n\n    return hyperparameter_grid\n</code></pre> <code></code> csv_to_dict <pre><code>csv_to_dict(file_path)\n</code></pre> <p>Read a csv file and interpret the content as a dictionary.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>path to csv file</p> <p> TYPE: <code>str</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/hyperparam_search_utils.py</code> <pre><code>def csv_to_dict(file_path):\n    \"\"\"Read a csv file and interpret the content as a dictionary.\n\n    Args:\n        file_path (str): path to csv file\n    \"\"\"\n    dict = {}\n    with open(file_path) as csvfile:\n        csvreader = csv.reader(csvfile)\n        # Skip the first line\n        next(csvreader)\n        for row in csvreader:\n            hyperparameter, value = row\n            # Check if the value is numeric and convert it to int or float accordingly\n            try:\n                if \".\" in value:\n                    value = float(value)\n                else:\n                    value = int(value)\n            except ValueError:\n                pass  # If conversion is not possible, keep the value as a string\n            dict[hyperparameter] = value\n    return dict\n</code></pre> <code></code> read_data <pre><code>read_data(path)\n</code></pre> <p>Read data from a csv file where each row is a data sample. The columns are the input features and the last column specifies a label.</p> <p>Return a 2-d array of inputs and an array of labels, X,y.</p> PARAMETER DESCRIPTION <code>path</code> <p>path to data</p> <p> TYPE: <code>str</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/hyperparam_search_utils.py</code> <pre><code>def read_data(path):\n    \"\"\"Read data from a csv file where each row is a data sample.\n    The columns are the input features and the last column specifies a label.\n\n    Return a 2-d array of inputs and an array of labels, X,y.\n\n    Args:\n        path (str): path to data\n    \"\"\"\n    # The data is stored on a CSV file with the last column being the label\n    data = pd.read_csv(path, header=None)\n    X = data.iloc[:, :-1].values\n    y = data.iloc[:, -1].values\n    return X, y\n</code></pre> <code></code> hyperparameter_settings <p>Hyperparameter settings for all models</p> <code></code> model_utils <p>Utility functions shared by models.</p> FUNCTION DESCRIPTION <code>chunk_grad</code> <p>Convert a <code>jax.grad</code> function to an equivalent version that evaluated in chunks of size max_vmap.</p> <code>chunk_loss</code> <p>Converts a loss function of the form <code>loss_fn(params, array1, array2)</code> to an equivalent version that</p> <code>chunk_vmapped_fn</code> <p>Convert a vmapped function to an equivalent function that evaluates in chunks of size</p> <code>get_batch</code> <p>A generator to get random batches of the data (X, y)</p> <code>get_from_dict</code> <p>Access a value from a nested dictionary.</p> <code>get_nested_keys</code> <p>Returns the nested keys of a nested dictionary.</p> <code>set_in_dict</code> <p>Set a value in a nested dictionary.</p> <code>train</code> <p>Trains a model using an optimizer and a loss function via gradient descent. We assume that the loss function</p> <code></code> chunk_grad <pre><code>chunk_grad(grad_fn, max_vmap)\n</code></pre> <p>Convert a <code>jax.grad</code> function to an equivalent version that evaluated in chunks of size max_vmap.</p> <p><code>grad_fn</code> should be of the form <code>jax.grad(fn(params, X, y), argnums=0)</code>, where <code>params</code> is a dictionary of <code>jnp.arrays</code>, <code>X, y</code> are <code>jnp.arrays</code> with the same-size leading axis, and <code>grad_fn</code> is a function that is vectorised along these axes (i.e. <code>in_axes = (None,0,0)</code>).</p> <p>The returned function evaluates the original function by splitting the batch evaluation into smaller chunks of size <code>max_vmap</code>, and has a lower memory footprint.</p> PARAMETER DESCRIPTION <code>model</code> <p>gradient function with the functional form jax.grad(loss(params, X,y), argnums=0)</p> <p> TYPE: <code>func</code> </p> <code>max_vmap</code> <p>the size of the chunks</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <p>chunked version of the function</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def chunk_grad(grad_fn, max_vmap):\n    \"\"\"\n    Convert a `jax.grad` function to an equivalent version that evaluated in chunks of size max_vmap.\n\n    `grad_fn` should be of the form `jax.grad(fn(params, X, y), argnums=0)`, where `params` is a\n    dictionary of `jnp.arrays`, `X, y` are `jnp.arrays` with the same-size leading axis, and `grad_fn`\n    is a function that is vectorised along these axes (i.e. `in_axes = (None,0,0)`).\n\n    The returned function evaluates the original function by splitting the batch evaluation into smaller chunks\n    of size `max_vmap`, and has a lower memory footprint.\n\n    Args:\n        model (func): gradient function with the functional form jax.grad(loss(params, X,y), argnums=0)\n        max_vmap (int): the size of the chunks\n\n    Returns:\n        chunked version of the function\n    \"\"\"\n\n    def chunked_grad(params, X, y):\n        batch_slices = list(gen_batches(len(X), max_vmap))\n        grads = [grad_fn(params, X[slice], y[slice]) for slice in batch_slices]\n        grad_dict = {}\n        for key_list in get_nested_keys(params):\n            set_in_dict(\n                grad_dict,\n                key_list,\n                jnp.mean(jnp.array([get_from_dict(grad, key_list) for grad in grads]), axis=0),\n            )\n        return grad_dict\n\n    return chunked_grad\n</code></pre> <code></code> chunk_loss <pre><code>chunk_loss(loss_fn, max_vmap)\n</code></pre> <p>Converts a loss function of the form <code>loss_fn(params, array1, array2)</code> to an equivalent version that evaluates <code>loss_fn</code> in chunks of size max_vmap. <code>loss_fn</code> should batch evaluate along the leading axis of <code>array1, array2</code> (i.e. <code>in_axes = (None,0,0)</code>).</p> PARAMETER DESCRIPTION <code>loss_fn</code> <p>function of form loss_fn(params, array1, array2)</p> <p> TYPE: <code>func</code> </p> <code>max_vmap</code> <p>maximum chunk size</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <p>chunked version of the function</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def chunk_loss(loss_fn, max_vmap):\n    \"\"\"\n    Converts a loss function of the form `loss_fn(params, array1, array2)` to an equivalent version that\n    evaluates `loss_fn` in chunks of size max_vmap. `loss_fn` should batch evaluate along the leading\n    axis of `array1, array2` (i.e. `in_axes = (None,0,0)`).\n\n    Args:\n        loss_fn (func): function of form loss_fn(params, array1, array2)\n        max_vmap (int): maximum chunk size\n\n    Returns:\n        chunked version of the function\n    \"\"\"\n\n    def chunked_loss(params, X, y):\n        batch_slices = list(gen_batches(len(X), max_vmap))\n        res = jnp.array([loss_fn(params, *[X[slice], y[slice]]) for slice in batch_slices])\n        return jnp.mean(res)\n\n    return chunked_loss\n</code></pre> <code></code> chunk_vmapped_fn <pre><code>chunk_vmapped_fn(vmapped_fn, start, max_vmap)\n</code></pre> <p>Convert a vmapped function to an equivalent function that evaluates in chunks of size max_vmap. The behaviour of chunked_fn should be the same as vmapped_fn, but with a lower memory cost.</p> <p>The input vmapped_fn should have in_axes = (None, None, ..., 0,0,...,0)</p> PARAMETER DESCRIPTION <code>vmapped</code> <p>vmapped function with in_axes = (None, None, ..., 0,0,...,0)</p> <p> TYPE: <code>func</code> </p> <code>start</code> <p>The index where the first 0 appears in in_axes</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <p>chunked version of the function</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def chunk_vmapped_fn(vmapped_fn, start, max_vmap):\n    \"\"\"\n    Convert a vmapped function to an equivalent function that evaluates in chunks of size\n    max_vmap. The behaviour of chunked_fn should be the same as vmapped_fn, but with a\n    lower memory cost.\n\n    The input vmapped_fn should have in_axes = (None, None, ..., 0,0,...,0)\n\n    Args:\n        vmapped (func): vmapped function with in_axes = (None, None, ..., 0,0,...,0)\n        start (int): The index where the first 0 appears in in_axes\n        max_vmap (int) The max chunk size with which to evaluate the function\n\n    Returns:\n        chunked version of the function\n    \"\"\"\n\n    def chunked_fn(*args):\n        batch_len = len(args[start])\n        batch_slices = list(gen_batches(batch_len, max_vmap))\n        res = [\n            vmapped_fn(*args[:start], *[arg[slice] for arg in args[start:]])\n            for slice in batch_slices\n        ]\n        # jnp.concatenate needs to act on arrays with the same shape, so pad the last array if necessary\n        if batch_len / max_vmap % 1 != 0.0:\n            diff = max_vmap - len(res[-1])\n            res[-1] = jnp.pad(res[-1], [(0, diff), *[(0, 0)] * (len(res[-1].shape) - 1)])\n            return jnp.concatenate(res)[:-diff]\n        return jnp.concatenate(res)\n\n    return chunked_fn\n</code></pre> <code></code> get_batch <pre><code>get_batch(X, y, rnd_key, batch_size=32)\n</code></pre> <p>A generator to get random batches of the data (X, y)</p> PARAMETER DESCRIPTION <code>X</code> <p>Input data with shape (n_samples, n_features).</p> <p> TYPE: <code>array[float]</code> </p> <code>y</code> <p>Target labels with shape (n_samples,)</p> <p> TYPE: <code>array[float]</code> </p> <code>rnd_key</code> <p>A jax random key object</p> <p> </p> <code>batch_size</code> <p>Number of elements in batch</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> RETURNS DESCRIPTION <p>array[float]: A batch of input data shape (batch_size, n_features)</p> <p>array[float]: A batch of target labels shaped (batch_size,)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def get_batch(X, y, rnd_key, batch_size=32):\n    \"\"\"\n    A generator to get random batches of the data (X, y)\n\n    Args:\n        X (array[float]): Input data with shape (n_samples, n_features).\n        y (array[float]): Target labels with shape (n_samples,)\n        rnd_key: A jax random key object\n        batch_size (int): Number of elements in batch\n\n    Returns:\n        array[float]: A batch of input data shape (batch_size, n_features)\n        array[float]: A batch of target labels shaped (batch_size,)\n    \"\"\"\n    all_indices = jnp.array(range(len(X)))\n    rnd_indices = jax.random.choice(key=rnd_key, a=all_indices, shape=(batch_size,), replace=True)\n    return X[rnd_indices], y[rnd_indices]\n</code></pre> <code></code> get_from_dict <pre><code>get_from_dict(dict, key_list)\n</code></pre> <p>Access a value from a nested dictionary. Inspired by https://stackoverflow.com/questions/14692690/access-nested-dictionary-items-via-a-list-of-keys</p> PARAMETER DESCRIPTION <code>dict</code> <p>nested dictionary</p> <p> TYPE: <code>dict</code> </p> <code>key_list</code> <p>list of keys to be accessed</p> <p> TYPE: <code>list</code> </p> RETURNS DESCRIPTION <p>the requested value</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def get_from_dict(dict, key_list):\n    \"\"\"\n    Access a value from a nested dictionary.\n    Inspired by https://stackoverflow.com/questions/14692690/access-nested-dictionary-items-via-a-list-of-keys\n\n    Args:\n        dict (dict): nested dictionary\n        key_list (list): list of keys to be accessed\n\n    Returns:\n         the requested value\n    \"\"\"\n    return reduce(operator.getitem, key_list, dict)\n</code></pre> <code></code> get_nested_keys <pre><code>get_nested_keys(d, parent_keys=[])\n</code></pre> <p>Returns the nested keys of a nested dictionary.</p> PARAMETER DESCRIPTION <code>d</code> <p>nested dictionary</p> <p> TYPE: <code>dict</code> </p> RETURNS DESCRIPTION <p>list where each element is a list of nested keys</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def get_nested_keys(d, parent_keys=[]):\n    \"\"\"\n    Returns the nested keys of a nested dictionary.\n\n    Args:\n        d (dict): nested dictionary\n\n    Returns:\n        list where each element is a list of nested keys\n    \"\"\"\n    keys_list = []\n    for key, value in d.items():\n        current_keys = parent_keys + [key]\n        if isinstance(value, dict):\n            keys_list.extend(get_nested_keys(value, current_keys))\n        else:\n            keys_list.append(current_keys)\n    return keys_list\n</code></pre> <code></code> set_in_dict <pre><code>set_in_dict(dict, keys, value)\n</code></pre> <p>Set a value in a nested dictionary.</p> PARAMETER DESCRIPTION <code>dict</code> <p>nested dictionary</p> <p> TYPE: <code>dict</code> </p> <code>keys</code> <p>list of keys in nested dictionary</p> <p> TYPE: <code>list</code> </p> <code>value</code> <p>value to be set</p> <p> TYPE: <code>Any</code> </p> RETURNS DESCRIPTION <p>nested dictionary with new value</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def set_in_dict(dict, keys, value):\n    \"\"\"\n    Set a value in a nested dictionary.\n\n    Args:\n        dict (dict): nested dictionary\n        keys (list): list of keys in nested dictionary\n        value (Any): value to be set\n\n    Returns:\n        nested dictionary with new value\n    \"\"\"\n    for key in keys[:-1]:\n        dict = dict.setdefault(key, {})\n    dict[keys[-1]] = value\n</code></pre> <code></code> train <pre><code>train(model, loss_fn, optimizer, X, y, random_key_generator, convergence_interval=200)\n</code></pre> <p>Trains a model using an optimizer and a loss function via gradient descent. We assume that the loss function is of the form <code>loss(params, X, y)</code> and that the trainable parameters are stored in model.params_ as a dictionary of jnp.arrays. The optimizer should be an Optax optimizer (e.g. optax.adam). <code>model</code> must have an attribute <code>learning_rate</code> to set the initial learning rate for the gradient descent.</p> <p>The model is trained until a convergence criterion is met that corresponds to the loss curve being flat over a number of optimization steps given by <code>convergence_inteval</code> (see plots for details).</p> <p>To reduce precompilation time and memory cost, the loss function and gradient functions are evaluated in chunks of size model.max_vmap.</p> PARAMETER DESCRIPTION <code>model</code> <p>Classifier class object to train. Trainable parameters must be stored in model.params_.</p> <p> TYPE: <code>class</code> </p> <code>loss_fn</code> <p>Loss function to be minimised. Must be of the form loss_fn(params, X, y).</p> <p> TYPE: <code>Callable</code> </p> <code>optimizer</code> <p>Optax optimizer (e.g. optax.adam).</p> <p> TYPE: <code>optax optimizer</code> </p> <code>X</code> <p>Input data array of shape (n_samples, n_features)</p> <p> TYPE: <code>array</code> </p> <code>y</code> <p>Array of shape (n_samples) containing the labels.</p> <p> TYPE: <code>array</code> </p> <code>random_key_generator</code> <p>JAX key generator object for pseudo-randomness generation.</p> <p> TYPE: <code>PRNGKey</code> </p> <code>convergence_interval</code> <p>Number of optimization steps over which to decide convergence. Larger values give a higher confidence that the model has converged but may increase training time.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> RETURNS DESCRIPTION <code>params</code> <p>The new parameters after training has completed.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def train(model, loss_fn, optimizer, X, y, random_key_generator, convergence_interval=200):\n    \"\"\"\n    Trains a model using an optimizer and a loss function via gradient descent. We assume that the loss function\n    is of the form `loss(params, X, y)` and that the trainable parameters are stored in model.params_ as a dictionary\n    of jnp.arrays. The optimizer should be an Optax optimizer (e.g. optax.adam). `model` must have an attribute\n    `learning_rate` to set the initial learning rate for the gradient descent.\n\n    The model is trained until a convergence criterion is met that corresponds to the loss curve being flat over\n    a number of optimization steps given by `convergence_inteval` (see plots for details).\n\n    To reduce precompilation time and memory cost, the loss function and gradient functions are evaluated in\n    chunks of size model.max_vmap.\n\n    Args:\n        model (class): Classifier class object to train. Trainable parameters must be stored in model.params_.\n        loss_fn (Callable): Loss function to be minimised. Must be of the form loss_fn(params, X, y).\n        optimizer (optax optimizer): Optax optimizer (e.g. optax.adam).\n        X (array): Input data array of shape (n_samples, n_features)\n        y (array): Array of shape (n_samples) containing the labels.\n        random_key_generator (jax.random.PRNGKey): JAX key generator object for pseudo-randomness generation.\n        convergence_interval (int, optional): Number of optimization steps over which to decide convergence. Larger\n            values give a higher confidence that the model has converged but may increase training time.\n\n    Returns:\n        params (dict): The new parameters after training has completed.\n    \"\"\"\n\n    if not model.batch_size / model.max_vmap % 1 == 0:\n        raise Exception(\"Batch size must be multiple of max_vmap.\")\n\n    params = model.params_\n    opt = optimizer(learning_rate=model.learning_rate)\n    opt_state = opt.init(params)\n    grad_fn = jax.grad(loss_fn)\n\n    # jitting through the chunked_grad function can take a long time,\n    # so we jit here and chunk after\n    if model.jit:\n        grad_fn = jax.jit(grad_fn)\n\n    # note: assumes that the loss function is a sample mean of\n    # some function over the input data set\n    chunked_grad_fn = chunk_grad(grad_fn, model.max_vmap)\n    chunked_loss_fn = chunk_loss(loss_fn, model.max_vmap)\n\n    def update(params, opt_state, x, y):\n        grads = chunked_grad_fn(params, x, y)\n        loss_val = chunked_loss_fn(params, x, y)\n        updates, opt_state = opt.update(grads, opt_state)\n        params = optax.apply_updates(params, updates)\n        return params, opt_state, loss_val\n\n    loss_history = []\n    converged = False\n    start = time.time()\n    for step in range(model.max_steps):\n        key = random_key_generator()\n        X_batch, y_batch = get_batch(X, y, key, batch_size=model.batch_size)\n        params, opt_state, loss_val = update(params, opt_state, X_batch, y_batch)\n        loss_history.append(loss_val)\n        logging.debug(f\"{step} - loss: {loss_val}\")\n\n        if np.isnan(loss_val):\n            logging.info(\"nan encountered. Training aborted.\")\n            break\n\n        # decide convergence\n        if step &gt; 2 * convergence_interval:\n            # get means of last two intervals and standard deviation of last interval\n            average1 = np.mean(loss_history[-convergence_interval:])\n            average2 = np.mean(loss_history[-2 * convergence_interval : -convergence_interval])\n            std1 = np.std(loss_history[-convergence_interval:])\n            # if the difference in averages is small compared to the statistical fluctuations, stop training.\n            if np.abs(average2 - average1) &lt;= std1 / np.sqrt(convergence_interval) / 2:\n                logging.info(f\"Model {model.__class__.__name__} converged after {step} steps.\")\n                converged = True\n                break\n\n    end = time.time()\n    loss_history = np.array(loss_history)\n    model.loss_history_ = loss_history / np.max(np.abs(loss_history))\n    model.training_time_ = end - start\n\n    if not converged:\n        print(\"Loss did not converge:\", loss_history)\n        raise ConvergenceWarning(\n            f\"Model {model.__class__.__name__} has not converged after the maximum number of {model.max_steps} steps.\"\n        )\n\n    return params\n</code></pre> <code></code> models <p>Module containing models to be used in benchmarks.</p> MODULE DESCRIPTION <code>circuit_centric</code> <code>convolutional_neural_network</code> <code>data_reuploading</code> <code>dressed_quantum_circuit</code> <code>iqp_kernel</code> <code>iqp_variational</code> <code>projected_quantum_kernel</code> <code>quantum_boltzmann_machine</code> <code>quantum_kitchen_sinks</code> <code>quantum_metric_learning</code> <code>quanvolutional_neural_network</code> <code>separable</code> <code>tree_tensor</code> <code>vanilla_qnn</code> <code>weinet</code> CLASS DESCRIPTION <code>CircuitCentricClassifier</code> <code>ConvolutionalNeuralNetwork</code> <code>DataReuploadingClassifier</code> <code>DataReuploadingClassifierNoCost</code> <code>DataReuploadingClassifierNoScaling</code> <code>DataReuploadingClassifierNoTrainableEmbedding</code> <code>DataReuploadingClassifierSeparable</code> <code>DressedQuantumCircuitClassifier</code> <code>IQPKernelClassifier</code> <code>IQPVariationalClassifier</code> <code>ProjectedQuantumKernel</code> <code>QuantumBoltzmannMachine</code> <code>QuantumKitchenSinks</code> <code>QuantumMetricLearner</code> <code>QuanvolutionalNeuralNetwork</code> <code>SeparableKernelClassifier</code> <code>SeparableVariationalClassifier</code> <code>TreeTensorClassifier</code> <code>VanillaQNN</code> <code>WeiNet</code> <code></code> CircuitCentricClassifier <pre><code>CircuitCentricClassifier(n_input_copies=2, n_layers=4, convergence_interval=200, max_steps=10000, learning_rate=0.001, batch_size=32, max_vmap=None, jit=True, scaling=1.0, random_state=42, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax-jit'})\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>A variational quantum model based on https://arxiv.org/pdf/1804.03680v2.pdf.</p> <p>Uses amplitude encoding as the data embedding, but implements several copies of the initial state. The data vector :math:<code>x</code> of size :math:<code>N</code> gets padded to the next power of 2 by values :math:<code>1/len(\\tilde{x})</code> (so that the new data vector :math:<code>\\tilde{x}</code> has length :math:<code>2^n &gt; N</code>). The padded vector gets normalised to :math:<code>\\bar{\\tilde{x}}</code>, and embedded into the amplitudes of several copies of :math:<code>n</code>-qubit quantum states. Altogether, the variational circuit acts on a quantum state of amplitudes :math:<code>\\bar{\\tilde{x}} \\otimes \\bar{\\tilde{x}} \\otimes ...</code>.</p> <p>The total number of qubits :math:<code>d\\lceil \\log(n_features) \\rceil</code> of this model depends on the number of features :math:<code>N</code> and the number of copies :math:<code>d</code>.</p> <p>The variational part of the circuit uses general single qubit rotations as trainable gates. Each layer in the ansatz first applies a rotation to each qubit, and then controlled rotations connecting each qubit :math:<code>i</code> to qubit :math:<code>i+r \\text{mod } n</code>, where :math:<code>r</code> repeatedly runs through the range :math:<code>[0,...,n-1]</code>. The number of layers in the ansatz is a hyperparameter of the model.</p> <p>The result of the model is the sigma-z expectation of the first qubit, plus a trainable scalar bias. Training is via the square loss.</p> PARAMETER DESCRIPTION <code>n_input_copies</code> <p>Number of copies of the amplitude embedded state to produce.</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>n_layers</code> <p>Number of layers in the variational ansatz.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>convergence_interval</code> <p>The number of loss values to consider to decide convergence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>max_steps</code> <p>Maximum number of training steps. A warning will be raised if training did not converge.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> <code>learning_rate</code> <p>Initial learning rate for gradient descent.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> <code>batch_size</code> <p>Size of batches used for computing parameter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dev_type</code> <p>Pennylane device type; e.g. 'default.qubit.jax'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>Keyword arguments for the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax-jit'}</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>generate_key</code> <p>Generates a random key used in sampling batches.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for batch of data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>The feature vectors padded to the next power of 2 and then normalised.</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/circuit_centric.py</code> <pre><code>def __init__(\n    self,\n    n_input_copies=2,\n    n_layers=4,\n    convergence_interval=200,\n    max_steps=10000,\n    learning_rate=0.001,\n    batch_size=32,\n    max_vmap=None,\n    jit=True,\n    scaling=1.0,\n    random_state=42,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax-jit\"},\n):\n    r\"\"\"\n    A variational quantum model based on https://arxiv.org/pdf/1804.03680v2.pdf.\n\n    Uses amplitude encoding as the data embedding, but implements several\n    copies of the initial state. The data vector :math:`x` of size :math:`N`\n    gets padded to the next power of 2 by values :math:`1/len(\\tilde{x})`\n    (so that the new data vector :math:`\\tilde{x}` has length :math:`2^n &gt; N`).\n    The padded vector gets normalised to :math:`\\bar{\\tilde{x}}`, and\n    embedded into the amplitudes of several copies of :math:`n`-qubit\n    quantum states. Altogether, the variational circuit acts on a quantum\n    state of amplitudes :math:`\\bar{\\tilde{x}} \\otimes \\bar{\\tilde{x}} \\otimes ...`.\n\n    The total number of qubits :math:`d\\lceil \\log(n_features) \\rceil` of\n    this model depends on the number of features :math:`N` and the number\n    of copies :math:`d`.\n\n    The variational part of the circuit uses general single qubit rotations\n    as trainable gates. Each layer in the ansatz first applies a rotation\n    to each qubit, and then controlled rotations connecting each qubit\n    :math:`i` to qubit :math:`i+r \\text{mod } n`, where :math:`r` repeatedly runs\n    through the range :math:`[0,...,n-1]`. The number of layers in the\n    ansatz is a hyperparameter of the model.\n\n    The result of the model is the sigma-z expectation of the first qubit,\n    plus a trainable scalar bias. Training is via the square loss.\n\n    Args:\n        n_input_copies (int): Number of copies of the amplitude embedded\n            state to produce.\n        n_layers (int): Number of layers in the variational ansatz.\n        convergence_interval (int): The number of loss values to consider to decide convergence.\n        max_steps (int): Maximum number of training steps. A warning will\n            be raised if training did not converge.\n        learning_rate (float): Initial learning rate for gradient descent.\n        batch_size (int): Size of batches used for computing parameter updates.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        dev_type (str): Pennylane device type; e.g. 'default.qubit.jax'.\n        qnode_kwargs (str): Keyword arguments for the circuit qnode.\n        scaling (float): Factor by which to scale the input data.\n        random_state (int): Seed used for pseudorandom number generation.\n    \"\"\"\n\n    # attributes that do not depend on data\n    self.n_input_copies = n_input_copies\n    self.n_layers = n_layers\n    self.convergence_interval = convergence_interval\n    self.max_steps = max_steps\n    self.learning_rate = learning_rate\n    self.batch_size = batch_size\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.jit = jit\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # Dictionary containing the trainable parameters\n    self.n_qubits_ = None\n    self.scaler = None\n    self.circuit = None\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/circuit_centric.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n\n    self.initialize(n_features=X.shape[1], classes=np.unique(y))\n\n    X = self.transform(X)\n\n    optimizer = optax.adam\n\n    def loss_fn(params, X, y):\n        pred = self.forward(params, X)  # jnp.stack([self.forward(params, x) for x in X])\n        return jnp.mean(optax.l2_loss(pred, y))\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        X,\n        y,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    return self\n</code></pre> <code></code> generate_key <pre><code>generate_key() -&gt; Array\n</code></pre> <p>Generates a random key used in sampling batches.</p> RETURNS DESCRIPTION <code>Array</code> <p>jax.Array: description</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/circuit_centric.py</code> <pre><code>def generate_key(self) -&gt; jax.Array:\n    \"\"\"\n    Generates a random key used in sampling batches.\n\n    Returns:\n        jax.Array: _description_\n    \"\"\"\n    return jax.random.PRNGKey(self.rng.integers(1000000))\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels. Args:     n_features (int): Number of features that the classifier expects     classes (array-like): class labels</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/circuit_centric.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    n_qubits_per_copy = int(np.ceil(np.log2(n_features)))\n    self.n_qubits_ = self.n_input_copies * n_qubits_per_copy\n\n    self.construct_model()\n    self.initialize_params()\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for batch of data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/circuit_centric.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for batch of data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/circuit_centric.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    predictions = self.chunked_forward(self.params_, X)\n    predictions_2d = np.c_[(1 - predictions) / 2, (1 + predictions) / 2]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=False)\n</code></pre> <p>The feature vectors padded to the next power of 2 and then normalised.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/circuit_centric.py</code> <pre><code>def transform(self, X, preprocess=False):\n    \"\"\"\n    The feature vectors padded to the next power of 2 and then normalised.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    n_features = X.shape[1]\n    X = X * self.scaling\n\n    n_qubits_per_copy = int(np.ceil(np.log2(n_features)))\n    max_n_features = 2**n_qubits_per_copy\n    n_padding = max_n_features - n_features\n    padding = np.ones(shape=(len(X), n_padding)) / max_n_features\n\n    X_padded = np.c_[X, padding]\n    X_normalised = np.divide(X_padded, np.expand_dims(np.linalg.norm(X_padded, axis=1), axis=1))\n    return X_normalised\n</code></pre> <code></code> ConvolutionalNeuralNetwork <pre><code>ConvolutionalNeuralNetwork(kernel_shape=3, output_channels=[32, 64], learning_rate=0.001, convergence_interval=200, max_steps=10000, batch_size=32, max_vmap=None, jit=True, random_state=42, scaling=1.0)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>This implements a vanilla convolutional neural network (CNN) two-class classifier with JAX and flax (https://github.com/google/flax).</p> <p>The structure of the neural network is as follows:</p> <ul> <li>a 2D convolutional layer with self.output_channels[0] output channels</li> <li>a max pool layer</li> <li>a 2D convolutional layer with self.output_channels[1] output channels</li> <li>a max pool layer</li> <li>a two layer fully connected feedforward neural network with 2*self.output_channels[1] hidden neurons     and one output neuron</li> </ul> <p>The probability of class 1 is given by :math:<code>P(+1\\vert \\vec{w},\\vec{x}) = \\sigma(f(\\vec{w}),\\vec{x})</code> where :math:<code>\\vec{w}</code> are the weights of the network and :math:<code>\\sigma</code> is the logistic function and :math:<code>f</code> gives the value of the neuron in the final later. These probabilities are fed to binary cross entropy loss for training.</p> <p>The 2d input data should be flattened to have shape (n_samples, height*width), where height and width are the dimensions of the 2d data. The model works for square data only, i.e. height=width.</p> PARAMETER DESCRIPTION <code>kernel_shape</code> <p>the shape of the kernel used in the CNN. e.g. kernel_shape=3 uses a 3x3 filter</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>output_channels</code> <p>Two integers specifying the output sizes of the convolutional layers in the CNN. Defaults to [32, 62].</p> <p> TYPE: <code>list[int]</code> DEFAULT: <code>[32, 64]</code> </p> <code>learning_rate</code> <p>Initial learning rate for training.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> <code>max_steps</code> <p>Maximum number of training steps. A warning will be raised if training did not converge.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> <code>convergence_interval</code> <p>The number of loss values to consider to decide convergence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>batch_size</code> <p>Size of batches used for computing parameter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> METHOD DESCRIPTION <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>If scaler is initialized, transform the inputs.</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/convolutional_neural_network.py</code> <pre><code>def __init__(\n    self,\n    kernel_shape=3,\n    output_channels=[32, 64],\n    learning_rate=0.001,\n    convergence_interval=200,\n    max_steps=10000,\n    batch_size=32,\n    max_vmap=None,\n    jit=True,\n    random_state=42,\n    scaling=1.0,\n):\n    r\"\"\"\n    This implements a vanilla convolutional neural network (CNN) two-class classifier with JAX and flax\n    (https://github.com/google/flax).\n\n    The structure of the neural network is as follows:\n\n    - a 2D convolutional layer with self.output_channels[0] output channels\n    - a max pool layer\n    - a 2D convolutional layer with self.output_channels[1] output channels\n    - a max pool layer\n    - a two layer fully connected feedforward neural network with 2*self.output_channels[1] hidden neurons\n        and one output neuron\n\n\n    The probability of class 1 is given by :math:`P(+1\\vert \\vec{w},\\vec{x}) = \\sigma(f(\\vec{w}),\\vec{x})`\n    where :math:`\\vec{w}` are the weights of the network and :math:`\\sigma` is the logistic function and\n    :math:`f` gives the value of the neuron in the final later. These probabilities are fed to binary cross entropy\n    loss for training.\n\n    The 2d input data should be flattened to have shape (n_samples, height*width), where height and width are the\n    dimensions of the 2d data. The model works for square data only, i.e. height=width.\n\n    Args:\n        kernel_shape (int): the shape of the kernel used in the CNN. e.g. kernel_shape=3 uses a 3x3 filter\n        output_channels (list[int]): Two integers specifying the output sizes of the convolutional layers\n            in the CNN. Defaults to [32, 62].\n        learning_rate (float): Initial learning rate for training.\n        max_steps (int): Maximum number of training steps. A warning will be raised if training did not converge.\n        convergence_interval (int): The number of loss values to consider to decide convergence.\n        batch_size (int): Size of batches used for computing parameter updates.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        random_state (int): Seed used for pseudorandom number generation.\n        scaling (float): Factor by which to scale the input data.\n    \"\"\"\n\n    # attributes that do not depend on data\n    self.learning_rate = learning_rate\n    self.max_steps = max_steps\n    self.scaling = scaling\n    self.jit = jit\n    self.kernel_shape = kernel_shape\n    self.output_channels = output_channels\n    self.convergence_interval = convergence_interval\n    self.batch_size = batch_size\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # Dictionary containing the trainable parameters\n    self.scaler = None  # data scaler will be fitted on training data\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/convolutional_neural_network.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n\n    self.initialize(X.shape[1], classes=np.unique(y))\n\n    y = jnp.array(y, dtype=int)\n\n    # scale input data\n    self.scaler = StandardScaler()\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    def loss_fn(params, X, y):\n        y = jax.nn.relu(y)  # convert to 0,1 labels\n        vals = self.forward.apply(params, X)[:, 0]\n        loss = jnp.mean(optax.sigmoid_binary_cross_entropy(vals, y))\n        return loss\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    optimizer = optax.adam\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        X,\n        y,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels. Args:     classes (array-like): class labels that the classifier expects</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/convolutional_neural_network.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n    Args:\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    # initialise the model\n    self.cnn = construct_cnn(self.output_channels, self.kernel_shape)\n    self.forward = self.cnn\n\n    # create dummy data input to initialise the cnn\n    height = int(jnp.sqrt(n_features))\n    X0 = jnp.ones(shape=(1, height, height, 1))\n    self.initialize_params(X0)\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred</code> <p>Predicted labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/convolutional_neural_network.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred (np.ndarray): Predicted labels of shape (n_samples,)\n    \"\"\"\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/convolutional_neural_network.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    # get probabilities of y=1\n    p1 = jax.nn.sigmoid(self.forward.apply(self.params_, X)[:, 0])\n    predictions_2d = jnp.c_[1 - p1, p1]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X)\n</code></pre> <p>If scaler is initialized, transform the inputs.</p> <p>Put into NCHW format. This assumes square images. Args:     X (np.ndarray): Data of shape (n_samples, n_features)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/convolutional_neural_network.py</code> <pre><code>def transform(self, X):\n    \"\"\"\n    If scaler is initialized, transform the inputs.\n\n    Put into NCHW format. This assumes square images.\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if self.scaler is None:\n        # if the model is unfitted, initialise the scaler here\n        self.scaler = StandardScaler()\n        self.scaler.fit(X)\n\n    X = self.scaler.transform(X) * self.scaling\n\n    # reshape data to square array\n    X = jnp.array(X)\n    height = int(jnp.sqrt(X.shape[1]))\n    X = jnp.reshape(X, (X.shape[0], height, height, 1))\n\n    return X\n</code></pre> <code></code> DataReuploadingClassifier <pre><code>DataReuploadingClassifier(n_layers=4, observable_type='single', convergence_interval=200, max_steps=10000, learning_rate=0.05, batch_size=32, max_vmap=None, jit=True, scaling=1.0, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax-jit'}, random_state=42)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Data reuploading classifier from https://arxiv.org/abs/1907.02085. Here we code the 'multi-qubit classifier'. The model consists of layers of trainable data encoding unitaries. Each subsequent sequence of 3 elements of an input vector x are encoded into a SU(2) rotation via pennylane.Rot. The model therefore uses ceil(d/3) qubits.</p> <p>The cost function is given by eqn (23) in the plots, and includes trainable parameters for scaling input data (omegas), the class weights (alphas), and the circuit parameters (thetas). The cost function is a weighted sum of the squared differences between the ideal fidelity to the corresponding class state (here, either |0&gt; or |1&gt;) and the output state of the circuit, for a number of qubits specified by observable_weight. More specifically</p> <p>.. math::</p> <pre><code>\\ell(\\vec{\\theta},\\vec{\\omega},\\vec{\\alpha},\\vec{x}_i) =\n\\sum_{j=1}^{n_{\\text{max}}}(\\alpha_j^0F^0_j-(1-y_i))^2 + (\\alpha_j^1F^1_j-y_i)^2\n</code></pre> <p>where :math:<code>n_max</code> is given by observable_weight and :math:<code>F^0_j</code> is the fidelity of the jth output quibt to the state |0&gt;.</p> <p>When observable_weight !=1 it is not clear from the plots how to predict labels. Here we look at the average fidelities to |0&gt; and |1&gt; of the qubits up to index self.observable_weight and take the label with the largest average fidelity. For self.observable_weight=1 this reverts to their strategy in the plots.</p> <p>Where possible, we have followed the numerical examples found in the repo which accompanies the plots:</p> <p>https://github.com/AdrianPerezSalinas/universal_qlassifier/tree/354d70f940ea737192de4063ff72f859c77e5760</p> PARAMETER DESCRIPTION <code>n_layers</code> <p>Number of blocks used in the trainable embedding. The data is uploaded n_layers+1 times.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>observable_type</code> <p>Defines the number of qubits used to evaluate the weighed cost fucntion, either 'single', 'half' or 'full'.  max_steps (int): Maximum number of training steps. A warning will be raised if training did not     converge.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'single'</code> </p> <code>learning_rate</code> <p>Initial learning rate for training.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.05</code> </p> <code>convergence_interval</code> <p>The number of loss values to consider to decide convergence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>batch_size</code> <p>Size of batches used for computing paraeeter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the keyword arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax-jit'}</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>construct_model</code> <p>Construct the quantum circuit used in the model.</p> <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for batch of data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def __init__(\n    self,\n    n_layers=4,\n    observable_type=\"single\",\n    convergence_interval=200,\n    max_steps=10000,\n    learning_rate=0.05,\n    batch_size=32,\n    max_vmap=None,\n    jit=True,\n    scaling=1.0,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax-jit\"},\n    random_state=42,\n):\n    r\"\"\"\n    Data reuploading classifier from https://arxiv.org/abs/1907.02085. Here we code the 'multi-qubit classifier'.\n    The model consists of layers of trainable data encoding unitaries. Each subsequent sequence of 3 elements of\n    an input vector x are encoded into a SU(2) rotation via pennylane.Rot. The model therefore uses ceil(d/3)\n    qubits.\n\n    The cost function is given by eqn (23) in the plots, and includes trainable parameters for scaling input data\n    (omegas), the class weights (alphas), and the circuit parameters (thetas). The cost\n    function is a weighted sum of the squared differences between the ideal fidelity to the corresponding class state\n    (here, either |0&gt; or |1&gt;) and the output state of the circuit, for a number of qubits specified by\n    observable_weight. More specifically\n\n    .. math::\n\n        \\ell(\\vec{\\theta},\\vec{\\omega},\\vec{\\alpha},\\vec{x}_i) =\n        \\sum_{j=1}^{n_{\\text{max}}}(\\alpha_j^0F^0_j-(1-y_i))^2 + (\\alpha_j^1F^1_j-y_i)^2\n\n    where :math:`n_max` is given by observable_weight and :math:`F^0_j` is the fidelity of the jth output\n    quibt to the state |0&gt;.\n\n    When observable_weight !=1 it is not clear from the plots how to predict labels. Here we look at the average\n    fidelities to |0&gt; and |1&gt; of the qubits up to index self.observable_weight and take the label with the largest\n    average fidelity. For self.observable_weight=1 this reverts to their strategy in the plots.\n\n    Where possible, we have followed the numerical examples found in the repo which accompanies the plots:\n\n    https://github.com/AdrianPerezSalinas/universal_qlassifier/tree/354d70f940ea737192de4063ff72f859c77e5760\n\n    Args:\n        n_layers (int): Number of blocks used in the trainable embedding. The data is uploaded n_layers+1 times.\n        observable_type (str): Defines the number of qubits used to evaluate the weighed cost fucntion,\n            either 'single', 'half' or 'full'.\n             max_steps (int): Maximum number of training steps. A warning will be raised if training did not\n                converge.\n        learning_rate (float): Initial learning rate for training.\n        convergence_interval (int): The number of loss values to consider to decide convergence.\n        batch_size (int): Size of batches used for computing paraeeter updates.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'.\n        qnode_kwargs (str): the keyword arguments passed to the circuit qnode.\n        scaling (float): Factor by which to scale the input data.\n        random_state (int): Seed used for pseudorandom number generation.\n    \"\"\"\n\n    # attributes that do not depend on data\n    self.n_layers = n_layers\n    self.observable_type = observable_type\n    self.convergence_interval = convergence_interval\n    self.max_steps = max_steps\n    self.learning_rate = learning_rate\n    self.batch_size = batch_size\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.jit = jit\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # dictionary containing the trainable parameters\n    self.n_qubits_ = None\n    self.scaler = None  # data preprocessing\n    self.circuit = None\n</code></pre> <code></code> construct_model <pre><code>construct_model()\n</code></pre> <p>Construct the quantum circuit used in the model.</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def construct_model(self):\n    \"\"\"Construct the quantum circuit used in the model.\"\"\"\n\n    dev = qml.device(self.dev_type, wires=self.n_qubits_)\n\n    @qml.qnode(dev, **self.qnode_kwargs)\n    def circuit(params, x):\n        \"\"\"A variational quantum circuit with data reuploading.\n\n        Args:\n            params (array[float]): dictionary of parameters\n            x (array[float]): input vector\n\n        Returns:\n            list: Expectation values of Pauli Z on each qubit\n        \"\"\"\n        for layer in range(self.n_layers):\n            x_idx = 0  # to keep track of the data index\n            for i in range(self.n_qubits_):\n                # scaled inputs\n                angles = x[x_idx : x_idx + 3] * params[\"omegas\"][layer, x_idx : x_idx + 3]\n                qml.Rot(*angles, wires=i)\n\n                # variational\n                angles = params[\"thetas\"][i, layer, :]\n                qml.Rot(*angles, wires=i)\n\n                x_idx += 3\n            if layer % 2 == 0:\n                qml.broadcast(qml.CZ, range(self.n_qubits_), pattern=\"double\")\n            else:\n                qml.broadcast(qml.CZ, range(self.n_qubits_), pattern=\"double_odd\")\n\n        # final reupload without CZs\n        x_idx = 0\n        for i in range(self.n_qubits_):\n            angles = (\n                x[x_idx : x_idx + 3] * params[\"omegas\"][self.n_layers, x_idx : x_idx + 3]\n                + params[\"thetas\"][i, self.n_layers, :]\n            )\n            qml.Rot(*angles, wires=i)\n            x_idx += 3\n\n        return [qml.expval(qml.PauliZ(wires=[i])) for i in range(self.n_qubits_)]\n\n    self.circuit = circuit\n\n    def circuit_as_array(params, x):\n        # pennylane returns a list in the circuit above, so we need this\n        return jnp.array(circuit(params, x))\n\n    if self.jit:\n        circuit_as_array = jax.jit(circuit_as_array)\n    self.forward = jax.vmap(circuit_as_array, in_axes=(None, 0))\n    self.chunked_forward = chunk_vmapped_fn(self.forward, 1, self.max_vmap)\n\n    return self.forward\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n    self.initialize(X.shape[1], classes=np.unique(y))\n\n    self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    def loss_fn(params, x, y):\n        y_mat = jnp.vstack(\n            [y for __ in range(self.observable_weight)]\n        ).T  # repeated columns of labels\n        y_mat0 = (1 - y_mat) / 2\n        y_mat1 = (1 + y_mat) / 2\n        alpha_mat0 = jnp.vstack(\n            [params[\"alphas\"][0, :] for __ in range(len(x))]\n        )  # repeated rows of alpha parameters\n        alpha_mat1 = jnp.vstack([params[\"alphas\"][1, :] for __ in range(len(x))])\n\n        expvals = self.forward(params, x)\n        probs0 = (1 - expvals[:, : self.observable_weight]) / 2  # fidelity with |0&gt;\n        probs1 = (1 + expvals[:, : self.observable_weight]) / 2  # fidelity with |1&gt;\n        loss = (\n            1\n            / 2\n            * (\n                jnp.sum(\n                    (alpha_mat0 * probs0 - y_mat0) ** 2 + (alpha_mat1 * probs1 - y_mat1) ** 2\n                )\n            )\n        )  # eqn 23 in plots\n        return loss / len(y)\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    self.construct_model()\n    self.initialize_params()\n    optimizer = optax.adam\n\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        X,\n        y,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels. Args:     n_features (int): Number of features that the classifier expects     classes (array-like): class labels that the classifier expects</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_features = n_features\n    self.n_qubits_ = ceil(self.n_features / 3)\n\n    if self.observable_type == \"single\":\n        self.observable_weight = 1\n    elif self.observable_type == \"half\":\n        if self.n_qubits_ == 1:\n            self.observable_weight = 1\n        else:\n            self.observable_weight = self.n_qubits_ // 2\n    elif self.observable_type == \"full\":\n        self.observable_weight = self.n_qubits_\n\n    self.initialize_params()\n    self.construct_model()\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for batch of data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for batch of data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    check_is_fitted(self)\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    predictions = self.chunked_forward(self.params_, X)\n    predictions = jnp.mean(\n        predictions[:, : self.observable_weight], axis=1\n    )  # use mean of expvals over relevant qubits\n    predictions_2d = np.c_[(1 - predictions) / 2, (1 + predictions) / 2]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n            self.scaler.fit(X)\n\n        X = self.scaler.transform(X)\n\n    X = X * self.scaling\n    X = jnp.pad(X, ((0, 0), (0, (3 - X.shape[1]) % 3)), \"constant\")\n    return X\n</code></pre> <code></code> DataReuploadingClassifierNoCost <pre><code>DataReuploadingClassifierNoCost(n_layers=4, observable_type='single', convergence_interval=200, max_steps=10000, learning_rate=0.05, batch_size=32, max_vmap=None, jit=True, scaling=1.0, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax-jit'}, random_state=42)\n</code></pre> <p>               Bases: <code>DataReuploadingClassifier</code></p> <p>Data reuploading classifier from https://arxiv.org/abs/1907.02085. Here we code the 'multi-qubit classifier'. The model consists of layers of trainable data encoding unitaries. Each subsequent sequence of 3 elements of an input vector x are encoded into a SU(2) rotation via pennylane.Rot. The model therefore uses ceil(d/3) qubits.</p> <p>The cost function is given by eqn (23) in the plots, and includes trainable parameters for scaling input data (omegas), the class weights (alphas), and the circuit parameters (thetas). The cost function is a weighted sum of the squared differences between the ideal fidelity to the corresponding class state (here, either |0&gt; or |1&gt;) and the output state of the circuit, for a number of qubits specified by observable_weight. More specifically</p> <p>.. math::</p> <pre><code>\\ell(\\vec{\\theta},\\vec{\\omega},\\vec{\\alpha},\\vec{x}_i) =\n\\sum_{j=1}^{n_{\\text{max}}}(\\alpha_j^0F^0_j-(1-y_i))^2 + (\\alpha_j^1F^1_j-y_i)^2\n</code></pre> <p>where :math:<code>n_max</code> is given by observable_weight and :math:<code>F^0_j</code> is the fidelity of the jth output quibt to the state |0&gt;.</p> <p>When observable_weight !=1 it is not clear from the plots how to predict labels. Here we look at the average fidelities to |0&gt; and |1&gt; of the qubits up to index self.observable_weight and take the label with the largest average fidelity. For self.observable_weight=1 this reverts to their strategy in the plots.</p> <p>Where possible, we have followed the numerical examples found in the repo which accompanies the plots:</p> <p>https://github.com/AdrianPerezSalinas/universal_qlassifier/tree/354d70f940ea737192de4063ff72f859c77e5760</p> PARAMETER DESCRIPTION <code>n_layers</code> <p>Number of blocks used in the trainable embedding. The data is uploaded n_layers+1 times.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>observable_type</code> <p>Defines the number of qubits used to evaluate the weighed cost fucntion, either 'single', 'half' or 'full'.  max_steps (int): Maximum number of training steps. A warning will be raised if training did not     converge.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'single'</code> </p> <code>learning_rate</code> <p>Initial learning rate for training.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.05</code> </p> <code>convergence_interval</code> <p>The number of loss values to consider to decide convergence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>batch_size</code> <p>Size of batches used for computing paraeeter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the keyword arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax-jit'}</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>construct_model</code> <p>Construct the quantum circuit used in the model.</p> <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for batch of data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def __init__(\n    self,\n    n_layers=4,\n    observable_type=\"single\",\n    convergence_interval=200,\n    max_steps=10000,\n    learning_rate=0.05,\n    batch_size=32,\n    max_vmap=None,\n    jit=True,\n    scaling=1.0,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax-jit\"},\n    random_state=42,\n):\n    r\"\"\"\n    Data reuploading classifier from https://arxiv.org/abs/1907.02085. Here we code the 'multi-qubit classifier'.\n    The model consists of layers of trainable data encoding unitaries. Each subsequent sequence of 3 elements of\n    an input vector x are encoded into a SU(2) rotation via pennylane.Rot. The model therefore uses ceil(d/3)\n    qubits.\n\n    The cost function is given by eqn (23) in the plots, and includes trainable parameters for scaling input data\n    (omegas), the class weights (alphas), and the circuit parameters (thetas). The cost\n    function is a weighted sum of the squared differences between the ideal fidelity to the corresponding class state\n    (here, either |0&gt; or |1&gt;) and the output state of the circuit, for a number of qubits specified by\n    observable_weight. More specifically\n\n    .. math::\n\n        \\ell(\\vec{\\theta},\\vec{\\omega},\\vec{\\alpha},\\vec{x}_i) =\n        \\sum_{j=1}^{n_{\\text{max}}}(\\alpha_j^0F^0_j-(1-y_i))^2 + (\\alpha_j^1F^1_j-y_i)^2\n\n    where :math:`n_max` is given by observable_weight and :math:`F^0_j` is the fidelity of the jth output\n    quibt to the state |0&gt;.\n\n    When observable_weight !=1 it is not clear from the plots how to predict labels. Here we look at the average\n    fidelities to |0&gt; and |1&gt; of the qubits up to index self.observable_weight and take the label with the largest\n    average fidelity. For self.observable_weight=1 this reverts to their strategy in the plots.\n\n    Where possible, we have followed the numerical examples found in the repo which accompanies the plots:\n\n    https://github.com/AdrianPerezSalinas/universal_qlassifier/tree/354d70f940ea737192de4063ff72f859c77e5760\n\n    Args:\n        n_layers (int): Number of blocks used in the trainable embedding. The data is uploaded n_layers+1 times.\n        observable_type (str): Defines the number of qubits used to evaluate the weighed cost fucntion,\n            either 'single', 'half' or 'full'.\n             max_steps (int): Maximum number of training steps. A warning will be raised if training did not\n                converge.\n        learning_rate (float): Initial learning rate for training.\n        convergence_interval (int): The number of loss values to consider to decide convergence.\n        batch_size (int): Size of batches used for computing paraeeter updates.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'.\n        qnode_kwargs (str): the keyword arguments passed to the circuit qnode.\n        scaling (float): Factor by which to scale the input data.\n        random_state (int): Seed used for pseudorandom number generation.\n    \"\"\"\n\n    # attributes that do not depend on data\n    self.n_layers = n_layers\n    self.observable_type = observable_type\n    self.convergence_interval = convergence_interval\n    self.max_steps = max_steps\n    self.learning_rate = learning_rate\n    self.batch_size = batch_size\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.jit = jit\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # dictionary containing the trainable parameters\n    self.n_qubits_ = None\n    self.scaler = None  # data preprocessing\n    self.circuit = None\n</code></pre> <code></code> construct_model <pre><code>construct_model()\n</code></pre> <p>Construct the quantum circuit used in the model.</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def construct_model(self):\n    \"\"\"Construct the quantum circuit used in the model.\"\"\"\n\n    dev = qml.device(self.dev_type, wires=self.n_qubits_)\n\n    @qml.qnode(dev, **self.qnode_kwargs)\n    def circuit(params, x):\n        \"\"\"A variational quantum circuit with data reuploading.\n\n        Args:\n            params (array[float]): dictionary of parameters\n            x (array[float]): input vector\n\n        Returns:\n            list: Expectation values of Pauli Z on each qubit\n        \"\"\"\n        for layer in range(self.n_layers):\n            x_idx = 0  # to keep track of the data index\n            for i in range(self.n_qubits_):\n                # scaled inputs\n                angles = x[x_idx : x_idx + 3] * params[\"omegas\"][layer, x_idx : x_idx + 3]\n                qml.Rot(*angles, wires=i)\n\n                # variational\n                angles = params[\"thetas\"][i, layer, :]\n                qml.Rot(*angles, wires=i)\n\n                x_idx += 3\n            if layer % 2 == 0:\n                qml.broadcast(qml.CZ, range(self.n_qubits_), pattern=\"double\")\n            else:\n                qml.broadcast(qml.CZ, range(self.n_qubits_), pattern=\"double_odd\")\n\n        # final reupload without CZs\n        x_idx = 0\n        for i in range(self.n_qubits_):\n            angles = (\n                x[x_idx : x_idx + 3] * params[\"omegas\"][self.n_layers, x_idx : x_idx + 3]\n                + params[\"thetas\"][i, self.n_layers, :]\n            )\n            qml.Rot(*angles, wires=i)\n            x_idx += 3\n\n        return [qml.expval(qml.PauliZ(wires=[i])) for i in range(self.n_qubits_)]\n\n    self.circuit = circuit\n\n    def circuit_as_array(params, x):\n        # pennylane returns a list in the circuit above, so we need this\n        return jnp.array(circuit(params, x))\n\n    if self.jit:\n        circuit_as_array = jax.jit(circuit_as_array)\n    self.forward = jax.vmap(circuit_as_array, in_axes=(None, 0))\n    self.chunked_forward = chunk_vmapped_fn(self.forward, 1, self.max_vmap)\n\n    return self.forward\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n    self.initialize(X.shape[1], classes=np.unique(y))\n\n    self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    def loss_fn(params, X, y):\n        expvals = jnp.sum(self.forward(params, X), axis=1)\n        probs = (1 - expvals * y) / 2  # the probs of incorrect classification\n        return jnp.mean(probs)\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    self.construct_model()\n    self.initialize_params()\n    optimizer = optax.adam\n\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        X,\n        y,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels. Args:     n_features (int): Number of features that the classifier expects     classes (array-like): class labels that the classifier expects</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_features = n_features\n    self.n_qubits_ = ceil(self.n_features / 3)\n\n    if self.observable_type == \"single\":\n        self.observable_weight = 1\n    elif self.observable_type == \"half\":\n        if self.n_qubits_ == 1:\n            self.observable_weight = 1\n        else:\n            self.observable_weight = self.n_qubits_ // 2\n    elif self.observable_type == \"full\":\n        self.observable_weight = self.n_qubits_\n\n    self.initialize_params()\n    self.construct_model()\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for batch of data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for batch of data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    check_is_fitted(self)\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    predictions = self.chunked_forward(self.params_, X)\n    predictions = jnp.mean(\n        predictions[:, : self.observable_weight], axis=1\n    )  # use mean of expvals over relevant qubits\n    predictions_2d = np.c_[(1 - predictions) / 2, (1 + predictions) / 2]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n            self.scaler.fit(X)\n\n        X = self.scaler.transform(X)\n\n    X = X * self.scaling\n    X = jnp.pad(X, ((0, 0), (0, (3 - X.shape[1]) % 3)), \"constant\")\n    return X\n</code></pre> <code></code> DataReuploadingClassifierNoScaling <pre><code>DataReuploadingClassifierNoScaling(n_layers=4, observable_type='single', convergence_interval=200, max_steps=10000, learning_rate=0.05, batch_size=32, max_vmap=None, jit=True, scaling=1.0, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax-jit'}, random_state=42)\n</code></pre> <p>               Bases: <code>DataReuploadingClassifier</code></p> <p>Data reuploading classifier from https://arxiv.org/abs/1907.02085. Here we code the 'multi-qubit classifier'. The model consists of layers of trainable data encoding unitaries. Each subsequent sequence of 3 elements of an input vector x are encoded into a SU(2) rotation via pennylane.Rot. The model therefore uses ceil(d/3) qubits.</p> <p>The cost function is given by eqn (23) in the plots, and includes trainable parameters for scaling input data (omegas), the class weights (alphas), and the circuit parameters (thetas). The cost function is a weighted sum of the squared differences between the ideal fidelity to the corresponding class state (here, either |0&gt; or |1&gt;) and the output state of the circuit, for a number of qubits specified by observable_weight. More specifically</p> <p>.. math::</p> <pre><code>\\ell(\\vec{\\theta},\\vec{\\omega},\\vec{\\alpha},\\vec{x}_i) =\n\\sum_{j=1}^{n_{\\text{max}}}(\\alpha_j^0F^0_j-(1-y_i))^2 + (\\alpha_j^1F^1_j-y_i)^2\n</code></pre> <p>where :math:<code>n_max</code> is given by observable_weight and :math:<code>F^0_j</code> is the fidelity of the jth output quibt to the state |0&gt;.</p> <p>When observable_weight !=1 it is not clear from the plots how to predict labels. Here we look at the average fidelities to |0&gt; and |1&gt; of the qubits up to index self.observable_weight and take the label with the largest average fidelity. For self.observable_weight=1 this reverts to their strategy in the plots.</p> <p>Where possible, we have followed the numerical examples found in the repo which accompanies the plots:</p> <p>https://github.com/AdrianPerezSalinas/universal_qlassifier/tree/354d70f940ea737192de4063ff72f859c77e5760</p> PARAMETER DESCRIPTION <code>n_layers</code> <p>Number of blocks used in the trainable embedding. The data is uploaded n_layers+1 times.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>observable_type</code> <p>Defines the number of qubits used to evaluate the weighed cost fucntion, either 'single', 'half' or 'full'.  max_steps (int): Maximum number of training steps. A warning will be raised if training did not     converge.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'single'</code> </p> <code>learning_rate</code> <p>Initial learning rate for training.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.05</code> </p> <code>convergence_interval</code> <p>The number of loss values to consider to decide convergence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>batch_size</code> <p>Size of batches used for computing paraeeter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the keyword arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax-jit'}</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>construct_model</code> <p>Construct the quantum circuit used in the model.</p> <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for batch of data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def __init__(\n    self,\n    n_layers=4,\n    observable_type=\"single\",\n    convergence_interval=200,\n    max_steps=10000,\n    learning_rate=0.05,\n    batch_size=32,\n    max_vmap=None,\n    jit=True,\n    scaling=1.0,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax-jit\"},\n    random_state=42,\n):\n    r\"\"\"\n    Data reuploading classifier from https://arxiv.org/abs/1907.02085. Here we code the 'multi-qubit classifier'.\n    The model consists of layers of trainable data encoding unitaries. Each subsequent sequence of 3 elements of\n    an input vector x are encoded into a SU(2) rotation via pennylane.Rot. The model therefore uses ceil(d/3)\n    qubits.\n\n    The cost function is given by eqn (23) in the plots, and includes trainable parameters for scaling input data\n    (omegas), the class weights (alphas), and the circuit parameters (thetas). The cost\n    function is a weighted sum of the squared differences between the ideal fidelity to the corresponding class state\n    (here, either |0&gt; or |1&gt;) and the output state of the circuit, for a number of qubits specified by\n    observable_weight. More specifically\n\n    .. math::\n\n        \\ell(\\vec{\\theta},\\vec{\\omega},\\vec{\\alpha},\\vec{x}_i) =\n        \\sum_{j=1}^{n_{\\text{max}}}(\\alpha_j^0F^0_j-(1-y_i))^2 + (\\alpha_j^1F^1_j-y_i)^2\n\n    where :math:`n_max` is given by observable_weight and :math:`F^0_j` is the fidelity of the jth output\n    quibt to the state |0&gt;.\n\n    When observable_weight !=1 it is not clear from the plots how to predict labels. Here we look at the average\n    fidelities to |0&gt; and |1&gt; of the qubits up to index self.observable_weight and take the label with the largest\n    average fidelity. For self.observable_weight=1 this reverts to their strategy in the plots.\n\n    Where possible, we have followed the numerical examples found in the repo which accompanies the plots:\n\n    https://github.com/AdrianPerezSalinas/universal_qlassifier/tree/354d70f940ea737192de4063ff72f859c77e5760\n\n    Args:\n        n_layers (int): Number of blocks used in the trainable embedding. The data is uploaded n_layers+1 times.\n        observable_type (str): Defines the number of qubits used to evaluate the weighed cost fucntion,\n            either 'single', 'half' or 'full'.\n             max_steps (int): Maximum number of training steps. A warning will be raised if training did not\n                converge.\n        learning_rate (float): Initial learning rate for training.\n        convergence_interval (int): The number of loss values to consider to decide convergence.\n        batch_size (int): Size of batches used for computing paraeeter updates.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'.\n        qnode_kwargs (str): the keyword arguments passed to the circuit qnode.\n        scaling (float): Factor by which to scale the input data.\n        random_state (int): Seed used for pseudorandom number generation.\n    \"\"\"\n\n    # attributes that do not depend on data\n    self.n_layers = n_layers\n    self.observable_type = observable_type\n    self.convergence_interval = convergence_interval\n    self.max_steps = max_steps\n    self.learning_rate = learning_rate\n    self.batch_size = batch_size\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.jit = jit\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # dictionary containing the trainable parameters\n    self.n_qubits_ = None\n    self.scaler = None  # data preprocessing\n    self.circuit = None\n</code></pre> <code></code> construct_model <pre><code>construct_model()\n</code></pre> <p>Construct the quantum circuit used in the model.</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def construct_model(self):\n    \"\"\"Construct the quantum circuit used in the model.\"\"\"\n\n    dev = qml.device(self.dev_type, wires=self.n_qubits_)\n\n    @qml.qnode(dev, **self.qnode_kwargs)\n    def circuit(params, x):\n        \"\"\"A variational quantum circuit with data reuploading.\n\n        Args:\n            params (array[float]): dictionary of parameters\n            x (array[float]): input vector\n\n        Returns:\n            list: Expectation values of Pauli Z on each qubit\n        \"\"\"\n        for layer in range(self.n_layers):\n            x_idx = 0  # to keep track of the data index\n            for i in range(self.n_qubits_):\n                angles = x[x_idx : x_idx + 3] + params[\"thetas\"][i, layer, :]\n                qml.Rot(*angles, wires=i)\n                x_idx += 3\n            if layer % 2 == 0:\n                qml.broadcast(qml.CZ, range(self.n_qubits_), pattern=\"double\")\n            else:\n                qml.broadcast(qml.CZ, range(self.n_qubits_), pattern=\"double_odd\")\n\n        # final reupload without CZs\n        x_idx = 0\n        for i in range(self.n_qubits_):\n            angles = x[x_idx : x_idx + 3] + params[\"thetas\"][i, self.n_layers, :]\n            qml.Rot(*angles, wires=i)\n            x_idx += 3\n\n        return [qml.expval(qml.PauliZ(wires=[i])) for i in range(self.n_qubits_)]\n\n    self.circuit = circuit\n\n    def circuit_as_array(params, x):\n        # pennylane returns a list in the circuit above, so we need this\n        return jnp.array(circuit(params, x))\n\n    if self.jit:\n        circuit_as_array = jax.jit(circuit_as_array)\n    self.forward = jax.vmap(circuit_as_array, in_axes=(None, 0))\n    self.chunked_forward = chunk_vmapped_fn(self.forward, 1, self.max_vmap)\n\n    return self.forward\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n    self.initialize(X.shape[1], classes=np.unique(y))\n\n    self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    def loss_fn(params, x, y):\n        y_mat = jnp.vstack(\n            [y for __ in range(self.observable_weight)]\n        ).T  # repeated columns of labels\n        y_mat0 = (1 - y_mat) / 2\n        y_mat1 = (1 + y_mat) / 2\n        alpha_mat0 = jnp.vstack(\n            [params[\"alphas\"][0, :] for __ in range(len(x))]\n        )  # repeated rows of alpha parameters\n        alpha_mat1 = jnp.vstack([params[\"alphas\"][1, :] for __ in range(len(x))])\n\n        expvals = self.forward(params, x)\n        probs0 = (1 - expvals[:, : self.observable_weight]) / 2  # fidelity with |0&gt;\n        probs1 = (1 + expvals[:, : self.observable_weight]) / 2  # fidelity with |1&gt;\n        loss = (\n            1\n            / 2\n            * (\n                jnp.sum(\n                    (alpha_mat0 * probs0 - y_mat0) ** 2 + (alpha_mat1 * probs1 - y_mat1) ** 2\n                )\n            )\n        )  # eqn 23 in plots\n        return loss / len(y)\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    self.construct_model()\n    self.initialize_params()\n    optimizer = optax.adam\n\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        X,\n        y,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels. Args:     n_features (int): Number of features that the classifier expects     classes (array-like): class labels that the classifier expects</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_features = n_features\n    self.n_qubits_ = ceil(self.n_features / 3)\n\n    if self.observable_type == \"single\":\n        self.observable_weight = 1\n    elif self.observable_type == \"half\":\n        if self.n_qubits_ == 1:\n            self.observable_weight = 1\n        else:\n            self.observable_weight = self.n_qubits_ // 2\n    elif self.observable_type == \"full\":\n        self.observable_weight = self.n_qubits_\n\n    self.initialize_params()\n    self.construct_model()\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for batch of data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for batch of data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    check_is_fitted(self)\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    predictions = self.chunked_forward(self.params_, X)\n    predictions = jnp.mean(\n        predictions[:, : self.observable_weight], axis=1\n    )  # use mean of expvals over relevant qubits\n    predictions_2d = np.c_[(1 - predictions) / 2, (1 + predictions) / 2]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n            self.scaler.fit(X)\n\n        X = self.scaler.transform(X)\n\n    X = X * self.scaling\n    X = jnp.pad(X, ((0, 0), (0, (3 - X.shape[1]) % 3)), \"constant\")\n    return X\n</code></pre> <code></code> DataReuploadingClassifierNoTrainableEmbedding <pre><code>DataReuploadingClassifierNoTrainableEmbedding(n_layers=4, observable_type='single', convergence_interval=200, max_steps=10000, learning_rate=0.05, batch_size=32, max_vmap=None, jit=True, scaling=1.0, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax-jit'}, random_state=42)\n</code></pre> <p>               Bases: <code>DataReuploadingClassifier</code></p> <p>Data reuploading classifier from https://arxiv.org/abs/1907.02085. Here we code the 'multi-qubit classifier'. The model consists of layers of trainable data encoding unitaries. Each subsequent sequence of 3 elements of an input vector x are encoded into a SU(2) rotation via pennylane.Rot. The model therefore uses ceil(d/3) qubits.</p> <p>The cost function is given by eqn (23) in the plots, and includes trainable parameters for scaling input data (omegas), the class weights (alphas), and the circuit parameters (thetas). The cost function is a weighted sum of the squared differences between the ideal fidelity to the corresponding class state (here, either |0&gt; or |1&gt;) and the output state of the circuit, for a number of qubits specified by observable_weight. More specifically</p> <p>.. math::</p> <pre><code>\\ell(\\vec{\\theta},\\vec{\\omega},\\vec{\\alpha},\\vec{x}_i) =\n\\sum_{j=1}^{n_{\\text{max}}}(\\alpha_j^0F^0_j-(1-y_i))^2 + (\\alpha_j^1F^1_j-y_i)^2\n</code></pre> <p>where :math:<code>n_max</code> is given by observable_weight and :math:<code>F^0_j</code> is the fidelity of the jth output quibt to the state |0&gt;.</p> <p>When observable_weight !=1 it is not clear from the plots how to predict labels. Here we look at the average fidelities to |0&gt; and |1&gt; of the qubits up to index self.observable_weight and take the label with the largest average fidelity. For self.observable_weight=1 this reverts to their strategy in the plots.</p> <p>Where possible, we have followed the numerical examples found in the repo which accompanies the plots:</p> <p>https://github.com/AdrianPerezSalinas/universal_qlassifier/tree/354d70f940ea737192de4063ff72f859c77e5760</p> PARAMETER DESCRIPTION <code>n_layers</code> <p>Number of blocks used in the trainable embedding. The data is uploaded n_layers+1 times.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>observable_type</code> <p>Defines the number of qubits used to evaluate the weighed cost fucntion, either 'single', 'half' or 'full'.  max_steps (int): Maximum number of training steps. A warning will be raised if training did not     converge.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'single'</code> </p> <code>learning_rate</code> <p>Initial learning rate for training.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.05</code> </p> <code>convergence_interval</code> <p>The number of loss values to consider to decide convergence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>batch_size</code> <p>Size of batches used for computing paraeeter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the keyword arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax-jit'}</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>construct_model</code> <p>Construct the quantum circuit used in the model.</p> <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for batch of data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def __init__(\n    self,\n    n_layers=4,\n    observable_type=\"single\",\n    convergence_interval=200,\n    max_steps=10000,\n    learning_rate=0.05,\n    batch_size=32,\n    max_vmap=None,\n    jit=True,\n    scaling=1.0,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax-jit\"},\n    random_state=42,\n):\n    r\"\"\"\n    Data reuploading classifier from https://arxiv.org/abs/1907.02085. Here we code the 'multi-qubit classifier'.\n    The model consists of layers of trainable data encoding unitaries. Each subsequent sequence of 3 elements of\n    an input vector x are encoded into a SU(2) rotation via pennylane.Rot. The model therefore uses ceil(d/3)\n    qubits.\n\n    The cost function is given by eqn (23) in the plots, and includes trainable parameters for scaling input data\n    (omegas), the class weights (alphas), and the circuit parameters (thetas). The cost\n    function is a weighted sum of the squared differences between the ideal fidelity to the corresponding class state\n    (here, either |0&gt; or |1&gt;) and the output state of the circuit, for a number of qubits specified by\n    observable_weight. More specifically\n\n    .. math::\n\n        \\ell(\\vec{\\theta},\\vec{\\omega},\\vec{\\alpha},\\vec{x}_i) =\n        \\sum_{j=1}^{n_{\\text{max}}}(\\alpha_j^0F^0_j-(1-y_i))^2 + (\\alpha_j^1F^1_j-y_i)^2\n\n    where :math:`n_max` is given by observable_weight and :math:`F^0_j` is the fidelity of the jth output\n    quibt to the state |0&gt;.\n\n    When observable_weight !=1 it is not clear from the plots how to predict labels. Here we look at the average\n    fidelities to |0&gt; and |1&gt; of the qubits up to index self.observable_weight and take the label with the largest\n    average fidelity. For self.observable_weight=1 this reverts to their strategy in the plots.\n\n    Where possible, we have followed the numerical examples found in the repo which accompanies the plots:\n\n    https://github.com/AdrianPerezSalinas/universal_qlassifier/tree/354d70f940ea737192de4063ff72f859c77e5760\n\n    Args:\n        n_layers (int): Number of blocks used in the trainable embedding. The data is uploaded n_layers+1 times.\n        observable_type (str): Defines the number of qubits used to evaluate the weighed cost fucntion,\n            either 'single', 'half' or 'full'.\n             max_steps (int): Maximum number of training steps. A warning will be raised if training did not\n                converge.\n        learning_rate (float): Initial learning rate for training.\n        convergence_interval (int): The number of loss values to consider to decide convergence.\n        batch_size (int): Size of batches used for computing paraeeter updates.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'.\n        qnode_kwargs (str): the keyword arguments passed to the circuit qnode.\n        scaling (float): Factor by which to scale the input data.\n        random_state (int): Seed used for pseudorandom number generation.\n    \"\"\"\n\n    # attributes that do not depend on data\n    self.n_layers = n_layers\n    self.observable_type = observable_type\n    self.convergence_interval = convergence_interval\n    self.max_steps = max_steps\n    self.learning_rate = learning_rate\n    self.batch_size = batch_size\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.jit = jit\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # dictionary containing the trainable parameters\n    self.n_qubits_ = None\n    self.scaler = None  # data preprocessing\n    self.circuit = None\n</code></pre> <code></code> construct_model <pre><code>construct_model()\n</code></pre> <p>Construct the quantum circuit used in the model.</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def construct_model(self):\n    \"\"\"Construct the quantum circuit used in the model.\"\"\"\n\n    dev = qml.device(self.dev_type, wires=self.n_qubits_)\n\n    @qml.qnode(dev, **self.qnode_kwargs)\n    def circuit(params, x):\n        \"\"\"A variational quantum circuit with data reuploading.\n\n        Args:\n            params (array[float]): dictionary of parameters\n            x (array[float]): input vector\n\n        Returns:\n            list: Expectation values of Pauli Z on each qubit\n        \"\"\"\n        for layer in range(self.n_layers):\n            x_idx = 0  # to keep track of the data index\n            for i in range(self.n_qubits_):\n                angles = x[x_idx : x_idx + 3] * params[\"omegas\"][layer, x_idx : x_idx + 3]\n                qml.Rot(*angles, wires=i)\n                x_idx += 3\n\n        # final reupload without CZs\n        x_idx = 0\n        for i in range(self.n_qubits_):\n            angles = x[x_idx : x_idx + 3] * params[\"omegas\"][self.n_layers, x_idx : x_idx + 3]\n            qml.Rot(*angles, wires=i)\n            x_idx += 3\n\n        for layer in range(self.n_layers):\n            x_idx = 0  # to keep track of the data index\n            for i in range(self.n_qubits_):\n                angles = params[\"thetas\"][i, layer, :]\n                qml.Rot(*angles, wires=i)\n                x_idx += 3\n            if layer % 2 == 0:\n                qml.broadcast(qml.CZ, range(self.n_qubits_), pattern=\"double\")\n            else:\n                qml.broadcast(qml.CZ, range(self.n_qubits_), pattern=\"double_odd\")\n\n            # final reupload without CZs\n        x_idx = 0\n        for i in range(self.n_qubits_):\n            angles = params[\"thetas\"][i, self.n_layers, :]\n            qml.Rot(*angles, wires=i)\n            x_idx += 3\n\n        return [qml.expval(qml.PauliZ(wires=[i])) for i in range(self.n_qubits_)]\n\n    self.circuit = circuit\n\n    def circuit_as_array(params, x):\n        # pennylane returns a list in the circuit above, so we need this\n        return jnp.array(circuit(params, x))\n\n    if self.jit:\n        circuit_as_array = jax.jit(circuit_as_array)\n    self.forward = jax.vmap(circuit_as_array, in_axes=(None, 0))\n    self.chunked_forward = chunk_vmapped_fn(self.forward, 1, self.max_vmap)\n\n    return self.forward\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n    self.initialize(X.shape[1], classes=np.unique(y))\n\n    self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    def loss_fn(params, x, y):\n        y_mat = jnp.vstack(\n            [y for __ in range(self.observable_weight)]\n        ).T  # repeated columns of labels\n        y_mat0 = (1 - y_mat) / 2\n        y_mat1 = (1 + y_mat) / 2\n        alpha_mat0 = jnp.vstack(\n            [params[\"alphas\"][0, :] for __ in range(len(x))]\n        )  # repeated rows of alpha parameters\n        alpha_mat1 = jnp.vstack([params[\"alphas\"][1, :] for __ in range(len(x))])\n\n        expvals = self.forward(params, x)\n        probs0 = (1 - expvals[:, : self.observable_weight]) / 2  # fidelity with |0&gt;\n        probs1 = (1 + expvals[:, : self.observable_weight]) / 2  # fidelity with |1&gt;\n        loss = (\n            1\n            / 2\n            * (\n                jnp.sum(\n                    (alpha_mat0 * probs0 - y_mat0) ** 2 + (alpha_mat1 * probs1 - y_mat1) ** 2\n                )\n            )\n        )  # eqn 23 in plots\n        return loss / len(y)\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    self.construct_model()\n    self.initialize_params()\n    optimizer = optax.adam\n\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        X,\n        y,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels. Args:     n_features (int): Number of features that the classifier expects     classes (array-like): class labels that the classifier expects</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_features = n_features\n    self.n_qubits_ = ceil(self.n_features / 3)\n\n    if self.observable_type == \"single\":\n        self.observable_weight = 1\n    elif self.observable_type == \"half\":\n        if self.n_qubits_ == 1:\n            self.observable_weight = 1\n        else:\n            self.observable_weight = self.n_qubits_ // 2\n    elif self.observable_type == \"full\":\n        self.observable_weight = self.n_qubits_\n\n    self.initialize_params()\n    self.construct_model()\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for batch of data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for batch of data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    check_is_fitted(self)\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    predictions = self.chunked_forward(self.params_, X)\n    predictions = jnp.mean(\n        predictions[:, : self.observable_weight], axis=1\n    )  # use mean of expvals over relevant qubits\n    predictions_2d = np.c_[(1 - predictions) / 2, (1 + predictions) / 2]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n            self.scaler.fit(X)\n\n        X = self.scaler.transform(X)\n\n    X = X * self.scaling\n    X = jnp.pad(X, ((0, 0), (0, (3 - X.shape[1]) % 3)), \"constant\")\n    return X\n</code></pre> <code></code> DataReuploadingClassifierSeparable <pre><code>DataReuploadingClassifierSeparable(n_layers=4, observable_type='single', convergence_interval=200, max_steps=10000, learning_rate=0.05, batch_size=32, max_vmap=None, jit=True, scaling=1.0, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax-jit'}, random_state=42)\n</code></pre> <p>               Bases: <code>DataReuploadingClassifier</code></p> <p>Data reuploading classifier from https://arxiv.org/abs/1907.02085. Here we code the 'multi-qubit classifier'. The model consists of layers of trainable data encoding unitaries. Each subsequent sequence of 3 elements of an input vector x are encoded into a SU(2) rotation via pennylane.Rot. The model therefore uses ceil(d/3) qubits.</p> <p>The cost function is given by eqn (23) in the plots, and includes trainable parameters for scaling input data (omegas), the class weights (alphas), and the circuit parameters (thetas). The cost function is a weighted sum of the squared differences between the ideal fidelity to the corresponding class state (here, either |0&gt; or |1&gt;) and the output state of the circuit, for a number of qubits specified by observable_weight. More specifically</p> <p>.. math::</p> <pre><code>\\ell(\\vec{\\theta},\\vec{\\omega},\\vec{\\alpha},\\vec{x}_i) =\n\\sum_{j=1}^{n_{\\text{max}}}(\\alpha_j^0F^0_j-(1-y_i))^2 + (\\alpha_j^1F^1_j-y_i)^2\n</code></pre> <p>where :math:<code>n_max</code> is given by observable_weight and :math:<code>F^0_j</code> is the fidelity of the jth output quibt to the state |0&gt;.</p> <p>When observable_weight !=1 it is not clear from the plots how to predict labels. Here we look at the average fidelities to |0&gt; and |1&gt; of the qubits up to index self.observable_weight and take the label with the largest average fidelity. For self.observable_weight=1 this reverts to their strategy in the plots.</p> <p>Where possible, we have followed the numerical examples found in the repo which accompanies the plots:</p> <p>https://github.com/AdrianPerezSalinas/universal_qlassifier/tree/354d70f940ea737192de4063ff72f859c77e5760</p> PARAMETER DESCRIPTION <code>n_layers</code> <p>Number of blocks used in the trainable embedding. The data is uploaded n_layers+1 times.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>observable_type</code> <p>Defines the number of qubits used to evaluate the weighed cost fucntion, either 'single', 'half' or 'full'.  max_steps (int): Maximum number of training steps. A warning will be raised if training did not     converge.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'single'</code> </p> <code>learning_rate</code> <p>Initial learning rate for training.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.05</code> </p> <code>convergence_interval</code> <p>The number of loss values to consider to decide convergence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>batch_size</code> <p>Size of batches used for computing paraeeter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the keyword arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax-jit'}</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>construct_model</code> <p>Construct the quantum circuit used in the model.</p> <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for batch of data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def __init__(\n    self,\n    n_layers=4,\n    observable_type=\"single\",\n    convergence_interval=200,\n    max_steps=10000,\n    learning_rate=0.05,\n    batch_size=32,\n    max_vmap=None,\n    jit=True,\n    scaling=1.0,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax-jit\"},\n    random_state=42,\n):\n    r\"\"\"\n    Data reuploading classifier from https://arxiv.org/abs/1907.02085. Here we code the 'multi-qubit classifier'.\n    The model consists of layers of trainable data encoding unitaries. Each subsequent sequence of 3 elements of\n    an input vector x are encoded into a SU(2) rotation via pennylane.Rot. The model therefore uses ceil(d/3)\n    qubits.\n\n    The cost function is given by eqn (23) in the plots, and includes trainable parameters for scaling input data\n    (omegas), the class weights (alphas), and the circuit parameters (thetas). The cost\n    function is a weighted sum of the squared differences between the ideal fidelity to the corresponding class state\n    (here, either |0&gt; or |1&gt;) and the output state of the circuit, for a number of qubits specified by\n    observable_weight. More specifically\n\n    .. math::\n\n        \\ell(\\vec{\\theta},\\vec{\\omega},\\vec{\\alpha},\\vec{x}_i) =\n        \\sum_{j=1}^{n_{\\text{max}}}(\\alpha_j^0F^0_j-(1-y_i))^2 + (\\alpha_j^1F^1_j-y_i)^2\n\n    where :math:`n_max` is given by observable_weight and :math:`F^0_j` is the fidelity of the jth output\n    quibt to the state |0&gt;.\n\n    When observable_weight !=1 it is not clear from the plots how to predict labels. Here we look at the average\n    fidelities to |0&gt; and |1&gt; of the qubits up to index self.observable_weight and take the label with the largest\n    average fidelity. For self.observable_weight=1 this reverts to their strategy in the plots.\n\n    Where possible, we have followed the numerical examples found in the repo which accompanies the plots:\n\n    https://github.com/AdrianPerezSalinas/universal_qlassifier/tree/354d70f940ea737192de4063ff72f859c77e5760\n\n    Args:\n        n_layers (int): Number of blocks used in the trainable embedding. The data is uploaded n_layers+1 times.\n        observable_type (str): Defines the number of qubits used to evaluate the weighed cost fucntion,\n            either 'single', 'half' or 'full'.\n             max_steps (int): Maximum number of training steps. A warning will be raised if training did not\n                converge.\n        learning_rate (float): Initial learning rate for training.\n        convergence_interval (int): The number of loss values to consider to decide convergence.\n        batch_size (int): Size of batches used for computing paraeeter updates.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'.\n        qnode_kwargs (str): the keyword arguments passed to the circuit qnode.\n        scaling (float): Factor by which to scale the input data.\n        random_state (int): Seed used for pseudorandom number generation.\n    \"\"\"\n\n    # attributes that do not depend on data\n    self.n_layers = n_layers\n    self.observable_type = observable_type\n    self.convergence_interval = convergence_interval\n    self.max_steps = max_steps\n    self.learning_rate = learning_rate\n    self.batch_size = batch_size\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.jit = jit\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # dictionary containing the trainable parameters\n    self.n_qubits_ = None\n    self.scaler = None  # data preprocessing\n    self.circuit = None\n</code></pre> <code></code> construct_model <pre><code>construct_model()\n</code></pre> <p>Construct the quantum circuit used in the model.</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def construct_model(self):\n    \"\"\"Construct the quantum circuit used in the model.\"\"\"\n\n    dev = qml.device(self.dev_type, wires=self.n_qubits_)\n\n    @qml.qnode(dev, **self.qnode_kwargs)\n    def circuit(params, x):\n        \"\"\"A variational quantum circuit with data reuploading.\n\n        Args:\n            params (array[float]): dictionary of parameters\n            x (array[float]): input vector\n\n        Returns:\n            list: Expectation values of Pauli Z on each qubit\n        \"\"\"\n        for layer in range(self.n_layers):\n            x_idx = 0  # to keep track of the data index\n            for i in range(self.n_qubits_):\n                angles = (\n                    x[x_idx : x_idx + 3] * params[\"omegas\"][layer, x_idx : x_idx + 3]\n                    + params[\"thetas\"][i, layer, :]\n                )\n                qml.Rot(*angles, wires=i)\n                x_idx += 3\n\n        # final reupload without CZs\n        x_idx = 0\n        for i in range(self.n_qubits_):\n            angles = (\n                x[x_idx : x_idx + 3] * params[\"omegas\"][self.n_layers, x_idx : x_idx + 3]\n                + params[\"thetas\"][i, self.n_layers, :]\n            )\n            qml.Rot(*angles, wires=i)\n            x_idx += 3\n\n        return [qml.expval(qml.PauliZ(wires=[i])) for i in range(self.n_qubits_)]\n\n    self.circuit = circuit\n\n    def circuit_as_array(params, x):\n        # pennylane returns a list in the circuit above, so we need this\n        return jnp.array(circuit(params, x))\n\n    if self.jit:\n        circuit_as_array = jax.jit(circuit_as_array)\n    self.forward = jax.vmap(circuit_as_array, in_axes=(None, 0))\n    self.chunked_forward = chunk_vmapped_fn(self.forward, 1, self.max_vmap)\n\n    return self.forward\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n    self.initialize(X.shape[1], classes=np.unique(y))\n\n    self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    def loss_fn(params, x, y):\n        y_mat = jnp.vstack(\n            [y for __ in range(self.observable_weight)]\n        ).T  # repeated columns of labels\n        y_mat0 = (1 - y_mat) / 2\n        y_mat1 = (1 + y_mat) / 2\n        alpha_mat0 = jnp.vstack(\n            [params[\"alphas\"][0, :] for __ in range(len(x))]\n        )  # repeated rows of alpha parameters\n        alpha_mat1 = jnp.vstack([params[\"alphas\"][1, :] for __ in range(len(x))])\n\n        expvals = self.forward(params, x)\n        probs0 = (1 - expvals[:, : self.observable_weight]) / 2  # fidelity with |0&gt;\n        probs1 = (1 + expvals[:, : self.observable_weight]) / 2  # fidelity with |1&gt;\n        loss = (\n            1\n            / 2\n            * (\n                jnp.sum(\n                    (alpha_mat0 * probs0 - y_mat0) ** 2 + (alpha_mat1 * probs1 - y_mat1) ** 2\n                )\n            )\n        )  # eqn 23 in plots\n        return loss / len(y)\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    self.construct_model()\n    self.initialize_params()\n    optimizer = optax.adam\n\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        X,\n        y,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels. Args:     n_features (int): Number of features that the classifier expects     classes (array-like): class labels that the classifier expects</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_features = n_features\n    self.n_qubits_ = ceil(self.n_features / 3)\n\n    if self.observable_type == \"single\":\n        self.observable_weight = 1\n    elif self.observable_type == \"half\":\n        if self.n_qubits_ == 1:\n            self.observable_weight = 1\n        else:\n            self.observable_weight = self.n_qubits_ // 2\n    elif self.observable_type == \"full\":\n        self.observable_weight = self.n_qubits_\n\n    self.initialize_params()\n    self.construct_model()\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for batch of data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for batch of data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    check_is_fitted(self)\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    predictions = self.chunked_forward(self.params_, X)\n    predictions = jnp.mean(\n        predictions[:, : self.observable_weight], axis=1\n    )  # use mean of expvals over relevant qubits\n    predictions_2d = np.c_[(1 - predictions) / 2, (1 + predictions) / 2]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n            self.scaler.fit(X)\n\n        X = self.scaler.transform(X)\n\n    X = X * self.scaling\n    X = jnp.pad(X, ((0, 0), (0, (3 - X.shape[1]) % 3)), \"constant\")\n    return X\n</code></pre> <code></code> DressedQuantumCircuitClassifier <pre><code>DressedQuantumCircuitClassifier(n_layers=3, learning_rate=0.001, batch_size=32, max_vmap=None, jit=True, max_steps=100000, convergence_interval=200, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax-jit'}, scaling=1.0, random_state=42)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Dressed quantum circuit from https://arxiv.org/abs/1912.08278. The model consists of the following sequence     * a single layer fully connected trainable neural network with tanh activation function     * a parameterised quantum circuit taking the above outputs as input     * a single layer fully connected trainable neural network taking local expectation values of the above       circuit as input</p> <p>The last neural network maps to two neurons that we take the softmax of to get class probabilities. The model is trained via binary cross entropy loss.</p> PARAMETER DESCRIPTION <code>n_layers</code> <p>number of layers in the variational part of the circuit.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>learning_rate</code> <p>initial learning rate for gradient descent.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> <code>max_steps</code> <p>Maximum number of training steps. A warning will be raised if training did not converge.</p> <p> TYPE: <code>int</code> DEFAULT: <code>100000</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>batch_size</code> <p>Size of batches used for computing parameter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>convergence_interval</code> <p>The number of loss values to consider to decide convergence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the keyword arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax-jit'}</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>input_transform</code> <p>The first neural network that we implment as matrix multiplication.</p> <code>output_transform</code> <p>The final neural network</p> <code>predict</code> <p>Predict labels for data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/dressed_quantum_circuit.py</code> <pre><code>def __init__(\n    self,\n    n_layers=3,\n    learning_rate=0.001,\n    batch_size=32,\n    max_vmap=None,\n    jit=True,\n    max_steps=100000,\n    convergence_interval=200,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax-jit\"},\n    scaling=1.0,\n    random_state=42,\n):\n    r\"\"\"\n    Dressed quantum circuit from https://arxiv.org/abs/1912.08278. The model consists of the following sequence\n        * a single layer fully connected trainable neural network with tanh activation function\n        * a parameterised quantum circuit taking the above outputs as input\n        * a single layer fully connected trainable neural network taking local expectation values of the above\n          circuit as input\n\n    The last neural network maps to two neurons that we take the softmax of to get class probabilities.\n    The model is trained via binary cross entropy loss.\n\n    Args:\n        n_layers (int): number of layers in the variational part of the circuit.\n        learning_rate (float): initial learning rate for gradient descent.\n        max_steps (int): Maximum number of training steps. A warning will be raised if training did not converge.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        batch_size (int): Size of batches used for computing parameter updates.\n        convergence_interval (int): The number of loss values to consider to decide convergence.\n        jit (bool): Whether to use just in time compilation.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'.\n        qnode_kwargs (str): the keyword arguments passed to the circuit qnode.\n        scaling (float): Factor by which to scale the input data.\n        random_state (int): Seed used for pseudorandom number generation.\n    \"\"\"\n    # attributes that do not depend on data\n    self.n_layers = n_layers\n    self.learning_rate = learning_rate\n    self.batch_size = batch_size\n    self.max_steps = max_steps\n    self.convergence_interval = convergence_interval\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.jit = jit\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # Dictionary containing the trainable parameters\n    self.n_qubits_ = None\n    self.n_features_ = None\n    self.scaler = None  # data scaler will be fitted on training data\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/dressed_quantum_circuit.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n\n    self.scaler = StandardScaler()\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    self.initialize(X.shape[1], classes=np.unique(y))\n\n    optimizer = optax.adam\n\n    def loss_fn(params, X, y):\n        vals = self.forward(params, X)\n        # convert to 0,1 one hot encoded labels\n        labels = jax.nn.one_hot(jax.nn.relu(y), 2)\n        return jnp.mean(optax.softmax_cross_entropy(vals, labels))\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        X,\n        y,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels. Args:     n_features (int): Number of features that the classifier expects     classes (array-like): class labels that the classifier expects</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/dressed_quantum_circuit.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_features_ = n_features\n    self.n_qubits_ = self.n_features_\n\n    self.initialize_params()\n    self.construct_model()\n</code></pre> <code></code> input_transform <pre><code>input_transform(params, x)\n</code></pre> <p>The first neural network that we implment as matrix multiplication.</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/dressed_quantum_circuit.py</code> <pre><code>def input_transform(self, params, x):\n    \"\"\"\n    The first neural network that we implment as matrix multiplication.\n    \"\"\"\n    x = jnp.matmul(params[\"input_weights\"], x)\n    x = jnp.tanh(x) * jnp.pi / 2\n    return x\n</code></pre> <code></code> output_transform <pre><code>output_transform(params, x)\n</code></pre> <p>The final neural network</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/dressed_quantum_circuit.py</code> <pre><code>def output_transform(self, params, x):\n    \"\"\"\n    The final neural network\n    \"\"\"\n    x = jnp.matmul(params[\"output_weights\"], x)\n    return x\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred</code> <p>Predicted labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/dressed_quantum_circuit.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred (np.ndarray): Predicted labels of shape (n_samples,)\n    \"\"\"\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/dressed_quantum_circuit.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    return jax.nn.softmax(self.chunked_forward(self.params_, X))\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/dressed_quantum_circuit.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = StandardScaler()\n            self.scaler.fit(X)\n\n        X = self.scaler.transform(X)\n    return X * self.scaling\n</code></pre> <code></code> IQPKernelClassifier <pre><code>IQPKernelClassifier(svm=SVC(kernel='precomputed', probability=True), repeats=2, C=1.0, jit=False, random_state=42, scaling=1.0, max_vmap=250, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax-jit', 'diff_method': None})\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Kernel version of the classifier from https://arxiv.org/pdf/1804.11326v2.pdf. The kernel function is given by</p> <p>.. math::     k(x,x')=\\vert\\langle 0 \\vert U^\\dagger(x')U(x)\\vert 0 \\rangle\\vert^2</p> <p>where :math:<code>U(x)</code> is an IQP circuit implemented via Pennylane's <code>IQPEmbedding</code>.</p> <p>We precompute the kernel matrix from the data directly, and pass it to scikit-learn's support vector classifier SVC. This  allows us to benefit from JAX parallelisation when computing the kernel matrices.</p> <p>The model requires evaluating a number of circuits given by the square of the number of data samples, and is therefore only appropriate for relatively small datasets.</p> PARAMETER DESCRIPTION <code>svm</code> <p>scikit-learn SVM class object used to fit the model from the kernel matrix</p> <p> TYPE: <code>SVC</code> DEFAULT: <code>SVC(kernel='precomputed', probability=True)</code> </p> <code>repeats</code> <p>number of times the IQP structure is repeated in the embedding circuit.</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>C</code> <p>regularization parameter for SVC. Lower values imply stronger regularization.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>250</code> </p> <code>qnode_kwargs</code> <p>the key word arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax-jit', 'diff_method': None}</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>random_state</code> <p>seed used for reproducibility.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>fit</code> <p>Fit the model to data X and labels y. Uses sklearn's SVM classifier</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>precompute_kernel</code> <p>compute the kernel matrix relative to data sets X1 and X2</p> <code>predict</code> <p>Predict labels for data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/iqp_kernel.py</code> <pre><code>def __init__(\n    self,\n    svm=SVC(kernel=\"precomputed\", probability=True),\n    repeats=2,\n    C=1.0,\n    jit=False,\n    random_state=42,\n    scaling=1.0,\n    max_vmap=250,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax-jit\", \"diff_method\": None},\n):\n    r\"\"\"\n    Kernel version of the classifier from https://arxiv.org/pdf/1804.11326v2.pdf.\n    The kernel function is given by\n\n    .. math::\n        k(x,x')=\\vert\\langle 0 \\vert U^\\dagger(x')U(x)\\vert 0 \\rangle\\vert^2\n\n    where :math:`U(x)` is an IQP circuit implemented via Pennylane's `IQPEmbedding`.\n\n    We precompute the kernel matrix from the data directly, and pass it to scikit-learn's support vector\n    classifier SVC. This  allows us to benefit from JAX parallelisation when computing the kernel\n    matrices.\n\n    The model requires evaluating a number of circuits given by the square of the number of data\n    samples, and is therefore only appropriate for relatively small datasets.\n\n    Args:\n        svm (sklearn.svm.SVC): scikit-learn SVM class object used to fit the model from the kernel matrix\n        repeats (int): number of times the IQP structure is repeated in the embedding circuit.\n        C (float): regularization parameter for SVC. Lower values imply stronger regularization.\n        jit (bool): Whether to use just in time compilation.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        qnode_kwargs (str): the key word arguments passed to the circuit qnode.\n        scaling (float): Factor by which to scale the input data.\n        random_state (int): seed used for reproducibility.\n    \"\"\"\n    # attributes that do not depend on data\n    self.repeats = repeats\n    self.C = C\n    self.jit = jit\n    self.max_vmap = max_vmap\n    self.svm = svm\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None\n    self.n_qubits_ = None\n    self.scaler = None  # data scaler will be fitted on training data\n    self.circuit = None\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y. Uses sklearn's SVM classifier</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/iqp_kernel.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y. Uses sklearn's SVM classifier\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n\n    self.svm.random_state = int(\n        jax.random.randint(self.generate_key(), shape=(1,), minval=0, maxval=1000000)\n    )\n\n    self.initialize(X.shape[1], np.unique(y))\n\n    self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    self.params_ = {\"x_train\": X}\n    kernel_matrix = self.precompute_kernel(X, X)\n\n    start = time.time()\n    # we are updating this value here, in case it was\n    # changed after initialising the model\n    self.svm.C = self.C\n    self.svm.fit(kernel_matrix, y)\n    end = time.time()\n    self.training_time_ = end - start\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels.</p> PARAMETER DESCRIPTION <code>n_features</code> <p>Number of features that the classifier expects</p> <p> TYPE: <code>int</code> </p> <code>classes</code> <p>class labels that the classifier expects</p> <p> TYPE: <code>array - like</code> DEFAULT: <code>None</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/iqp_kernel.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_qubits_ = n_features\n\n    self.construct_circuit()\n</code></pre> <code></code> precompute_kernel <pre><code>precompute_kernel(X1, X2)\n</code></pre> <p>compute the kernel matrix relative to data sets X1 and X2 Args:     X1 (np.array): first dataset of input vectors     X2 (np.array): second dataset of input vectors Returns:     kernel_matrix (np.array): matrix of size (len(X1),len(X2)) with elements K(x_1,x_2)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/iqp_kernel.py</code> <pre><code>def precompute_kernel(self, X1, X2):\n    \"\"\"\n    compute the kernel matrix relative to data sets X1 and X2\n    Args:\n        X1 (np.array): first dataset of input vectors\n        X2 (np.array): second dataset of input vectors\n    Returns:\n        kernel_matrix (np.array): matrix of size (len(X1),len(X2)) with elements K(x_1,x_2)\n    \"\"\"\n    dim1 = len(X1)\n    dim2 = len(X2)\n\n    # concatenate all pairs of vectors\n    Z = jnp.array([np.concatenate((X1[i], X2[j])) for i in range(dim1) for j in range(dim2)])\n\n    circuit = self.construct_circuit()\n    self.batched_circuit = chunk_vmapped_fn(\n        jax.vmap(circuit, 0), start=0, max_vmap=self.max_vmap\n    )\n    kernel_values = self.batched_circuit(Z)[:, 0]\n\n    # reshape the values into the kernel matrix\n    kernel_matrix = np.reshape(kernel_values, (dim1, dim2))\n\n    return kernel_matrix\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred</code> <p>Predicted labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/iqp_kernel.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred (np.ndarray): Predicted labels of shape (n_samples,)\n    \"\"\"\n    X = self.transform(X)\n    kernel_matrix = self.precompute_kernel(X, self.params_[\"x_train\"])\n    return self.svm.predict(kernel_matrix)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X. note that this may be inconsistent with predict; see the sklearn docummentation for details.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/iqp_kernel.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n    note that this may be inconsistent with predict; see the sklearn docummentation for details.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n\n    if \"x_train\" not in self.params_:\n        raise ValueError(\"Model cannot predict without fitting to data first.\")\n\n    X = self.transform(X)\n    kernel_matrix = self.precompute_kernel(X, self.params_[\"x_train\"])\n    return self.svm.predict_proba(kernel_matrix)\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/iqp_kernel.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n            self.scaler.fit(X)\n        X = self.scaler.transform(X)\n\n    return X * self.scaling\n</code></pre> <code></code> IQPVariationalClassifier <pre><code>IQPVariationalClassifier(repeats=1, n_layers=10, learning_rate=0.001, batch_size=32, max_vmap=None, jit=True, max_steps=10000, convergence_interval=200, random_state=42, scaling=1.0, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax'})\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Variational verison of the classifier from https://arxiv.org/pdf/1804.11326v2.pdf. The model is a standard variational quantum classifier</p> <p>.. math::</p> <pre><code>f(x)=\\langle 0 \\vert U^\\dagger(x)V^\\dagger(\\theta) H V(\\theta)U(x)\\vert 0 \\rangle\n</code></pre> <p>where the data embedding unitary :math:<code>U(x)</code> is based on an IQP circuit stucture and implemented via pennylane.IQPEmbedding, and the trainable unitay :math:<code>V(\\theta)</code> is implemented via pennylane.StronglyEntanglingLayers.</p> <p>The model is trained using a linear loss function equivalent to the probability of incorrect classification.</p> PARAMETER DESCRIPTION <code>repeats</code> <p>Number of times to repeat the IQP embedding circuit structure.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>n_layers</code> <p>Number of layers in the variational part of the circuit.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>learning_rate</code> <p>Learning rate for gradient descent.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> <code>batch_size</code> <p>Size of batches used for computing paraemeter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>convergence_threshold</code> <p>If loss changes less than this threshold for 10 consecutive steps we stop training.</p> <p> TYPE: <code>float</code> </p> <code>max_steps</code> <p>Maximum number of training steps. A warning will be raised if training did not converge.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the keyword arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax'}</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/iqp_variational.py</code> <pre><code>def __init__(\n    self,\n    repeats=1,\n    n_layers=10,\n    learning_rate=0.001,\n    batch_size=32,\n    max_vmap=None,\n    jit=True,\n    max_steps=10000,\n    convergence_interval=200,\n    random_state=42,\n    scaling=1.0,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax\"},\n):\n    r\"\"\"\n    Variational verison of the classifier from https://arxiv.org/pdf/1804.11326v2.pdf.\n    The model is a standard variational quantum classifier\n\n    .. math::\n\n        f(x)=\\langle 0 \\vert U^\\dagger(x)V^\\dagger(\\theta) H V(\\theta)U(x)\\vert 0 \\rangle\n\n    where the data embedding unitary :math:`U(x)` is based on an IQP circuit stucture and implemented via\n    pennylane.IQPEmbedding, and the trainable unitay :math:`V(\\theta)` is implemented via\n    pennylane.StronglyEntanglingLayers.\n\n    The model is trained using a linear loss function equivalent to the probability of incorrect classification.\n\n    Args:\n        repeats (int): Number of times to repeat the IQP embedding circuit structure.\n        n_layers (int): Number of layers in the variational part of the circuit.\n        learning_rate (float): Learning rate for gradient descent.\n        batch_size (int): Size of batches used for computing paraemeter updates.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        convergence_threshold (float): If loss changes less than this threshold for 10 consecutive steps we stop training.\n        max_steps (int): Maximum number of training steps. A warning will be raised if training did not converge.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'.\n        qnode_kwargs (str): the keyword arguments passed to the circuit qnode.\n        random_state (int): Seed used for pseudorandom number generation.\n    \"\"\"\n\n    # attributes that do not depend on data\n    self.repeats = repeats\n    self.n_layers = n_layers\n    self.learning_rate = learning_rate\n    self.batch_size = batch_size\n    self.max_steps = max_steps\n    self.convergence_interval = convergence_interval\n    self.batch_size = batch_size\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.jit = jit\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # Dictionary containing the trainable parameters\n    self.n_qubits_ = None\n    self.scaler = None  # data scaler will be fitted on training data\n    self.circuit = None\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/iqp_variational.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n\n    self.initialize(n_features=X.shape[1], classes=np.unique(y))\n\n    self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    optimizer = optax.adam\n\n    def loss_fn(params, X, y):\n        expvals = self.forward(params, X)\n        probs = (1 - expvals * y) / 2  # the probs of incorrect classification\n        return jnp.mean(probs)\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        X,\n        y,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels.</p> PARAMETER DESCRIPTION <code>n_features</code> <p>Number of features that the classifier expects</p> <p> TYPE: <code>int</code> </p> <code>classes</code> <p>class labels that the classifier expects</p> <p> TYPE: <code>array - like</code> DEFAULT: <code>None</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/iqp_variational.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_qubits_ = n_features\n    self.initialize_params()\n    self.construct_model()\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred</code> <p>Predicted labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/iqp_variational.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred (np.ndarray): Predicted labels of shape (n_samples,)\n    \"\"\"\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/iqp_variational.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    predictions = self.chunked_forward(self.params_, X)\n    predictions_2d = np.c_[(1 - predictions) / 2, (1 + predictions) / 2]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/iqp_variational.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n            self.scaler.fit(X)\n        X = self.scaler.transform(X)\n    return X * self.scaling\n</code></pre> <code></code> ProjectedQuantumKernel <pre><code>ProjectedQuantumKernel(svm=SVC(kernel='precomputed', probability=True), gamma_factor=1.0, C=1.0, embedding='Hamiltonian', t=1.0 / 3, trotter_steps=5, jit=True, max_vmap=None, scaling=1.0, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax-jit', 'diff_method': None}, random_state=42)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Kernel based classifier from https://arxiv.org/pdf/2011.01938v2.pdf.</p> <p>The Kernel function is</p> <p>.. math::     k(x_i, x_j) = \\exp (-\\gamma  \\sum_k \\sum_{P \\in {X, Y, Z}} ( \\text{tr}(P \\rho(x_i)_k)     - \\text{tr}( P \\rho(x_j)_k))^2)</p> <p>where :math:<code>\\rho_k(x_i)</code> is the reduced state of the kth qubit of the data embedded density matrix and :math:<code>\\gamma</code> is a hyperparameter of the model that we scale from the default value given in the plots.</p> <p>For embedding='Hamiltonian' a layer or random single qubit rotations are performed, followed by a Hamiltonian time evolution corresponding to a trotterised evolution of a Heisenberg Hamiltonian:</p> <p>.. math::     \\prod_{j=1}^n \\exp(-i \\frac{t}{L} x_{j} (X_j X_{j+1} + Y_j Y_{j+1} + Z_j Z_{j+1})).</p> <p>where :math:<code>t</code> and :math:<code>L</code> are the evolution time and the number of trotter steps that are controlled by hyperparameters <code>t</code> and <code>trotter_steps</code>.</p> <p>For emedding='IQP' an IQP embedding is used via PennyLanes's IQPEmbedding class.</p> <p>We precompute the kernel matrix from data and pass it to scikit-learn's support vector machine class SVC, which fits a classifier.</p> PARAMETER DESCRIPTION <code>svm</code> <p>scikit-learn SVC class object.</p> <p> TYPE: <code>SVC</code> DEFAULT: <code>SVC(kernel='precomputed', probability=True)</code> </p> <code>gamma_factor</code> <p>the factor that multiplies the default scaling parameter in the kernel.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>C</code> <p>regularization parameter when fitting the kernel model.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>embedding</code> <p>The choice of embedding circuit used to construct the kernel. Either 'IQP' or 'Hamiltonian'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'Hamiltonian'</code> </p> <code>t</code> <p>The evolution time used in the 'Hamiltonian' data embedding. The time is given by n_features*t.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0 / 3</code> </p> <code>trotter_steps</code> <p>the number of trotter steps used in the 'Hamiltonian' embedding circuit.</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the key word arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax-jit', 'diff_method': None}</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>construct_circuit</code> <p>Constructs the circuit to get the expvals of a given qubit and Pauli operator</p> <code>fit</code> <p>Fit the model to data X and labels y. Uses sklearn's SVM classifier</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>precompute_kernel</code> <p>compute the kernel matrix relative to data sets X1 and X2</p> <code>predict</code> <p>Predict labels for data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/projected_quantum_kernel.py</code> <pre><code>def __init__(\n    self,\n    svm=SVC(kernel=\"precomputed\", probability=True),\n    gamma_factor=1.0,\n    C=1.0,\n    embedding=\"Hamiltonian\",\n    t=1.0 / 3,\n    trotter_steps=5,\n    jit=True,\n    max_vmap=None,\n    scaling=1.0,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax-jit\", \"diff_method\": None},\n    random_state=42,\n):\n    r\"\"\"\n    Kernel based classifier from https://arxiv.org/pdf/2011.01938v2.pdf.\n\n    The Kernel function is\n\n    .. math::\n        k(x_i, x_j) = \\exp (-\\gamma  \\sum_k \\sum_{P \\in \\{X, Y, Z\\}} ( \\text{tr}(P \\rho(x_i)_k)\n        - \\text{tr}( P \\rho(x_j)_k))^2)\n\n    where :math:`\\rho_k(x_i)` is the reduced state of the kth qubit of the data embedded density matrix and\n    :math:`\\gamma` is a hyperparameter of the model that we scale from the default value given in the plots.\n\n    For embedding='Hamiltonian' a layer or random single qubit rotations are performed, followed by a Hamiltonian\n    time evolution corresponding to a trotterised evolution of a Heisenberg Hamiltonian:\n\n    .. math::\n        \\prod_{j=1}^n \\exp(-i \\frac{t}{L} x_{j} (X_j X_{j+1} + Y_j Y_{j+1} + Z_j Z_{j+1})).\n\n    where :math:`t` and :math:`L` are the evolution time and the number of trotter steps that are controlled by\n    hyperparameters `t` and `trotter_steps`.\n\n    For emedding='IQP' an IQP embedding is used via PennyLanes's IQPEmbedding class.\n\n    We precompute the kernel matrix from data and pass it to scikit-learn's support vector machine class SVC,\n    which fits a classifier.\n\n    Args:\n        svm (sklearn.svm.SVC): scikit-learn SVC class object.\n        gamma_factor (float): the factor that multiplies the default scaling parameter in the kernel.\n        C (float): regularization parameter when fitting the kernel model.\n        embedding (str): The choice of embedding circuit used to construct the kernel.\n            Either 'IQP' or 'Hamiltonian'.\n        t (float): The evolution time used in the 'Hamiltonian' data embedding. The time is\n            given by n_features*t.\n        trotter_steps (int): the number of trotter steps used in the 'Hamiltonian' embedding circuit.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'\n        qnode_kwargs (str): the key word arguments passed to the circuit qnode.\n        scaling (float): Factor by which to scale the input data.\n        random_state (int): Seed used for pseudorandom number generation.\n    \"\"\"\n    # attributes that do not depend on data\n    self.gamma_factor = gamma_factor\n    self.svm = svm\n    self.C = C\n    self.embedding = embedding\n    self.t = t\n    self.trotter_steps = trotter_steps\n    self.jit = jit\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = 50\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None\n    self.n_qubits_ = None\n    self.n_features_ = None\n    self.rotation_angles_ = None  # for hamiltonian embedding\n    self.scaler = None  # data scaler will be fitted on training data\n    self.circuit = None\n</code></pre> <code></code> construct_circuit <pre><code>construct_circuit()\n</code></pre> <p>Constructs the circuit to get the expvals of a given qubit and Pauli operator We will use JAX to parallelize over these circuits in precompute kernel. Args:     P: a pennylane Pauli X,Y,Z operator on a given qubit</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/projected_quantum_kernel.py</code> <pre><code>def construct_circuit(self):\n    \"\"\"\n    Constructs the circuit to get the expvals of a given qubit and Pauli operator\n    We will use JAX to parallelize over these circuits in precompute kernel.\n    Args:\n        P: a pennylane Pauli X,Y,Z operator on a given qubit\n    \"\"\"\n    if self.embedding == \"IQP\":\n\n        def embedding(x):\n            qml.IQPEmbedding(x, wires=range(self.n_qubits_), n_repeats=2)\n\n    elif self.embedding == \"Hamiltonian\":\n\n        def embedding(x):\n            evol_time = self.t / self.trotter_steps * (self.n_qubits_ - 1)\n            for i in range(self.n_qubits_):\n                qml.Rot(\n                    self.rotation_angles_[i, 0],\n                    self.rotation_angles_[i, 1],\n                    self.rotation_angles_[i, 2],\n                    wires=i,\n                )\n            for __ in range(self.trotter_steps):\n                for j in range(self.n_qubits_ - 1):\n                    qml.IsingXX(x[j] * evol_time, wires=[j, j + 1])\n                    qml.IsingYY(x[j] * evol_time, wires=[j, j + 1])\n                    qml.IsingZZ(x[j] * evol_time, wires=[j, j + 1])\n\n    dev = qml.device(self.dev_type, wires=self.n_qubits_)\n\n    @qml.qnode(dev, **self.qnode_kwargs)\n    def circuit(x):\n        embedding(x)\n        return (\n            [qml.expval(qml.PauliX(wires=i)) for i in range(self.n_qubits_)]\n            + [qml.expval(qml.PauliY(wires=i)) for i in range(self.n_qubits_)]\n            + [qml.expval(qml.PauliZ(wires=i)) for i in range(self.n_qubits_)]\n        )\n\n    self.circuit = circuit\n\n    def circuit_as_array(x):\n        return jnp.array(circuit(x))\n\n    if self.jit:\n        circuit_as_array = jax.jit(circuit_as_array)\n    circuit_as_array = jax.vmap(circuit_as_array, in_axes=(0))\n    circuit_as_array = chunk_vmapped_fn(circuit_as_array, 0, self.max_vmap)\n\n    return circuit_as_array\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y. Uses sklearn's SVM classifier</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/projected_quantum_kernel.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y. Uses sklearn's SVM classifier\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n    self.svm.random_state = self.rng.integers(100000)\n\n    self.initialize(X.shape[1], classes=np.unique(y))\n\n    self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    self.params_ = {\"X_train\": X}\n    kernel_matrix = self.precompute_kernel(X, X)\n\n    start = time.time()\n    self.svm.C = self.C\n    self.svm.fit(kernel_matrix, y)\n    end = time.time()\n    self.training_time_ = end - start\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels.</p> PARAMETER DESCRIPTION <code>n_features</code> <p>Number of features that the classifier expects</p> <p> TYPE: <code>int</code> </p> <code>classes</code> <p>class labels that the classifier expects</p> <p> TYPE: <code>array - like</code> DEFAULT: <code>None</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/projected_quantum_kernel.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_features_ = n_features\n\n    if self.embedding == \"IQP\":\n        self.n_qubits_ = self.n_features_\n    elif self.embedding == \"Hamiltonian\":\n        self.n_qubits_ = self.n_features_ + 1\n        self.rotation_angles_ = jnp.array(\n            self.rng.uniform(size=(self.n_qubits_, 3)) * np.pi * 2\n        )\n    self.construct_circuit()\n</code></pre> <code></code> precompute_kernel <pre><code>precompute_kernel(X1, X2)\n</code></pre> <p>compute the kernel matrix relative to data sets X1 and X2 Args:     X1 (np.array): first dataset of input vectors     X2 (np.array): second dataset of input vectors Returns:     kernel_matrix (np.array): matrix of size (len(X1),len(X2)) with elements K(x_1,x_2)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/projected_quantum_kernel.py</code> <pre><code>def precompute_kernel(self, X1, X2):\n    \"\"\"\n    compute the kernel matrix relative to data sets X1 and X2\n    Args:\n        X1 (np.array): first dataset of input vectors\n        X2 (np.array): second dataset of input vectors\n    Returns:\n        kernel_matrix (np.array): matrix of size (len(X1),len(X2)) with elements K(x_1,x_2)\n    \"\"\"\n    dim1 = len(X1)\n    dim2 = len(X2)\n\n    # get all of the Pauli expvals needed to constrcut the kernel\n    self.circuit = self.construct_circuit()\n\n    valsX1 = np.array(self.circuit(X1))\n    valsX1 = np.reshape(valsX1, (dim1, 3, -1))\n    valsX2 = np.array(self.circuit(X2))\n    valsX2 = np.reshape(valsX2, (dim2, 3, -1))\n\n    valsX_X1 = valsX1[:, 0]\n    valsX_X2 = valsX2[:, 0]\n    valsY_X1 = valsX1[:, 1]\n    valsY_X2 = valsX2[:, 1]\n    valsZ_X1 = valsX1[:, 2]\n    valsZ_X2 = valsX2[:, 2]\n\n    all_vals_X1 = np.reshape(np.concatenate((valsX_X1, valsY_X1, valsZ_X1)), -1)\n    default_gamma = 1 / np.var(all_vals_X1) / self.n_features_\n\n    # construct kernel following plots\n    kernel_matrix = np.zeros([dim1, dim2])\n\n    for i in range(dim1):\n        for j in range(dim2):\n            sumX = sum([(valsX_X1[i, q] - valsX_X2[j, q]) ** 2 for q in range(self.n_qubits_)])\n            sumY = sum([(valsY_X1[i, q] - valsY_X2[j, q]) ** 2 for q in range(self.n_qubits_)])\n            sumZ = sum([(valsZ_X1[i, q] - valsZ_X2[j, q]) ** 2 for q in range(self.n_qubits_)])\n\n            kernel_matrix[i, j] = np.exp(\n                -default_gamma * self.gamma_factor * (sumX + sumY + sumZ)\n            )\n    return kernel_matrix\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred</code> <p>Predicted labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/projected_quantum_kernel.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred (np.ndarray): Predicted labels of shape (n_samples,)\n    \"\"\"\n    X = self.transform(X)\n    kernel_matrix = self.precompute_kernel(X, self.params_[\"X_train\"])\n    return self.svm.predict(kernel_matrix)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X. Note that this may be inconsistent with predict; see the sklearn docummentation for details.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/projected_quantum_kernel.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n    Note that this may be inconsistent with predict; see the sklearn docummentation for details.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n\n    if \"X_train\" not in self.params_:\n        raise ValueError(\"Model cannot predict without fitting to data first.\")\n\n    X = self.transform(X)\n    kernel_matrix = self.precompute_kernel(X, self.params_[\"X_train\"])\n    return self.svm.predict_proba(kernel_matrix)\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/projected_quantum_kernel.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n            self.scaler.fit(X)\n        X = self.scaler.transform(X)\n    return X * self.scaling\n</code></pre> <code></code> QuantumBoltzmannMachine <pre><code>QuantumBoltzmannMachine(visible_qubits='single', observable_type='sum', temperature=1, learning_rate=0.001, batch_size=32, max_vmap=None, jit=True, max_steps=10000, convergence_threshold=1e-06, random_state=42, scaling=1.0)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Variational Quantum Boltzmann Machine from https://arxiv.org/abs/2006.06004</p> <p>The model works as follows 1. One prepares a gibbs state :math:<code>e^{-H(\u000bec{theta},x)/K_b T}/Z</code>, where :math:<code>H(     heta,x)</code> is a parameterised n_qubit     Hamiltonian and :math:<code>Z</code> is the partition function normalisation. Here we take n_qubits equal to the number of features 2. A :math:<code>\\pm1</code> valued observable :math:<code>O</code> is measured on a subset of qubits (called 'visible qubits'). The forward     function of the model is then</p> <p>.. maths::</p> <pre><code>f(x,        heta) = Tr[O e^{-H(\u000bec{theta},x)/k_b T}/Z]\n</code></pre> <p>from this expectation value, the class probabilities are computed and used in a binary cross entropy loss.</p> <p>The specific Hamiltonian we use is a generalisation of the one used in the plots:</p> <p>.. maths::</p> <pre><code>H(x,        heta) = \\sum_i Z_i      heta_i\\cdot x + \\sum_i X_i      heta_{i+n_qubits}\\cdot x + \\sum_{ij} Z_iZ_j     heta_{i+2*n_qubits}\\cdot x\n</code></pre> <p>The observable use can be either a sum or tensor product of Z operators on the visible qubits.</p> <p>In practice, the full algorithm involves parameterising a trial state for the gibbs state and performing variational imaginary time evolution to approximate the true gibbs state in each optimisation step. Since this is quite computationally involved, we here assume (as they do in the plots numerics) that we have access to the perfect Gibbs state. It is thus unclear whether the full algorithm can be expected to perform as well as this implementation.</p> PARAMETER DESCRIPTION <code>visible_qubits</code> <p>The subset of qubits used for prediction. if 'single' a single qubit is used if 'half' half are used and if 'all' all are used.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'single'</code> </p> <code>observable_type</code> <p>If 'sum' a sum of Z operators is used, if 'product' a tensor product is used.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'sum'</code> </p> <code>temperature</code> <p>The temperature of the Gibbs state in units of K_bT. e.g. temperature = 2 is equivalent to K_bT=2.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>learning_rate</code> <p>learning rate for gradient descent.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> <code>batch_size</code> <p>Number of data points to subsample in each training step.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>max_vmap</code> <p>The largest size of vmap used (to control memory).</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>convergence_threshold</code> <p>If loss changes less than this threshold for 10 consecutive steps we stop training.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-06</code> </p> <code>max_steps</code> <p>Maximum number of training steps. A warning will be raised if training did not converge.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> <code>scaling</code> <p>The data is scaled by this factor after standardisation</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> METHOD DESCRIPTION <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_boltzmann_machine.py</code> <pre><code>def __init__(\n    self,\n    visible_qubits=\"single\",\n    observable_type=\"sum\",\n    temperature=1,\n    learning_rate=0.001,\n    batch_size=32,\n    max_vmap=None,\n    jit=True,\n    max_steps=10000,\n    convergence_threshold=1e-6,\n    random_state=42,\n    scaling=1.0,\n):\n    \"\"\"\n    Variational Quantum Boltzmann Machine from https://arxiv.org/abs/2006.06004\n\n    The model works as follows\n    1. One prepares a gibbs state :math:`e^{-H(\\vec{theta},x)/K_b T}/Z`, where :math:`H(\\theta,x)` is a parameterised n_qubit\n        Hamiltonian and :math:`Z` is the partition function normalisation. Here we take n_qubits equal to the number of features\n    2. A :math:`\\\\pm1` valued observable :math:`O` is measured on a subset of qubits (called 'visible qubits'). The forward\n        function of the model is then\n\n    .. maths::\n\n        f(x, \\theta) = Tr[O e^{-H(\\vec{theta},x)/k_b T}/Z]\n\n    from this expectation value, the class probabilities are computed and used in a binary cross entropy loss.\n\n    The specific Hamiltonian we use is a generalisation of the one used in the plots:\n\n    .. maths::\n\n        H(x, \\theta) = \\\\sum_i Z_i \\theta_i\\\\cdot x + \\\\sum_i X_i \\theta_{i+n_qubits}\\\\cdot x + \\\\sum_{ij} Z_iZ_j \\theta_{i+2*n_qubits}\\\\cdot x\n\n    The observable use can be either a sum or tensor product of Z operators on the visible qubits.\n\n    In practice, the full algorithm involves parameterising a trial state for the gibbs state and performing variational imaginary\n    time evolution to approximate the true gibbs state in each optimisation step. Since this is quite computationally involved, we\n    here assume (as they do in the plots numerics) that we have access to the perfect Gibbs state. It is thus unclear whether the full\n    algorithm can be expected to perform as well as this implementation.\n\n    Args:\n        visible_qubits (str): The subset of qubits used for prediction. if 'single' a single qubit is used\n            if 'half' half are used and if 'all' all are used.\n        observable_type (str): If 'sum' a sum of Z operators is used, if 'product' a tensor product is used.\n        temperature (int): The temperature of the Gibbs state in units of K_bT. e.g. temperature = 2 is equivalent to K_bT=2.\n        learning_rate (float): learning rate for gradient descent.\n        batch_size (int): Number of data points to subsample in each training step.\n        max_vmap (int): The largest size of vmap used (to control memory).\n        jit (bool): Whether to use just in time compilation.\n        convergence_threshold (float): If loss changes less than this threshold for 10 consecutive steps we stop training.\n        max_steps (int): Maximum number of training steps. A warning will be raised if training did not converge.\n        random_state (int): Seed used for pseudorandom number generation\n        scaling (float): The data is scaled by this factor after standardisation\n    \"\"\"\n\n    # attributes that do not depend on data\n    self.visible_qubits = visible_qubits\n    self.observable_type = observable_type\n    self.temperature = temperature\n    self.learning_rate = learning_rate\n    self.batch_size = batch_size\n    self.max_vmap = max_vmap\n    self.jit = jit\n    self.max_steps = max_steps\n    self.convergence_threshold = convergence_threshold\n    self.batch_size = batch_size\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # Dictionary containing the trainable parameters\n    self.n_qubits = None\n    self.n_visible = None  # number of visible qubits\n    self.scaler = None  # data scaler will be fitted on training data\n    self.circuit = None\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_boltzmann_machine.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n    self.initialize(X.shape[1], classes=np.unique(y))\n\n    self.scaler = StandardScaler()\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    optimizer = optax.adam\n\n    def loss_fn(params, X, y):\n        # binary cross entropy loss\n        vals = self.forward(params[\"thetas\"], X)\n        probs = (1 + vals) / 2\n        y = jax.nn.relu(y)  # convert to 0,1\n        return jnp.mean(-y * jnp.log(probs) - (1 - y) * jnp.log(1 - probs))\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    self.params_ = train(self, loss_fn, optimizer, X, y, self.generate_key)\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels.</p> PARAMETER DESCRIPTION <code>n_features</code> <p>Number of features that the classifier expects</p> <p> TYPE: <code>int</code> </p> <code>classes</code> <p>class labels that the classifier expects</p> <p> TYPE: <code>array - like</code> DEFAULT: <code>None</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_boltzmann_machine.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_qubits = n_features\n\n    if self.visible_qubits == \"single\":\n        self.n_visible = 1\n    elif self.visible_qubits == \"half\":\n        self.n_visible = self.n_qubits // 2\n    elif self.visible_qubits == \"full\":\n        self.n_visible = self.n_qubits\n\n    self.construct_model()\n    self.initialize_params()\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred</code> <p>Predicted labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_boltzmann_machine.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred (np.ndarray): Predicted labels of shape (n_samples,)\n    \"\"\"\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_boltzmann_machine.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    predictions = self.forward(self.params_[\"thetas\"], X)\n    predictions_2d = np.c_[(1 - predictions) / 2, (1 + predictions) / 2]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_boltzmann_machine.py</code> <pre><code>def transform(self, X):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if self.scaler is None:\n        # if the model is unfitted, initialise the scaler here\n        self.scaler = StandardScaler()\n        self.scaler.fit(X)\n\n    return self.scaler.transform(X) * self.scaling\n</code></pre> <code></code> QuantumKitchenSinks <pre><code>QuantumKitchenSinks(linear_model=LogisticRegression(penalty=None, solver='lbfgs', tol=0.001), n_episodes=100, n_qfeatures='full', var=1.0, jit=True, max_vmap=None, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax', 'diff_method': None}, scaling=1.0, random_state=42)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Quantum kitchen sinks model classical data classification.</p> <p>Based on: https://arxiv.org/pdf/1806.08321.pdf</p> <p>The quantum computer is used to generate random feature vectors which are fed to a linear classifier that we implement using scikit-learn's LogisticRegression (note logistic regression is a standard method to perform linear classification despite the name)</p> <p>The feature map procedure works as follows:</p> <ol> <li> <p>Linearly transform an input feature vector :math:<code>x</code> of length :math:<code>n</code> via :math:<code>x' = \\omega x + \\beta</code> using random :math:<code>\\omega, \\beta</code>. The output is of shape <code>(n_episodes, n_qfeatures)</code>.</p> </li> <li> <p>Feed each row vector of the matrix :math:<code>x'</code> into a quantum circuit that returns <code>n_qfeatures</code> samples. The samples are concatenated in a feature vector :math:<code>x</code> of length <code>n_episodes*n_qfeatures</code>, which is the input to the linear model.</p> </li> </ol> <p>.. note::</p> <pre><code>It is not stated in the plots how to generalise the circuits to higher qubit numbers;\nthis implementation is a simple generalisation of the circuit in Fig 2(c), which consists\nof encoding the features into single qubits via X rotaiton gates, and perfoming a sequence\nof CNOT gates on nearest neighbour and next-nearest neighbour qubits.\n</code></pre> PARAMETER DESCRIPTION <code>linear_model</code> <p>linear model to use with the transformed features</p> <p> TYPE: <code>sklearn Estimator</code> DEFAULT: <code>LogisticRegression(penalty=None, solver='lbfgs', tol=0.001)</code> </p> <code>n_episodes</code> <p>Number of features fed into the linear model after data transformation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>n_qfeatures</code> <p>Determines the number of features fed into the quantum circuit to transform the data. This is the number of qubits used by the model. If 'full', the number of qubits is equal to the number of input features, if 'half' it is half the number of input features.</p> <p> TYPE: <code>(str, int)</code> DEFAULT: <code>'full'</code> </p> <code>var</code> <p>detemined the variance of the matrix <code>\\omega</code> used to lienarly transform the input features.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the key word arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax', 'diff_method': None}</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>construct_model</code> <p>The circuit used to generate the feature vectors</p> <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for batch of data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Apply the feature map: The inputs go through a random linear transformation</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_kitchen_sinks.py</code> <pre><code>def __init__(\n    self,\n    linear_model=LogisticRegression(penalty=None, solver=\"lbfgs\", tol=10e-4),\n    n_episodes=100,\n    n_qfeatures=\"full\",\n    var=1.0,\n    jit=True,\n    max_vmap=None,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax\", \"diff_method\": None},\n    scaling=1.0,\n    random_state=42,\n):\n    r\"\"\"\n    Quantum kitchen sinks model classical data classification.\n\n    Based on: https://arxiv.org/pdf/1806.08321.pdf\n\n    The quantum computer is used to generate random feature vectors which are\n    fed to a linear classifier that we implement using scikit-learn's LogisticRegression (note logistic\n    regression is a standard method to perform linear classification despite the name)\n\n    The feature map procedure works as follows:\n\n    1. Linearly transform an input feature vector :math:`x` of length :math:`n`\n    via :math:`x' = \\omega x + \\beta` using random :math:`\\omega, \\beta`.\n    The output is of shape `(n_episodes, n_qfeatures)`.\n\n    2. Feed each row vector of the matrix :math:`x'` into a quantum circuit that\n    returns `n_qfeatures` samples. The samples are concatenated in a feature vector :math:`x`\n    of length `n_episodes*n_qfeatures`, which is the input to the linear model.\n\n    .. note::\n\n        It is not stated in the plots how to generalise the circuits to higher qubit numbers;\n        this implementation is a simple generalisation of the circuit in Fig 2(c), which consists\n        of encoding the features into single qubits via X rotaiton gates, and perfoming a sequence\n        of CNOT gates on nearest neighbour and next-nearest neighbour qubits.\n\n    Args:\n        linear_model (sklearn Estimator): linear model to use with the transformed features\n        n_episodes (int): Number of features fed into the linear model after data transformation.\n        n_qfeatures (str, int): Determines the number of features fed into the quantum circuit to transform the\n            data. This is the number of qubits used by the model. If 'full', the number of qubits is equal\n            to the number of input features, if 'half' it is half the number of input features.\n        var (float): detemined the variance of the matrix `\\omega` used to lienarly transform the input\n            features.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'\n        qnode_kwargs (str): the key word arguments passed to the circuit qnode.\n        random_state (int): Seed used for pseudorandom number generation.\n    \"\"\"\n\n    # attributes that do not depend on data\n    self.linear_model = linear_model\n    self.n_episodes = n_episodes\n    self.n_qfeatures = n_qfeatures\n    self.var = var\n    self.jit = jit\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = 100000\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None\n    self.n_qubits_ = None\n    self.scaler = None  # data scaler for inputs\n    self.scaler2 = None  # data scaler for quantum generated features\n    self.circuit = None\n</code></pre> <code></code> construct_model <pre><code>construct_model()\n</code></pre> <p>The circuit used to generate the feature vectors It is not clear from the plots how to generalise this to higher qubit numbers. This is a simple generalisation of the circuit in Fig 2(c).</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_kitchen_sinks.py</code> <pre><code>def construct_model(self):\n    \"\"\"\n    The circuit used to generate the feature vectors\n    It is not clear from the plots how to generalise this to higher qubit numbers.\n    This is a simple generalisation of the circuit in Fig 2(c).\n    \"\"\"\n\n    pattern = [[i, i + 2] for i in range(self.n_qubits_ - 2)]\n\n    dev = qml.device(\n        self.dev_type,\n        wires=self.n_qubits_,\n        shots=1,\n        prng_key=self.generate_key(),\n    )\n\n    @qml.qnode(dev, **self.qnode_kwargs)\n    def circuit(Q):\n        for i, q in enumerate(Q):\n            qml.RX(q, wires=i)\n        qml.broadcast(qml.CNOT, wires=range(self.n_qubits_), pattern=\"double\")\n        qml.broadcast(qml.CNOT, wires=range(self.n_qubits_), pattern=pattern)\n\n        return qml.sample(wires=range(self.n_qubits_))\n\n    self.circuit = circuit\n\n    if self.jit:\n        circuit = jax.jit(circuit)\n    circuit = chunk_vmapped_fn(jax.vmap(circuit), 0, self.max_vmap)\n\n    self.forward = circuit\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels 1-, 1 of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_kitchen_sinks.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels 1-, 1 of shape (n_samples,)\n    \"\"\"\n\n    self.linear_model.random_state = self.rng.integers(100000)\n\n    self.initialize(X.shape[1], np.unique(y))\n\n    self.scaler = MinMaxScaler(feature_range=(-1, 1))\n    self.scaler.fit(X)\n    features = self.transform(X)\n\n    start = time.time()\n    self.linear_model.fit(features, y)\n    end = time.time()\n    self.params_[\"weights\"] = self.linear_model.coef_\n    self.training_time_ = end - start\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels.</p> PARAMETER DESCRIPTION <code>n_features</code> <p>Number of features that the classifier expects</p> <p> TYPE: <code>int</code> </p> <code>classes</code> <p>class labels that the classifier expects</p> <p> TYPE: <code>array - like</code> DEFAULT: <code>None</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_kitchen_sinks.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_features_ = n_features\n\n    if self.n_qfeatures == \"full\":\n        self.n_qubits_ = n_features\n    elif self.n_qfeatures == \"half\":\n        self.n_qubits_ = int(np.ceil(n_features / 2))\n    else:\n        self.n_qubits_ = int(self.n_qfeatures)\n\n    self.construct_model()\n    self.initialize_params()\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for batch of data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_kitchen_sinks.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for batch of data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_kitchen_sinks.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    predictions_2d = self.linear_model.predict_proba(X)\n    return softmax(predictions_2d, copy=False)\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> <p>Apply the feature map: The inputs go through a random linear transformation followed by a quantum circuit.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_kitchen_sinks.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Apply the feature map: The inputs go through a random linear transformation\n    followed by a quantum circuit.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if self.params_[\"betas\"] is None or self.params_[\"omegas\"] is None or self.circuit is None:\n        raise ValueError(\"Model cannot predict without fitting to data first.\")\n\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = MinMaxScaler(feature_range=(-1, 1))\n            self.scaler.fit(X)\n        X = self.scaler.transform(X)\n    X = X * self.scaling\n\n    n_data = X.shape[0]\n    input_features = np.zeros([self.n_episodes, n_data, self.n_qubits_])\n    for e in range(self.n_episodes):\n        stacked_beta = np.stack([self.params_[\"betas\"][e] for __ in range(n_data)])\n        input_features[e] = (self.params_[\"omegas\"][e] @ X.T + stacked_beta.T).T\n    input_features = np.reshape(input_features, (n_data * self.n_episodes, -1))\n\n    features = self.forward(input_features)\n    features = np.reshape(features, (self.n_episodes, n_data, -1))\n    features = np.array([features[:, i, :] for i in range(n_data)])\n    features = np.reshape(features, (n_data, -1))\n\n    if self.scaler2 is None:\n        self.scaler2 = StandardScaler().fit(features)\n\n    return features\n</code></pre> <code></code> QuantumMetricLearner <pre><code>QuantumMetricLearner(n_layers=3, n_examples_predict=32, convergence_interval=200, max_steps=50000, learning_rate=0.01, batch_size=32, max_vmap=4, jit=True, random_state=42, scaling=1.0, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax-jit'})\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Following https://arxiv.org/abs/2001.03622.</p> <p>This classifier uses a trainable embedding to encode inputs into quantum states. Training and prediction relies on comparing these states with each other using the fidelity/state overlap:</p> <pre><code>* During training, the embedding is optimised to place data from the same class close together and data\n  from different classes far apart from each other.\n* Prediction compares a new embedded input with memorised training samples from each class and predicts\n  the class whose samples are closest.\n</code></pre> <p>Since pairwise comparison between data points are expensive, training and classification only uses samples from the data.</p> <p>The trainable embedding uses PennyLane's <code>QAOAEmbedding</code>.</p> <p>The classifier uses <code>batch_size*3</code> circuits for an evaluation of the loss function, and <code>n_examples_predict*2</code> circuits for prediction.</p> PARAMETER DESCRIPTION <code>n_examples_predict</code> <p>Number of examples from each class of the training set used for prediction.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>n_layers</code> <p>Number of layers used in the trainable embedding.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>convergence_interval</code> <p>The number of loss values to consider to decide convergence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>max_steps</code> <p>Maximum number of training steps. A warning will be raised if training did not converge.</p> <p> TYPE: <code>int</code> DEFAULT: <code>50000</code> </p> <code>batch_size</code> <p>Size of batches used for computing parameter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>learning_rate</code> <p>Initial learning rate for training.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.01</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>4</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dev_type</code> <p>Wtring specifying the pennylane device type; e.g. 'default.qubit'</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>The keyword arguments passed to the circuit qnode</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax-jit'}</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> METHOD DESCRIPTION <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for batch of data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X using a batch of training examples.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_metric_learning.py</code> <pre><code>def __init__(\n    self,\n    n_layers=3,\n    n_examples_predict=32,\n    convergence_interval=200,\n    max_steps=50000,\n    learning_rate=0.01,\n    batch_size=32,\n    max_vmap=4,\n    jit=True,\n    random_state=42,\n    scaling=1.0,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax-jit\"},\n):\n    \"\"\"\n    Following https://arxiv.org/abs/2001.03622.\n\n    This classifier uses a trainable embedding to encode inputs into quantum states. Training and prediction\n    relies on comparing these states with each other using the fidelity/state overlap:\n\n        * During training, the embedding is optimised to place data from the same class close together and data\n          from different classes far apart from each other.\n        * Prediction compares a new embedded input with memorised training samples from each class and predicts\n          the class whose samples are closest.\n\n    Since pairwise comparison between data points are expensive, training and classification only\n    uses samples from the data.\n\n    The trainable embedding uses PennyLane's `QAOAEmbedding`.\n\n    The classifier uses `batch_size*3` circuits for an evaluation of the loss function, and `n_examples_predict*2`\n    circuits for prediction.\n\n    Args:\n        n_examples_predict (int): Number of examples from each class of the training set used for prediction.\n        n_layers (int): Number of layers used in the trainable embedding.\n        convergence_interval (int): The number of loss values to consider to decide convergence.\n        max_steps (int): Maximum number of training steps. A warning will be raised if training did not converge.\n        batch_size (int): Size of batches used for computing parameter updates.\n        learning_rate (float): Initial learning rate for training.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        dev_type (str): Wtring specifying the pennylane device type; e.g. 'default.qubit'\n        qnode_kwargs (str): The keyword arguments passed to the circuit qnode\n        scaling (float): Factor by which to scale the input data.\n\n    \"\"\"\n\n    # attributes that do not depend on data\n    self.n_examples_predict = n_examples_predict\n    self.n_layers = n_layers\n    self.convergence_interval = convergence_interval\n    self.max_steps = max_steps\n    self.learning_rate = learning_rate\n    self.batch_size = batch_size\n    self.jit = jit\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # Dictionary containing the trainable parameters\n    self.n_qubits_ = None\n    self.n_features_ = None\n    self.scaler = None  # data scaler will be fitted on training data\n    self.circuit = None\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_metric_learning.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n\n    self.initialize(X.shape[1], classes=np.unique(y))\n\n    self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    # split data\n    A = jnp.array(X[y == -1])\n    B = jnp.array(X[y == 1])\n\n    if self.batch_size &gt; min(len(A), len(B)):\n        warnings.warn(\"batch size too large, setting to \" + str(min(len(A), len(B))))\n        self.batch_size = min(len(A), len(B))\n\n    def loss_fn(params, A=None, B=None):\n        aa = self.forward(params, X1=A, X2=A)\n        bb = self.forward(params, X1=B, X2=B)\n        ab = self.forward(params, X1=A, X2=B)\n\n        d_hs = -ab + 0.5 * (aa + bb)\n        return 1 - d_hs\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n\n    optimizer = optax.adam\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        A,\n        B,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    self.params_[\"examples_-1\"] = A\n    self.params_[\"examples_+1\"] = B\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels.</p> PARAMETER DESCRIPTION <code>n_features</code> <p>Number of features that the classifier expects</p> <p> TYPE: <code>int</code> </p> <code>classes</code> <p>class labels that the classifier expects</p> <p> TYPE: <code>array - like</code> DEFAULT: <code>None</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_metric_learning.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_features_ = n_features\n    self.n_qubits_ = (\n        self.n_features_ + 1\n    )  # +1 to add constant features as described in the plots\n    self.initialize_params()\n    self.construct_model()\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for batch of data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_metric_learning.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for batch of data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X using a batch of training examples. The examples are stored in the parameter dictionary.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_metric_learning.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X using a batch of training examples.\n    The examples are stored in the parameter dictionary.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    if \"examples_-1\" not in self.params_:\n        raise ValueError(\"Model cannot predict without fitting to data first.\")\n\n    X = self.transform(X)\n\n    max_examples = min(len(self.params_[\"examples_-1\"]), len(self.params_[\"examples_+1\"]))\n    if self.n_examples_predict &gt; max_examples:\n        warnings.warn(\"n_examples_predict too large, setting to \" + str(max_examples))\n        self.n_examples_predict = max_examples\n\n    A_examples, B_examples = get_batch(\n        self.n_examples_predict,\n        self.params_[\"examples_-1\"],\n        self.params_[\"examples_+1\"],\n        [self.generate_key(), self.generate_key()],\n    )\n\n    predictions = []\n    for x in X:\n        # create list [x, x, x, ...] to get overlaps with A_examples = [a1, a2, a3...] and B_examples\n        x_tiled = jnp.tile(x, (self.n_examples_predict, 1))\n\n        pred_a = jnp.mean(self.chunked_forward(self.params_, A_examples, x_tiled))\n        pred_b = jnp.mean(self.chunked_forward(self.params_, B_examples, x_tiled))\n\n        # normalise to [0,1]\n        predictions.append([pred_a / (pred_a + pred_b), pred_b / (pred_a + pred_b)])\n\n    return jnp.array(predictions)\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_metric_learning.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n            self.scaler.fit(X)\n        X = self.scaler.transform(X)\n    return X * self.scaling\n</code></pre> <code></code> QuanvolutionalNeuralNetwork <pre><code>QuanvolutionalNeuralNetwork(qkernel_shape=2, n_qchannels=1, rand_depth=10, rand_rot=20, threshold=0.0, kernel_shape=3, output_channels=[32, 64], max_vmap=None, jit=True, learning_rate=0.001, max_steps=10000, convergence_interval=0.001, batch_size=32, random_state=42, scaling=1.0, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax-jit'})\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Quanvolutional Neural network described in https://arxiv.org/pdf/1904.04767v1.pdf.</p> <p>The model is an adaptation of the ConvolutionalNeuralNetwork model. The only difference is that the data is preprocessed by a quantum convolutional layer before being fed to the classical CNN. This quantum preprocessing is a fixed, non-trainiable feature map.</p> <p>The feature map consists of the following:</p> <pre><code>1. A step function that converts the input to 0,1 valued. If the value is below `threshold` it is 0,\notherwise it is 1.\n\n2. A 2d convolution layer is a applied to the binarised data. The filter is given by a random quantum\ncircuit acting on `qkernel_shape*qkernel_shape` qubits (a square grid), and there are `n_qchannels` output\nchannels. The scalar output of the filter is given by the number of ones appearing in the output bitstring\nof the circuit with the highest probability to be sampled. We implement the random quantum circuit\nvia PennyLane's `RandomLayers`.\n\n3. The transformed data is fed into a classical CNN of the same form as ConvolutionalNeuralNetwork and\nthe model is equivalent from that point onwards.\n</code></pre> <p>The input data X should have shape (dataset_size,height*width) and will be reshaped to (dataset_size, height, width, 1) in the model. We assume height=width.</p> PARAMETER DESCRIPTION <code>qkernel_shape</code> <p>The size of the quantum filter: a circuit with <code>qkernel_shape*qkernel_shape</code> qubits will be used.</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>n_qchannels</code> <p>The number of output channels in the quanvolutional layer.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>rand_depth</code> <p>The depth of the random circuit in pennylane.RandomLayers</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>rand_rot</code> <p>The number of random rotations in pennylane.RandomLayers</p> <p> TYPE: <code>int</code> DEFAULT: <code>20</code> </p> <code>threshold</code> <p>The threshold that determines the binarisation of the input data. Since we use a StadardScaler this is set to 0.0 as default.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>kernel_shape</code> <p>the size of the kernel used in the CNN. e.g. kernel_shape=3 uses a 3x3 filter</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>output_channels</code> <p>a list of integers specifying the output size of the convolutional layers in the CNN.</p> <p> TYPE: <code>list[int]</code> DEFAULT: <code>[32, 64]</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>convergence_interval</code> <p>The number of loss values to consider to decide convergence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0.001</code> </p> <code>learning_rate</code> <p>Initial learning rate for training.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> <code>max_steps</code> <p>Maximum number of training steps. A warning will be raised if training did not converge.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> <code>batch_size</code> <p>Size of batches used for computing parameter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the key word arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax-jit'}</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>construct_quanvolutional_layer</code> <p>construct the quantum feature map.</p> <code>construct_random_circuit</code> <p>construct a random circuit to be used as a filter in the quanvolutional layer</p> <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quanvolutional_neural_network.py</code> <pre><code>def __init__(\n    self,\n    qkernel_shape=2,\n    n_qchannels=1,\n    rand_depth=10,\n    rand_rot=20,\n    threshold=0.0,\n    kernel_shape=3,\n    output_channels=[32, 64],\n    max_vmap=None,\n    jit=True,\n    learning_rate=0.001,\n    max_steps=10000,\n    convergence_interval=10e-4,\n    batch_size=32,\n    random_state=42,\n    scaling=1.0,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax-jit\"},\n):\n    r\"\"\"\n    Quanvolutional Neural network described in https://arxiv.org/pdf/1904.04767v1.pdf.\n\n    The model is an adaptation of the ConvolutionalNeuralNetwork model. The only difference is that the data is\n    preprocessed by a quantum convolutional layer before being fed to the classical CNN. This quantum preprocessing\n    is a fixed, non-trainiable feature map.\n\n    The feature map consists of the following:\n\n        1. A step function that converts the input to 0,1 valued. If the value is below `threshold` it is 0,\n        otherwise it is 1.\n\n        2. A 2d convolution layer is a applied to the binarised data. The filter is given by a random quantum\n        circuit acting on `qkernel_shape*qkernel_shape` qubits (a square grid), and there are `n_qchannels` output\n        channels. The scalar output of the filter is given by the number of ones appearing in the output bitstring\n        of the circuit with the highest probability to be sampled. We implement the random quantum circuit\n        via PennyLane's `RandomLayers`.\n\n        3. The transformed data is fed into a classical CNN of the same form as ConvolutionalNeuralNetwork and\n        the model is equivalent from that point onwards.\n\n    The input data X should have shape (dataset_size,height*width) and will be reshaped to\n    (dataset_size, height, width, 1) in the model. We assume height=width.\n\n    Args:\n        qkernel_shape (int): The size of the quantum filter: a circuit with `qkernel_shape*qkernel_shape`\n            qubits will be used.\n        n_qchannels (int): The number of output channels in the quanvolutional layer.\n        rand_depth (int): The depth of the random circuit in pennylane.RandomLayers\n        rand_rot (int): The number of random rotations in pennylane.RandomLayers\n        threshold (float): The threshold that determines the binarisation of the input data. Since we use a\n            StadardScaler this is set to 0.0 as default.\n        kernel_shape (int): the size of the kernel used in the CNN. e.g. kernel_shape=3 uses a 3x3 filter\n        output_channels (list[int]): a list of integers specifying the output size of the convolutional layers\n            in the CNN.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        convergence_interval (int): The number of loss values to consider to decide convergence.\n        learning_rate (float): Initial learning rate for training.\n        max_steps (int): Maximum number of training steps. A warning will be raised if training did not converge.\n        batch_size (int): Size of batches used for computing parameter updates.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'\n        qnode_kwargs (str): the key word arguments passed to the circuit qnode.\n        random_state (int): Seed used for pseudorandom number generation.\n    \"\"\"\n    # attributes that do not depend on data\n    self.learning_rate = learning_rate\n    self.max_steps = max_steps\n    self.convergence_interval = convergence_interval\n    self.n_qchannels = n_qchannels\n    self.rand_depth = rand_depth\n    self.rand_rot = rand_rot\n    self.threshold = threshold\n    self.kernel_shape = kernel_shape\n    self.qkernel_shape = qkernel_shape\n    self.output_channels = output_channels\n    self.batch_size = batch_size\n    self.jit = jit\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.batch_size = batch_size\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # Dictionary containing the trainable parameters\n    self.scaler = None  # data scaler will be fitted on training data\n    self.circuit = None\n</code></pre> <code></code> construct_quanvolutional_layer <pre><code>construct_quanvolutional_layer()\n</code></pre> <p>construct the quantum feature map.</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quanvolutional_neural_network.py</code> <pre><code>def construct_quanvolutional_layer(self):\n    \"\"\"\n    construct the quantum feature map.\n    \"\"\"\n    random_circuits = [self.construct_random_circuit() for __ in range(self.n_qchannels)]\n\n    # construct an array that specifies the indices of the 'windows' of the image used for the convolution.\n    idx_mat = jnp.array([[(i, j) for j in range(self.width)] for i in range(self.height)])\n    idxs = jnp.array(\n        [\n            idx_mat[j : j + self.qkernel_shape, k : k + self.qkernel_shape]\n            for k in range(self.height - self.qkernel_shape + 1)\n            for j in range(self.height - self.qkernel_shape + 1)\n        ]\n    )\n    idxs = idxs.reshape(len(idxs), -1, 2)\n    zerovec = jnp.zeros(len(idxs[0, :, 0]))  # needed for last axis of NHWC format\n    idxs = jnp.array(\n        [[idxs[i, :, 0], idxs[i, :, 1], zerovec] for i in range(len(idxs))],\n        dtype=int,\n    )\n\n    def quanv_layer(x):\n        \"\"\"\n        A convolutional layer where the filter is given by a random quantum circuit. The layer has a stride of 1.\n        Args:\n            x (jnp.array): input data of shape (n_data, height, width, 1)\n        \"\"\"\n\n        # the windows from the image to be fed into the quantum circuits\n        x_windows = x[idxs[:, 0, :], idxs[:, 1, :], idxs[:, 2, :]]\n\n        layer_out = []\n        for channel in range(self.n_qchannels):\n            out = []\n            # find most likely outputs\n            probs = random_circuits[channel](x_windows)\n            # convert to scalars based on the number of ones in the state vec\n            max_idxs = jnp.argmax(probs, axis=1)\n            state_vecs = [\n                jnp.unravel_index(idx, [2 for __ in range(self.qkernel_shape**2)])\n                for idx in max_idxs\n            ]\n            out = jnp.sum(jnp.array(state_vecs), axis=1)\n            out = jnp.array(out, dtype=\"float64\")\n            # put back to correct shape\n            out = jnp.reshape(\n                out,\n                (\n                    self.height - self.qkernel_shape + 1,\n                    self.width - self.qkernel_shape + 1,\n                ),\n            )\n            layer_out.append(out)\n\n        layer_out = jnp.array(layer_out)\n        layer_out = jnp.moveaxis(\n            layer_out, 0, -1\n        )  # the above is in CHW format, so we switch to WHC\n        return layer_out\n\n    return quanv_layer\n</code></pre> <code></code> construct_random_circuit <pre><code>construct_random_circuit()\n</code></pre> <p>construct a random circuit to be used as a filter in the quanvolutional layer</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quanvolutional_neural_network.py</code> <pre><code>def construct_random_circuit(self):\n    \"\"\"\n    construct a random circuit to be used as a filter in the quanvolutional layer\n    \"\"\"\n    wires = range(self.qkernel_shape**2)\n    dev = qml.device(self.dev_type, wires=wires)\n    weights = (\n        jnp.pi\n        * 2\n        * jnp.array(\n            jax.random.uniform(self.generate_key(), shape=(self.rand_depth, self.rand_rot))\n        )\n    )\n\n    @qml.qnode(dev, **self.qnode_kwargs)\n    def circuit(x):\n        \"\"\"\n        Apply a random circuit and return the probabilities of the output strings.\n        Here we use Pennylane's RandomLayers which deviates slightly from the desciption in the plots,\n        but we expect similar behaviour since they are both random circuit generators.\n        \"\"\"\n        for i in wires:\n            qml.RY(x[i] * jnp.pi, wires=i)\n        qml.RandomLayers(weights, wires=wires)\n        return qml.probs(wires=wires)\n\n    self.circuit = circuit\n\n    if self.jit:\n        circuit = jax.jit(circuit)\n    circuit = chunk_vmapped_fn(jax.vmap(circuit, in_axes=(0)), 0, self.max_vmap)\n    return circuit\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quanvolutional_neural_network.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n\n    self.initialize(X.shape[1], classes=np.unique(y))\n\n    y = jnp.array(y, dtype=int)\n    self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    # quantum feature map the entire dataset for training. Assuming we use more than one epoch of training,\n    # it is more efficient to do this first\n    X = self.batched_quanv_layer(X)\n\n    def loss_fn(params, X, y):\n        \"\"\"\n        this takes the quantum feature mapped data as input and returns the sigmoid binary cross entropy\n        \"\"\"\n        y = jax.nn.relu(y)  # convert to 0,1 labels\n        vals = self.cnn.apply(params, X)[:, 0]\n        loss = jnp.mean(optax.sigmoid_binary_cross_entropy(vals, y))\n        return loss\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n\n    optimizer = optax.adam\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        X,\n        y,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels. Args:     classes (array-like): class labels that the classifier expects</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quanvolutional_neural_network.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n    Args:\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.height = int(jnp.sqrt(n_features))\n    self.width = self.height\n\n    # initialise the model\n    self.quanv_layer = self.construct_quanvolutional_layer()\n    self.batched_quanv_layer = chunk_vmapped_fn(\n        jax.vmap(self.quanv_layer, in_axes=(0)), 0, self.max_vmap\n    )\n    self.cnn = construct_cnn(self.output_channels, self.kernel_shape)\n\n    # create dummy data input to initialise the cnn\n    X0 = jnp.ones(shape=(1, self.height, self.height, 1))\n    X0 = self.batched_quanv_layer(X0)\n    self.initialize_params(X0)\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred</code> <p>Predicted labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quanvolutional_neural_network.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred (np.ndarray): Predicted labels of shape (n_samples,)\n    \"\"\"\n    predictions = self.predict_proba(X)\n    mapped_predictions = jnp.argmax(predictions, axis=1)\n    return jnp.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quanvolutional_neural_network.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    p1 = jax.nn.sigmoid(self.forward(self.params_, X)[:, 0])\n    predictions_2d = jnp.c_[1 - p1, p1]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quanvolutional_neural_network.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n            self.scaler.fit(X)\n        X = self.scaler.transform(X)\n    X = X * self.scaling\n    X = jnp.array(X)\n\n    # put in NHWC format. We assume square images\n    self.height = int(jnp.sqrt(X.shape[1]))\n    self.width = self.height\n    X = jnp.reshape(X, (X.shape[0], self.height, self.width, 1))\n    X = jnp.heaviside(X - self.threshold, 0.0)  # binarise input\n\n    return X\n</code></pre> <code></code> SeparableKernelClassifier <pre><code>SeparableKernelClassifier(encoding_layers=1, svm=SVC(kernel='precomputed', probability=True), C=1.0, jit=True, random_state=42, scaling=1.0, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax', 'diff_method': None})\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>A kernel model that uses a separable embedding. The embedding consists of layers of fixed single qubit X rotations by an angle :math:<code>pi/4</code> on the Bloch sphere and single qubit Y rotation data encoding gates.</p> <p>The kernel is given by</p> <p>.. math::     k(x,x')=\\vert\\langle 0 \\vert U^\\dagger(x')U(x)\\vert 0 \\rangle\\vert^2</p> PARAMETER DESCRIPTION <code>encoding_layers</code> <p>number of layers in the data encoding circuit.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>svm</code> <p>scikit-learn SVC class object used to fit the model from the kernel matrix.</p> <p> TYPE: <code>SVC</code> DEFAULT: <code>SVC(kernel='precomputed', probability=True)</code> </p> <code>C</code> <p>regularization parameter for the SVC. Lower values imply stronger regularization.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the keyword arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax', 'diff_method': None}</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> METHOD DESCRIPTION <code>fit</code> <p>Fit the model to data X and labels y. Uses sklearn's SVM classifier</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>precompute_kernel</code> <p>compute the kernel matrix relative to data sets X1 and X2</p> <code>predict</code> <p>Predict labels for data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/separable.py</code> <pre><code>def __init__(\n    self,\n    encoding_layers=1,\n    svm=SVC(kernel=\"precomputed\", probability=True),\n    C=1.0,\n    jit=True,\n    random_state=42,\n    scaling=1.0,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax\", \"diff_method\": None},\n):\n    r\"\"\"\n    A kernel model that uses a separable embedding. The embedding consists of layers of fixed single qubit X\n    rotations by an angle :math:`pi/4` on the Bloch sphere and single qubit Y rotation data encoding gates.\n\n    The kernel is given by\n\n    .. math::\n        k(x,x')=\\vert\\langle 0 \\vert U^\\dagger(x')U(x)\\vert 0 \\rangle\\vert^2\n\n\n    Args:\n        encoding_layers (int): number of layers in the data encoding circuit.\n        svm (sklearn.svm.SVC): scikit-learn SVC class object used to fit the model from the kernel matrix.\n        C (float): regularization parameter for the SVC. Lower values imply stronger regularization.\n        jit (bool): Whether to use just in time compilation.\n        random_state (int): Seed used for pseudorandom number generation.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'.\n        qnode_kwargs (str): the keyword arguments passed to the circuit qnode.\n        scaling (float): Factor by which to scale the input data.\n    \"\"\"\n    # attributes that do not depend on data\n    self.encoding_layers = encoding_layers\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.jit = jit\n    self.svm = svm\n    self.C = C\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # Dictionary containing the trainable parameters\n    self.n_qubits_ = None\n    self.scaler = None  # data scaler will be fitted on training data\n    self.circuit = None\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y. Uses sklearn's SVM classifier</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/separable.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y. Uses sklearn's SVM classifier\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n\n    self.svm.random_state = int(\n        jax.random.randint(self.generate_key(), shape=(1,), minval=0, maxval=1000000)\n    )\n\n    self.initialize(X.shape[1], np.unique(y))\n\n    self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    self.params_ = {\"x_train\": X}\n    kernel_matrix = self.precompute_kernel(X, X)\n\n    start = time.time()\n    # we are updating this value here, in case it was\n    # changed after initialising the model\n    self.svm.C = self.C\n    self.svm.fit(kernel_matrix, y)\n    end = time.time()\n    self.training_time_ = end - start\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels.</p> PARAMETER DESCRIPTION <code>n_features</code> <p>Number of features that the classifier expects</p> <p> TYPE: <code>int</code> </p> <code>classes</code> <p>class labels that the classifier expects</p> <p> TYPE: <code>array - like</code> DEFAULT: <code>None</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/separable.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_qubits_ = n_features\n    self.params_ = {}\n    self.construct_circuit()\n</code></pre> <code></code> precompute_kernel <pre><code>precompute_kernel(X1, X2)\n</code></pre> <p>compute the kernel matrix relative to data sets X1 and X2 Args:     X1 (np.array): first dataset of input vectors     X2 (np.array): second dataset of input vectors Returns:     kernel_matrix (np.array): matrix of size (len(X1),len(X2)) with elements K(x_1,x_2)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/separable.py</code> <pre><code>def precompute_kernel(self, X1, X2):\n    \"\"\"\n    compute the kernel matrix relative to data sets X1 and X2\n    Args:\n        X1 (np.array): first dataset of input vectors\n        X2 (np.array): second dataset of input vectors\n    Returns:\n        kernel_matrix (np.array): matrix of size (len(X1),len(X2)) with elements K(x_1,x_2)\n    \"\"\"\n    dim1 = len(X1)\n    dim2 = len(X2)\n\n    # concatenate all pairs of vectors\n    Z = np.array([np.concatenate((X1[i], X2[j])) for i in range(dim1) for j in range(dim2)])\n    self.construct_circuit()\n    kernel_values = [self.forward(z) for z in Z]\n    # reshape the values into the kernel matrix\n    kernel_matrix = np.reshape(kernel_values, (dim1, dim2))\n\n    return kernel_matrix\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred</code> <p>Predicted labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/separable.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred (np.ndarray): Predicted labels of shape (n_samples,)\n    \"\"\"\n    X = self.transform(X)\n    kernel_matrix = self.precompute_kernel(X, self.params_[\"x_train\"])\n    return self.svm.predict(kernel_matrix)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X. note that this may be inconsistent with predict; see the sklearn docummentation for details.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/separable.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n    note that this may be inconsistent with predict; see the sklearn docummentation for details.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    if \"x_train\" not in self.params_:\n        raise ValueError(\"Model cannot predict without fitting to data first.\")\n\n    X = self.transform(X)\n    kernel_matrix = self.precompute_kernel(X, self.params_[\"x_train\"])\n    return self.svm.predict_proba(kernel_matrix)\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/separable.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n            self.scaler.fit(X)\n        X = self.scaler.transform(X)\n    return X * self.scaling\n</code></pre> <code></code> SeparableVariationalClassifier <pre><code>SeparableVariationalClassifier(encoding_layers=1, learning_rate=0.001, batch_size=32, max_vmap=None, jit=True, max_steps=10000, random_state=42, scaling=1.0, convergence_interval=200, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax'})\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Variational model that uses only separable operations (i.e. there is no entanglement in the model). The circuit consists of layers of encoding gates and parameterised unitaries followed by measurement of an observable.</p> <p>Each encoding layer consists of a trainiable arbitrary qubit rotation on each qubit followed by a product angle embedding of the input data, using RY gates. A final layer of trainable qubit rotations is applied at the end of the circuit.</p> <p>The obserable O is the mean value of Pauli Z observables on each of the output qubits. The value of this observable is used to predict the probability for class 1 as :math:<code>P(+1)=\\sigma(6\\langle O \\rangle)</code> where :math<code>\\sigma</code> is the logistic funciton. The model is then fit using the cross entropy loss.</p> PARAMETER DESCRIPTION <code>encoding_layers</code> <p>number of layers in the data encoding circuit.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>learning_rate</code> <p>learning rate for gradient descent.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> <code>batch_size</code> <p>Size of batches used for computing parameter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>convergence_interval</code> <p>The number of loss values to consider to decide convergence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>max_steps</code> <p>Maximum number of training steps. A warning will be raised if training did not converge.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the keyword arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax'}</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/separable.py</code> <pre><code>def __init__(\n    self,\n    encoding_layers=1,\n    learning_rate=0.001,\n    batch_size=32,\n    max_vmap=None,\n    jit=True,\n    max_steps=10000,\n    random_state=42,\n    scaling=1.0,\n    convergence_interval=200,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax\"},\n):\n    r\"\"\"\n    Variational model that uses only separable operations (i.e. there is no entanglement in the model). The circuit\n    consists of layers of encoding gates and parameterised unitaries followed by measurement of an observable.\n\n    Each encoding layer consists of a trainiable arbitrary qubit rotation on each qubit followed by\n    a product angle embedding of the input data, using RY gates. A final layer of trainable qubit rotations is\n    applied at the end of the circuit.\n\n    The obserable O is the mean value of Pauli Z observables on each of the output qubits. The value of this\n    observable is used to predict the probability for class 1 as :math:`P(+1)=\\sigma(6\\langle O \\rangle)`\n    where :math`\\sigma` is the logistic funciton. The model is then fit using the cross entropy loss.\n\n    Args:\n        encoding_layers (int): number of layers in the data encoding circuit.\n        learning_rate (float): learning rate for gradient descent.\n        batch_size (int): Size of batches used for computing parameter updates.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        convergence_interval (int): The number of loss values to consider to decide convergence.\n        max_steps (int): Maximum number of training steps. A warning will be raised if training did not converge.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'.\n        qnode_kwargs (str): the keyword arguments passed to the circuit qnode.\n        random_state (int): Seed used for pseudorandom number generation\n    \"\"\"\n    # attributes that do not depend on data\n    self.encoding_layers = encoding_layers\n    self.learning_rate = learning_rate\n    self.batch_size = batch_size\n    self.max_steps = max_steps\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.jit = jit\n    self.convergence_interval = convergence_interval\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # Dictionary containing the trainable parameters\n    self.n_qubits_ = None\n    self.scaler = None  # data scaler will be fitted on training data\n    self.circuit = None\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/separable.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n    self.initialize(X.shape[1], np.unique(y))\n\n    self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    optimizer = optax.adam\n\n    def loss_fn(params, X, y):\n        # we multiply by 6 because a relevant domain of the sigmoid function is [-6,6]\n        vals = self.forward(params, X) * 6\n        y = jax.nn.relu(y)  # convert to 0,1\n        return jnp.mean(optax.sigmoid_binary_cross_entropy(vals, y))\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        X,\n        y,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels.</p> PARAMETER DESCRIPTION <code>n_features</code> <p>Number of features that the classifier expects</p> <p> TYPE: <code>int</code> </p> <code>classes</code> <p>class labels that the classifier expects</p> <p> TYPE: <code>array - like</code> DEFAULT: <code>None</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/separable.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_qubits_ = n_features\n    self.initialize_params()\n    self.construct_model()\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred</code> <p>Predicted labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/separable.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred (np.ndarray): Predicted labels of shape (n_samples,)\n    \"\"\"\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/separable.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    predictions = self.chunked_forward(self.params_, X)\n    predictions_2d = np.c_[(1 - predictions) / 2, (1 + predictions) / 2]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/separable.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n            self.scaler.fit(X)\n        X = self.scaler.transform(X)\n    return X * self.scaling\n</code></pre> <code></code> TreeTensorClassifier <pre><code>TreeTensorClassifier(learning_rate=0.01, batch_size=32, max_steps=10000, convergence_interval=200, random_state=42, max_vmap=None, jit=True, scaling=1.0, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax-jit'})\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Tree tensor network classifier from https://arxiv.org/abs/2011.06258v2 (see figure 1)</p> <p>This is a variational model where data is amplitude embedded and a trainiable circuit with a tree like structure is used for prediction. Due to the tree structure, the number of qubits must be a power of 2.</p> <p>In the plots, the data encoding state for a given input x is approximated  using a variational circuit. In practice, this means one has to train an encoding circuit for every training and test input. Since this is very expensive, we just use the exact amplitude encoded state. If the input data dimension is smaller than the state vector dimension, we pad with a value :math:<code>1/2^{n_qubits}</code>.</p> <p>The classification is performed via a Z measurement on the first qubit plus a trainable bias. Training is via the square loss.</p> PARAMETER DESCRIPTION <code>learning_rate</code> <p>Initial learning rate for gradient descent.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.01</code> </p> <code>batch_size</code> <p>Size of batches used for computing parameter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>convergence_interval</code> <p>The number of loss values to consider to decide convergence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>max_steps</code> <p>Maximum number of training steps. A warning will be raised if training did not converge.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the key word arguments passed to the circuit qnode</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax-jit'}</code> </p> METHOD DESCRIPTION <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>The feature vectors padded to the next power of 2 and then normalised.</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/tree_tensor.py</code> <pre><code>def __init__(\n    self,\n    learning_rate=0.01,\n    batch_size=32,\n    max_steps=10000,\n    convergence_interval=200,\n    random_state=42,\n    max_vmap=None,\n    jit=True,\n    scaling=1.0,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax-jit\"},\n):\n    r\"\"\"\n    Tree tensor network classifier from https://arxiv.org/abs/2011.06258v2 (see figure 1)\n\n    This is a variational model where data is amplitude embedded and a trainiable circuit with a tree like\n    structure is used for prediction. Due to the tree structure, the number of qubits must be a power of 2.\n\n    In the plots, the data encoding state for a given input x is approximated  using a variational circuit.\n    In practice, this means one has to train an encoding circuit for every training and test\n    input. Since this is very expensive, we just use the exact amplitude encoded state. If the input data\n    dimension is smaller than the state vector dimension, we pad with a value :math:`1/2^{n_qubits}`.\n\n    The classification is performed via a Z measurement on the first qubit plus a trainable bias. Training\n    is via the square loss.\n\n    Args:\n        learning_rate (float): Initial learning rate for gradient descent.\n        batch_size (int): Size of batches used for computing parameter updates.\n        convergence_interval (int): The number of loss values to consider to decide convergence.\n        max_steps (int): Maximum number of training steps. A warning will be raised if training did not converge.\n        random_state (int): Seed used for pseudorandom number generation\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        scaling (float): Factor by which to scale the input data.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'\n        qnode_kwargs (str): the key word arguments passed to the circuit qnode\n    \"\"\"\n\n    # attributes that do not depend on data\n    self.learning_rate = learning_rate\n    self.max_steps = max_steps\n    self.convergence_interval = convergence_interval\n    self.batch_size = batch_size\n    self.dev_type = dev_type\n    self.jit = jit\n    self.qnode_kwargs = qnode_kwargs\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # Dictionary containing the trainable parameters\n    self.circuit = None\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/tree_tensor.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n    self.initialize(X.shape[1], classes=np.unique(y))\n\n    X = self.transform(X)\n\n    def loss_fn(params, X, y):\n        # square loss\n        predictions = self.forward(params, X)\n        return jnp.mean((predictions - y) ** 2)\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    optimizer = optax.adam\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        X,\n        y,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels.</p> PARAMETER DESCRIPTION <code>n_features</code> <p>Number of features that the classifier expects</p> <p> TYPE: <code>int</code> </p> <code>classes</code> <p>class labels that the classifier expects</p> <p> TYPE: <code>array - like</code> DEFAULT: <code>None</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/tree_tensor.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    if n_features == 1:\n        self.n_qubits = 1\n    else:\n        n_qubits_ae = int(\n            np.ceil(np.log2(n_features))\n        )  # the num qubits needed to amplitude encode\n        n_qubits = 2 ** int(\n            np.ceil(np.log2(n_qubits_ae))\n        )  # the model needs 2**m qubits, for some m\n        self.n_qubits = n_qubits\n\n    self.initialize_params()\n    self.construct_model()\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred</code> <p>Predicted labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/tree_tensor.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred (np.ndarray): Predicted labels of shape (n_samples,)\n    \"\"\"\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/tree_tensor.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    predictions = self.chunked_forward(self.params_, X)\n    predictions_2d = np.c_[(1 - predictions) / 2, (1 + predictions) / 2]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> <p>The feature vectors padded to the next power of 2 and then normalised.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/tree_tensor.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    The feature vectors padded to the next power of 2 and then normalised.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    n_features = X.shape[1]\n    X = X * self.scaling\n\n    n_qubits_ae = int(np.ceil(np.log2(n_features)))  # the num qubits needed to amplitude encode\n    n_qubits = 2 ** int(\n        np.ceil(np.log2(n_qubits_ae))\n    )  # the model needs 2**m qubits, for some m\n    max_n_features = 2**n_qubits\n    n_padding = max_n_features - n_features\n    padding = np.ones(shape=(len(X), n_padding)) / max_n_features\n\n    X_padded = np.c_[X, padding]\n    X_normalised = np.divide(X_padded, np.expand_dims(np.linalg.norm(X_padded, axis=1), axis=1))\n    return X_normalised\n</code></pre> <code></code> VanillaQNN <pre><code>VanillaQNN(embedding_layers=2, variational_layers=3, learning_rate=0.01, batch_size=32, max_vmap=None, jit=True, max_steps=10000, convergence_threshold=1e-06, random_state=42, scaling=1.0, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax'})\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>A vanilla implementation of a quantum neural network with layer-wise angle embedding and a layered variational circuit.</p> PARAMETER DESCRIPTION <code>embedding_layers</code> <p>number of times to repeat the embedding circuit structure.</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>variational_layers</code> <p>number of layers in the variational part of the circuit.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>learning_rate</code> <p>learning rate for gradient descent.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.01</code> </p> <code>batch_size</code> <p>Number of data points to subsample.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>max_vmap</code> <p>The largest size of vmap used (to control memory)</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>convergence_threshold</code> <p>If loss changes less than this threshold for 10 consecutive steps we stop training.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-06</code> </p> <code>max_steps</code> <p>Maximum number of training steps. A warning will be raised if training did not converge.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the keyword arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax'}</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/vanilla_qnn.py</code> <pre><code>def __init__(\n    self,\n    embedding_layers=2,\n    variational_layers=3,\n    learning_rate=0.01,\n    batch_size=32,\n    max_vmap=None,\n    jit=True,\n    max_steps=10000,\n    convergence_threshold=1e-6,\n    random_state=42,\n    scaling=1.0,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax\"},\n):\n    \"\"\"\n    A vanilla implementation of a quantum neural network with layer-wise angle embedding and a layered\n    variational circuit.\n\n    Args:\n        embedding_layers (int): number of times to repeat the embedding circuit structure.\n        variational_layers (int): number of layers in the variational part of the circuit.\n        learning_rate (float): learning rate for gradient descent.\n        batch_size (int): Number of data points to subsample.\n        max_vmap (int): The largest size of vmap used (to control memory)\n        jit (bool): Whether to use just in time compilation.\n        convergence_threshold (float): If loss changes less than this threshold for 10 consecutive steps we stop training.\n        max_steps (int): Maximum number of training steps. A warning will be raised if training did not converge.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'.\n        qnode_kwargs (str): the keyword arguments passed to the circuit qnode.\n        random_state (int): Seed used for pseudorandom number generation.\n    \"\"\"\n\n    # attributes that do not depend on data\n    self.embedding_layers = embedding_layers\n    self.variational_layers = variational_layers\n    self.learning_rate = learning_rate\n    self.batch_size = batch_size\n    self.max_steps = max_steps\n    self.convergence_threshold = convergence_threshold\n    self.batch_size = batch_size\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.jit = jit\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # Dictionary containing the trainable parameters\n    self.n_qubits_ = None\n    self.scaler = None  # data scaler will be fitted on training data\n    self.circuit = None\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/vanilla_qnn.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n\n    self.initialize(n_features=X.shape[1], classes=np.unique(y))\n\n    self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    optimizer = optax.adam\n\n    def loss_fn(params, X, y):\n        # we multiply by 6 because a relevant domain of the sigmoid function is [-6,6]\n        vals = self.forward(params, X) * 6\n        y = jax.nn.relu(y)  # convert to 0,1\n        return jnp.mean(optax.sigmoid_binary_cross_entropy(vals, y))\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    self.params_ = train(self, loss_fn, optimizer, X, y, self.generate_key)\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels.</p> PARAMETER DESCRIPTION <code>n_features</code> <p>Number of features that the classifier expects</p> <p> TYPE: <code>int</code> </p> <code>classes</code> <p>class labels that the classifier expects</p> <p> TYPE: <code>array - like</code> DEFAULT: <code>None</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/vanilla_qnn.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_features = n_features\n    self.n_qubits_ = n_features\n    self.initialize_params()\n    self.construct_model()\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred</code> <p>Predicted labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/vanilla_qnn.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred (np.ndarray): Predicted labels of shape (n_samples,)\n    \"\"\"\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/vanilla_qnn.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    predictions = self.chunked_forward(self.params_, X)\n    predictions_2d = np.c_[(1 - predictions) / 2, (1 + predictions) / 2]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/vanilla_qnn.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n            self.scaler.fit(X)\n        X = self.scaler.transform(X)\n    return X * self.scaling\n</code></pre> <code></code> WeiNet <pre><code>WeiNet(filter_name='edge_detect', learning_rate=0.1, max_steps=10000, convergence_interval=200, random_state=42, max_vmap=None, jit=True, scaling=1.0, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax-jit'}, batch_size=32)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Quantum convolutional neural network from https://arxiv.org/abs/2104.06918v3  (see fig 2 of plots)</p> <p>The model has two registers: the ancilliary register and the work register. The ancilliary register is used to parameterise a 4 qubit state which in turn controls a number of unitaries that act on the work register, where the data is encoded via amplitude encoding.</p> <p>The qubits with index -1 and height-1 are then traced out, which is equivalent to a type of pooling. All single and double correlators  and  are measured, and a linear model on these values is used for classification.</p> <p>The plots does not specify the loss: we use the binary cross entropy.</p> <p>The input data X should have shape (dataset_size,height*width) and will be reshaped to (dataset_size,1, height, width) in the model. We assume height=width.</p> <p>Note that in figure 2 of the plots, the Hadamards on the ancilla register have no effect since we trace this register out. The effect of this register is then to simply perform  a classical mixture of the unitaries Q_i on the work register. For simplicity (and to save qubit numbers), we parameterise this distribution via params_['s'] rather than model the ancilla qubits themselves.</p> PARAMETER DESCRIPTION <code>filter_name</code> <p>The classical filter that defines the unitaries Q_i. either 'edge_detect', 'smooth', or 'sharpen'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'edge_detect'</code> </p> <code>learning_rate</code> <p>Initial learning rate for gradient descent.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>convergence_interval</code> <p>The number of loss values to consider to decide convergence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>max_steps</code> <p>Maximum number of training steps. A warning will be raised if training did not converge.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> <code>batch_size</code> <p>Size of batches used for computing parameter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the key word arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax-jit'}</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> METHOD DESCRIPTION <code>construct_models</code> <p>constructs the 9 circuits used for the convolutional layer (Q_k in the plots).</p> <code>construct_unitaries</code> <p>Construct the unitaries V' defined in the plots</p> <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>forward_fn</code> <p>We have taken some shortcuts here compared to the plots description but the result is the same.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>initialize_params</code> <p>initialise the trainable parameters</p> <code>predict</code> <p>Predict labels for data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/weinet.py</code> <pre><code>def __init__(\n    self,\n    filter_name=\"edge_detect\",\n    learning_rate=0.1,\n    max_steps=10000,\n    convergence_interval=200,\n    random_state=42,\n    max_vmap=None,\n    jit=True,\n    scaling=1.0,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax-jit\"},\n    batch_size=32,\n):\n    \"\"\"\n    Quantum convolutional neural network from https://arxiv.org/abs/2104.06918v3  (see fig 2 of plots)\n\n    The model has two registers: the ancilliary register and the work register. The ancilliary register is used to\n    parameterise a 4 qubit state which in turn controls a number of unitaries that act on the work register, where\n    the data is encoded via amplitude encoding.\n\n    The qubits with index -1 and height-1 are then traced out, which is equivalent to a type of pooling. All single\n    and double correlators &lt;Z&gt; and &lt;ZZ&gt; are measured, and a linear model on these values is used for classification.\n\n    The plots does not specify the loss: we use the binary cross entropy.\n\n    The input data X should have shape (dataset_size,height*width) and will be reshaped to\n    (dataset_size,1, height, width) in the model. We assume height=width.\n\n    Note that in figure 2 of the plots, the Hadamards on the ancilla register have no effect since we trace this\n    register out. The effect of this register is then to simply perform  a classical mixture of the unitaries\n    Q_i on the work register. For simplicity (and to save qubit numbers), we parameterise this distribution\n    via params_['s'] rather than model the ancilla qubits themselves.\n\n    Args:\n        filter_name (str): The classical filter that defines the unitaries Q_i. either 'edge_detect', 'smooth', or\n            'sharpen'.\n        learning_rate (float): Initial learning rate for gradient descent.\n        convergence_interval (int): The number of loss values to consider to decide convergence.\n        max_steps (int): Maximum number of training steps. A warning will be raised if training did not converge.\n        batch_size (int): Size of batches used for computing parameter updates.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'.\n        qnode_kwargs (str): the key word arguments passed to the circuit qnode.\n        random_state (int): Seed used for pseudorandom number generation.\n        scaling (float): Factor by which to scale the input data.\n    \"\"\"\n    # attributes that do not depend on data\n    self.learning_rate = learning_rate\n    self.max_steps = max_steps\n    self.filter_name = filter_name\n    self.convergence_interval = convergence_interval\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.scaling = scaling\n    self.batch_size = batch_size\n    self.jit = jit\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n    self.unitaries = []\n\n    if filter_name == \"edge_detect\":\n        self.filter = jnp.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]])\n    elif filter_name == \"smooth\":\n        self.filter = jnp.array([[1, 1, 1], [1, 5, 1], [1, 1, 1]]) / 13\n    elif filter_name == \"sharpen\":\n        self.filter = jnp.array([[-2, -2, -2], [-2, 32, -2], [-2, -2, -2]]) / 16\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.n_qubits_ = None\n    self.params_ = None  # Dictionary containing the trainable parameters\n    self.height_ = None  # height of image data\n    self.width_ = None  # width of image data\n    self.circuit = None\n</code></pre> <code></code> construct_models <pre><code>construct_models()\n</code></pre> <p>constructs the 9 circuits used for the convolutional layer (Q_k in the plots).</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/weinet.py</code> <pre><code>def construct_models(self):\n    \"\"\"\n    constructs the 9 circuits used for the convolutional layer (Q_k in the plots).\n    \"\"\"\n\n    # get the operators that are used for prediction. We don't include the self.height_ qubit or the last\n    # qubit as per the plots, since there are lost in the pooling layer\n    operators = []\n    for i in range(self.n_qubits_ - 1):\n        if i != int(self.n_qubits_ / 2) - 1:\n            operators.append(qml.PauliZ(wires=i))\n            for j in range(i + 1, self.n_qubits_ - 1):\n                if j != int(self.n_qubits_ / 2) - 1:\n                    operators.append(qml.PauliZ(wires=i) @ qml.PauliZ(wires=j))\n    operators.append(qml.Identity(wires=0))\n\n    wires = range(self.n_qubits_)\n    dev = qml.device(self.dev_type, wires=wires)\n    circuits = []\n    for nu in range(3):\n        for mu in range(3):\n\n            @qml.qnode(dev, **self.qnode_kwargs)\n            def circuit(x):\n                qml.AmplitudeEmbedding(\n                    jnp.reshape(x, -1), wires=wires, normalize=True, pad_with=0.0\n                )\n                qml.QubitUnitary(\n                    jnp.kron(self.unitaries[nu][nu], jnp.array(self.unitaries[nu][mu])),\n                    wires=wires,\n                )\n                return [qml.expval(op) for op in operators]\n\n            self.circuit = circuit  # we use the last one of the circuits here as an example\n\n            if self.jit:\n                circuit = jax.jit(circuit)\n            circuits.append(circuit)\n\n    self.circuits = circuits\n</code></pre> <code></code> construct_unitaries <pre><code>construct_unitaries()\n</code></pre> <p>Construct the unitaries V' defined in the plots</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/weinet.py</code> <pre><code>def construct_unitaries(self):\n    \"\"\"\n    Construct the unitaries V' defined in the plots\n    \"\"\"\n    self.unitaries = [[None for __ in range(3)] for __ in range(3)]\n    for mu in range(3):\n        for nu, k in enumerate([-1, 0, 1]):\n            V = np.zeros([self.height_, self.width_])\n            for i in range(self.height_):\n                V[i, (i + k) % self.height_] = self.filter[nu, mu]\n            self.unitaries[nu][mu] = V / self.filter[nu, mu]\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Image data of shape (n_samples, height**2)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/weinet.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Image data of shape (n_samples, height**2)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n\n    self.initialize(X.shape[1], classes=np.unique(y))\n    y = jnp.array(y, dtype=int)\n    X = self.transform(X)\n\n    # initialise the model\n    self.construct_unitaries()\n    self.construct_models()\n    self.forward = jax.vmap(self.forward_fn, in_axes=(None, 0))\n    self.chunked_forward = chunk_vmapped_fn(self.forward, 1, self.max_vmap)\n\n    def loss_fn(params, X, y):\n        # we use the usual cross entropy\n        y = jax.nn.relu(y)  # convert to 0,1 labels\n        vals = self.forward(params, X)\n        loss = jnp.mean(optax.sigmoid_binary_cross_entropy(vals, y))\n        return loss\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    optimizer = optax.adam\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        X,\n        y,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    return self\n</code></pre> <code></code> forward_fn <pre><code>forward_fn(params, x)\n</code></pre> <p>We have taken some shortcuts here compared to the plots description but the result is the same. Since we trace out the ancilla register, the final hadamards in the circuit diagram have no effect, and the process is equivalent to classically sampling one of the unitaries Q_i, parameterised by params['s'].</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/weinet.py</code> <pre><code>def forward_fn(self, params, x):\n    \"\"\"\n    We have taken some shortcuts here compared to the plots description but the result is the same.\n    Since we trace out the ancilla register, the final hadamards in the circuit diagram have no effect, and the process\n    is equivalent to classically sampling one of the unitaries Q_i, parameterised by params['s'].\n    \"\"\"\n    probs = jax.nn.softmax(params[\"s\"])\n    expvals = jnp.array([probs[i] * jnp.array(self.circuits[i](x)).T for i in range(9)])\n    expvals = jnp.sum(expvals, axis=0)\n    out = jnp.sum(params[\"weights\"] * expvals)\n    # out = jax.nn.sigmoid(out)  # convert to a probability\n    # out = jnp.vstack((out, 1 - out)).T  # convert to 'two neurons'\n    # out = jnp.reshape(out, (2))\n    return out\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels.</p> PARAMETER DESCRIPTION <code>n_features</code> <p>Number of features that the classifier expects</p> <p> TYPE: <code>int</code> </p> <code>classes</code> <p>class labels that the classifier expects</p> <p> TYPE: <code>array - like</code> DEFAULT: <code>None</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/weinet.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    im_height = int(jnp.sqrt(n_features))\n    self.n_qubits_ = 2 * ceil(jnp.log2(im_height))\n    self.height_ = 2 ** (self.n_qubits_ // 2)\n    self.width_ = 2 ** (self.n_qubits_ // 2)\n    self.initialize_params()\n    self.construct_unitaries()\n    self.construct_models()\n</code></pre> <code></code> initialize_params <pre><code>initialize_params()\n</code></pre> <p>initialise the trainable parameters</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/weinet.py</code> <pre><code>def initialize_params(self):\n    \"\"\"\n    initialise the trainable parameters\n    \"\"\"\n    # no of expvals that are combined with weights\n    n_expvals = int(\n        self.n_qubits_ - 1 + factorial(self.n_qubits_ - 2) / 2 / factorial(self.n_qubits_ - 4)\n    )\n\n    self.params_ = {\n        \"s\": jax.random.normal(self.generate_key(), shape=(9,)),\n        \"weights\": jax.random.normal(self.generate_key(), shape=(n_expvals,)) / n_expvals,\n    }\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred</code> <p>Predicted labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/weinet.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred (np.ndarray): Predicted labels of shape (n_samples,)\n    \"\"\"\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/weinet.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    p1 = jax.nn.sigmoid(self.chunked_forward(self.params_, X))\n    predictions_2d = jnp.c_[1 - p1, p1]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/weinet.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n\n    # put in NCHW format. We assume square images\n    im_height = int(jnp.sqrt(X.shape[1]))\n    X = jnp.reshape(X, (X.shape[0], 1, im_height, im_height))\n\n    X = self.scaling * X\n\n    padded_X = np.zeros([X.shape[0], X.shape[1], self.height_, self.width_])\n    padded_X[: X.shape[0], : X.shape[1], : X.shape[2], : X.shape[3]] = X\n    return jnp.array(padded_X)\n</code></pre> <code></code> circuit_centric CLASS DESCRIPTION <code>CircuitCentricClassifier</code> <code></code> CircuitCentricClassifier <pre><code>CircuitCentricClassifier(n_input_copies=2, n_layers=4, convergence_interval=200, max_steps=10000, learning_rate=0.001, batch_size=32, max_vmap=None, jit=True, scaling=1.0, random_state=42, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax-jit'})\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>A variational quantum model based on https://arxiv.org/pdf/1804.03680v2.pdf.</p> <p>Uses amplitude encoding as the data embedding, but implements several copies of the initial state. The data vector :math:<code>x</code> of size :math:<code>N</code> gets padded to the next power of 2 by values :math:<code>1/len(\\tilde{x})</code> (so that the new data vector :math:<code>\\tilde{x}</code> has length :math:<code>2^n &gt; N</code>). The padded vector gets normalised to :math:<code>\\bar{\\tilde{x}}</code>, and embedded into the amplitudes of several copies of :math:<code>n</code>-qubit quantum states. Altogether, the variational circuit acts on a quantum state of amplitudes :math:<code>\\bar{\\tilde{x}} \\otimes \\bar{\\tilde{x}} \\otimes ...</code>.</p> <p>The total number of qubits :math:<code>d\\lceil \\log(n_features) \\rceil</code> of this model depends on the number of features :math:<code>N</code> and the number of copies :math:<code>d</code>.</p> <p>The variational part of the circuit uses general single qubit rotations as trainable gates. Each layer in the ansatz first applies a rotation to each qubit, and then controlled rotations connecting each qubit :math:<code>i</code> to qubit :math:<code>i+r \\text{mod } n</code>, where :math:<code>r</code> repeatedly runs through the range :math:<code>[0,...,n-1]</code>. The number of layers in the ansatz is a hyperparameter of the model.</p> <p>The result of the model is the sigma-z expectation of the first qubit, plus a trainable scalar bias. Training is via the square loss.</p> PARAMETER DESCRIPTION <code>n_input_copies</code> <p>Number of copies of the amplitude embedded state to produce.</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>n_layers</code> <p>Number of layers in the variational ansatz.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>convergence_interval</code> <p>The number of loss values to consider to decide convergence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>max_steps</code> <p>Maximum number of training steps. A warning will be raised if training did not converge.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> <code>learning_rate</code> <p>Initial learning rate for gradient descent.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> <code>batch_size</code> <p>Size of batches used for computing parameter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dev_type</code> <p>Pennylane device type; e.g. 'default.qubit.jax'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>Keyword arguments for the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax-jit'}</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>generate_key</code> <p>Generates a random key used in sampling batches.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for batch of data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>The feature vectors padded to the next power of 2 and then normalised.</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/circuit_centric.py</code> <pre><code>def __init__(\n    self,\n    n_input_copies=2,\n    n_layers=4,\n    convergence_interval=200,\n    max_steps=10000,\n    learning_rate=0.001,\n    batch_size=32,\n    max_vmap=None,\n    jit=True,\n    scaling=1.0,\n    random_state=42,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax-jit\"},\n):\n    r\"\"\"\n    A variational quantum model based on https://arxiv.org/pdf/1804.03680v2.pdf.\n\n    Uses amplitude encoding as the data embedding, but implements several\n    copies of the initial state. The data vector :math:`x` of size :math:`N`\n    gets padded to the next power of 2 by values :math:`1/len(\\tilde{x})`\n    (so that the new data vector :math:`\\tilde{x}` has length :math:`2^n &gt; N`).\n    The padded vector gets normalised to :math:`\\bar{\\tilde{x}}`, and\n    embedded into the amplitudes of several copies of :math:`n`-qubit\n    quantum states. Altogether, the variational circuit acts on a quantum\n    state of amplitudes :math:`\\bar{\\tilde{x}} \\otimes \\bar{\\tilde{x}} \\otimes ...`.\n\n    The total number of qubits :math:`d\\lceil \\log(n_features) \\rceil` of\n    this model depends on the number of features :math:`N` and the number\n    of copies :math:`d`.\n\n    The variational part of the circuit uses general single qubit rotations\n    as trainable gates. Each layer in the ansatz first applies a rotation\n    to each qubit, and then controlled rotations connecting each qubit\n    :math:`i` to qubit :math:`i+r \\text{mod } n`, where :math:`r` repeatedly runs\n    through the range :math:`[0,...,n-1]`. The number of layers in the\n    ansatz is a hyperparameter of the model.\n\n    The result of the model is the sigma-z expectation of the first qubit,\n    plus a trainable scalar bias. Training is via the square loss.\n\n    Args:\n        n_input_copies (int): Number of copies of the amplitude embedded\n            state to produce.\n        n_layers (int): Number of layers in the variational ansatz.\n        convergence_interval (int): The number of loss values to consider to decide convergence.\n        max_steps (int): Maximum number of training steps. A warning will\n            be raised if training did not converge.\n        learning_rate (float): Initial learning rate for gradient descent.\n        batch_size (int): Size of batches used for computing parameter updates.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        dev_type (str): Pennylane device type; e.g. 'default.qubit.jax'.\n        qnode_kwargs (str): Keyword arguments for the circuit qnode.\n        scaling (float): Factor by which to scale the input data.\n        random_state (int): Seed used for pseudorandom number generation.\n    \"\"\"\n\n    # attributes that do not depend on data\n    self.n_input_copies = n_input_copies\n    self.n_layers = n_layers\n    self.convergence_interval = convergence_interval\n    self.max_steps = max_steps\n    self.learning_rate = learning_rate\n    self.batch_size = batch_size\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.jit = jit\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # Dictionary containing the trainable parameters\n    self.n_qubits_ = None\n    self.scaler = None\n    self.circuit = None\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/circuit_centric.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n\n    self.initialize(n_features=X.shape[1], classes=np.unique(y))\n\n    X = self.transform(X)\n\n    optimizer = optax.adam\n\n    def loss_fn(params, X, y):\n        pred = self.forward(params, X)  # jnp.stack([self.forward(params, x) for x in X])\n        return jnp.mean(optax.l2_loss(pred, y))\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        X,\n        y,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    return self\n</code></pre> <code></code> generate_key <pre><code>generate_key() -&gt; Array\n</code></pre> <p>Generates a random key used in sampling batches.</p> RETURNS DESCRIPTION <code>Array</code> <p>jax.Array: description</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/circuit_centric.py</code> <pre><code>def generate_key(self) -&gt; jax.Array:\n    \"\"\"\n    Generates a random key used in sampling batches.\n\n    Returns:\n        jax.Array: _description_\n    \"\"\"\n    return jax.random.PRNGKey(self.rng.integers(1000000))\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels. Args:     n_features (int): Number of features that the classifier expects     classes (array-like): class labels</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/circuit_centric.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    n_qubits_per_copy = int(np.ceil(np.log2(n_features)))\n    self.n_qubits_ = self.n_input_copies * n_qubits_per_copy\n\n    self.construct_model()\n    self.initialize_params()\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for batch of data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/circuit_centric.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for batch of data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/circuit_centric.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    predictions = self.chunked_forward(self.params_, X)\n    predictions_2d = np.c_[(1 - predictions) / 2, (1 + predictions) / 2]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=False)\n</code></pre> <p>The feature vectors padded to the next power of 2 and then normalised.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/circuit_centric.py</code> <pre><code>def transform(self, X, preprocess=False):\n    \"\"\"\n    The feature vectors padded to the next power of 2 and then normalised.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    n_features = X.shape[1]\n    X = X * self.scaling\n\n    n_qubits_per_copy = int(np.ceil(np.log2(n_features)))\n    max_n_features = 2**n_qubits_per_copy\n    n_padding = max_n_features - n_features\n    padding = np.ones(shape=(len(X), n_padding)) / max_n_features\n\n    X_padded = np.c_[X, padding]\n    X_normalised = np.divide(X_padded, np.expand_dims(np.linalg.norm(X_padded, axis=1), axis=1))\n    return X_normalised\n</code></pre> <code></code> convolutional_neural_network CLASS DESCRIPTION <code>ConvolutionalNeuralNetwork</code> <code></code> ConvolutionalNeuralNetwork <pre><code>ConvolutionalNeuralNetwork(kernel_shape=3, output_channels=[32, 64], learning_rate=0.001, convergence_interval=200, max_steps=10000, batch_size=32, max_vmap=None, jit=True, random_state=42, scaling=1.0)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>This implements a vanilla convolutional neural network (CNN) two-class classifier with JAX and flax (https://github.com/google/flax).</p> <p>The structure of the neural network is as follows:</p> <ul> <li>a 2D convolutional layer with self.output_channels[0] output channels</li> <li>a max pool layer</li> <li>a 2D convolutional layer with self.output_channels[1] output channels</li> <li>a max pool layer</li> <li>a two layer fully connected feedforward neural network with 2*self.output_channels[1] hidden neurons     and one output neuron</li> </ul> <p>The probability of class 1 is given by :math:<code>P(+1\\vert \\vec{w},\\vec{x}) = \\sigma(f(\\vec{w}),\\vec{x})</code> where :math:<code>\\vec{w}</code> are the weights of the network and :math:<code>\\sigma</code> is the logistic function and :math:<code>f</code> gives the value of the neuron in the final later. These probabilities are fed to binary cross entropy loss for training.</p> <p>The 2d input data should be flattened to have shape (n_samples, height*width), where height and width are the dimensions of the 2d data. The model works for square data only, i.e. height=width.</p> PARAMETER DESCRIPTION <code>kernel_shape</code> <p>the shape of the kernel used in the CNN. e.g. kernel_shape=3 uses a 3x3 filter</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>output_channels</code> <p>Two integers specifying the output sizes of the convolutional layers in the CNN. Defaults to [32, 62].</p> <p> TYPE: <code>list[int]</code> DEFAULT: <code>[32, 64]</code> </p> <code>learning_rate</code> <p>Initial learning rate for training.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> <code>max_steps</code> <p>Maximum number of training steps. A warning will be raised if training did not converge.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> <code>convergence_interval</code> <p>The number of loss values to consider to decide convergence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>batch_size</code> <p>Size of batches used for computing parameter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> METHOD DESCRIPTION <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>If scaler is initialized, transform the inputs.</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/convolutional_neural_network.py</code> <pre><code>def __init__(\n    self,\n    kernel_shape=3,\n    output_channels=[32, 64],\n    learning_rate=0.001,\n    convergence_interval=200,\n    max_steps=10000,\n    batch_size=32,\n    max_vmap=None,\n    jit=True,\n    random_state=42,\n    scaling=1.0,\n):\n    r\"\"\"\n    This implements a vanilla convolutional neural network (CNN) two-class classifier with JAX and flax\n    (https://github.com/google/flax).\n\n    The structure of the neural network is as follows:\n\n    - a 2D convolutional layer with self.output_channels[0] output channels\n    - a max pool layer\n    - a 2D convolutional layer with self.output_channels[1] output channels\n    - a max pool layer\n    - a two layer fully connected feedforward neural network with 2*self.output_channels[1] hidden neurons\n        and one output neuron\n\n\n    The probability of class 1 is given by :math:`P(+1\\vert \\vec{w},\\vec{x}) = \\sigma(f(\\vec{w}),\\vec{x})`\n    where :math:`\\vec{w}` are the weights of the network and :math:`\\sigma` is the logistic function and\n    :math:`f` gives the value of the neuron in the final later. These probabilities are fed to binary cross entropy\n    loss for training.\n\n    The 2d input data should be flattened to have shape (n_samples, height*width), where height and width are the\n    dimensions of the 2d data. The model works for square data only, i.e. height=width.\n\n    Args:\n        kernel_shape (int): the shape of the kernel used in the CNN. e.g. kernel_shape=3 uses a 3x3 filter\n        output_channels (list[int]): Two integers specifying the output sizes of the convolutional layers\n            in the CNN. Defaults to [32, 62].\n        learning_rate (float): Initial learning rate for training.\n        max_steps (int): Maximum number of training steps. A warning will be raised if training did not converge.\n        convergence_interval (int): The number of loss values to consider to decide convergence.\n        batch_size (int): Size of batches used for computing parameter updates.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        random_state (int): Seed used for pseudorandom number generation.\n        scaling (float): Factor by which to scale the input data.\n    \"\"\"\n\n    # attributes that do not depend on data\n    self.learning_rate = learning_rate\n    self.max_steps = max_steps\n    self.scaling = scaling\n    self.jit = jit\n    self.kernel_shape = kernel_shape\n    self.output_channels = output_channels\n    self.convergence_interval = convergence_interval\n    self.batch_size = batch_size\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # Dictionary containing the trainable parameters\n    self.scaler = None  # data scaler will be fitted on training data\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/convolutional_neural_network.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n\n    self.initialize(X.shape[1], classes=np.unique(y))\n\n    y = jnp.array(y, dtype=int)\n\n    # scale input data\n    self.scaler = StandardScaler()\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    def loss_fn(params, X, y):\n        y = jax.nn.relu(y)  # convert to 0,1 labels\n        vals = self.forward.apply(params, X)[:, 0]\n        loss = jnp.mean(optax.sigmoid_binary_cross_entropy(vals, y))\n        return loss\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    optimizer = optax.adam\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        X,\n        y,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels. Args:     classes (array-like): class labels that the classifier expects</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/convolutional_neural_network.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n    Args:\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    # initialise the model\n    self.cnn = construct_cnn(self.output_channels, self.kernel_shape)\n    self.forward = self.cnn\n\n    # create dummy data input to initialise the cnn\n    height = int(jnp.sqrt(n_features))\n    X0 = jnp.ones(shape=(1, height, height, 1))\n    self.initialize_params(X0)\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred</code> <p>Predicted labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/convolutional_neural_network.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred (np.ndarray): Predicted labels of shape (n_samples,)\n    \"\"\"\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/convolutional_neural_network.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    # get probabilities of y=1\n    p1 = jax.nn.sigmoid(self.forward.apply(self.params_, X)[:, 0])\n    predictions_2d = jnp.c_[1 - p1, p1]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X)\n</code></pre> <p>If scaler is initialized, transform the inputs.</p> <p>Put into NCHW format. This assumes square images. Args:     X (np.ndarray): Data of shape (n_samples, n_features)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/convolutional_neural_network.py</code> <pre><code>def transform(self, X):\n    \"\"\"\n    If scaler is initialized, transform the inputs.\n\n    Put into NCHW format. This assumes square images.\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if self.scaler is None:\n        # if the model is unfitted, initialise the scaler here\n        self.scaler = StandardScaler()\n        self.scaler.fit(X)\n\n    X = self.scaler.transform(X) * self.scaling\n\n    # reshape data to square array\n    X = jnp.array(X)\n    height = int(jnp.sqrt(X.shape[1]))\n    X = jnp.reshape(X, (X.shape[0], height, height, 1))\n\n    return X\n</code></pre> <code></code> data_reuploading CLASS DESCRIPTION <code>DataReuploadingClassifier</code> <code>DataReuploadingClassifierNoCost</code> <code>DataReuploadingClassifierNoScaling</code> <code>DataReuploadingClassifierNoTrainableEmbedding</code> <code>DataReuploadingClassifierSeparable</code> <code></code> DataReuploadingClassifier <pre><code>DataReuploadingClassifier(n_layers=4, observable_type='single', convergence_interval=200, max_steps=10000, learning_rate=0.05, batch_size=32, max_vmap=None, jit=True, scaling=1.0, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax-jit'}, random_state=42)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Data reuploading classifier from https://arxiv.org/abs/1907.02085. Here we code the 'multi-qubit classifier'. The model consists of layers of trainable data encoding unitaries. Each subsequent sequence of 3 elements of an input vector x are encoded into a SU(2) rotation via pennylane.Rot. The model therefore uses ceil(d/3) qubits.</p> <p>The cost function is given by eqn (23) in the plots, and includes trainable parameters for scaling input data (omegas), the class weights (alphas), and the circuit parameters (thetas). The cost function is a weighted sum of the squared differences between the ideal fidelity to the corresponding class state (here, either |0&gt; or |1&gt;) and the output state of the circuit, for a number of qubits specified by observable_weight. More specifically</p> <p>.. math::</p> <pre><code>\\ell(\\vec{\\theta},\\vec{\\omega},\\vec{\\alpha},\\vec{x}_i) =\n\\sum_{j=1}^{n_{\\text{max}}}(\\alpha_j^0F^0_j-(1-y_i))^2 + (\\alpha_j^1F^1_j-y_i)^2\n</code></pre> <p>where :math:<code>n_max</code> is given by observable_weight and :math:<code>F^0_j</code> is the fidelity of the jth output quibt to the state |0&gt;.</p> <p>When observable_weight !=1 it is not clear from the plots how to predict labels. Here we look at the average fidelities to |0&gt; and |1&gt; of the qubits up to index self.observable_weight and take the label with the largest average fidelity. For self.observable_weight=1 this reverts to their strategy in the plots.</p> <p>Where possible, we have followed the numerical examples found in the repo which accompanies the plots:</p> <p>https://github.com/AdrianPerezSalinas/universal_qlassifier/tree/354d70f940ea737192de4063ff72f859c77e5760</p> PARAMETER DESCRIPTION <code>n_layers</code> <p>Number of blocks used in the trainable embedding. The data is uploaded n_layers+1 times.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>observable_type</code> <p>Defines the number of qubits used to evaluate the weighed cost fucntion, either 'single', 'half' or 'full'.  max_steps (int): Maximum number of training steps. A warning will be raised if training did not     converge.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'single'</code> </p> <code>learning_rate</code> <p>Initial learning rate for training.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.05</code> </p> <code>convergence_interval</code> <p>The number of loss values to consider to decide convergence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>batch_size</code> <p>Size of batches used for computing paraeeter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the keyword arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax-jit'}</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>construct_model</code> <p>Construct the quantum circuit used in the model.</p> <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for batch of data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def __init__(\n    self,\n    n_layers=4,\n    observable_type=\"single\",\n    convergence_interval=200,\n    max_steps=10000,\n    learning_rate=0.05,\n    batch_size=32,\n    max_vmap=None,\n    jit=True,\n    scaling=1.0,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax-jit\"},\n    random_state=42,\n):\n    r\"\"\"\n    Data reuploading classifier from https://arxiv.org/abs/1907.02085. Here we code the 'multi-qubit classifier'.\n    The model consists of layers of trainable data encoding unitaries. Each subsequent sequence of 3 elements of\n    an input vector x are encoded into a SU(2) rotation via pennylane.Rot. The model therefore uses ceil(d/3)\n    qubits.\n\n    The cost function is given by eqn (23) in the plots, and includes trainable parameters for scaling input data\n    (omegas), the class weights (alphas), and the circuit parameters (thetas). The cost\n    function is a weighted sum of the squared differences between the ideal fidelity to the corresponding class state\n    (here, either |0&gt; or |1&gt;) and the output state of the circuit, for a number of qubits specified by\n    observable_weight. More specifically\n\n    .. math::\n\n        \\ell(\\vec{\\theta},\\vec{\\omega},\\vec{\\alpha},\\vec{x}_i) =\n        \\sum_{j=1}^{n_{\\text{max}}}(\\alpha_j^0F^0_j-(1-y_i))^2 + (\\alpha_j^1F^1_j-y_i)^2\n\n    where :math:`n_max` is given by observable_weight and :math:`F^0_j` is the fidelity of the jth output\n    quibt to the state |0&gt;.\n\n    When observable_weight !=1 it is not clear from the plots how to predict labels. Here we look at the average\n    fidelities to |0&gt; and |1&gt; of the qubits up to index self.observable_weight and take the label with the largest\n    average fidelity. For self.observable_weight=1 this reverts to their strategy in the plots.\n\n    Where possible, we have followed the numerical examples found in the repo which accompanies the plots:\n\n    https://github.com/AdrianPerezSalinas/universal_qlassifier/tree/354d70f940ea737192de4063ff72f859c77e5760\n\n    Args:\n        n_layers (int): Number of blocks used in the trainable embedding. The data is uploaded n_layers+1 times.\n        observable_type (str): Defines the number of qubits used to evaluate the weighed cost fucntion,\n            either 'single', 'half' or 'full'.\n             max_steps (int): Maximum number of training steps. A warning will be raised if training did not\n                converge.\n        learning_rate (float): Initial learning rate for training.\n        convergence_interval (int): The number of loss values to consider to decide convergence.\n        batch_size (int): Size of batches used for computing paraeeter updates.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'.\n        qnode_kwargs (str): the keyword arguments passed to the circuit qnode.\n        scaling (float): Factor by which to scale the input data.\n        random_state (int): Seed used for pseudorandom number generation.\n    \"\"\"\n\n    # attributes that do not depend on data\n    self.n_layers = n_layers\n    self.observable_type = observable_type\n    self.convergence_interval = convergence_interval\n    self.max_steps = max_steps\n    self.learning_rate = learning_rate\n    self.batch_size = batch_size\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.jit = jit\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # dictionary containing the trainable parameters\n    self.n_qubits_ = None\n    self.scaler = None  # data preprocessing\n    self.circuit = None\n</code></pre> <code></code> construct_model <pre><code>construct_model()\n</code></pre> <p>Construct the quantum circuit used in the model.</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def construct_model(self):\n    \"\"\"Construct the quantum circuit used in the model.\"\"\"\n\n    dev = qml.device(self.dev_type, wires=self.n_qubits_)\n\n    @qml.qnode(dev, **self.qnode_kwargs)\n    def circuit(params, x):\n        \"\"\"A variational quantum circuit with data reuploading.\n\n        Args:\n            params (array[float]): dictionary of parameters\n            x (array[float]): input vector\n\n        Returns:\n            list: Expectation values of Pauli Z on each qubit\n        \"\"\"\n        for layer in range(self.n_layers):\n            x_idx = 0  # to keep track of the data index\n            for i in range(self.n_qubits_):\n                # scaled inputs\n                angles = x[x_idx : x_idx + 3] * params[\"omegas\"][layer, x_idx : x_idx + 3]\n                qml.Rot(*angles, wires=i)\n\n                # variational\n                angles = params[\"thetas\"][i, layer, :]\n                qml.Rot(*angles, wires=i)\n\n                x_idx += 3\n            if layer % 2 == 0:\n                qml.broadcast(qml.CZ, range(self.n_qubits_), pattern=\"double\")\n            else:\n                qml.broadcast(qml.CZ, range(self.n_qubits_), pattern=\"double_odd\")\n\n        # final reupload without CZs\n        x_idx = 0\n        for i in range(self.n_qubits_):\n            angles = (\n                x[x_idx : x_idx + 3] * params[\"omegas\"][self.n_layers, x_idx : x_idx + 3]\n                + params[\"thetas\"][i, self.n_layers, :]\n            )\n            qml.Rot(*angles, wires=i)\n            x_idx += 3\n\n        return [qml.expval(qml.PauliZ(wires=[i])) for i in range(self.n_qubits_)]\n\n    self.circuit = circuit\n\n    def circuit_as_array(params, x):\n        # pennylane returns a list in the circuit above, so we need this\n        return jnp.array(circuit(params, x))\n\n    if self.jit:\n        circuit_as_array = jax.jit(circuit_as_array)\n    self.forward = jax.vmap(circuit_as_array, in_axes=(None, 0))\n    self.chunked_forward = chunk_vmapped_fn(self.forward, 1, self.max_vmap)\n\n    return self.forward\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n    self.initialize(X.shape[1], classes=np.unique(y))\n\n    self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    def loss_fn(params, x, y):\n        y_mat = jnp.vstack(\n            [y for __ in range(self.observable_weight)]\n        ).T  # repeated columns of labels\n        y_mat0 = (1 - y_mat) / 2\n        y_mat1 = (1 + y_mat) / 2\n        alpha_mat0 = jnp.vstack(\n            [params[\"alphas\"][0, :] for __ in range(len(x))]\n        )  # repeated rows of alpha parameters\n        alpha_mat1 = jnp.vstack([params[\"alphas\"][1, :] for __ in range(len(x))])\n\n        expvals = self.forward(params, x)\n        probs0 = (1 - expvals[:, : self.observable_weight]) / 2  # fidelity with |0&gt;\n        probs1 = (1 + expvals[:, : self.observable_weight]) / 2  # fidelity with |1&gt;\n        loss = (\n            1\n            / 2\n            * (\n                jnp.sum(\n                    (alpha_mat0 * probs0 - y_mat0) ** 2 + (alpha_mat1 * probs1 - y_mat1) ** 2\n                )\n            )\n        )  # eqn 23 in plots\n        return loss / len(y)\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    self.construct_model()\n    self.initialize_params()\n    optimizer = optax.adam\n\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        X,\n        y,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels. Args:     n_features (int): Number of features that the classifier expects     classes (array-like): class labels that the classifier expects</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_features = n_features\n    self.n_qubits_ = ceil(self.n_features / 3)\n\n    if self.observable_type == \"single\":\n        self.observable_weight = 1\n    elif self.observable_type == \"half\":\n        if self.n_qubits_ == 1:\n            self.observable_weight = 1\n        else:\n            self.observable_weight = self.n_qubits_ // 2\n    elif self.observable_type == \"full\":\n        self.observable_weight = self.n_qubits_\n\n    self.initialize_params()\n    self.construct_model()\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for batch of data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for batch of data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    check_is_fitted(self)\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    predictions = self.chunked_forward(self.params_, X)\n    predictions = jnp.mean(\n        predictions[:, : self.observable_weight], axis=1\n    )  # use mean of expvals over relevant qubits\n    predictions_2d = np.c_[(1 - predictions) / 2, (1 + predictions) / 2]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n            self.scaler.fit(X)\n\n        X = self.scaler.transform(X)\n\n    X = X * self.scaling\n    X = jnp.pad(X, ((0, 0), (0, (3 - X.shape[1]) % 3)), \"constant\")\n    return X\n</code></pre> <code></code> DataReuploadingClassifierNoCost <pre><code>DataReuploadingClassifierNoCost(n_layers=4, observable_type='single', convergence_interval=200, max_steps=10000, learning_rate=0.05, batch_size=32, max_vmap=None, jit=True, scaling=1.0, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax-jit'}, random_state=42)\n</code></pre> <p>               Bases: <code>DataReuploadingClassifier</code></p> <p>Data reuploading classifier from https://arxiv.org/abs/1907.02085. Here we code the 'multi-qubit classifier'. The model consists of layers of trainable data encoding unitaries. Each subsequent sequence of 3 elements of an input vector x are encoded into a SU(2) rotation via pennylane.Rot. The model therefore uses ceil(d/3) qubits.</p> <p>The cost function is given by eqn (23) in the plots, and includes trainable parameters for scaling input data (omegas), the class weights (alphas), and the circuit parameters (thetas). The cost function is a weighted sum of the squared differences between the ideal fidelity to the corresponding class state (here, either |0&gt; or |1&gt;) and the output state of the circuit, for a number of qubits specified by observable_weight. More specifically</p> <p>.. math::</p> <pre><code>\\ell(\\vec{\\theta},\\vec{\\omega},\\vec{\\alpha},\\vec{x}_i) =\n\\sum_{j=1}^{n_{\\text{max}}}(\\alpha_j^0F^0_j-(1-y_i))^2 + (\\alpha_j^1F^1_j-y_i)^2\n</code></pre> <p>where :math:<code>n_max</code> is given by observable_weight and :math:<code>F^0_j</code> is the fidelity of the jth output quibt to the state |0&gt;.</p> <p>When observable_weight !=1 it is not clear from the plots how to predict labels. Here we look at the average fidelities to |0&gt; and |1&gt; of the qubits up to index self.observable_weight and take the label with the largest average fidelity. For self.observable_weight=1 this reverts to their strategy in the plots.</p> <p>Where possible, we have followed the numerical examples found in the repo which accompanies the plots:</p> <p>https://github.com/AdrianPerezSalinas/universal_qlassifier/tree/354d70f940ea737192de4063ff72f859c77e5760</p> PARAMETER DESCRIPTION <code>n_layers</code> <p>Number of blocks used in the trainable embedding. The data is uploaded n_layers+1 times.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>observable_type</code> <p>Defines the number of qubits used to evaluate the weighed cost fucntion, either 'single', 'half' or 'full'.  max_steps (int): Maximum number of training steps. A warning will be raised if training did not     converge.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'single'</code> </p> <code>learning_rate</code> <p>Initial learning rate for training.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.05</code> </p> <code>convergence_interval</code> <p>The number of loss values to consider to decide convergence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>batch_size</code> <p>Size of batches used for computing paraeeter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the keyword arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax-jit'}</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>construct_model</code> <p>Construct the quantum circuit used in the model.</p> <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for batch of data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def __init__(\n    self,\n    n_layers=4,\n    observable_type=\"single\",\n    convergence_interval=200,\n    max_steps=10000,\n    learning_rate=0.05,\n    batch_size=32,\n    max_vmap=None,\n    jit=True,\n    scaling=1.0,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax-jit\"},\n    random_state=42,\n):\n    r\"\"\"\n    Data reuploading classifier from https://arxiv.org/abs/1907.02085. Here we code the 'multi-qubit classifier'.\n    The model consists of layers of trainable data encoding unitaries. Each subsequent sequence of 3 elements of\n    an input vector x are encoded into a SU(2) rotation via pennylane.Rot. The model therefore uses ceil(d/3)\n    qubits.\n\n    The cost function is given by eqn (23) in the plots, and includes trainable parameters for scaling input data\n    (omegas), the class weights (alphas), and the circuit parameters (thetas). The cost\n    function is a weighted sum of the squared differences between the ideal fidelity to the corresponding class state\n    (here, either |0&gt; or |1&gt;) and the output state of the circuit, for a number of qubits specified by\n    observable_weight. More specifically\n\n    .. math::\n\n        \\ell(\\vec{\\theta},\\vec{\\omega},\\vec{\\alpha},\\vec{x}_i) =\n        \\sum_{j=1}^{n_{\\text{max}}}(\\alpha_j^0F^0_j-(1-y_i))^2 + (\\alpha_j^1F^1_j-y_i)^2\n\n    where :math:`n_max` is given by observable_weight and :math:`F^0_j` is the fidelity of the jth output\n    quibt to the state |0&gt;.\n\n    When observable_weight !=1 it is not clear from the plots how to predict labels. Here we look at the average\n    fidelities to |0&gt; and |1&gt; of the qubits up to index self.observable_weight and take the label with the largest\n    average fidelity. For self.observable_weight=1 this reverts to their strategy in the plots.\n\n    Where possible, we have followed the numerical examples found in the repo which accompanies the plots:\n\n    https://github.com/AdrianPerezSalinas/universal_qlassifier/tree/354d70f940ea737192de4063ff72f859c77e5760\n\n    Args:\n        n_layers (int): Number of blocks used in the trainable embedding. The data is uploaded n_layers+1 times.\n        observable_type (str): Defines the number of qubits used to evaluate the weighed cost fucntion,\n            either 'single', 'half' or 'full'.\n             max_steps (int): Maximum number of training steps. A warning will be raised if training did not\n                converge.\n        learning_rate (float): Initial learning rate for training.\n        convergence_interval (int): The number of loss values to consider to decide convergence.\n        batch_size (int): Size of batches used for computing paraeeter updates.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'.\n        qnode_kwargs (str): the keyword arguments passed to the circuit qnode.\n        scaling (float): Factor by which to scale the input data.\n        random_state (int): Seed used for pseudorandom number generation.\n    \"\"\"\n\n    # attributes that do not depend on data\n    self.n_layers = n_layers\n    self.observable_type = observable_type\n    self.convergence_interval = convergence_interval\n    self.max_steps = max_steps\n    self.learning_rate = learning_rate\n    self.batch_size = batch_size\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.jit = jit\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # dictionary containing the trainable parameters\n    self.n_qubits_ = None\n    self.scaler = None  # data preprocessing\n    self.circuit = None\n</code></pre> <code></code> construct_model <pre><code>construct_model()\n</code></pre> <p>Construct the quantum circuit used in the model.</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def construct_model(self):\n    \"\"\"Construct the quantum circuit used in the model.\"\"\"\n\n    dev = qml.device(self.dev_type, wires=self.n_qubits_)\n\n    @qml.qnode(dev, **self.qnode_kwargs)\n    def circuit(params, x):\n        \"\"\"A variational quantum circuit with data reuploading.\n\n        Args:\n            params (array[float]): dictionary of parameters\n            x (array[float]): input vector\n\n        Returns:\n            list: Expectation values of Pauli Z on each qubit\n        \"\"\"\n        for layer in range(self.n_layers):\n            x_idx = 0  # to keep track of the data index\n            for i in range(self.n_qubits_):\n                # scaled inputs\n                angles = x[x_idx : x_idx + 3] * params[\"omegas\"][layer, x_idx : x_idx + 3]\n                qml.Rot(*angles, wires=i)\n\n                # variational\n                angles = params[\"thetas\"][i, layer, :]\n                qml.Rot(*angles, wires=i)\n\n                x_idx += 3\n            if layer % 2 == 0:\n                qml.broadcast(qml.CZ, range(self.n_qubits_), pattern=\"double\")\n            else:\n                qml.broadcast(qml.CZ, range(self.n_qubits_), pattern=\"double_odd\")\n\n        # final reupload without CZs\n        x_idx = 0\n        for i in range(self.n_qubits_):\n            angles = (\n                x[x_idx : x_idx + 3] * params[\"omegas\"][self.n_layers, x_idx : x_idx + 3]\n                + params[\"thetas\"][i, self.n_layers, :]\n            )\n            qml.Rot(*angles, wires=i)\n            x_idx += 3\n\n        return [qml.expval(qml.PauliZ(wires=[i])) for i in range(self.n_qubits_)]\n\n    self.circuit = circuit\n\n    def circuit_as_array(params, x):\n        # pennylane returns a list in the circuit above, so we need this\n        return jnp.array(circuit(params, x))\n\n    if self.jit:\n        circuit_as_array = jax.jit(circuit_as_array)\n    self.forward = jax.vmap(circuit_as_array, in_axes=(None, 0))\n    self.chunked_forward = chunk_vmapped_fn(self.forward, 1, self.max_vmap)\n\n    return self.forward\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n    self.initialize(X.shape[1], classes=np.unique(y))\n\n    self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    def loss_fn(params, X, y):\n        expvals = jnp.sum(self.forward(params, X), axis=1)\n        probs = (1 - expvals * y) / 2  # the probs of incorrect classification\n        return jnp.mean(probs)\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    self.construct_model()\n    self.initialize_params()\n    optimizer = optax.adam\n\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        X,\n        y,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels. Args:     n_features (int): Number of features that the classifier expects     classes (array-like): class labels that the classifier expects</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_features = n_features\n    self.n_qubits_ = ceil(self.n_features / 3)\n\n    if self.observable_type == \"single\":\n        self.observable_weight = 1\n    elif self.observable_type == \"half\":\n        if self.n_qubits_ == 1:\n            self.observable_weight = 1\n        else:\n            self.observable_weight = self.n_qubits_ // 2\n    elif self.observable_type == \"full\":\n        self.observable_weight = self.n_qubits_\n\n    self.initialize_params()\n    self.construct_model()\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for batch of data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for batch of data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    check_is_fitted(self)\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    predictions = self.chunked_forward(self.params_, X)\n    predictions = jnp.mean(\n        predictions[:, : self.observable_weight], axis=1\n    )  # use mean of expvals over relevant qubits\n    predictions_2d = np.c_[(1 - predictions) / 2, (1 + predictions) / 2]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n            self.scaler.fit(X)\n\n        X = self.scaler.transform(X)\n\n    X = X * self.scaling\n    X = jnp.pad(X, ((0, 0), (0, (3 - X.shape[1]) % 3)), \"constant\")\n    return X\n</code></pre> <code></code> DataReuploadingClassifierNoScaling <pre><code>DataReuploadingClassifierNoScaling(n_layers=4, observable_type='single', convergence_interval=200, max_steps=10000, learning_rate=0.05, batch_size=32, max_vmap=None, jit=True, scaling=1.0, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax-jit'}, random_state=42)\n</code></pre> <p>               Bases: <code>DataReuploadingClassifier</code></p> <p>Data reuploading classifier from https://arxiv.org/abs/1907.02085. Here we code the 'multi-qubit classifier'. The model consists of layers of trainable data encoding unitaries. Each subsequent sequence of 3 elements of an input vector x are encoded into a SU(2) rotation via pennylane.Rot. The model therefore uses ceil(d/3) qubits.</p> <p>The cost function is given by eqn (23) in the plots, and includes trainable parameters for scaling input data (omegas), the class weights (alphas), and the circuit parameters (thetas). The cost function is a weighted sum of the squared differences between the ideal fidelity to the corresponding class state (here, either |0&gt; or |1&gt;) and the output state of the circuit, for a number of qubits specified by observable_weight. More specifically</p> <p>.. math::</p> <pre><code>\\ell(\\vec{\\theta},\\vec{\\omega},\\vec{\\alpha},\\vec{x}_i) =\n\\sum_{j=1}^{n_{\\text{max}}}(\\alpha_j^0F^0_j-(1-y_i))^2 + (\\alpha_j^1F^1_j-y_i)^2\n</code></pre> <p>where :math:<code>n_max</code> is given by observable_weight and :math:<code>F^0_j</code> is the fidelity of the jth output quibt to the state |0&gt;.</p> <p>When observable_weight !=1 it is not clear from the plots how to predict labels. Here we look at the average fidelities to |0&gt; and |1&gt; of the qubits up to index self.observable_weight and take the label with the largest average fidelity. For self.observable_weight=1 this reverts to their strategy in the plots.</p> <p>Where possible, we have followed the numerical examples found in the repo which accompanies the plots:</p> <p>https://github.com/AdrianPerezSalinas/universal_qlassifier/tree/354d70f940ea737192de4063ff72f859c77e5760</p> PARAMETER DESCRIPTION <code>n_layers</code> <p>Number of blocks used in the trainable embedding. The data is uploaded n_layers+1 times.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>observable_type</code> <p>Defines the number of qubits used to evaluate the weighed cost fucntion, either 'single', 'half' or 'full'.  max_steps (int): Maximum number of training steps. A warning will be raised if training did not     converge.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'single'</code> </p> <code>learning_rate</code> <p>Initial learning rate for training.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.05</code> </p> <code>convergence_interval</code> <p>The number of loss values to consider to decide convergence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>batch_size</code> <p>Size of batches used for computing paraeeter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the keyword arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax-jit'}</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>construct_model</code> <p>Construct the quantum circuit used in the model.</p> <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for batch of data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def __init__(\n    self,\n    n_layers=4,\n    observable_type=\"single\",\n    convergence_interval=200,\n    max_steps=10000,\n    learning_rate=0.05,\n    batch_size=32,\n    max_vmap=None,\n    jit=True,\n    scaling=1.0,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax-jit\"},\n    random_state=42,\n):\n    r\"\"\"\n    Data reuploading classifier from https://arxiv.org/abs/1907.02085. Here we code the 'multi-qubit classifier'.\n    The model consists of layers of trainable data encoding unitaries. Each subsequent sequence of 3 elements of\n    an input vector x are encoded into a SU(2) rotation via pennylane.Rot. The model therefore uses ceil(d/3)\n    qubits.\n\n    The cost function is given by eqn (23) in the plots, and includes trainable parameters for scaling input data\n    (omegas), the class weights (alphas), and the circuit parameters (thetas). The cost\n    function is a weighted sum of the squared differences between the ideal fidelity to the corresponding class state\n    (here, either |0&gt; or |1&gt;) and the output state of the circuit, for a number of qubits specified by\n    observable_weight. More specifically\n\n    .. math::\n\n        \\ell(\\vec{\\theta},\\vec{\\omega},\\vec{\\alpha},\\vec{x}_i) =\n        \\sum_{j=1}^{n_{\\text{max}}}(\\alpha_j^0F^0_j-(1-y_i))^2 + (\\alpha_j^1F^1_j-y_i)^2\n\n    where :math:`n_max` is given by observable_weight and :math:`F^0_j` is the fidelity of the jth output\n    quibt to the state |0&gt;.\n\n    When observable_weight !=1 it is not clear from the plots how to predict labels. Here we look at the average\n    fidelities to |0&gt; and |1&gt; of the qubits up to index self.observable_weight and take the label with the largest\n    average fidelity. For self.observable_weight=1 this reverts to their strategy in the plots.\n\n    Where possible, we have followed the numerical examples found in the repo which accompanies the plots:\n\n    https://github.com/AdrianPerezSalinas/universal_qlassifier/tree/354d70f940ea737192de4063ff72f859c77e5760\n\n    Args:\n        n_layers (int): Number of blocks used in the trainable embedding. The data is uploaded n_layers+1 times.\n        observable_type (str): Defines the number of qubits used to evaluate the weighed cost fucntion,\n            either 'single', 'half' or 'full'.\n             max_steps (int): Maximum number of training steps. A warning will be raised if training did not\n                converge.\n        learning_rate (float): Initial learning rate for training.\n        convergence_interval (int): The number of loss values to consider to decide convergence.\n        batch_size (int): Size of batches used for computing paraeeter updates.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'.\n        qnode_kwargs (str): the keyword arguments passed to the circuit qnode.\n        scaling (float): Factor by which to scale the input data.\n        random_state (int): Seed used for pseudorandom number generation.\n    \"\"\"\n\n    # attributes that do not depend on data\n    self.n_layers = n_layers\n    self.observable_type = observable_type\n    self.convergence_interval = convergence_interval\n    self.max_steps = max_steps\n    self.learning_rate = learning_rate\n    self.batch_size = batch_size\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.jit = jit\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # dictionary containing the trainable parameters\n    self.n_qubits_ = None\n    self.scaler = None  # data preprocessing\n    self.circuit = None\n</code></pre> <code></code> construct_model <pre><code>construct_model()\n</code></pre> <p>Construct the quantum circuit used in the model.</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def construct_model(self):\n    \"\"\"Construct the quantum circuit used in the model.\"\"\"\n\n    dev = qml.device(self.dev_type, wires=self.n_qubits_)\n\n    @qml.qnode(dev, **self.qnode_kwargs)\n    def circuit(params, x):\n        \"\"\"A variational quantum circuit with data reuploading.\n\n        Args:\n            params (array[float]): dictionary of parameters\n            x (array[float]): input vector\n\n        Returns:\n            list: Expectation values of Pauli Z on each qubit\n        \"\"\"\n        for layer in range(self.n_layers):\n            x_idx = 0  # to keep track of the data index\n            for i in range(self.n_qubits_):\n                angles = x[x_idx : x_idx + 3] + params[\"thetas\"][i, layer, :]\n                qml.Rot(*angles, wires=i)\n                x_idx += 3\n            if layer % 2 == 0:\n                qml.broadcast(qml.CZ, range(self.n_qubits_), pattern=\"double\")\n            else:\n                qml.broadcast(qml.CZ, range(self.n_qubits_), pattern=\"double_odd\")\n\n        # final reupload without CZs\n        x_idx = 0\n        for i in range(self.n_qubits_):\n            angles = x[x_idx : x_idx + 3] + params[\"thetas\"][i, self.n_layers, :]\n            qml.Rot(*angles, wires=i)\n            x_idx += 3\n\n        return [qml.expval(qml.PauliZ(wires=[i])) for i in range(self.n_qubits_)]\n\n    self.circuit = circuit\n\n    def circuit_as_array(params, x):\n        # pennylane returns a list in the circuit above, so we need this\n        return jnp.array(circuit(params, x))\n\n    if self.jit:\n        circuit_as_array = jax.jit(circuit_as_array)\n    self.forward = jax.vmap(circuit_as_array, in_axes=(None, 0))\n    self.chunked_forward = chunk_vmapped_fn(self.forward, 1, self.max_vmap)\n\n    return self.forward\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n    self.initialize(X.shape[1], classes=np.unique(y))\n\n    self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    def loss_fn(params, x, y):\n        y_mat = jnp.vstack(\n            [y for __ in range(self.observable_weight)]\n        ).T  # repeated columns of labels\n        y_mat0 = (1 - y_mat) / 2\n        y_mat1 = (1 + y_mat) / 2\n        alpha_mat0 = jnp.vstack(\n            [params[\"alphas\"][0, :] for __ in range(len(x))]\n        )  # repeated rows of alpha parameters\n        alpha_mat1 = jnp.vstack([params[\"alphas\"][1, :] for __ in range(len(x))])\n\n        expvals = self.forward(params, x)\n        probs0 = (1 - expvals[:, : self.observable_weight]) / 2  # fidelity with |0&gt;\n        probs1 = (1 + expvals[:, : self.observable_weight]) / 2  # fidelity with |1&gt;\n        loss = (\n            1\n            / 2\n            * (\n                jnp.sum(\n                    (alpha_mat0 * probs0 - y_mat0) ** 2 + (alpha_mat1 * probs1 - y_mat1) ** 2\n                )\n            )\n        )  # eqn 23 in plots\n        return loss / len(y)\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    self.construct_model()\n    self.initialize_params()\n    optimizer = optax.adam\n\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        X,\n        y,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels. Args:     n_features (int): Number of features that the classifier expects     classes (array-like): class labels that the classifier expects</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_features = n_features\n    self.n_qubits_ = ceil(self.n_features / 3)\n\n    if self.observable_type == \"single\":\n        self.observable_weight = 1\n    elif self.observable_type == \"half\":\n        if self.n_qubits_ == 1:\n            self.observable_weight = 1\n        else:\n            self.observable_weight = self.n_qubits_ // 2\n    elif self.observable_type == \"full\":\n        self.observable_weight = self.n_qubits_\n\n    self.initialize_params()\n    self.construct_model()\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for batch of data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for batch of data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    check_is_fitted(self)\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    predictions = self.chunked_forward(self.params_, X)\n    predictions = jnp.mean(\n        predictions[:, : self.observable_weight], axis=1\n    )  # use mean of expvals over relevant qubits\n    predictions_2d = np.c_[(1 - predictions) / 2, (1 + predictions) / 2]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n            self.scaler.fit(X)\n\n        X = self.scaler.transform(X)\n\n    X = X * self.scaling\n    X = jnp.pad(X, ((0, 0), (0, (3 - X.shape[1]) % 3)), \"constant\")\n    return X\n</code></pre> <code></code> DataReuploadingClassifierNoTrainableEmbedding <pre><code>DataReuploadingClassifierNoTrainableEmbedding(n_layers=4, observable_type='single', convergence_interval=200, max_steps=10000, learning_rate=0.05, batch_size=32, max_vmap=None, jit=True, scaling=1.0, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax-jit'}, random_state=42)\n</code></pre> <p>               Bases: <code>DataReuploadingClassifier</code></p> <p>Data reuploading classifier from https://arxiv.org/abs/1907.02085. Here we code the 'multi-qubit classifier'. The model consists of layers of trainable data encoding unitaries. Each subsequent sequence of 3 elements of an input vector x are encoded into a SU(2) rotation via pennylane.Rot. The model therefore uses ceil(d/3) qubits.</p> <p>The cost function is given by eqn (23) in the plots, and includes trainable parameters for scaling input data (omegas), the class weights (alphas), and the circuit parameters (thetas). The cost function is a weighted sum of the squared differences between the ideal fidelity to the corresponding class state (here, either |0&gt; or |1&gt;) and the output state of the circuit, for a number of qubits specified by observable_weight. More specifically</p> <p>.. math::</p> <pre><code>\\ell(\\vec{\\theta},\\vec{\\omega},\\vec{\\alpha},\\vec{x}_i) =\n\\sum_{j=1}^{n_{\\text{max}}}(\\alpha_j^0F^0_j-(1-y_i))^2 + (\\alpha_j^1F^1_j-y_i)^2\n</code></pre> <p>where :math:<code>n_max</code> is given by observable_weight and :math:<code>F^0_j</code> is the fidelity of the jth output quibt to the state |0&gt;.</p> <p>When observable_weight !=1 it is not clear from the plots how to predict labels. Here we look at the average fidelities to |0&gt; and |1&gt; of the qubits up to index self.observable_weight and take the label with the largest average fidelity. For self.observable_weight=1 this reverts to their strategy in the plots.</p> <p>Where possible, we have followed the numerical examples found in the repo which accompanies the plots:</p> <p>https://github.com/AdrianPerezSalinas/universal_qlassifier/tree/354d70f940ea737192de4063ff72f859c77e5760</p> PARAMETER DESCRIPTION <code>n_layers</code> <p>Number of blocks used in the trainable embedding. The data is uploaded n_layers+1 times.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>observable_type</code> <p>Defines the number of qubits used to evaluate the weighed cost fucntion, either 'single', 'half' or 'full'.  max_steps (int): Maximum number of training steps. A warning will be raised if training did not     converge.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'single'</code> </p> <code>learning_rate</code> <p>Initial learning rate for training.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.05</code> </p> <code>convergence_interval</code> <p>The number of loss values to consider to decide convergence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>batch_size</code> <p>Size of batches used for computing paraeeter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the keyword arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax-jit'}</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>construct_model</code> <p>Construct the quantum circuit used in the model.</p> <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for batch of data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def __init__(\n    self,\n    n_layers=4,\n    observable_type=\"single\",\n    convergence_interval=200,\n    max_steps=10000,\n    learning_rate=0.05,\n    batch_size=32,\n    max_vmap=None,\n    jit=True,\n    scaling=1.0,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax-jit\"},\n    random_state=42,\n):\n    r\"\"\"\n    Data reuploading classifier from https://arxiv.org/abs/1907.02085. Here we code the 'multi-qubit classifier'.\n    The model consists of layers of trainable data encoding unitaries. Each subsequent sequence of 3 elements of\n    an input vector x are encoded into a SU(2) rotation via pennylane.Rot. The model therefore uses ceil(d/3)\n    qubits.\n\n    The cost function is given by eqn (23) in the plots, and includes trainable parameters for scaling input data\n    (omegas), the class weights (alphas), and the circuit parameters (thetas). The cost\n    function is a weighted sum of the squared differences between the ideal fidelity to the corresponding class state\n    (here, either |0&gt; or |1&gt;) and the output state of the circuit, for a number of qubits specified by\n    observable_weight. More specifically\n\n    .. math::\n\n        \\ell(\\vec{\\theta},\\vec{\\omega},\\vec{\\alpha},\\vec{x}_i) =\n        \\sum_{j=1}^{n_{\\text{max}}}(\\alpha_j^0F^0_j-(1-y_i))^2 + (\\alpha_j^1F^1_j-y_i)^2\n\n    where :math:`n_max` is given by observable_weight and :math:`F^0_j` is the fidelity of the jth output\n    quibt to the state |0&gt;.\n\n    When observable_weight !=1 it is not clear from the plots how to predict labels. Here we look at the average\n    fidelities to |0&gt; and |1&gt; of the qubits up to index self.observable_weight and take the label with the largest\n    average fidelity. For self.observable_weight=1 this reverts to their strategy in the plots.\n\n    Where possible, we have followed the numerical examples found in the repo which accompanies the plots:\n\n    https://github.com/AdrianPerezSalinas/universal_qlassifier/tree/354d70f940ea737192de4063ff72f859c77e5760\n\n    Args:\n        n_layers (int): Number of blocks used in the trainable embedding. The data is uploaded n_layers+1 times.\n        observable_type (str): Defines the number of qubits used to evaluate the weighed cost fucntion,\n            either 'single', 'half' or 'full'.\n             max_steps (int): Maximum number of training steps. A warning will be raised if training did not\n                converge.\n        learning_rate (float): Initial learning rate for training.\n        convergence_interval (int): The number of loss values to consider to decide convergence.\n        batch_size (int): Size of batches used for computing paraeeter updates.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'.\n        qnode_kwargs (str): the keyword arguments passed to the circuit qnode.\n        scaling (float): Factor by which to scale the input data.\n        random_state (int): Seed used for pseudorandom number generation.\n    \"\"\"\n\n    # attributes that do not depend on data\n    self.n_layers = n_layers\n    self.observable_type = observable_type\n    self.convergence_interval = convergence_interval\n    self.max_steps = max_steps\n    self.learning_rate = learning_rate\n    self.batch_size = batch_size\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.jit = jit\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # dictionary containing the trainable parameters\n    self.n_qubits_ = None\n    self.scaler = None  # data preprocessing\n    self.circuit = None\n</code></pre> <code></code> construct_model <pre><code>construct_model()\n</code></pre> <p>Construct the quantum circuit used in the model.</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def construct_model(self):\n    \"\"\"Construct the quantum circuit used in the model.\"\"\"\n\n    dev = qml.device(self.dev_type, wires=self.n_qubits_)\n\n    @qml.qnode(dev, **self.qnode_kwargs)\n    def circuit(params, x):\n        \"\"\"A variational quantum circuit with data reuploading.\n\n        Args:\n            params (array[float]): dictionary of parameters\n            x (array[float]): input vector\n\n        Returns:\n            list: Expectation values of Pauli Z on each qubit\n        \"\"\"\n        for layer in range(self.n_layers):\n            x_idx = 0  # to keep track of the data index\n            for i in range(self.n_qubits_):\n                angles = x[x_idx : x_idx + 3] * params[\"omegas\"][layer, x_idx : x_idx + 3]\n                qml.Rot(*angles, wires=i)\n                x_idx += 3\n\n        # final reupload without CZs\n        x_idx = 0\n        for i in range(self.n_qubits_):\n            angles = x[x_idx : x_idx + 3] * params[\"omegas\"][self.n_layers, x_idx : x_idx + 3]\n            qml.Rot(*angles, wires=i)\n            x_idx += 3\n\n        for layer in range(self.n_layers):\n            x_idx = 0  # to keep track of the data index\n            for i in range(self.n_qubits_):\n                angles = params[\"thetas\"][i, layer, :]\n                qml.Rot(*angles, wires=i)\n                x_idx += 3\n            if layer % 2 == 0:\n                qml.broadcast(qml.CZ, range(self.n_qubits_), pattern=\"double\")\n            else:\n                qml.broadcast(qml.CZ, range(self.n_qubits_), pattern=\"double_odd\")\n\n            # final reupload without CZs\n        x_idx = 0\n        for i in range(self.n_qubits_):\n            angles = params[\"thetas\"][i, self.n_layers, :]\n            qml.Rot(*angles, wires=i)\n            x_idx += 3\n\n        return [qml.expval(qml.PauliZ(wires=[i])) for i in range(self.n_qubits_)]\n\n    self.circuit = circuit\n\n    def circuit_as_array(params, x):\n        # pennylane returns a list in the circuit above, so we need this\n        return jnp.array(circuit(params, x))\n\n    if self.jit:\n        circuit_as_array = jax.jit(circuit_as_array)\n    self.forward = jax.vmap(circuit_as_array, in_axes=(None, 0))\n    self.chunked_forward = chunk_vmapped_fn(self.forward, 1, self.max_vmap)\n\n    return self.forward\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n    self.initialize(X.shape[1], classes=np.unique(y))\n\n    self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    def loss_fn(params, x, y):\n        y_mat = jnp.vstack(\n            [y for __ in range(self.observable_weight)]\n        ).T  # repeated columns of labels\n        y_mat0 = (1 - y_mat) / 2\n        y_mat1 = (1 + y_mat) / 2\n        alpha_mat0 = jnp.vstack(\n            [params[\"alphas\"][0, :] for __ in range(len(x))]\n        )  # repeated rows of alpha parameters\n        alpha_mat1 = jnp.vstack([params[\"alphas\"][1, :] for __ in range(len(x))])\n\n        expvals = self.forward(params, x)\n        probs0 = (1 - expvals[:, : self.observable_weight]) / 2  # fidelity with |0&gt;\n        probs1 = (1 + expvals[:, : self.observable_weight]) / 2  # fidelity with |1&gt;\n        loss = (\n            1\n            / 2\n            * (\n                jnp.sum(\n                    (alpha_mat0 * probs0 - y_mat0) ** 2 + (alpha_mat1 * probs1 - y_mat1) ** 2\n                )\n            )\n        )  # eqn 23 in plots\n        return loss / len(y)\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    self.construct_model()\n    self.initialize_params()\n    optimizer = optax.adam\n\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        X,\n        y,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels. Args:     n_features (int): Number of features that the classifier expects     classes (array-like): class labels that the classifier expects</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_features = n_features\n    self.n_qubits_ = ceil(self.n_features / 3)\n\n    if self.observable_type == \"single\":\n        self.observable_weight = 1\n    elif self.observable_type == \"half\":\n        if self.n_qubits_ == 1:\n            self.observable_weight = 1\n        else:\n            self.observable_weight = self.n_qubits_ // 2\n    elif self.observable_type == \"full\":\n        self.observable_weight = self.n_qubits_\n\n    self.initialize_params()\n    self.construct_model()\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for batch of data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for batch of data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    check_is_fitted(self)\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    predictions = self.chunked_forward(self.params_, X)\n    predictions = jnp.mean(\n        predictions[:, : self.observable_weight], axis=1\n    )  # use mean of expvals over relevant qubits\n    predictions_2d = np.c_[(1 - predictions) / 2, (1 + predictions) / 2]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n            self.scaler.fit(X)\n\n        X = self.scaler.transform(X)\n\n    X = X * self.scaling\n    X = jnp.pad(X, ((0, 0), (0, (3 - X.shape[1]) % 3)), \"constant\")\n    return X\n</code></pre> <code></code> DataReuploadingClassifierSeparable <pre><code>DataReuploadingClassifierSeparable(n_layers=4, observable_type='single', convergence_interval=200, max_steps=10000, learning_rate=0.05, batch_size=32, max_vmap=None, jit=True, scaling=1.0, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax-jit'}, random_state=42)\n</code></pre> <p>               Bases: <code>DataReuploadingClassifier</code></p> <p>Data reuploading classifier from https://arxiv.org/abs/1907.02085. Here we code the 'multi-qubit classifier'. The model consists of layers of trainable data encoding unitaries. Each subsequent sequence of 3 elements of an input vector x are encoded into a SU(2) rotation via pennylane.Rot. The model therefore uses ceil(d/3) qubits.</p> <p>The cost function is given by eqn (23) in the plots, and includes trainable parameters for scaling input data (omegas), the class weights (alphas), and the circuit parameters (thetas). The cost function is a weighted sum of the squared differences between the ideal fidelity to the corresponding class state (here, either |0&gt; or |1&gt;) and the output state of the circuit, for a number of qubits specified by observable_weight. More specifically</p> <p>.. math::</p> <pre><code>\\ell(\\vec{\\theta},\\vec{\\omega},\\vec{\\alpha},\\vec{x}_i) =\n\\sum_{j=1}^{n_{\\text{max}}}(\\alpha_j^0F^0_j-(1-y_i))^2 + (\\alpha_j^1F^1_j-y_i)^2\n</code></pre> <p>where :math:<code>n_max</code> is given by observable_weight and :math:<code>F^0_j</code> is the fidelity of the jth output quibt to the state |0&gt;.</p> <p>When observable_weight !=1 it is not clear from the plots how to predict labels. Here we look at the average fidelities to |0&gt; and |1&gt; of the qubits up to index self.observable_weight and take the label with the largest average fidelity. For self.observable_weight=1 this reverts to their strategy in the plots.</p> <p>Where possible, we have followed the numerical examples found in the repo which accompanies the plots:</p> <p>https://github.com/AdrianPerezSalinas/universal_qlassifier/tree/354d70f940ea737192de4063ff72f859c77e5760</p> PARAMETER DESCRIPTION <code>n_layers</code> <p>Number of blocks used in the trainable embedding. The data is uploaded n_layers+1 times.</p> <p> TYPE: <code>int</code> DEFAULT: <code>4</code> </p> <code>observable_type</code> <p>Defines the number of qubits used to evaluate the weighed cost fucntion, either 'single', 'half' or 'full'.  max_steps (int): Maximum number of training steps. A warning will be raised if training did not     converge.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'single'</code> </p> <code>learning_rate</code> <p>Initial learning rate for training.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.05</code> </p> <code>convergence_interval</code> <p>The number of loss values to consider to decide convergence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>batch_size</code> <p>Size of batches used for computing paraeeter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the keyword arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax-jit'}</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>construct_model</code> <p>Construct the quantum circuit used in the model.</p> <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for batch of data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def __init__(\n    self,\n    n_layers=4,\n    observable_type=\"single\",\n    convergence_interval=200,\n    max_steps=10000,\n    learning_rate=0.05,\n    batch_size=32,\n    max_vmap=None,\n    jit=True,\n    scaling=1.0,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax-jit\"},\n    random_state=42,\n):\n    r\"\"\"\n    Data reuploading classifier from https://arxiv.org/abs/1907.02085. Here we code the 'multi-qubit classifier'.\n    The model consists of layers of trainable data encoding unitaries. Each subsequent sequence of 3 elements of\n    an input vector x are encoded into a SU(2) rotation via pennylane.Rot. The model therefore uses ceil(d/3)\n    qubits.\n\n    The cost function is given by eqn (23) in the plots, and includes trainable parameters for scaling input data\n    (omegas), the class weights (alphas), and the circuit parameters (thetas). The cost\n    function is a weighted sum of the squared differences between the ideal fidelity to the corresponding class state\n    (here, either |0&gt; or |1&gt;) and the output state of the circuit, for a number of qubits specified by\n    observable_weight. More specifically\n\n    .. math::\n\n        \\ell(\\vec{\\theta},\\vec{\\omega},\\vec{\\alpha},\\vec{x}_i) =\n        \\sum_{j=1}^{n_{\\text{max}}}(\\alpha_j^0F^0_j-(1-y_i))^2 + (\\alpha_j^1F^1_j-y_i)^2\n\n    where :math:`n_max` is given by observable_weight and :math:`F^0_j` is the fidelity of the jth output\n    quibt to the state |0&gt;.\n\n    When observable_weight !=1 it is not clear from the plots how to predict labels. Here we look at the average\n    fidelities to |0&gt; and |1&gt; of the qubits up to index self.observable_weight and take the label with the largest\n    average fidelity. For self.observable_weight=1 this reverts to their strategy in the plots.\n\n    Where possible, we have followed the numerical examples found in the repo which accompanies the plots:\n\n    https://github.com/AdrianPerezSalinas/universal_qlassifier/tree/354d70f940ea737192de4063ff72f859c77e5760\n\n    Args:\n        n_layers (int): Number of blocks used in the trainable embedding. The data is uploaded n_layers+1 times.\n        observable_type (str): Defines the number of qubits used to evaluate the weighed cost fucntion,\n            either 'single', 'half' or 'full'.\n             max_steps (int): Maximum number of training steps. A warning will be raised if training did not\n                converge.\n        learning_rate (float): Initial learning rate for training.\n        convergence_interval (int): The number of loss values to consider to decide convergence.\n        batch_size (int): Size of batches used for computing paraeeter updates.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'.\n        qnode_kwargs (str): the keyword arguments passed to the circuit qnode.\n        scaling (float): Factor by which to scale the input data.\n        random_state (int): Seed used for pseudorandom number generation.\n    \"\"\"\n\n    # attributes that do not depend on data\n    self.n_layers = n_layers\n    self.observable_type = observable_type\n    self.convergence_interval = convergence_interval\n    self.max_steps = max_steps\n    self.learning_rate = learning_rate\n    self.batch_size = batch_size\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.jit = jit\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # dictionary containing the trainable parameters\n    self.n_qubits_ = None\n    self.scaler = None  # data preprocessing\n    self.circuit = None\n</code></pre> <code></code> construct_model <pre><code>construct_model()\n</code></pre> <p>Construct the quantum circuit used in the model.</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def construct_model(self):\n    \"\"\"Construct the quantum circuit used in the model.\"\"\"\n\n    dev = qml.device(self.dev_type, wires=self.n_qubits_)\n\n    @qml.qnode(dev, **self.qnode_kwargs)\n    def circuit(params, x):\n        \"\"\"A variational quantum circuit with data reuploading.\n\n        Args:\n            params (array[float]): dictionary of parameters\n            x (array[float]): input vector\n\n        Returns:\n            list: Expectation values of Pauli Z on each qubit\n        \"\"\"\n        for layer in range(self.n_layers):\n            x_idx = 0  # to keep track of the data index\n            for i in range(self.n_qubits_):\n                angles = (\n                    x[x_idx : x_idx + 3] * params[\"omegas\"][layer, x_idx : x_idx + 3]\n                    + params[\"thetas\"][i, layer, :]\n                )\n                qml.Rot(*angles, wires=i)\n                x_idx += 3\n\n        # final reupload without CZs\n        x_idx = 0\n        for i in range(self.n_qubits_):\n            angles = (\n                x[x_idx : x_idx + 3] * params[\"omegas\"][self.n_layers, x_idx : x_idx + 3]\n                + params[\"thetas\"][i, self.n_layers, :]\n            )\n            qml.Rot(*angles, wires=i)\n            x_idx += 3\n\n        return [qml.expval(qml.PauliZ(wires=[i])) for i in range(self.n_qubits_)]\n\n    self.circuit = circuit\n\n    def circuit_as_array(params, x):\n        # pennylane returns a list in the circuit above, so we need this\n        return jnp.array(circuit(params, x))\n\n    if self.jit:\n        circuit_as_array = jax.jit(circuit_as_array)\n    self.forward = jax.vmap(circuit_as_array, in_axes=(None, 0))\n    self.chunked_forward = chunk_vmapped_fn(self.forward, 1, self.max_vmap)\n\n    return self.forward\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n    self.initialize(X.shape[1], classes=np.unique(y))\n\n    self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    def loss_fn(params, x, y):\n        y_mat = jnp.vstack(\n            [y for __ in range(self.observable_weight)]\n        ).T  # repeated columns of labels\n        y_mat0 = (1 - y_mat) / 2\n        y_mat1 = (1 + y_mat) / 2\n        alpha_mat0 = jnp.vstack(\n            [params[\"alphas\"][0, :] for __ in range(len(x))]\n        )  # repeated rows of alpha parameters\n        alpha_mat1 = jnp.vstack([params[\"alphas\"][1, :] for __ in range(len(x))])\n\n        expvals = self.forward(params, x)\n        probs0 = (1 - expvals[:, : self.observable_weight]) / 2  # fidelity with |0&gt;\n        probs1 = (1 + expvals[:, : self.observable_weight]) / 2  # fidelity with |1&gt;\n        loss = (\n            1\n            / 2\n            * (\n                jnp.sum(\n                    (alpha_mat0 * probs0 - y_mat0) ** 2 + (alpha_mat1 * probs1 - y_mat1) ** 2\n                )\n            )\n        )  # eqn 23 in plots\n        return loss / len(y)\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    self.construct_model()\n    self.initialize_params()\n    optimizer = optax.adam\n\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        X,\n        y,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels. Args:     n_features (int): Number of features that the classifier expects     classes (array-like): class labels that the classifier expects</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_features = n_features\n    self.n_qubits_ = ceil(self.n_features / 3)\n\n    if self.observable_type == \"single\":\n        self.observable_weight = 1\n    elif self.observable_type == \"half\":\n        if self.n_qubits_ == 1:\n            self.observable_weight = 1\n        else:\n            self.observable_weight = self.n_qubits_ // 2\n    elif self.observable_type == \"full\":\n        self.observable_weight = self.n_qubits_\n\n    self.initialize_params()\n    self.construct_model()\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for batch of data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for batch of data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    check_is_fitted(self)\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    predictions = self.chunked_forward(self.params_, X)\n    predictions = jnp.mean(\n        predictions[:, : self.observable_weight], axis=1\n    )  # use mean of expvals over relevant qubits\n    predictions_2d = np.c_[(1 - predictions) / 2, (1 + predictions) / 2]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/data_reuploading.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n            self.scaler.fit(X)\n\n        X = self.scaler.transform(X)\n\n    X = X * self.scaling\n    X = jnp.pad(X, ((0, 0), (0, (3 - X.shape[1]) % 3)), \"constant\")\n    return X\n</code></pre> <code></code> dressed_quantum_circuit CLASS DESCRIPTION <code>DressedQuantumCircuitClassifier</code> FUNCTION DESCRIPTION <code>chunk_grad</code> <p>Convert a <code>jax.grad</code> function to an equivalent version that evaluated in chunks of size max_vmap.</p> <code>chunk_loss</code> <p>Converts a loss function of the form <code>loss_fn(params, array1, array2)</code> to an equivalent version that</p> <code>chunk_vmapped_fn</code> <p>Convert a vmapped function to an equivalent function that evaluates in chunks of size</p> <code>get_batch</code> <p>A generator to get random batches of the data (X, y)</p> <code>get_from_dict</code> <p>Access a value from a nested dictionary.</p> <code>get_nested_keys</code> <p>Returns the nested keys of a nested dictionary.</p> <code>set_in_dict</code> <p>Set a value in a nested dictionary.</p> <code>train</code> <p>Trains a model using an optimizer and a loss function via gradient descent. We assume that the loss function</p> <code></code> DressedQuantumCircuitClassifier <pre><code>DressedQuantumCircuitClassifier(n_layers=3, learning_rate=0.001, batch_size=32, max_vmap=None, jit=True, max_steps=100000, convergence_interval=200, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax-jit'}, scaling=1.0, random_state=42)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Dressed quantum circuit from https://arxiv.org/abs/1912.08278. The model consists of the following sequence     * a single layer fully connected trainable neural network with tanh activation function     * a parameterised quantum circuit taking the above outputs as input     * a single layer fully connected trainable neural network taking local expectation values of the above       circuit as input</p> <p>The last neural network maps to two neurons that we take the softmax of to get class probabilities. The model is trained via binary cross entropy loss.</p> PARAMETER DESCRIPTION <code>n_layers</code> <p>number of layers in the variational part of the circuit.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>learning_rate</code> <p>initial learning rate for gradient descent.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> <code>max_steps</code> <p>Maximum number of training steps. A warning will be raised if training did not converge.</p> <p> TYPE: <code>int</code> DEFAULT: <code>100000</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>batch_size</code> <p>Size of batches used for computing parameter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>convergence_interval</code> <p>The number of loss values to consider to decide convergence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the keyword arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax-jit'}</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>input_transform</code> <p>The first neural network that we implment as matrix multiplication.</p> <code>output_transform</code> <p>The final neural network</p> <code>predict</code> <p>Predict labels for data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/dressed_quantum_circuit.py</code> <pre><code>def __init__(\n    self,\n    n_layers=3,\n    learning_rate=0.001,\n    batch_size=32,\n    max_vmap=None,\n    jit=True,\n    max_steps=100000,\n    convergence_interval=200,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax-jit\"},\n    scaling=1.0,\n    random_state=42,\n):\n    r\"\"\"\n    Dressed quantum circuit from https://arxiv.org/abs/1912.08278. The model consists of the following sequence\n        * a single layer fully connected trainable neural network with tanh activation function\n        * a parameterised quantum circuit taking the above outputs as input\n        * a single layer fully connected trainable neural network taking local expectation values of the above\n          circuit as input\n\n    The last neural network maps to two neurons that we take the softmax of to get class probabilities.\n    The model is trained via binary cross entropy loss.\n\n    Args:\n        n_layers (int): number of layers in the variational part of the circuit.\n        learning_rate (float): initial learning rate for gradient descent.\n        max_steps (int): Maximum number of training steps. A warning will be raised if training did not converge.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        batch_size (int): Size of batches used for computing parameter updates.\n        convergence_interval (int): The number of loss values to consider to decide convergence.\n        jit (bool): Whether to use just in time compilation.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'.\n        qnode_kwargs (str): the keyword arguments passed to the circuit qnode.\n        scaling (float): Factor by which to scale the input data.\n        random_state (int): Seed used for pseudorandom number generation.\n    \"\"\"\n    # attributes that do not depend on data\n    self.n_layers = n_layers\n    self.learning_rate = learning_rate\n    self.batch_size = batch_size\n    self.max_steps = max_steps\n    self.convergence_interval = convergence_interval\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.jit = jit\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # Dictionary containing the trainable parameters\n    self.n_qubits_ = None\n    self.n_features_ = None\n    self.scaler = None  # data scaler will be fitted on training data\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/dressed_quantum_circuit.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n\n    self.scaler = StandardScaler()\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    self.initialize(X.shape[1], classes=np.unique(y))\n\n    optimizer = optax.adam\n\n    def loss_fn(params, X, y):\n        vals = self.forward(params, X)\n        # convert to 0,1 one hot encoded labels\n        labels = jax.nn.one_hot(jax.nn.relu(y), 2)\n        return jnp.mean(optax.softmax_cross_entropy(vals, labels))\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        X,\n        y,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels. Args:     n_features (int): Number of features that the classifier expects     classes (array-like): class labels that the classifier expects</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/dressed_quantum_circuit.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_features_ = n_features\n    self.n_qubits_ = self.n_features_\n\n    self.initialize_params()\n    self.construct_model()\n</code></pre> <code></code> input_transform <pre><code>input_transform(params, x)\n</code></pre> <p>The first neural network that we implment as matrix multiplication.</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/dressed_quantum_circuit.py</code> <pre><code>def input_transform(self, params, x):\n    \"\"\"\n    The first neural network that we implment as matrix multiplication.\n    \"\"\"\n    x = jnp.matmul(params[\"input_weights\"], x)\n    x = jnp.tanh(x) * jnp.pi / 2\n    return x\n</code></pre> <code></code> output_transform <pre><code>output_transform(params, x)\n</code></pre> <p>The final neural network</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/dressed_quantum_circuit.py</code> <pre><code>def output_transform(self, params, x):\n    \"\"\"\n    The final neural network\n    \"\"\"\n    x = jnp.matmul(params[\"output_weights\"], x)\n    return x\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred</code> <p>Predicted labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/dressed_quantum_circuit.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred (np.ndarray): Predicted labels of shape (n_samples,)\n    \"\"\"\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/dressed_quantum_circuit.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    return jax.nn.softmax(self.chunked_forward(self.params_, X))\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/dressed_quantum_circuit.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = StandardScaler()\n            self.scaler.fit(X)\n\n        X = self.scaler.transform(X)\n    return X * self.scaling\n</code></pre> <code></code> chunk_grad <pre><code>chunk_grad(grad_fn, max_vmap)\n</code></pre> <p>Convert a <code>jax.grad</code> function to an equivalent version that evaluated in chunks of size max_vmap.</p> <p><code>grad_fn</code> should be of the form <code>jax.grad(fn(params, X, y), argnums=0)</code>, where <code>params</code> is a dictionary of <code>jnp.arrays</code>, <code>X, y</code> are <code>jnp.arrays</code> with the same-size leading axis, and <code>grad_fn</code> is a function that is vectorised along these axes (i.e. <code>in_axes = (None,0,0)</code>).</p> <p>The returned function evaluates the original function by splitting the batch evaluation into smaller chunks of size <code>max_vmap</code>, and has a lower memory footprint.</p> PARAMETER DESCRIPTION <code>model</code> <p>gradient function with the functional form jax.grad(loss(params, X,y), argnums=0)</p> <p> TYPE: <code>func</code> </p> <code>max_vmap</code> <p>the size of the chunks</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <p>chunked version of the function</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def chunk_grad(grad_fn, max_vmap):\n    \"\"\"\n    Convert a `jax.grad` function to an equivalent version that evaluated in chunks of size max_vmap.\n\n    `grad_fn` should be of the form `jax.grad(fn(params, X, y), argnums=0)`, where `params` is a\n    dictionary of `jnp.arrays`, `X, y` are `jnp.arrays` with the same-size leading axis, and `grad_fn`\n    is a function that is vectorised along these axes (i.e. `in_axes = (None,0,0)`).\n\n    The returned function evaluates the original function by splitting the batch evaluation into smaller chunks\n    of size `max_vmap`, and has a lower memory footprint.\n\n    Args:\n        model (func): gradient function with the functional form jax.grad(loss(params, X,y), argnums=0)\n        max_vmap (int): the size of the chunks\n\n    Returns:\n        chunked version of the function\n    \"\"\"\n\n    def chunked_grad(params, X, y):\n        batch_slices = list(gen_batches(len(X), max_vmap))\n        grads = [grad_fn(params, X[slice], y[slice]) for slice in batch_slices]\n        grad_dict = {}\n        for key_list in get_nested_keys(params):\n            set_in_dict(\n                grad_dict,\n                key_list,\n                jnp.mean(jnp.array([get_from_dict(grad, key_list) for grad in grads]), axis=0),\n            )\n        return grad_dict\n\n    return chunked_grad\n</code></pre> <code></code> chunk_loss <pre><code>chunk_loss(loss_fn, max_vmap)\n</code></pre> <p>Converts a loss function of the form <code>loss_fn(params, array1, array2)</code> to an equivalent version that evaluates <code>loss_fn</code> in chunks of size max_vmap. <code>loss_fn</code> should batch evaluate along the leading axis of <code>array1, array2</code> (i.e. <code>in_axes = (None,0,0)</code>).</p> PARAMETER DESCRIPTION <code>loss_fn</code> <p>function of form loss_fn(params, array1, array2)</p> <p> TYPE: <code>func</code> </p> <code>max_vmap</code> <p>maximum chunk size</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <p>chunked version of the function</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def chunk_loss(loss_fn, max_vmap):\n    \"\"\"\n    Converts a loss function of the form `loss_fn(params, array1, array2)` to an equivalent version that\n    evaluates `loss_fn` in chunks of size max_vmap. `loss_fn` should batch evaluate along the leading\n    axis of `array1, array2` (i.e. `in_axes = (None,0,0)`).\n\n    Args:\n        loss_fn (func): function of form loss_fn(params, array1, array2)\n        max_vmap (int): maximum chunk size\n\n    Returns:\n        chunked version of the function\n    \"\"\"\n\n    def chunked_loss(params, X, y):\n        batch_slices = list(gen_batches(len(X), max_vmap))\n        res = jnp.array([loss_fn(params, *[X[slice], y[slice]]) for slice in batch_slices])\n        return jnp.mean(res)\n\n    return chunked_loss\n</code></pre> <code></code> chunk_vmapped_fn <pre><code>chunk_vmapped_fn(vmapped_fn, start, max_vmap)\n</code></pre> <p>Convert a vmapped function to an equivalent function that evaluates in chunks of size max_vmap. The behaviour of chunked_fn should be the same as vmapped_fn, but with a lower memory cost.</p> <p>The input vmapped_fn should have in_axes = (None, None, ..., 0,0,...,0)</p> PARAMETER DESCRIPTION <code>vmapped</code> <p>vmapped function with in_axes = (None, None, ..., 0,0,...,0)</p> <p> TYPE: <code>func</code> </p> <code>start</code> <p>The index where the first 0 appears in in_axes</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <p>chunked version of the function</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def chunk_vmapped_fn(vmapped_fn, start, max_vmap):\n    \"\"\"\n    Convert a vmapped function to an equivalent function that evaluates in chunks of size\n    max_vmap. The behaviour of chunked_fn should be the same as vmapped_fn, but with a\n    lower memory cost.\n\n    The input vmapped_fn should have in_axes = (None, None, ..., 0,0,...,0)\n\n    Args:\n        vmapped (func): vmapped function with in_axes = (None, None, ..., 0,0,...,0)\n        start (int): The index where the first 0 appears in in_axes\n        max_vmap (int) The max chunk size with which to evaluate the function\n\n    Returns:\n        chunked version of the function\n    \"\"\"\n\n    def chunked_fn(*args):\n        batch_len = len(args[start])\n        batch_slices = list(gen_batches(batch_len, max_vmap))\n        res = [\n            vmapped_fn(*args[:start], *[arg[slice] for arg in args[start:]])\n            for slice in batch_slices\n        ]\n        # jnp.concatenate needs to act on arrays with the same shape, so pad the last array if necessary\n        if batch_len / max_vmap % 1 != 0.0:\n            diff = max_vmap - len(res[-1])\n            res[-1] = jnp.pad(res[-1], [(0, diff), *[(0, 0)] * (len(res[-1].shape) - 1)])\n            return jnp.concatenate(res)[:-diff]\n        return jnp.concatenate(res)\n\n    return chunked_fn\n</code></pre> <code></code> get_batch <pre><code>get_batch(X, y, rnd_key, batch_size=32)\n</code></pre> <p>A generator to get random batches of the data (X, y)</p> PARAMETER DESCRIPTION <code>X</code> <p>Input data with shape (n_samples, n_features).</p> <p> TYPE: <code>array[float]</code> </p> <code>y</code> <p>Target labels with shape (n_samples,)</p> <p> TYPE: <code>array[float]</code> </p> <code>rnd_key</code> <p>A jax random key object</p> <p> </p> <code>batch_size</code> <p>Number of elements in batch</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> RETURNS DESCRIPTION <p>array[float]: A batch of input data shape (batch_size, n_features)</p> <p>array[float]: A batch of target labels shaped (batch_size,)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def get_batch(X, y, rnd_key, batch_size=32):\n    \"\"\"\n    A generator to get random batches of the data (X, y)\n\n    Args:\n        X (array[float]): Input data with shape (n_samples, n_features).\n        y (array[float]): Target labels with shape (n_samples,)\n        rnd_key: A jax random key object\n        batch_size (int): Number of elements in batch\n\n    Returns:\n        array[float]: A batch of input data shape (batch_size, n_features)\n        array[float]: A batch of target labels shaped (batch_size,)\n    \"\"\"\n    all_indices = jnp.array(range(len(X)))\n    rnd_indices = jax.random.choice(key=rnd_key, a=all_indices, shape=(batch_size,), replace=True)\n    return X[rnd_indices], y[rnd_indices]\n</code></pre> <code></code> get_from_dict <pre><code>get_from_dict(dict, key_list)\n</code></pre> <p>Access a value from a nested dictionary. Inspired by https://stackoverflow.com/questions/14692690/access-nested-dictionary-items-via-a-list-of-keys</p> PARAMETER DESCRIPTION <code>dict</code> <p>nested dictionary</p> <p> TYPE: <code>dict</code> </p> <code>key_list</code> <p>list of keys to be accessed</p> <p> TYPE: <code>list</code> </p> RETURNS DESCRIPTION <p>the requested value</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def get_from_dict(dict, key_list):\n    \"\"\"\n    Access a value from a nested dictionary.\n    Inspired by https://stackoverflow.com/questions/14692690/access-nested-dictionary-items-via-a-list-of-keys\n\n    Args:\n        dict (dict): nested dictionary\n        key_list (list): list of keys to be accessed\n\n    Returns:\n         the requested value\n    \"\"\"\n    return reduce(operator.getitem, key_list, dict)\n</code></pre> <code></code> get_nested_keys <pre><code>get_nested_keys(d, parent_keys=[])\n</code></pre> <p>Returns the nested keys of a nested dictionary.</p> PARAMETER DESCRIPTION <code>d</code> <p>nested dictionary</p> <p> TYPE: <code>dict</code> </p> RETURNS DESCRIPTION <p>list where each element is a list of nested keys</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def get_nested_keys(d, parent_keys=[]):\n    \"\"\"\n    Returns the nested keys of a nested dictionary.\n\n    Args:\n        d (dict): nested dictionary\n\n    Returns:\n        list where each element is a list of nested keys\n    \"\"\"\n    keys_list = []\n    for key, value in d.items():\n        current_keys = parent_keys + [key]\n        if isinstance(value, dict):\n            keys_list.extend(get_nested_keys(value, current_keys))\n        else:\n            keys_list.append(current_keys)\n    return keys_list\n</code></pre> <code></code> set_in_dict <pre><code>set_in_dict(dict, keys, value)\n</code></pre> <p>Set a value in a nested dictionary.</p> PARAMETER DESCRIPTION <code>dict</code> <p>nested dictionary</p> <p> TYPE: <code>dict</code> </p> <code>keys</code> <p>list of keys in nested dictionary</p> <p> TYPE: <code>list</code> </p> <code>value</code> <p>value to be set</p> <p> TYPE: <code>Any</code> </p> RETURNS DESCRIPTION <p>nested dictionary with new value</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def set_in_dict(dict, keys, value):\n    \"\"\"\n    Set a value in a nested dictionary.\n\n    Args:\n        dict (dict): nested dictionary\n        keys (list): list of keys in nested dictionary\n        value (Any): value to be set\n\n    Returns:\n        nested dictionary with new value\n    \"\"\"\n    for key in keys[:-1]:\n        dict = dict.setdefault(key, {})\n    dict[keys[-1]] = value\n</code></pre> <code></code> train <pre><code>train(model, loss_fn, optimizer, X, y, random_key_generator, convergence_interval=200)\n</code></pre> <p>Trains a model using an optimizer and a loss function via gradient descent. We assume that the loss function is of the form <code>loss(params, X, y)</code> and that the trainable parameters are stored in model.params_ as a dictionary of jnp.arrays. The optimizer should be an Optax optimizer (e.g. optax.adam). <code>model</code> must have an attribute <code>learning_rate</code> to set the initial learning rate for the gradient descent.</p> <p>The model is trained until a convergence criterion is met that corresponds to the loss curve being flat over a number of optimization steps given by <code>convergence_inteval</code> (see plots for details).</p> <p>To reduce precompilation time and memory cost, the loss function and gradient functions are evaluated in chunks of size model.max_vmap.</p> PARAMETER DESCRIPTION <code>model</code> <p>Classifier class object to train. Trainable parameters must be stored in model.params_.</p> <p> TYPE: <code>class</code> </p> <code>loss_fn</code> <p>Loss function to be minimised. Must be of the form loss_fn(params, X, y).</p> <p> TYPE: <code>Callable</code> </p> <code>optimizer</code> <p>Optax optimizer (e.g. optax.adam).</p> <p> TYPE: <code>optax optimizer</code> </p> <code>X</code> <p>Input data array of shape (n_samples, n_features)</p> <p> TYPE: <code>array</code> </p> <code>y</code> <p>Array of shape (n_samples) containing the labels.</p> <p> TYPE: <code>array</code> </p> <code>random_key_generator</code> <p>JAX key generator object for pseudo-randomness generation.</p> <p> TYPE: <code>PRNGKey</code> </p> <code>convergence_interval</code> <p>Number of optimization steps over which to decide convergence. Larger values give a higher confidence that the model has converged but may increase training time.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> RETURNS DESCRIPTION <code>params</code> <p>The new parameters after training has completed.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def train(model, loss_fn, optimizer, X, y, random_key_generator, convergence_interval=200):\n    \"\"\"\n    Trains a model using an optimizer and a loss function via gradient descent. We assume that the loss function\n    is of the form `loss(params, X, y)` and that the trainable parameters are stored in model.params_ as a dictionary\n    of jnp.arrays. The optimizer should be an Optax optimizer (e.g. optax.adam). `model` must have an attribute\n    `learning_rate` to set the initial learning rate for the gradient descent.\n\n    The model is trained until a convergence criterion is met that corresponds to the loss curve being flat over\n    a number of optimization steps given by `convergence_inteval` (see plots for details).\n\n    To reduce precompilation time and memory cost, the loss function and gradient functions are evaluated in\n    chunks of size model.max_vmap.\n\n    Args:\n        model (class): Classifier class object to train. Trainable parameters must be stored in model.params_.\n        loss_fn (Callable): Loss function to be minimised. Must be of the form loss_fn(params, X, y).\n        optimizer (optax optimizer): Optax optimizer (e.g. optax.adam).\n        X (array): Input data array of shape (n_samples, n_features)\n        y (array): Array of shape (n_samples) containing the labels.\n        random_key_generator (jax.random.PRNGKey): JAX key generator object for pseudo-randomness generation.\n        convergence_interval (int, optional): Number of optimization steps over which to decide convergence. Larger\n            values give a higher confidence that the model has converged but may increase training time.\n\n    Returns:\n        params (dict): The new parameters after training has completed.\n    \"\"\"\n\n    if not model.batch_size / model.max_vmap % 1 == 0:\n        raise Exception(\"Batch size must be multiple of max_vmap.\")\n\n    params = model.params_\n    opt = optimizer(learning_rate=model.learning_rate)\n    opt_state = opt.init(params)\n    grad_fn = jax.grad(loss_fn)\n\n    # jitting through the chunked_grad function can take a long time,\n    # so we jit here and chunk after\n    if model.jit:\n        grad_fn = jax.jit(grad_fn)\n\n    # note: assumes that the loss function is a sample mean of\n    # some function over the input data set\n    chunked_grad_fn = chunk_grad(grad_fn, model.max_vmap)\n    chunked_loss_fn = chunk_loss(loss_fn, model.max_vmap)\n\n    def update(params, opt_state, x, y):\n        grads = chunked_grad_fn(params, x, y)\n        loss_val = chunked_loss_fn(params, x, y)\n        updates, opt_state = opt.update(grads, opt_state)\n        params = optax.apply_updates(params, updates)\n        return params, opt_state, loss_val\n\n    loss_history = []\n    converged = False\n    start = time.time()\n    for step in range(model.max_steps):\n        key = random_key_generator()\n        X_batch, y_batch = get_batch(X, y, key, batch_size=model.batch_size)\n        params, opt_state, loss_val = update(params, opt_state, X_batch, y_batch)\n        loss_history.append(loss_val)\n        logging.debug(f\"{step} - loss: {loss_val}\")\n\n        if np.isnan(loss_val):\n            logging.info(\"nan encountered. Training aborted.\")\n            break\n\n        # decide convergence\n        if step &gt; 2 * convergence_interval:\n            # get means of last two intervals and standard deviation of last interval\n            average1 = np.mean(loss_history[-convergence_interval:])\n            average2 = np.mean(loss_history[-2 * convergence_interval : -convergence_interval])\n            std1 = np.std(loss_history[-convergence_interval:])\n            # if the difference in averages is small compared to the statistical fluctuations, stop training.\n            if np.abs(average2 - average1) &lt;= std1 / np.sqrt(convergence_interval) / 2:\n                logging.info(f\"Model {model.__class__.__name__} converged after {step} steps.\")\n                converged = True\n                break\n\n    end = time.time()\n    loss_history = np.array(loss_history)\n    model.loss_history_ = loss_history / np.max(np.abs(loss_history))\n    model.training_time_ = end - start\n\n    if not converged:\n        print(\"Loss did not converge:\", loss_history)\n        raise ConvergenceWarning(\n            f\"Model {model.__class__.__name__} has not converged after the maximum number of {model.max_steps} steps.\"\n        )\n\n    return params\n</code></pre> <code></code> iqp_kernel CLASS DESCRIPTION <code>IQPKernelClassifier</code> <code></code> IQPKernelClassifier <pre><code>IQPKernelClassifier(svm=SVC(kernel='precomputed', probability=True), repeats=2, C=1.0, jit=False, random_state=42, scaling=1.0, max_vmap=250, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax-jit', 'diff_method': None})\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Kernel version of the classifier from https://arxiv.org/pdf/1804.11326v2.pdf. The kernel function is given by</p> <p>.. math::     k(x,x')=\\vert\\langle 0 \\vert U^\\dagger(x')U(x)\\vert 0 \\rangle\\vert^2</p> <p>where :math:<code>U(x)</code> is an IQP circuit implemented via Pennylane's <code>IQPEmbedding</code>.</p> <p>We precompute the kernel matrix from the data directly, and pass it to scikit-learn's support vector classifier SVC. This  allows us to benefit from JAX parallelisation when computing the kernel matrices.</p> <p>The model requires evaluating a number of circuits given by the square of the number of data samples, and is therefore only appropriate for relatively small datasets.</p> PARAMETER DESCRIPTION <code>svm</code> <p>scikit-learn SVM class object used to fit the model from the kernel matrix</p> <p> TYPE: <code>SVC</code> DEFAULT: <code>SVC(kernel='precomputed', probability=True)</code> </p> <code>repeats</code> <p>number of times the IQP structure is repeated in the embedding circuit.</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>C</code> <p>regularization parameter for SVC. Lower values imply stronger regularization.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>250</code> </p> <code>qnode_kwargs</code> <p>the key word arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax-jit', 'diff_method': None}</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>random_state</code> <p>seed used for reproducibility.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>fit</code> <p>Fit the model to data X and labels y. Uses sklearn's SVM classifier</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>precompute_kernel</code> <p>compute the kernel matrix relative to data sets X1 and X2</p> <code>predict</code> <p>Predict labels for data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/iqp_kernel.py</code> <pre><code>def __init__(\n    self,\n    svm=SVC(kernel=\"precomputed\", probability=True),\n    repeats=2,\n    C=1.0,\n    jit=False,\n    random_state=42,\n    scaling=1.0,\n    max_vmap=250,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax-jit\", \"diff_method\": None},\n):\n    r\"\"\"\n    Kernel version of the classifier from https://arxiv.org/pdf/1804.11326v2.pdf.\n    The kernel function is given by\n\n    .. math::\n        k(x,x')=\\vert\\langle 0 \\vert U^\\dagger(x')U(x)\\vert 0 \\rangle\\vert^2\n\n    where :math:`U(x)` is an IQP circuit implemented via Pennylane's `IQPEmbedding`.\n\n    We precompute the kernel matrix from the data directly, and pass it to scikit-learn's support vector\n    classifier SVC. This  allows us to benefit from JAX parallelisation when computing the kernel\n    matrices.\n\n    The model requires evaluating a number of circuits given by the square of the number of data\n    samples, and is therefore only appropriate for relatively small datasets.\n\n    Args:\n        svm (sklearn.svm.SVC): scikit-learn SVM class object used to fit the model from the kernel matrix\n        repeats (int): number of times the IQP structure is repeated in the embedding circuit.\n        C (float): regularization parameter for SVC. Lower values imply stronger regularization.\n        jit (bool): Whether to use just in time compilation.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        qnode_kwargs (str): the key word arguments passed to the circuit qnode.\n        scaling (float): Factor by which to scale the input data.\n        random_state (int): seed used for reproducibility.\n    \"\"\"\n    # attributes that do not depend on data\n    self.repeats = repeats\n    self.C = C\n    self.jit = jit\n    self.max_vmap = max_vmap\n    self.svm = svm\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None\n    self.n_qubits_ = None\n    self.scaler = None  # data scaler will be fitted on training data\n    self.circuit = None\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y. Uses sklearn's SVM classifier</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/iqp_kernel.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y. Uses sklearn's SVM classifier\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n\n    self.svm.random_state = int(\n        jax.random.randint(self.generate_key(), shape=(1,), minval=0, maxval=1000000)\n    )\n\n    self.initialize(X.shape[1], np.unique(y))\n\n    self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    self.params_ = {\"x_train\": X}\n    kernel_matrix = self.precompute_kernel(X, X)\n\n    start = time.time()\n    # we are updating this value here, in case it was\n    # changed after initialising the model\n    self.svm.C = self.C\n    self.svm.fit(kernel_matrix, y)\n    end = time.time()\n    self.training_time_ = end - start\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels.</p> PARAMETER DESCRIPTION <code>n_features</code> <p>Number of features that the classifier expects</p> <p> TYPE: <code>int</code> </p> <code>classes</code> <p>class labels that the classifier expects</p> <p> TYPE: <code>array - like</code> DEFAULT: <code>None</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/iqp_kernel.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_qubits_ = n_features\n\n    self.construct_circuit()\n</code></pre> <code></code> precompute_kernel <pre><code>precompute_kernel(X1, X2)\n</code></pre> <p>compute the kernel matrix relative to data sets X1 and X2 Args:     X1 (np.array): first dataset of input vectors     X2 (np.array): second dataset of input vectors Returns:     kernel_matrix (np.array): matrix of size (len(X1),len(X2)) with elements K(x_1,x_2)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/iqp_kernel.py</code> <pre><code>def precompute_kernel(self, X1, X2):\n    \"\"\"\n    compute the kernel matrix relative to data sets X1 and X2\n    Args:\n        X1 (np.array): first dataset of input vectors\n        X2 (np.array): second dataset of input vectors\n    Returns:\n        kernel_matrix (np.array): matrix of size (len(X1),len(X2)) with elements K(x_1,x_2)\n    \"\"\"\n    dim1 = len(X1)\n    dim2 = len(X2)\n\n    # concatenate all pairs of vectors\n    Z = jnp.array([np.concatenate((X1[i], X2[j])) for i in range(dim1) for j in range(dim2)])\n\n    circuit = self.construct_circuit()\n    self.batched_circuit = chunk_vmapped_fn(\n        jax.vmap(circuit, 0), start=0, max_vmap=self.max_vmap\n    )\n    kernel_values = self.batched_circuit(Z)[:, 0]\n\n    # reshape the values into the kernel matrix\n    kernel_matrix = np.reshape(kernel_values, (dim1, dim2))\n\n    return kernel_matrix\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred</code> <p>Predicted labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/iqp_kernel.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred (np.ndarray): Predicted labels of shape (n_samples,)\n    \"\"\"\n    X = self.transform(X)\n    kernel_matrix = self.precompute_kernel(X, self.params_[\"x_train\"])\n    return self.svm.predict(kernel_matrix)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X. note that this may be inconsistent with predict; see the sklearn docummentation for details.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/iqp_kernel.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n    note that this may be inconsistent with predict; see the sklearn docummentation for details.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n\n    if \"x_train\" not in self.params_:\n        raise ValueError(\"Model cannot predict without fitting to data first.\")\n\n    X = self.transform(X)\n    kernel_matrix = self.precompute_kernel(X, self.params_[\"x_train\"])\n    return self.svm.predict_proba(kernel_matrix)\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/iqp_kernel.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n            self.scaler.fit(X)\n        X = self.scaler.transform(X)\n\n    return X * self.scaling\n</code></pre> <code></code> iqp_variational CLASS DESCRIPTION <code>IQPVariationalClassifier</code> FUNCTION DESCRIPTION <code>chunk_grad</code> <p>Convert a <code>jax.grad</code> function to an equivalent version that evaluated in chunks of size max_vmap.</p> <code>chunk_loss</code> <p>Converts a loss function of the form <code>loss_fn(params, array1, array2)</code> to an equivalent version that</p> <code>chunk_vmapped_fn</code> <p>Convert a vmapped function to an equivalent function that evaluates in chunks of size</p> <code>get_batch</code> <p>A generator to get random batches of the data (X, y)</p> <code>get_from_dict</code> <p>Access a value from a nested dictionary.</p> <code>get_nested_keys</code> <p>Returns the nested keys of a nested dictionary.</p> <code>set_in_dict</code> <p>Set a value in a nested dictionary.</p> <code>train</code> <p>Trains a model using an optimizer and a loss function via gradient descent. We assume that the loss function</p> <code></code> IQPVariationalClassifier <pre><code>IQPVariationalClassifier(repeats=1, n_layers=10, learning_rate=0.001, batch_size=32, max_vmap=None, jit=True, max_steps=10000, convergence_interval=200, random_state=42, scaling=1.0, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax'})\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Variational verison of the classifier from https://arxiv.org/pdf/1804.11326v2.pdf. The model is a standard variational quantum classifier</p> <p>.. math::</p> <pre><code>f(x)=\\langle 0 \\vert U^\\dagger(x)V^\\dagger(\\theta) H V(\\theta)U(x)\\vert 0 \\rangle\n</code></pre> <p>where the data embedding unitary :math:<code>U(x)</code> is based on an IQP circuit stucture and implemented via pennylane.IQPEmbedding, and the trainable unitay :math:<code>V(\\theta)</code> is implemented via pennylane.StronglyEntanglingLayers.</p> <p>The model is trained using a linear loss function equivalent to the probability of incorrect classification.</p> PARAMETER DESCRIPTION <code>repeats</code> <p>Number of times to repeat the IQP embedding circuit structure.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>n_layers</code> <p>Number of layers in the variational part of the circuit.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>learning_rate</code> <p>Learning rate for gradient descent.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> <code>batch_size</code> <p>Size of batches used for computing paraemeter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>convergence_threshold</code> <p>If loss changes less than this threshold for 10 consecutive steps we stop training.</p> <p> TYPE: <code>float</code> </p> <code>max_steps</code> <p>Maximum number of training steps. A warning will be raised if training did not converge.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the keyword arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax'}</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/iqp_variational.py</code> <pre><code>def __init__(\n    self,\n    repeats=1,\n    n_layers=10,\n    learning_rate=0.001,\n    batch_size=32,\n    max_vmap=None,\n    jit=True,\n    max_steps=10000,\n    convergence_interval=200,\n    random_state=42,\n    scaling=1.0,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax\"},\n):\n    r\"\"\"\n    Variational verison of the classifier from https://arxiv.org/pdf/1804.11326v2.pdf.\n    The model is a standard variational quantum classifier\n\n    .. math::\n\n        f(x)=\\langle 0 \\vert U^\\dagger(x)V^\\dagger(\\theta) H V(\\theta)U(x)\\vert 0 \\rangle\n\n    where the data embedding unitary :math:`U(x)` is based on an IQP circuit stucture and implemented via\n    pennylane.IQPEmbedding, and the trainable unitay :math:`V(\\theta)` is implemented via\n    pennylane.StronglyEntanglingLayers.\n\n    The model is trained using a linear loss function equivalent to the probability of incorrect classification.\n\n    Args:\n        repeats (int): Number of times to repeat the IQP embedding circuit structure.\n        n_layers (int): Number of layers in the variational part of the circuit.\n        learning_rate (float): Learning rate for gradient descent.\n        batch_size (int): Size of batches used for computing paraemeter updates.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        convergence_threshold (float): If loss changes less than this threshold for 10 consecutive steps we stop training.\n        max_steps (int): Maximum number of training steps. A warning will be raised if training did not converge.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'.\n        qnode_kwargs (str): the keyword arguments passed to the circuit qnode.\n        random_state (int): Seed used for pseudorandom number generation.\n    \"\"\"\n\n    # attributes that do not depend on data\n    self.repeats = repeats\n    self.n_layers = n_layers\n    self.learning_rate = learning_rate\n    self.batch_size = batch_size\n    self.max_steps = max_steps\n    self.convergence_interval = convergence_interval\n    self.batch_size = batch_size\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.jit = jit\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # Dictionary containing the trainable parameters\n    self.n_qubits_ = None\n    self.scaler = None  # data scaler will be fitted on training data\n    self.circuit = None\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/iqp_variational.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n\n    self.initialize(n_features=X.shape[1], classes=np.unique(y))\n\n    self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    optimizer = optax.adam\n\n    def loss_fn(params, X, y):\n        expvals = self.forward(params, X)\n        probs = (1 - expvals * y) / 2  # the probs of incorrect classification\n        return jnp.mean(probs)\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        X,\n        y,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels.</p> PARAMETER DESCRIPTION <code>n_features</code> <p>Number of features that the classifier expects</p> <p> TYPE: <code>int</code> </p> <code>classes</code> <p>class labels that the classifier expects</p> <p> TYPE: <code>array - like</code> DEFAULT: <code>None</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/iqp_variational.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_qubits_ = n_features\n    self.initialize_params()\n    self.construct_model()\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred</code> <p>Predicted labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/iqp_variational.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred (np.ndarray): Predicted labels of shape (n_samples,)\n    \"\"\"\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/iqp_variational.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    predictions = self.chunked_forward(self.params_, X)\n    predictions_2d = np.c_[(1 - predictions) / 2, (1 + predictions) / 2]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/iqp_variational.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n            self.scaler.fit(X)\n        X = self.scaler.transform(X)\n    return X * self.scaling\n</code></pre> <code></code> chunk_grad <pre><code>chunk_grad(grad_fn, max_vmap)\n</code></pre> <p>Convert a <code>jax.grad</code> function to an equivalent version that evaluated in chunks of size max_vmap.</p> <p><code>grad_fn</code> should be of the form <code>jax.grad(fn(params, X, y), argnums=0)</code>, where <code>params</code> is a dictionary of <code>jnp.arrays</code>, <code>X, y</code> are <code>jnp.arrays</code> with the same-size leading axis, and <code>grad_fn</code> is a function that is vectorised along these axes (i.e. <code>in_axes = (None,0,0)</code>).</p> <p>The returned function evaluates the original function by splitting the batch evaluation into smaller chunks of size <code>max_vmap</code>, and has a lower memory footprint.</p> PARAMETER DESCRIPTION <code>model</code> <p>gradient function with the functional form jax.grad(loss(params, X,y), argnums=0)</p> <p> TYPE: <code>func</code> </p> <code>max_vmap</code> <p>the size of the chunks</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <p>chunked version of the function</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def chunk_grad(grad_fn, max_vmap):\n    \"\"\"\n    Convert a `jax.grad` function to an equivalent version that evaluated in chunks of size max_vmap.\n\n    `grad_fn` should be of the form `jax.grad(fn(params, X, y), argnums=0)`, where `params` is a\n    dictionary of `jnp.arrays`, `X, y` are `jnp.arrays` with the same-size leading axis, and `grad_fn`\n    is a function that is vectorised along these axes (i.e. `in_axes = (None,0,0)`).\n\n    The returned function evaluates the original function by splitting the batch evaluation into smaller chunks\n    of size `max_vmap`, and has a lower memory footprint.\n\n    Args:\n        model (func): gradient function with the functional form jax.grad(loss(params, X,y), argnums=0)\n        max_vmap (int): the size of the chunks\n\n    Returns:\n        chunked version of the function\n    \"\"\"\n\n    def chunked_grad(params, X, y):\n        batch_slices = list(gen_batches(len(X), max_vmap))\n        grads = [grad_fn(params, X[slice], y[slice]) for slice in batch_slices]\n        grad_dict = {}\n        for key_list in get_nested_keys(params):\n            set_in_dict(\n                grad_dict,\n                key_list,\n                jnp.mean(jnp.array([get_from_dict(grad, key_list) for grad in grads]), axis=0),\n            )\n        return grad_dict\n\n    return chunked_grad\n</code></pre> <code></code> chunk_loss <pre><code>chunk_loss(loss_fn, max_vmap)\n</code></pre> <p>Converts a loss function of the form <code>loss_fn(params, array1, array2)</code> to an equivalent version that evaluates <code>loss_fn</code> in chunks of size max_vmap. <code>loss_fn</code> should batch evaluate along the leading axis of <code>array1, array2</code> (i.e. <code>in_axes = (None,0,0)</code>).</p> PARAMETER DESCRIPTION <code>loss_fn</code> <p>function of form loss_fn(params, array1, array2)</p> <p> TYPE: <code>func</code> </p> <code>max_vmap</code> <p>maximum chunk size</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <p>chunked version of the function</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def chunk_loss(loss_fn, max_vmap):\n    \"\"\"\n    Converts a loss function of the form `loss_fn(params, array1, array2)` to an equivalent version that\n    evaluates `loss_fn` in chunks of size max_vmap. `loss_fn` should batch evaluate along the leading\n    axis of `array1, array2` (i.e. `in_axes = (None,0,0)`).\n\n    Args:\n        loss_fn (func): function of form loss_fn(params, array1, array2)\n        max_vmap (int): maximum chunk size\n\n    Returns:\n        chunked version of the function\n    \"\"\"\n\n    def chunked_loss(params, X, y):\n        batch_slices = list(gen_batches(len(X), max_vmap))\n        res = jnp.array([loss_fn(params, *[X[slice], y[slice]]) for slice in batch_slices])\n        return jnp.mean(res)\n\n    return chunked_loss\n</code></pre> <code></code> chunk_vmapped_fn <pre><code>chunk_vmapped_fn(vmapped_fn, start, max_vmap)\n</code></pre> <p>Convert a vmapped function to an equivalent function that evaluates in chunks of size max_vmap. The behaviour of chunked_fn should be the same as vmapped_fn, but with a lower memory cost.</p> <p>The input vmapped_fn should have in_axes = (None, None, ..., 0,0,...,0)</p> PARAMETER DESCRIPTION <code>vmapped</code> <p>vmapped function with in_axes = (None, None, ..., 0,0,...,0)</p> <p> TYPE: <code>func</code> </p> <code>start</code> <p>The index where the first 0 appears in in_axes</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <p>chunked version of the function</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def chunk_vmapped_fn(vmapped_fn, start, max_vmap):\n    \"\"\"\n    Convert a vmapped function to an equivalent function that evaluates in chunks of size\n    max_vmap. The behaviour of chunked_fn should be the same as vmapped_fn, but with a\n    lower memory cost.\n\n    The input vmapped_fn should have in_axes = (None, None, ..., 0,0,...,0)\n\n    Args:\n        vmapped (func): vmapped function with in_axes = (None, None, ..., 0,0,...,0)\n        start (int): The index where the first 0 appears in in_axes\n        max_vmap (int) The max chunk size with which to evaluate the function\n\n    Returns:\n        chunked version of the function\n    \"\"\"\n\n    def chunked_fn(*args):\n        batch_len = len(args[start])\n        batch_slices = list(gen_batches(batch_len, max_vmap))\n        res = [\n            vmapped_fn(*args[:start], *[arg[slice] for arg in args[start:]])\n            for slice in batch_slices\n        ]\n        # jnp.concatenate needs to act on arrays with the same shape, so pad the last array if necessary\n        if batch_len / max_vmap % 1 != 0.0:\n            diff = max_vmap - len(res[-1])\n            res[-1] = jnp.pad(res[-1], [(0, diff), *[(0, 0)] * (len(res[-1].shape) - 1)])\n            return jnp.concatenate(res)[:-diff]\n        return jnp.concatenate(res)\n\n    return chunked_fn\n</code></pre> <code></code> get_batch <pre><code>get_batch(X, y, rnd_key, batch_size=32)\n</code></pre> <p>A generator to get random batches of the data (X, y)</p> PARAMETER DESCRIPTION <code>X</code> <p>Input data with shape (n_samples, n_features).</p> <p> TYPE: <code>array[float]</code> </p> <code>y</code> <p>Target labels with shape (n_samples,)</p> <p> TYPE: <code>array[float]</code> </p> <code>rnd_key</code> <p>A jax random key object</p> <p> </p> <code>batch_size</code> <p>Number of elements in batch</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> RETURNS DESCRIPTION <p>array[float]: A batch of input data shape (batch_size, n_features)</p> <p>array[float]: A batch of target labels shaped (batch_size,)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def get_batch(X, y, rnd_key, batch_size=32):\n    \"\"\"\n    A generator to get random batches of the data (X, y)\n\n    Args:\n        X (array[float]): Input data with shape (n_samples, n_features).\n        y (array[float]): Target labels with shape (n_samples,)\n        rnd_key: A jax random key object\n        batch_size (int): Number of elements in batch\n\n    Returns:\n        array[float]: A batch of input data shape (batch_size, n_features)\n        array[float]: A batch of target labels shaped (batch_size,)\n    \"\"\"\n    all_indices = jnp.array(range(len(X)))\n    rnd_indices = jax.random.choice(key=rnd_key, a=all_indices, shape=(batch_size,), replace=True)\n    return X[rnd_indices], y[rnd_indices]\n</code></pre> <code></code> get_from_dict <pre><code>get_from_dict(dict, key_list)\n</code></pre> <p>Access a value from a nested dictionary. Inspired by https://stackoverflow.com/questions/14692690/access-nested-dictionary-items-via-a-list-of-keys</p> PARAMETER DESCRIPTION <code>dict</code> <p>nested dictionary</p> <p> TYPE: <code>dict</code> </p> <code>key_list</code> <p>list of keys to be accessed</p> <p> TYPE: <code>list</code> </p> RETURNS DESCRIPTION <p>the requested value</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def get_from_dict(dict, key_list):\n    \"\"\"\n    Access a value from a nested dictionary.\n    Inspired by https://stackoverflow.com/questions/14692690/access-nested-dictionary-items-via-a-list-of-keys\n\n    Args:\n        dict (dict): nested dictionary\n        key_list (list): list of keys to be accessed\n\n    Returns:\n         the requested value\n    \"\"\"\n    return reduce(operator.getitem, key_list, dict)\n</code></pre> <code></code> get_nested_keys <pre><code>get_nested_keys(d, parent_keys=[])\n</code></pre> <p>Returns the nested keys of a nested dictionary.</p> PARAMETER DESCRIPTION <code>d</code> <p>nested dictionary</p> <p> TYPE: <code>dict</code> </p> RETURNS DESCRIPTION <p>list where each element is a list of nested keys</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def get_nested_keys(d, parent_keys=[]):\n    \"\"\"\n    Returns the nested keys of a nested dictionary.\n\n    Args:\n        d (dict): nested dictionary\n\n    Returns:\n        list where each element is a list of nested keys\n    \"\"\"\n    keys_list = []\n    for key, value in d.items():\n        current_keys = parent_keys + [key]\n        if isinstance(value, dict):\n            keys_list.extend(get_nested_keys(value, current_keys))\n        else:\n            keys_list.append(current_keys)\n    return keys_list\n</code></pre> <code></code> set_in_dict <pre><code>set_in_dict(dict, keys, value)\n</code></pre> <p>Set a value in a nested dictionary.</p> PARAMETER DESCRIPTION <code>dict</code> <p>nested dictionary</p> <p> TYPE: <code>dict</code> </p> <code>keys</code> <p>list of keys in nested dictionary</p> <p> TYPE: <code>list</code> </p> <code>value</code> <p>value to be set</p> <p> TYPE: <code>Any</code> </p> RETURNS DESCRIPTION <p>nested dictionary with new value</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def set_in_dict(dict, keys, value):\n    \"\"\"\n    Set a value in a nested dictionary.\n\n    Args:\n        dict (dict): nested dictionary\n        keys (list): list of keys in nested dictionary\n        value (Any): value to be set\n\n    Returns:\n        nested dictionary with new value\n    \"\"\"\n    for key in keys[:-1]:\n        dict = dict.setdefault(key, {})\n    dict[keys[-1]] = value\n</code></pre> <code></code> train <pre><code>train(model, loss_fn, optimizer, X, y, random_key_generator, convergence_interval=200)\n</code></pre> <p>Trains a model using an optimizer and a loss function via gradient descent. We assume that the loss function is of the form <code>loss(params, X, y)</code> and that the trainable parameters are stored in model.params_ as a dictionary of jnp.arrays. The optimizer should be an Optax optimizer (e.g. optax.adam). <code>model</code> must have an attribute <code>learning_rate</code> to set the initial learning rate for the gradient descent.</p> <p>The model is trained until a convergence criterion is met that corresponds to the loss curve being flat over a number of optimization steps given by <code>convergence_inteval</code> (see plots for details).</p> <p>To reduce precompilation time and memory cost, the loss function and gradient functions are evaluated in chunks of size model.max_vmap.</p> PARAMETER DESCRIPTION <code>model</code> <p>Classifier class object to train. Trainable parameters must be stored in model.params_.</p> <p> TYPE: <code>class</code> </p> <code>loss_fn</code> <p>Loss function to be minimised. Must be of the form loss_fn(params, X, y).</p> <p> TYPE: <code>Callable</code> </p> <code>optimizer</code> <p>Optax optimizer (e.g. optax.adam).</p> <p> TYPE: <code>optax optimizer</code> </p> <code>X</code> <p>Input data array of shape (n_samples, n_features)</p> <p> TYPE: <code>array</code> </p> <code>y</code> <p>Array of shape (n_samples) containing the labels.</p> <p> TYPE: <code>array</code> </p> <code>random_key_generator</code> <p>JAX key generator object for pseudo-randomness generation.</p> <p> TYPE: <code>PRNGKey</code> </p> <code>convergence_interval</code> <p>Number of optimization steps over which to decide convergence. Larger values give a higher confidence that the model has converged but may increase training time.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> RETURNS DESCRIPTION <code>params</code> <p>The new parameters after training has completed.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def train(model, loss_fn, optimizer, X, y, random_key_generator, convergence_interval=200):\n    \"\"\"\n    Trains a model using an optimizer and a loss function via gradient descent. We assume that the loss function\n    is of the form `loss(params, X, y)` and that the trainable parameters are stored in model.params_ as a dictionary\n    of jnp.arrays. The optimizer should be an Optax optimizer (e.g. optax.adam). `model` must have an attribute\n    `learning_rate` to set the initial learning rate for the gradient descent.\n\n    The model is trained until a convergence criterion is met that corresponds to the loss curve being flat over\n    a number of optimization steps given by `convergence_inteval` (see plots for details).\n\n    To reduce precompilation time and memory cost, the loss function and gradient functions are evaluated in\n    chunks of size model.max_vmap.\n\n    Args:\n        model (class): Classifier class object to train. Trainable parameters must be stored in model.params_.\n        loss_fn (Callable): Loss function to be minimised. Must be of the form loss_fn(params, X, y).\n        optimizer (optax optimizer): Optax optimizer (e.g. optax.adam).\n        X (array): Input data array of shape (n_samples, n_features)\n        y (array): Array of shape (n_samples) containing the labels.\n        random_key_generator (jax.random.PRNGKey): JAX key generator object for pseudo-randomness generation.\n        convergence_interval (int, optional): Number of optimization steps over which to decide convergence. Larger\n            values give a higher confidence that the model has converged but may increase training time.\n\n    Returns:\n        params (dict): The new parameters after training has completed.\n    \"\"\"\n\n    if not model.batch_size / model.max_vmap % 1 == 0:\n        raise Exception(\"Batch size must be multiple of max_vmap.\")\n\n    params = model.params_\n    opt = optimizer(learning_rate=model.learning_rate)\n    opt_state = opt.init(params)\n    grad_fn = jax.grad(loss_fn)\n\n    # jitting through the chunked_grad function can take a long time,\n    # so we jit here and chunk after\n    if model.jit:\n        grad_fn = jax.jit(grad_fn)\n\n    # note: assumes that the loss function is a sample mean of\n    # some function over the input data set\n    chunked_grad_fn = chunk_grad(grad_fn, model.max_vmap)\n    chunked_loss_fn = chunk_loss(loss_fn, model.max_vmap)\n\n    def update(params, opt_state, x, y):\n        grads = chunked_grad_fn(params, x, y)\n        loss_val = chunked_loss_fn(params, x, y)\n        updates, opt_state = opt.update(grads, opt_state)\n        params = optax.apply_updates(params, updates)\n        return params, opt_state, loss_val\n\n    loss_history = []\n    converged = False\n    start = time.time()\n    for step in range(model.max_steps):\n        key = random_key_generator()\n        X_batch, y_batch = get_batch(X, y, key, batch_size=model.batch_size)\n        params, opt_state, loss_val = update(params, opt_state, X_batch, y_batch)\n        loss_history.append(loss_val)\n        logging.debug(f\"{step} - loss: {loss_val}\")\n\n        if np.isnan(loss_val):\n            logging.info(\"nan encountered. Training aborted.\")\n            break\n\n        # decide convergence\n        if step &gt; 2 * convergence_interval:\n            # get means of last two intervals and standard deviation of last interval\n            average1 = np.mean(loss_history[-convergence_interval:])\n            average2 = np.mean(loss_history[-2 * convergence_interval : -convergence_interval])\n            std1 = np.std(loss_history[-convergence_interval:])\n            # if the difference in averages is small compared to the statistical fluctuations, stop training.\n            if np.abs(average2 - average1) &lt;= std1 / np.sqrt(convergence_interval) / 2:\n                logging.info(f\"Model {model.__class__.__name__} converged after {step} steps.\")\n                converged = True\n                break\n\n    end = time.time()\n    loss_history = np.array(loss_history)\n    model.loss_history_ = loss_history / np.max(np.abs(loss_history))\n    model.training_time_ = end - start\n\n    if not converged:\n        print(\"Loss did not converge:\", loss_history)\n        raise ConvergenceWarning(\n            f\"Model {model.__class__.__name__} has not converged after the maximum number of {model.max_steps} steps.\"\n        )\n\n    return params\n</code></pre> <code></code> projected_quantum_kernel CLASS DESCRIPTION <code>ProjectedQuantumKernel</code> <code></code> ProjectedQuantumKernel <pre><code>ProjectedQuantumKernel(svm=SVC(kernel='precomputed', probability=True), gamma_factor=1.0, C=1.0, embedding='Hamiltonian', t=1.0 / 3, trotter_steps=5, jit=True, max_vmap=None, scaling=1.0, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax-jit', 'diff_method': None}, random_state=42)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Kernel based classifier from https://arxiv.org/pdf/2011.01938v2.pdf.</p> <p>The Kernel function is</p> <p>.. math::     k(x_i, x_j) = \\exp (-\\gamma  \\sum_k \\sum_{P \\in {X, Y, Z}} ( \\text{tr}(P \\rho(x_i)_k)     - \\text{tr}( P \\rho(x_j)_k))^2)</p> <p>where :math:<code>\\rho_k(x_i)</code> is the reduced state of the kth qubit of the data embedded density matrix and :math:<code>\\gamma</code> is a hyperparameter of the model that we scale from the default value given in the plots.</p> <p>For embedding='Hamiltonian' a layer or random single qubit rotations are performed, followed by a Hamiltonian time evolution corresponding to a trotterised evolution of a Heisenberg Hamiltonian:</p> <p>.. math::     \\prod_{j=1}^n \\exp(-i \\frac{t}{L} x_{j} (X_j X_{j+1} + Y_j Y_{j+1} + Z_j Z_{j+1})).</p> <p>where :math:<code>t</code> and :math:<code>L</code> are the evolution time and the number of trotter steps that are controlled by hyperparameters <code>t</code> and <code>trotter_steps</code>.</p> <p>For emedding='IQP' an IQP embedding is used via PennyLanes's IQPEmbedding class.</p> <p>We precompute the kernel matrix from data and pass it to scikit-learn's support vector machine class SVC, which fits a classifier.</p> PARAMETER DESCRIPTION <code>svm</code> <p>scikit-learn SVC class object.</p> <p> TYPE: <code>SVC</code> DEFAULT: <code>SVC(kernel='precomputed', probability=True)</code> </p> <code>gamma_factor</code> <p>the factor that multiplies the default scaling parameter in the kernel.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>C</code> <p>regularization parameter when fitting the kernel model.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>embedding</code> <p>The choice of embedding circuit used to construct the kernel. Either 'IQP' or 'Hamiltonian'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'Hamiltonian'</code> </p> <code>t</code> <p>The evolution time used in the 'Hamiltonian' data embedding. The time is given by n_features*t.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0 / 3</code> </p> <code>trotter_steps</code> <p>the number of trotter steps used in the 'Hamiltonian' embedding circuit.</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the key word arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax-jit', 'diff_method': None}</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>construct_circuit</code> <p>Constructs the circuit to get the expvals of a given qubit and Pauli operator</p> <code>fit</code> <p>Fit the model to data X and labels y. Uses sklearn's SVM classifier</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>precompute_kernel</code> <p>compute the kernel matrix relative to data sets X1 and X2</p> <code>predict</code> <p>Predict labels for data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/projected_quantum_kernel.py</code> <pre><code>def __init__(\n    self,\n    svm=SVC(kernel=\"precomputed\", probability=True),\n    gamma_factor=1.0,\n    C=1.0,\n    embedding=\"Hamiltonian\",\n    t=1.0 / 3,\n    trotter_steps=5,\n    jit=True,\n    max_vmap=None,\n    scaling=1.0,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax-jit\", \"diff_method\": None},\n    random_state=42,\n):\n    r\"\"\"\n    Kernel based classifier from https://arxiv.org/pdf/2011.01938v2.pdf.\n\n    The Kernel function is\n\n    .. math::\n        k(x_i, x_j) = \\exp (-\\gamma  \\sum_k \\sum_{P \\in \\{X, Y, Z\\}} ( \\text{tr}(P \\rho(x_i)_k)\n        - \\text{tr}( P \\rho(x_j)_k))^2)\n\n    where :math:`\\rho_k(x_i)` is the reduced state of the kth qubit of the data embedded density matrix and\n    :math:`\\gamma` is a hyperparameter of the model that we scale from the default value given in the plots.\n\n    For embedding='Hamiltonian' a layer or random single qubit rotations are performed, followed by a Hamiltonian\n    time evolution corresponding to a trotterised evolution of a Heisenberg Hamiltonian:\n\n    .. math::\n        \\prod_{j=1}^n \\exp(-i \\frac{t}{L} x_{j} (X_j X_{j+1} + Y_j Y_{j+1} + Z_j Z_{j+1})).\n\n    where :math:`t` and :math:`L` are the evolution time and the number of trotter steps that are controlled by\n    hyperparameters `t` and `trotter_steps`.\n\n    For emedding='IQP' an IQP embedding is used via PennyLanes's IQPEmbedding class.\n\n    We precompute the kernel matrix from data and pass it to scikit-learn's support vector machine class SVC,\n    which fits a classifier.\n\n    Args:\n        svm (sklearn.svm.SVC): scikit-learn SVC class object.\n        gamma_factor (float): the factor that multiplies the default scaling parameter in the kernel.\n        C (float): regularization parameter when fitting the kernel model.\n        embedding (str): The choice of embedding circuit used to construct the kernel.\n            Either 'IQP' or 'Hamiltonian'.\n        t (float): The evolution time used in the 'Hamiltonian' data embedding. The time is\n            given by n_features*t.\n        trotter_steps (int): the number of trotter steps used in the 'Hamiltonian' embedding circuit.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'\n        qnode_kwargs (str): the key word arguments passed to the circuit qnode.\n        scaling (float): Factor by which to scale the input data.\n        random_state (int): Seed used for pseudorandom number generation.\n    \"\"\"\n    # attributes that do not depend on data\n    self.gamma_factor = gamma_factor\n    self.svm = svm\n    self.C = C\n    self.embedding = embedding\n    self.t = t\n    self.trotter_steps = trotter_steps\n    self.jit = jit\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = 50\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None\n    self.n_qubits_ = None\n    self.n_features_ = None\n    self.rotation_angles_ = None  # for hamiltonian embedding\n    self.scaler = None  # data scaler will be fitted on training data\n    self.circuit = None\n</code></pre> <code></code> construct_circuit <pre><code>construct_circuit()\n</code></pre> <p>Constructs the circuit to get the expvals of a given qubit and Pauli operator We will use JAX to parallelize over these circuits in precompute kernel. Args:     P: a pennylane Pauli X,Y,Z operator on a given qubit</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/projected_quantum_kernel.py</code> <pre><code>def construct_circuit(self):\n    \"\"\"\n    Constructs the circuit to get the expvals of a given qubit and Pauli operator\n    We will use JAX to parallelize over these circuits in precompute kernel.\n    Args:\n        P: a pennylane Pauli X,Y,Z operator on a given qubit\n    \"\"\"\n    if self.embedding == \"IQP\":\n\n        def embedding(x):\n            qml.IQPEmbedding(x, wires=range(self.n_qubits_), n_repeats=2)\n\n    elif self.embedding == \"Hamiltonian\":\n\n        def embedding(x):\n            evol_time = self.t / self.trotter_steps * (self.n_qubits_ - 1)\n            for i in range(self.n_qubits_):\n                qml.Rot(\n                    self.rotation_angles_[i, 0],\n                    self.rotation_angles_[i, 1],\n                    self.rotation_angles_[i, 2],\n                    wires=i,\n                )\n            for __ in range(self.trotter_steps):\n                for j in range(self.n_qubits_ - 1):\n                    qml.IsingXX(x[j] * evol_time, wires=[j, j + 1])\n                    qml.IsingYY(x[j] * evol_time, wires=[j, j + 1])\n                    qml.IsingZZ(x[j] * evol_time, wires=[j, j + 1])\n\n    dev = qml.device(self.dev_type, wires=self.n_qubits_)\n\n    @qml.qnode(dev, **self.qnode_kwargs)\n    def circuit(x):\n        embedding(x)\n        return (\n            [qml.expval(qml.PauliX(wires=i)) for i in range(self.n_qubits_)]\n            + [qml.expval(qml.PauliY(wires=i)) for i in range(self.n_qubits_)]\n            + [qml.expval(qml.PauliZ(wires=i)) for i in range(self.n_qubits_)]\n        )\n\n    self.circuit = circuit\n\n    def circuit_as_array(x):\n        return jnp.array(circuit(x))\n\n    if self.jit:\n        circuit_as_array = jax.jit(circuit_as_array)\n    circuit_as_array = jax.vmap(circuit_as_array, in_axes=(0))\n    circuit_as_array = chunk_vmapped_fn(circuit_as_array, 0, self.max_vmap)\n\n    return circuit_as_array\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y. Uses sklearn's SVM classifier</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/projected_quantum_kernel.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y. Uses sklearn's SVM classifier\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n    self.svm.random_state = self.rng.integers(100000)\n\n    self.initialize(X.shape[1], classes=np.unique(y))\n\n    self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    self.params_ = {\"X_train\": X}\n    kernel_matrix = self.precompute_kernel(X, X)\n\n    start = time.time()\n    self.svm.C = self.C\n    self.svm.fit(kernel_matrix, y)\n    end = time.time()\n    self.training_time_ = end - start\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels.</p> PARAMETER DESCRIPTION <code>n_features</code> <p>Number of features that the classifier expects</p> <p> TYPE: <code>int</code> </p> <code>classes</code> <p>class labels that the classifier expects</p> <p> TYPE: <code>array - like</code> DEFAULT: <code>None</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/projected_quantum_kernel.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_features_ = n_features\n\n    if self.embedding == \"IQP\":\n        self.n_qubits_ = self.n_features_\n    elif self.embedding == \"Hamiltonian\":\n        self.n_qubits_ = self.n_features_ + 1\n        self.rotation_angles_ = jnp.array(\n            self.rng.uniform(size=(self.n_qubits_, 3)) * np.pi * 2\n        )\n    self.construct_circuit()\n</code></pre> <code></code> precompute_kernel <pre><code>precompute_kernel(X1, X2)\n</code></pre> <p>compute the kernel matrix relative to data sets X1 and X2 Args:     X1 (np.array): first dataset of input vectors     X2 (np.array): second dataset of input vectors Returns:     kernel_matrix (np.array): matrix of size (len(X1),len(X2)) with elements K(x_1,x_2)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/projected_quantum_kernel.py</code> <pre><code>def precompute_kernel(self, X1, X2):\n    \"\"\"\n    compute the kernel matrix relative to data sets X1 and X2\n    Args:\n        X1 (np.array): first dataset of input vectors\n        X2 (np.array): second dataset of input vectors\n    Returns:\n        kernel_matrix (np.array): matrix of size (len(X1),len(X2)) with elements K(x_1,x_2)\n    \"\"\"\n    dim1 = len(X1)\n    dim2 = len(X2)\n\n    # get all of the Pauli expvals needed to constrcut the kernel\n    self.circuit = self.construct_circuit()\n\n    valsX1 = np.array(self.circuit(X1))\n    valsX1 = np.reshape(valsX1, (dim1, 3, -1))\n    valsX2 = np.array(self.circuit(X2))\n    valsX2 = np.reshape(valsX2, (dim2, 3, -1))\n\n    valsX_X1 = valsX1[:, 0]\n    valsX_X2 = valsX2[:, 0]\n    valsY_X1 = valsX1[:, 1]\n    valsY_X2 = valsX2[:, 1]\n    valsZ_X1 = valsX1[:, 2]\n    valsZ_X2 = valsX2[:, 2]\n\n    all_vals_X1 = np.reshape(np.concatenate((valsX_X1, valsY_X1, valsZ_X1)), -1)\n    default_gamma = 1 / np.var(all_vals_X1) / self.n_features_\n\n    # construct kernel following plots\n    kernel_matrix = np.zeros([dim1, dim2])\n\n    for i in range(dim1):\n        for j in range(dim2):\n            sumX = sum([(valsX_X1[i, q] - valsX_X2[j, q]) ** 2 for q in range(self.n_qubits_)])\n            sumY = sum([(valsY_X1[i, q] - valsY_X2[j, q]) ** 2 for q in range(self.n_qubits_)])\n            sumZ = sum([(valsZ_X1[i, q] - valsZ_X2[j, q]) ** 2 for q in range(self.n_qubits_)])\n\n            kernel_matrix[i, j] = np.exp(\n                -default_gamma * self.gamma_factor * (sumX + sumY + sumZ)\n            )\n    return kernel_matrix\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred</code> <p>Predicted labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/projected_quantum_kernel.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred (np.ndarray): Predicted labels of shape (n_samples,)\n    \"\"\"\n    X = self.transform(X)\n    kernel_matrix = self.precompute_kernel(X, self.params_[\"X_train\"])\n    return self.svm.predict(kernel_matrix)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X. Note that this may be inconsistent with predict; see the sklearn docummentation for details.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/projected_quantum_kernel.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n    Note that this may be inconsistent with predict; see the sklearn docummentation for details.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n\n    if \"X_train\" not in self.params_:\n        raise ValueError(\"Model cannot predict without fitting to data first.\")\n\n    X = self.transform(X)\n    kernel_matrix = self.precompute_kernel(X, self.params_[\"X_train\"])\n    return self.svm.predict_proba(kernel_matrix)\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/projected_quantum_kernel.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n            self.scaler.fit(X)\n        X = self.scaler.transform(X)\n    return X * self.scaling\n</code></pre> <code></code> quantum_boltzmann_machine CLASS DESCRIPTION <code>QuantumBoltzmannMachine</code> FUNCTION DESCRIPTION <code>tensor_ops</code> <p>Returns a tensor product of two operators acting at indexes idxs in an n_qubit system</p> <code></code> QuantumBoltzmannMachine <pre><code>QuantumBoltzmannMachine(visible_qubits='single', observable_type='sum', temperature=1, learning_rate=0.001, batch_size=32, max_vmap=None, jit=True, max_steps=10000, convergence_threshold=1e-06, random_state=42, scaling=1.0)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Variational Quantum Boltzmann Machine from https://arxiv.org/abs/2006.06004</p> <p>The model works as follows 1. One prepares a gibbs state :math:<code>e^{-H(\u000bec{theta},x)/K_b T}/Z</code>, where :math:<code>H(     heta,x)</code> is a parameterised n_qubit     Hamiltonian and :math:<code>Z</code> is the partition function normalisation. Here we take n_qubits equal to the number of features 2. A :math:<code>\\pm1</code> valued observable :math:<code>O</code> is measured on a subset of qubits (called 'visible qubits'). The forward     function of the model is then</p> <p>.. maths::</p> <pre><code>f(x,        heta) = Tr[O e^{-H(\u000bec{theta},x)/k_b T}/Z]\n</code></pre> <p>from this expectation value, the class probabilities are computed and used in a binary cross entropy loss.</p> <p>The specific Hamiltonian we use is a generalisation of the one used in the plots:</p> <p>.. maths::</p> <pre><code>H(x,        heta) = \\sum_i Z_i      heta_i\\cdot x + \\sum_i X_i      heta_{i+n_qubits}\\cdot x + \\sum_{ij} Z_iZ_j     heta_{i+2*n_qubits}\\cdot x\n</code></pre> <p>The observable use can be either a sum or tensor product of Z operators on the visible qubits.</p> <p>In practice, the full algorithm involves parameterising a trial state for the gibbs state and performing variational imaginary time evolution to approximate the true gibbs state in each optimisation step. Since this is quite computationally involved, we here assume (as they do in the plots numerics) that we have access to the perfect Gibbs state. It is thus unclear whether the full algorithm can be expected to perform as well as this implementation.</p> PARAMETER DESCRIPTION <code>visible_qubits</code> <p>The subset of qubits used for prediction. if 'single' a single qubit is used if 'half' half are used and if 'all' all are used.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'single'</code> </p> <code>observable_type</code> <p>If 'sum' a sum of Z operators is used, if 'product' a tensor product is used.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'sum'</code> </p> <code>temperature</code> <p>The temperature of the Gibbs state in units of K_bT. e.g. temperature = 2 is equivalent to K_bT=2.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>learning_rate</code> <p>learning rate for gradient descent.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> <code>batch_size</code> <p>Number of data points to subsample in each training step.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>max_vmap</code> <p>The largest size of vmap used (to control memory).</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>convergence_threshold</code> <p>If loss changes less than this threshold for 10 consecutive steps we stop training.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-06</code> </p> <code>max_steps</code> <p>Maximum number of training steps. A warning will be raised if training did not converge.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> <code>scaling</code> <p>The data is scaled by this factor after standardisation</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> METHOD DESCRIPTION <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_boltzmann_machine.py</code> <pre><code>def __init__(\n    self,\n    visible_qubits=\"single\",\n    observable_type=\"sum\",\n    temperature=1,\n    learning_rate=0.001,\n    batch_size=32,\n    max_vmap=None,\n    jit=True,\n    max_steps=10000,\n    convergence_threshold=1e-6,\n    random_state=42,\n    scaling=1.0,\n):\n    \"\"\"\n    Variational Quantum Boltzmann Machine from https://arxiv.org/abs/2006.06004\n\n    The model works as follows\n    1. One prepares a gibbs state :math:`e^{-H(\\vec{theta},x)/K_b T}/Z`, where :math:`H(\\theta,x)` is a parameterised n_qubit\n        Hamiltonian and :math:`Z` is the partition function normalisation. Here we take n_qubits equal to the number of features\n    2. A :math:`\\\\pm1` valued observable :math:`O` is measured on a subset of qubits (called 'visible qubits'). The forward\n        function of the model is then\n\n    .. maths::\n\n        f(x, \\theta) = Tr[O e^{-H(\\vec{theta},x)/k_b T}/Z]\n\n    from this expectation value, the class probabilities are computed and used in a binary cross entropy loss.\n\n    The specific Hamiltonian we use is a generalisation of the one used in the plots:\n\n    .. maths::\n\n        H(x, \\theta) = \\\\sum_i Z_i \\theta_i\\\\cdot x + \\\\sum_i X_i \\theta_{i+n_qubits}\\\\cdot x + \\\\sum_{ij} Z_iZ_j \\theta_{i+2*n_qubits}\\\\cdot x\n\n    The observable use can be either a sum or tensor product of Z operators on the visible qubits.\n\n    In practice, the full algorithm involves parameterising a trial state for the gibbs state and performing variational imaginary\n    time evolution to approximate the true gibbs state in each optimisation step. Since this is quite computationally involved, we\n    here assume (as they do in the plots numerics) that we have access to the perfect Gibbs state. It is thus unclear whether the full\n    algorithm can be expected to perform as well as this implementation.\n\n    Args:\n        visible_qubits (str): The subset of qubits used for prediction. if 'single' a single qubit is used\n            if 'half' half are used and if 'all' all are used.\n        observable_type (str): If 'sum' a sum of Z operators is used, if 'product' a tensor product is used.\n        temperature (int): The temperature of the Gibbs state in units of K_bT. e.g. temperature = 2 is equivalent to K_bT=2.\n        learning_rate (float): learning rate for gradient descent.\n        batch_size (int): Number of data points to subsample in each training step.\n        max_vmap (int): The largest size of vmap used (to control memory).\n        jit (bool): Whether to use just in time compilation.\n        convergence_threshold (float): If loss changes less than this threshold for 10 consecutive steps we stop training.\n        max_steps (int): Maximum number of training steps. A warning will be raised if training did not converge.\n        random_state (int): Seed used for pseudorandom number generation\n        scaling (float): The data is scaled by this factor after standardisation\n    \"\"\"\n\n    # attributes that do not depend on data\n    self.visible_qubits = visible_qubits\n    self.observable_type = observable_type\n    self.temperature = temperature\n    self.learning_rate = learning_rate\n    self.batch_size = batch_size\n    self.max_vmap = max_vmap\n    self.jit = jit\n    self.max_steps = max_steps\n    self.convergence_threshold = convergence_threshold\n    self.batch_size = batch_size\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # Dictionary containing the trainable parameters\n    self.n_qubits = None\n    self.n_visible = None  # number of visible qubits\n    self.scaler = None  # data scaler will be fitted on training data\n    self.circuit = None\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_boltzmann_machine.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n    self.initialize(X.shape[1], classes=np.unique(y))\n\n    self.scaler = StandardScaler()\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    optimizer = optax.adam\n\n    def loss_fn(params, X, y):\n        # binary cross entropy loss\n        vals = self.forward(params[\"thetas\"], X)\n        probs = (1 + vals) / 2\n        y = jax.nn.relu(y)  # convert to 0,1\n        return jnp.mean(-y * jnp.log(probs) - (1 - y) * jnp.log(1 - probs))\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    self.params_ = train(self, loss_fn, optimizer, X, y, self.generate_key)\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels.</p> PARAMETER DESCRIPTION <code>n_features</code> <p>Number of features that the classifier expects</p> <p> TYPE: <code>int</code> </p> <code>classes</code> <p>class labels that the classifier expects</p> <p> TYPE: <code>array - like</code> DEFAULT: <code>None</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_boltzmann_machine.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_qubits = n_features\n\n    if self.visible_qubits == \"single\":\n        self.n_visible = 1\n    elif self.visible_qubits == \"half\":\n        self.n_visible = self.n_qubits // 2\n    elif self.visible_qubits == \"full\":\n        self.n_visible = self.n_qubits\n\n    self.construct_model()\n    self.initialize_params()\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred</code> <p>Predicted labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_boltzmann_machine.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred (np.ndarray): Predicted labels of shape (n_samples,)\n    \"\"\"\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_boltzmann_machine.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    predictions = self.forward(self.params_[\"thetas\"], X)\n    predictions_2d = np.c_[(1 - predictions) / 2, (1 + predictions) / 2]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_boltzmann_machine.py</code> <pre><code>def transform(self, X):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if self.scaler is None:\n        # if the model is unfitted, initialise the scaler here\n        self.scaler = StandardScaler()\n        self.scaler.fit(X)\n\n    return self.scaler.transform(X) * self.scaling\n</code></pre> <code></code> tensor_ops <pre><code>tensor_ops(ops, idxs, n_qubits)\n</code></pre> <p>Returns a tensor product of two operators acting at indexes idxs in an n_qubit system</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_boltzmann_machine.py</code> <pre><code>def tensor_ops(ops, idxs, n_qubits):\n    \"\"\"\n    Returns a tensor product of two operators acting at indexes idxs in an n_qubit system\n    \"\"\"\n    tensor_op = 1.0\n    for i in range(n_qubits):\n        if i in idxs:\n            j = idxs.index(i)\n            tensor_op = jnp.kron(tensor_op, ops[j])\n        else:\n            tensor_op = jnp.kron(tensor_op, jnp.eye(2))\n    return tensor_op\n</code></pre> <code></code> quantum_kitchen_sinks CLASS DESCRIPTION <code>QuantumKitchenSinks</code> <code></code> QuantumKitchenSinks <pre><code>QuantumKitchenSinks(linear_model=LogisticRegression(penalty=None, solver='lbfgs', tol=0.001), n_episodes=100, n_qfeatures='full', var=1.0, jit=True, max_vmap=None, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax', 'diff_method': None}, scaling=1.0, random_state=42)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Quantum kitchen sinks model classical data classification.</p> <p>Based on: https://arxiv.org/pdf/1806.08321.pdf</p> <p>The quantum computer is used to generate random feature vectors which are fed to a linear classifier that we implement using scikit-learn's LogisticRegression (note logistic regression is a standard method to perform linear classification despite the name)</p> <p>The feature map procedure works as follows:</p> <ol> <li> <p>Linearly transform an input feature vector :math:<code>x</code> of length :math:<code>n</code> via :math:<code>x' = \\omega x + \\beta</code> using random :math:<code>\\omega, \\beta</code>. The output is of shape <code>(n_episodes, n_qfeatures)</code>.</p> </li> <li> <p>Feed each row vector of the matrix :math:<code>x'</code> into a quantum circuit that returns <code>n_qfeatures</code> samples. The samples are concatenated in a feature vector :math:<code>x</code> of length <code>n_episodes*n_qfeatures</code>, which is the input to the linear model.</p> </li> </ol> <p>.. note::</p> <pre><code>It is not stated in the plots how to generalise the circuits to higher qubit numbers;\nthis implementation is a simple generalisation of the circuit in Fig 2(c), which consists\nof encoding the features into single qubits via X rotaiton gates, and perfoming a sequence\nof CNOT gates on nearest neighbour and next-nearest neighbour qubits.\n</code></pre> PARAMETER DESCRIPTION <code>linear_model</code> <p>linear model to use with the transformed features</p> <p> TYPE: <code>sklearn Estimator</code> DEFAULT: <code>LogisticRegression(penalty=None, solver='lbfgs', tol=0.001)</code> </p> <code>n_episodes</code> <p>Number of features fed into the linear model after data transformation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>n_qfeatures</code> <p>Determines the number of features fed into the quantum circuit to transform the data. This is the number of qubits used by the model. If 'full', the number of qubits is equal to the number of input features, if 'half' it is half the number of input features.</p> <p> TYPE: <code>(str, int)</code> DEFAULT: <code>'full'</code> </p> <code>var</code> <p>detemined the variance of the matrix <code>\\omega</code> used to lienarly transform the input features.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the key word arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax', 'diff_method': None}</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>construct_model</code> <p>The circuit used to generate the feature vectors</p> <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for batch of data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Apply the feature map: The inputs go through a random linear transformation</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_kitchen_sinks.py</code> <pre><code>def __init__(\n    self,\n    linear_model=LogisticRegression(penalty=None, solver=\"lbfgs\", tol=10e-4),\n    n_episodes=100,\n    n_qfeatures=\"full\",\n    var=1.0,\n    jit=True,\n    max_vmap=None,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax\", \"diff_method\": None},\n    scaling=1.0,\n    random_state=42,\n):\n    r\"\"\"\n    Quantum kitchen sinks model classical data classification.\n\n    Based on: https://arxiv.org/pdf/1806.08321.pdf\n\n    The quantum computer is used to generate random feature vectors which are\n    fed to a linear classifier that we implement using scikit-learn's LogisticRegression (note logistic\n    regression is a standard method to perform linear classification despite the name)\n\n    The feature map procedure works as follows:\n\n    1. Linearly transform an input feature vector :math:`x` of length :math:`n`\n    via :math:`x' = \\omega x + \\beta` using random :math:`\\omega, \\beta`.\n    The output is of shape `(n_episodes, n_qfeatures)`.\n\n    2. Feed each row vector of the matrix :math:`x'` into a quantum circuit that\n    returns `n_qfeatures` samples. The samples are concatenated in a feature vector :math:`x`\n    of length `n_episodes*n_qfeatures`, which is the input to the linear model.\n\n    .. note::\n\n        It is not stated in the plots how to generalise the circuits to higher qubit numbers;\n        this implementation is a simple generalisation of the circuit in Fig 2(c), which consists\n        of encoding the features into single qubits via X rotaiton gates, and perfoming a sequence\n        of CNOT gates on nearest neighbour and next-nearest neighbour qubits.\n\n    Args:\n        linear_model (sklearn Estimator): linear model to use with the transformed features\n        n_episodes (int): Number of features fed into the linear model after data transformation.\n        n_qfeatures (str, int): Determines the number of features fed into the quantum circuit to transform the\n            data. This is the number of qubits used by the model. If 'full', the number of qubits is equal\n            to the number of input features, if 'half' it is half the number of input features.\n        var (float): detemined the variance of the matrix `\\omega` used to lienarly transform the input\n            features.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'\n        qnode_kwargs (str): the key word arguments passed to the circuit qnode.\n        random_state (int): Seed used for pseudorandom number generation.\n    \"\"\"\n\n    # attributes that do not depend on data\n    self.linear_model = linear_model\n    self.n_episodes = n_episodes\n    self.n_qfeatures = n_qfeatures\n    self.var = var\n    self.jit = jit\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = 100000\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None\n    self.n_qubits_ = None\n    self.scaler = None  # data scaler for inputs\n    self.scaler2 = None  # data scaler for quantum generated features\n    self.circuit = None\n</code></pre> <code></code> construct_model <pre><code>construct_model()\n</code></pre> <p>The circuit used to generate the feature vectors It is not clear from the plots how to generalise this to higher qubit numbers. This is a simple generalisation of the circuit in Fig 2(c).</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_kitchen_sinks.py</code> <pre><code>def construct_model(self):\n    \"\"\"\n    The circuit used to generate the feature vectors\n    It is not clear from the plots how to generalise this to higher qubit numbers.\n    This is a simple generalisation of the circuit in Fig 2(c).\n    \"\"\"\n\n    pattern = [[i, i + 2] for i in range(self.n_qubits_ - 2)]\n\n    dev = qml.device(\n        self.dev_type,\n        wires=self.n_qubits_,\n        shots=1,\n        prng_key=self.generate_key(),\n    )\n\n    @qml.qnode(dev, **self.qnode_kwargs)\n    def circuit(Q):\n        for i, q in enumerate(Q):\n            qml.RX(q, wires=i)\n        qml.broadcast(qml.CNOT, wires=range(self.n_qubits_), pattern=\"double\")\n        qml.broadcast(qml.CNOT, wires=range(self.n_qubits_), pattern=pattern)\n\n        return qml.sample(wires=range(self.n_qubits_))\n\n    self.circuit = circuit\n\n    if self.jit:\n        circuit = jax.jit(circuit)\n    circuit = chunk_vmapped_fn(jax.vmap(circuit), 0, self.max_vmap)\n\n    self.forward = circuit\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels 1-, 1 of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_kitchen_sinks.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels 1-, 1 of shape (n_samples,)\n    \"\"\"\n\n    self.linear_model.random_state = self.rng.integers(100000)\n\n    self.initialize(X.shape[1], np.unique(y))\n\n    self.scaler = MinMaxScaler(feature_range=(-1, 1))\n    self.scaler.fit(X)\n    features = self.transform(X)\n\n    start = time.time()\n    self.linear_model.fit(features, y)\n    end = time.time()\n    self.params_[\"weights\"] = self.linear_model.coef_\n    self.training_time_ = end - start\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels.</p> PARAMETER DESCRIPTION <code>n_features</code> <p>Number of features that the classifier expects</p> <p> TYPE: <code>int</code> </p> <code>classes</code> <p>class labels that the classifier expects</p> <p> TYPE: <code>array - like</code> DEFAULT: <code>None</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_kitchen_sinks.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_features_ = n_features\n\n    if self.n_qfeatures == \"full\":\n        self.n_qubits_ = n_features\n    elif self.n_qfeatures == \"half\":\n        self.n_qubits_ = int(np.ceil(n_features / 2))\n    else:\n        self.n_qubits_ = int(self.n_qfeatures)\n\n    self.construct_model()\n    self.initialize_params()\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for batch of data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_kitchen_sinks.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for batch of data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_kitchen_sinks.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    predictions_2d = self.linear_model.predict_proba(X)\n    return softmax(predictions_2d, copy=False)\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> <p>Apply the feature map: The inputs go through a random linear transformation followed by a quantum circuit.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_kitchen_sinks.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Apply the feature map: The inputs go through a random linear transformation\n    followed by a quantum circuit.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if self.params_[\"betas\"] is None or self.params_[\"omegas\"] is None or self.circuit is None:\n        raise ValueError(\"Model cannot predict without fitting to data first.\")\n\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = MinMaxScaler(feature_range=(-1, 1))\n            self.scaler.fit(X)\n        X = self.scaler.transform(X)\n    X = X * self.scaling\n\n    n_data = X.shape[0]\n    input_features = np.zeros([self.n_episodes, n_data, self.n_qubits_])\n    for e in range(self.n_episodes):\n        stacked_beta = np.stack([self.params_[\"betas\"][e] for __ in range(n_data)])\n        input_features[e] = (self.params_[\"omegas\"][e] @ X.T + stacked_beta.T).T\n    input_features = np.reshape(input_features, (n_data * self.n_episodes, -1))\n\n    features = self.forward(input_features)\n    features = np.reshape(features, (self.n_episodes, n_data, -1))\n    features = np.array([features[:, i, :] for i in range(n_data)])\n    features = np.reshape(features, (n_data, -1))\n\n    if self.scaler2 is None:\n        self.scaler2 = StandardScaler().fit(features)\n\n    return features\n</code></pre> <code></code> quantum_metric_learning CLASS DESCRIPTION <code>QuantumMetricLearner</code> FUNCTION DESCRIPTION <code>get_batch</code> <p>Convenience function to get a batch of data.</p> <code></code> QuantumMetricLearner <pre><code>QuantumMetricLearner(n_layers=3, n_examples_predict=32, convergence_interval=200, max_steps=50000, learning_rate=0.01, batch_size=32, max_vmap=4, jit=True, random_state=42, scaling=1.0, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax-jit'})\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Following https://arxiv.org/abs/2001.03622.</p> <p>This classifier uses a trainable embedding to encode inputs into quantum states. Training and prediction relies on comparing these states with each other using the fidelity/state overlap:</p> <pre><code>* During training, the embedding is optimised to place data from the same class close together and data\n  from different classes far apart from each other.\n* Prediction compares a new embedded input with memorised training samples from each class and predicts\n  the class whose samples are closest.\n</code></pre> <p>Since pairwise comparison between data points are expensive, training and classification only uses samples from the data.</p> <p>The trainable embedding uses PennyLane's <code>QAOAEmbedding</code>.</p> <p>The classifier uses <code>batch_size*3</code> circuits for an evaluation of the loss function, and <code>n_examples_predict*2</code> circuits for prediction.</p> PARAMETER DESCRIPTION <code>n_examples_predict</code> <p>Number of examples from each class of the training set used for prediction.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>n_layers</code> <p>Number of layers used in the trainable embedding.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>convergence_interval</code> <p>The number of loss values to consider to decide convergence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>max_steps</code> <p>Maximum number of training steps. A warning will be raised if training did not converge.</p> <p> TYPE: <code>int</code> DEFAULT: <code>50000</code> </p> <code>batch_size</code> <p>Size of batches used for computing parameter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>learning_rate</code> <p>Initial learning rate for training.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.01</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>4</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dev_type</code> <p>Wtring specifying the pennylane device type; e.g. 'default.qubit'</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>The keyword arguments passed to the circuit qnode</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax-jit'}</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> METHOD DESCRIPTION <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for batch of data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X using a batch of training examples.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_metric_learning.py</code> <pre><code>def __init__(\n    self,\n    n_layers=3,\n    n_examples_predict=32,\n    convergence_interval=200,\n    max_steps=50000,\n    learning_rate=0.01,\n    batch_size=32,\n    max_vmap=4,\n    jit=True,\n    random_state=42,\n    scaling=1.0,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax-jit\"},\n):\n    \"\"\"\n    Following https://arxiv.org/abs/2001.03622.\n\n    This classifier uses a trainable embedding to encode inputs into quantum states. Training and prediction\n    relies on comparing these states with each other using the fidelity/state overlap:\n\n        * During training, the embedding is optimised to place data from the same class close together and data\n          from different classes far apart from each other.\n        * Prediction compares a new embedded input with memorised training samples from each class and predicts\n          the class whose samples are closest.\n\n    Since pairwise comparison between data points are expensive, training and classification only\n    uses samples from the data.\n\n    The trainable embedding uses PennyLane's `QAOAEmbedding`.\n\n    The classifier uses `batch_size*3` circuits for an evaluation of the loss function, and `n_examples_predict*2`\n    circuits for prediction.\n\n    Args:\n        n_examples_predict (int): Number of examples from each class of the training set used for prediction.\n        n_layers (int): Number of layers used in the trainable embedding.\n        convergence_interval (int): The number of loss values to consider to decide convergence.\n        max_steps (int): Maximum number of training steps. A warning will be raised if training did not converge.\n        batch_size (int): Size of batches used for computing parameter updates.\n        learning_rate (float): Initial learning rate for training.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        dev_type (str): Wtring specifying the pennylane device type; e.g. 'default.qubit'\n        qnode_kwargs (str): The keyword arguments passed to the circuit qnode\n        scaling (float): Factor by which to scale the input data.\n\n    \"\"\"\n\n    # attributes that do not depend on data\n    self.n_examples_predict = n_examples_predict\n    self.n_layers = n_layers\n    self.convergence_interval = convergence_interval\n    self.max_steps = max_steps\n    self.learning_rate = learning_rate\n    self.batch_size = batch_size\n    self.jit = jit\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # Dictionary containing the trainable parameters\n    self.n_qubits_ = None\n    self.n_features_ = None\n    self.scaler = None  # data scaler will be fitted on training data\n    self.circuit = None\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_metric_learning.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n\n    self.initialize(X.shape[1], classes=np.unique(y))\n\n    self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    # split data\n    A = jnp.array(X[y == -1])\n    B = jnp.array(X[y == 1])\n\n    if self.batch_size &gt; min(len(A), len(B)):\n        warnings.warn(\"batch size too large, setting to \" + str(min(len(A), len(B))))\n        self.batch_size = min(len(A), len(B))\n\n    def loss_fn(params, A=None, B=None):\n        aa = self.forward(params, X1=A, X2=A)\n        bb = self.forward(params, X1=B, X2=B)\n        ab = self.forward(params, X1=A, X2=B)\n\n        d_hs = -ab + 0.5 * (aa + bb)\n        return 1 - d_hs\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n\n    optimizer = optax.adam\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        A,\n        B,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    self.params_[\"examples_-1\"] = A\n    self.params_[\"examples_+1\"] = B\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels.</p> PARAMETER DESCRIPTION <code>n_features</code> <p>Number of features that the classifier expects</p> <p> TYPE: <code>int</code> </p> <code>classes</code> <p>class labels that the classifier expects</p> <p> TYPE: <code>array - like</code> DEFAULT: <code>None</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_metric_learning.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_features_ = n_features\n    self.n_qubits_ = (\n        self.n_features_ + 1\n    )  # +1 to add constant features as described in the plots\n    self.initialize_params()\n    self.construct_model()\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for batch of data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_metric_learning.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for batch of data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X using a batch of training examples. The examples are stored in the parameter dictionary.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_metric_learning.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X using a batch of training examples.\n    The examples are stored in the parameter dictionary.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    if \"examples_-1\" not in self.params_:\n        raise ValueError(\"Model cannot predict without fitting to data first.\")\n\n    X = self.transform(X)\n\n    max_examples = min(len(self.params_[\"examples_-1\"]), len(self.params_[\"examples_+1\"]))\n    if self.n_examples_predict &gt; max_examples:\n        warnings.warn(\"n_examples_predict too large, setting to \" + str(max_examples))\n        self.n_examples_predict = max_examples\n\n    A_examples, B_examples = get_batch(\n        self.n_examples_predict,\n        self.params_[\"examples_-1\"],\n        self.params_[\"examples_+1\"],\n        [self.generate_key(), self.generate_key()],\n    )\n\n    predictions = []\n    for x in X:\n        # create list [x, x, x, ...] to get overlaps with A_examples = [a1, a2, a3...] and B_examples\n        x_tiled = jnp.tile(x, (self.n_examples_predict, 1))\n\n        pred_a = jnp.mean(self.chunked_forward(self.params_, A_examples, x_tiled))\n        pred_b = jnp.mean(self.chunked_forward(self.params_, B_examples, x_tiled))\n\n        # normalise to [0,1]\n        predictions.append([pred_a / (pred_a + pred_b), pred_b / (pred_a + pred_b)])\n\n    return jnp.array(predictions)\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_metric_learning.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n            self.scaler.fit(X)\n        X = self.scaler.transform(X)\n    return X * self.scaling\n</code></pre> <code></code> get_batch <pre><code>get_batch(batch_size, A, B, keys)\n</code></pre> <p>Convenience function to get a batch of data.</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quantum_metric_learning.py</code> <pre><code>def get_batch(batch_size, A, B, keys):\n    \"\"\"Convenience function to get a batch of data.\"\"\"\n    if batch_size &gt; min(len(A), len(B)):\n        raise ValueError(\n            f\"Trying to select {batch_size} of {min(len(A), len(B))} data. \"\n            f\"Is the batch size too low?\"\n        )\n    select_A = jax.random.choice(\n        keys[0], jnp.array(range(len(A))), shape=(batch_size,), replace=False\n    )\n    select_B = jax.random.choice(\n        keys[1], jnp.array(range(len(B))), shape=(batch_size,), replace=False\n    )\n    return A[select_A], B[select_B]\n</code></pre> <code></code> quanvolutional_neural_network CLASS DESCRIPTION <code>QuanvolutionalNeuralNetwork</code> <code></code> QuanvolutionalNeuralNetwork <pre><code>QuanvolutionalNeuralNetwork(qkernel_shape=2, n_qchannels=1, rand_depth=10, rand_rot=20, threshold=0.0, kernel_shape=3, output_channels=[32, 64], max_vmap=None, jit=True, learning_rate=0.001, max_steps=10000, convergence_interval=0.001, batch_size=32, random_state=42, scaling=1.0, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax-jit'})\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Quanvolutional Neural network described in https://arxiv.org/pdf/1904.04767v1.pdf.</p> <p>The model is an adaptation of the ConvolutionalNeuralNetwork model. The only difference is that the data is preprocessed by a quantum convolutional layer before being fed to the classical CNN. This quantum preprocessing is a fixed, non-trainiable feature map.</p> <p>The feature map consists of the following:</p> <pre><code>1. A step function that converts the input to 0,1 valued. If the value is below `threshold` it is 0,\notherwise it is 1.\n\n2. A 2d convolution layer is a applied to the binarised data. The filter is given by a random quantum\ncircuit acting on `qkernel_shape*qkernel_shape` qubits (a square grid), and there are `n_qchannels` output\nchannels. The scalar output of the filter is given by the number of ones appearing in the output bitstring\nof the circuit with the highest probability to be sampled. We implement the random quantum circuit\nvia PennyLane's `RandomLayers`.\n\n3. The transformed data is fed into a classical CNN of the same form as ConvolutionalNeuralNetwork and\nthe model is equivalent from that point onwards.\n</code></pre> <p>The input data X should have shape (dataset_size,height*width) and will be reshaped to (dataset_size, height, width, 1) in the model. We assume height=width.</p> PARAMETER DESCRIPTION <code>qkernel_shape</code> <p>The size of the quantum filter: a circuit with <code>qkernel_shape*qkernel_shape</code> qubits will be used.</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>n_qchannels</code> <p>The number of output channels in the quanvolutional layer.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>rand_depth</code> <p>The depth of the random circuit in pennylane.RandomLayers</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>rand_rot</code> <p>The number of random rotations in pennylane.RandomLayers</p> <p> TYPE: <code>int</code> DEFAULT: <code>20</code> </p> <code>threshold</code> <p>The threshold that determines the binarisation of the input data. Since we use a StadardScaler this is set to 0.0 as default.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>kernel_shape</code> <p>the size of the kernel used in the CNN. e.g. kernel_shape=3 uses a 3x3 filter</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>output_channels</code> <p>a list of integers specifying the output size of the convolutional layers in the CNN.</p> <p> TYPE: <code>list[int]</code> DEFAULT: <code>[32, 64]</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>convergence_interval</code> <p>The number of loss values to consider to decide convergence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0.001</code> </p> <code>learning_rate</code> <p>Initial learning rate for training.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> <code>max_steps</code> <p>Maximum number of training steps. A warning will be raised if training did not converge.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> <code>batch_size</code> <p>Size of batches used for computing parameter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the key word arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax-jit'}</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>construct_quanvolutional_layer</code> <p>construct the quantum feature map.</p> <code>construct_random_circuit</code> <p>construct a random circuit to be used as a filter in the quanvolutional layer</p> <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quanvolutional_neural_network.py</code> <pre><code>def __init__(\n    self,\n    qkernel_shape=2,\n    n_qchannels=1,\n    rand_depth=10,\n    rand_rot=20,\n    threshold=0.0,\n    kernel_shape=3,\n    output_channels=[32, 64],\n    max_vmap=None,\n    jit=True,\n    learning_rate=0.001,\n    max_steps=10000,\n    convergence_interval=10e-4,\n    batch_size=32,\n    random_state=42,\n    scaling=1.0,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax-jit\"},\n):\n    r\"\"\"\n    Quanvolutional Neural network described in https://arxiv.org/pdf/1904.04767v1.pdf.\n\n    The model is an adaptation of the ConvolutionalNeuralNetwork model. The only difference is that the data is\n    preprocessed by a quantum convolutional layer before being fed to the classical CNN. This quantum preprocessing\n    is a fixed, non-trainiable feature map.\n\n    The feature map consists of the following:\n\n        1. A step function that converts the input to 0,1 valued. If the value is below `threshold` it is 0,\n        otherwise it is 1.\n\n        2. A 2d convolution layer is a applied to the binarised data. The filter is given by a random quantum\n        circuit acting on `qkernel_shape*qkernel_shape` qubits (a square grid), and there are `n_qchannels` output\n        channels. The scalar output of the filter is given by the number of ones appearing in the output bitstring\n        of the circuit with the highest probability to be sampled. We implement the random quantum circuit\n        via PennyLane's `RandomLayers`.\n\n        3. The transformed data is fed into a classical CNN of the same form as ConvolutionalNeuralNetwork and\n        the model is equivalent from that point onwards.\n\n    The input data X should have shape (dataset_size,height*width) and will be reshaped to\n    (dataset_size, height, width, 1) in the model. We assume height=width.\n\n    Args:\n        qkernel_shape (int): The size of the quantum filter: a circuit with `qkernel_shape*qkernel_shape`\n            qubits will be used.\n        n_qchannels (int): The number of output channels in the quanvolutional layer.\n        rand_depth (int): The depth of the random circuit in pennylane.RandomLayers\n        rand_rot (int): The number of random rotations in pennylane.RandomLayers\n        threshold (float): The threshold that determines the binarisation of the input data. Since we use a\n            StadardScaler this is set to 0.0 as default.\n        kernel_shape (int): the size of the kernel used in the CNN. e.g. kernel_shape=3 uses a 3x3 filter\n        output_channels (list[int]): a list of integers specifying the output size of the convolutional layers\n            in the CNN.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        convergence_interval (int): The number of loss values to consider to decide convergence.\n        learning_rate (float): Initial learning rate for training.\n        max_steps (int): Maximum number of training steps. A warning will be raised if training did not converge.\n        batch_size (int): Size of batches used for computing parameter updates.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'\n        qnode_kwargs (str): the key word arguments passed to the circuit qnode.\n        random_state (int): Seed used for pseudorandom number generation.\n    \"\"\"\n    # attributes that do not depend on data\n    self.learning_rate = learning_rate\n    self.max_steps = max_steps\n    self.convergence_interval = convergence_interval\n    self.n_qchannels = n_qchannels\n    self.rand_depth = rand_depth\n    self.rand_rot = rand_rot\n    self.threshold = threshold\n    self.kernel_shape = kernel_shape\n    self.qkernel_shape = qkernel_shape\n    self.output_channels = output_channels\n    self.batch_size = batch_size\n    self.jit = jit\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.batch_size = batch_size\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # Dictionary containing the trainable parameters\n    self.scaler = None  # data scaler will be fitted on training data\n    self.circuit = None\n</code></pre> <code></code> construct_quanvolutional_layer <pre><code>construct_quanvolutional_layer()\n</code></pre> <p>construct the quantum feature map.</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quanvolutional_neural_network.py</code> <pre><code>def construct_quanvolutional_layer(self):\n    \"\"\"\n    construct the quantum feature map.\n    \"\"\"\n    random_circuits = [self.construct_random_circuit() for __ in range(self.n_qchannels)]\n\n    # construct an array that specifies the indices of the 'windows' of the image used for the convolution.\n    idx_mat = jnp.array([[(i, j) for j in range(self.width)] for i in range(self.height)])\n    idxs = jnp.array(\n        [\n            idx_mat[j : j + self.qkernel_shape, k : k + self.qkernel_shape]\n            for k in range(self.height - self.qkernel_shape + 1)\n            for j in range(self.height - self.qkernel_shape + 1)\n        ]\n    )\n    idxs = idxs.reshape(len(idxs), -1, 2)\n    zerovec = jnp.zeros(len(idxs[0, :, 0]))  # needed for last axis of NHWC format\n    idxs = jnp.array(\n        [[idxs[i, :, 0], idxs[i, :, 1], zerovec] for i in range(len(idxs))],\n        dtype=int,\n    )\n\n    def quanv_layer(x):\n        \"\"\"\n        A convolutional layer where the filter is given by a random quantum circuit. The layer has a stride of 1.\n        Args:\n            x (jnp.array): input data of shape (n_data, height, width, 1)\n        \"\"\"\n\n        # the windows from the image to be fed into the quantum circuits\n        x_windows = x[idxs[:, 0, :], idxs[:, 1, :], idxs[:, 2, :]]\n\n        layer_out = []\n        for channel in range(self.n_qchannels):\n            out = []\n            # find most likely outputs\n            probs = random_circuits[channel](x_windows)\n            # convert to scalars based on the number of ones in the state vec\n            max_idxs = jnp.argmax(probs, axis=1)\n            state_vecs = [\n                jnp.unravel_index(idx, [2 for __ in range(self.qkernel_shape**2)])\n                for idx in max_idxs\n            ]\n            out = jnp.sum(jnp.array(state_vecs), axis=1)\n            out = jnp.array(out, dtype=\"float64\")\n            # put back to correct shape\n            out = jnp.reshape(\n                out,\n                (\n                    self.height - self.qkernel_shape + 1,\n                    self.width - self.qkernel_shape + 1,\n                ),\n            )\n            layer_out.append(out)\n\n        layer_out = jnp.array(layer_out)\n        layer_out = jnp.moveaxis(\n            layer_out, 0, -1\n        )  # the above is in CHW format, so we switch to WHC\n        return layer_out\n\n    return quanv_layer\n</code></pre> <code></code> construct_random_circuit <pre><code>construct_random_circuit()\n</code></pre> <p>construct a random circuit to be used as a filter in the quanvolutional layer</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quanvolutional_neural_network.py</code> <pre><code>def construct_random_circuit(self):\n    \"\"\"\n    construct a random circuit to be used as a filter in the quanvolutional layer\n    \"\"\"\n    wires = range(self.qkernel_shape**2)\n    dev = qml.device(self.dev_type, wires=wires)\n    weights = (\n        jnp.pi\n        * 2\n        * jnp.array(\n            jax.random.uniform(self.generate_key(), shape=(self.rand_depth, self.rand_rot))\n        )\n    )\n\n    @qml.qnode(dev, **self.qnode_kwargs)\n    def circuit(x):\n        \"\"\"\n        Apply a random circuit and return the probabilities of the output strings.\n        Here we use Pennylane's RandomLayers which deviates slightly from the desciption in the plots,\n        but we expect similar behaviour since they are both random circuit generators.\n        \"\"\"\n        for i in wires:\n            qml.RY(x[i] * jnp.pi, wires=i)\n        qml.RandomLayers(weights, wires=wires)\n        return qml.probs(wires=wires)\n\n    self.circuit = circuit\n\n    if self.jit:\n        circuit = jax.jit(circuit)\n    circuit = chunk_vmapped_fn(jax.vmap(circuit, in_axes=(0)), 0, self.max_vmap)\n    return circuit\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quanvolutional_neural_network.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n\n    self.initialize(X.shape[1], classes=np.unique(y))\n\n    y = jnp.array(y, dtype=int)\n    self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    # quantum feature map the entire dataset for training. Assuming we use more than one epoch of training,\n    # it is more efficient to do this first\n    X = self.batched_quanv_layer(X)\n\n    def loss_fn(params, X, y):\n        \"\"\"\n        this takes the quantum feature mapped data as input and returns the sigmoid binary cross entropy\n        \"\"\"\n        y = jax.nn.relu(y)  # convert to 0,1 labels\n        vals = self.cnn.apply(params, X)[:, 0]\n        loss = jnp.mean(optax.sigmoid_binary_cross_entropy(vals, y))\n        return loss\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n\n    optimizer = optax.adam\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        X,\n        y,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels. Args:     classes (array-like): class labels that the classifier expects</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quanvolutional_neural_network.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n    Args:\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.height = int(jnp.sqrt(n_features))\n    self.width = self.height\n\n    # initialise the model\n    self.quanv_layer = self.construct_quanvolutional_layer()\n    self.batched_quanv_layer = chunk_vmapped_fn(\n        jax.vmap(self.quanv_layer, in_axes=(0)), 0, self.max_vmap\n    )\n    self.cnn = construct_cnn(self.output_channels, self.kernel_shape)\n\n    # create dummy data input to initialise the cnn\n    X0 = jnp.ones(shape=(1, self.height, self.height, 1))\n    X0 = self.batched_quanv_layer(X0)\n    self.initialize_params(X0)\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred</code> <p>Predicted labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quanvolutional_neural_network.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred (np.ndarray): Predicted labels of shape (n_samples,)\n    \"\"\"\n    predictions = self.predict_proba(X)\n    mapped_predictions = jnp.argmax(predictions, axis=1)\n    return jnp.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quanvolutional_neural_network.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    p1 = jax.nn.sigmoid(self.forward(self.params_, X)[:, 0])\n    predictions_2d = jnp.c_[1 - p1, p1]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/quanvolutional_neural_network.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n            self.scaler.fit(X)\n        X = self.scaler.transform(X)\n    X = X * self.scaling\n    X = jnp.array(X)\n\n    # put in NHWC format. We assume square images\n    self.height = int(jnp.sqrt(X.shape[1]))\n    self.width = self.height\n    X = jnp.reshape(X, (X.shape[0], self.height, self.width, 1))\n    X = jnp.heaviside(X - self.threshold, 0.0)  # binarise input\n\n    return X\n</code></pre> <code></code> separable CLASS DESCRIPTION <code>SeparableKernelClassifier</code> <code>SeparableVariationalClassifier</code> FUNCTION DESCRIPTION <code>chunk_grad</code> <p>Convert a <code>jax.grad</code> function to an equivalent version that evaluated in chunks of size max_vmap.</p> <code>chunk_loss</code> <p>Converts a loss function of the form <code>loss_fn(params, array1, array2)</code> to an equivalent version that</p> <code>chunk_vmapped_fn</code> <p>Convert a vmapped function to an equivalent function that evaluates in chunks of size</p> <code>get_batch</code> <p>A generator to get random batches of the data (X, y)</p> <code>get_from_dict</code> <p>Access a value from a nested dictionary.</p> <code>get_nested_keys</code> <p>Returns the nested keys of a nested dictionary.</p> <code>set_in_dict</code> <p>Set a value in a nested dictionary.</p> <code>train</code> <p>Trains a model using an optimizer and a loss function via gradient descent. We assume that the loss function</p> <code></code> SeparableKernelClassifier <pre><code>SeparableKernelClassifier(encoding_layers=1, svm=SVC(kernel='precomputed', probability=True), C=1.0, jit=True, random_state=42, scaling=1.0, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax', 'diff_method': None})\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>A kernel model that uses a separable embedding. The embedding consists of layers of fixed single qubit X rotations by an angle :math:<code>pi/4</code> on the Bloch sphere and single qubit Y rotation data encoding gates.</p> <p>The kernel is given by</p> <p>.. math::     k(x,x')=\\vert\\langle 0 \\vert U^\\dagger(x')U(x)\\vert 0 \\rangle\\vert^2</p> PARAMETER DESCRIPTION <code>encoding_layers</code> <p>number of layers in the data encoding circuit.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>svm</code> <p>scikit-learn SVC class object used to fit the model from the kernel matrix.</p> <p> TYPE: <code>SVC</code> DEFAULT: <code>SVC(kernel='precomputed', probability=True)</code> </p> <code>C</code> <p>regularization parameter for the SVC. Lower values imply stronger regularization.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the keyword arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax', 'diff_method': None}</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> METHOD DESCRIPTION <code>fit</code> <p>Fit the model to data X and labels y. Uses sklearn's SVM classifier</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>precompute_kernel</code> <p>compute the kernel matrix relative to data sets X1 and X2</p> <code>predict</code> <p>Predict labels for data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/separable.py</code> <pre><code>def __init__(\n    self,\n    encoding_layers=1,\n    svm=SVC(kernel=\"precomputed\", probability=True),\n    C=1.0,\n    jit=True,\n    random_state=42,\n    scaling=1.0,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax\", \"diff_method\": None},\n):\n    r\"\"\"\n    A kernel model that uses a separable embedding. The embedding consists of layers of fixed single qubit X\n    rotations by an angle :math:`pi/4` on the Bloch sphere and single qubit Y rotation data encoding gates.\n\n    The kernel is given by\n\n    .. math::\n        k(x,x')=\\vert\\langle 0 \\vert U^\\dagger(x')U(x)\\vert 0 \\rangle\\vert^2\n\n\n    Args:\n        encoding_layers (int): number of layers in the data encoding circuit.\n        svm (sklearn.svm.SVC): scikit-learn SVC class object used to fit the model from the kernel matrix.\n        C (float): regularization parameter for the SVC. Lower values imply stronger regularization.\n        jit (bool): Whether to use just in time compilation.\n        random_state (int): Seed used for pseudorandom number generation.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'.\n        qnode_kwargs (str): the keyword arguments passed to the circuit qnode.\n        scaling (float): Factor by which to scale the input data.\n    \"\"\"\n    # attributes that do not depend on data\n    self.encoding_layers = encoding_layers\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.jit = jit\n    self.svm = svm\n    self.C = C\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # Dictionary containing the trainable parameters\n    self.n_qubits_ = None\n    self.scaler = None  # data scaler will be fitted on training data\n    self.circuit = None\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y. Uses sklearn's SVM classifier</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/separable.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y. Uses sklearn's SVM classifier\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n\n    self.svm.random_state = int(\n        jax.random.randint(self.generate_key(), shape=(1,), minval=0, maxval=1000000)\n    )\n\n    self.initialize(X.shape[1], np.unique(y))\n\n    self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    self.params_ = {\"x_train\": X}\n    kernel_matrix = self.precompute_kernel(X, X)\n\n    start = time.time()\n    # we are updating this value here, in case it was\n    # changed after initialising the model\n    self.svm.C = self.C\n    self.svm.fit(kernel_matrix, y)\n    end = time.time()\n    self.training_time_ = end - start\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels.</p> PARAMETER DESCRIPTION <code>n_features</code> <p>Number of features that the classifier expects</p> <p> TYPE: <code>int</code> </p> <code>classes</code> <p>class labels that the classifier expects</p> <p> TYPE: <code>array - like</code> DEFAULT: <code>None</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/separable.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_qubits_ = n_features\n    self.params_ = {}\n    self.construct_circuit()\n</code></pre> <code></code> precompute_kernel <pre><code>precompute_kernel(X1, X2)\n</code></pre> <p>compute the kernel matrix relative to data sets X1 and X2 Args:     X1 (np.array): first dataset of input vectors     X2 (np.array): second dataset of input vectors Returns:     kernel_matrix (np.array): matrix of size (len(X1),len(X2)) with elements K(x_1,x_2)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/separable.py</code> <pre><code>def precompute_kernel(self, X1, X2):\n    \"\"\"\n    compute the kernel matrix relative to data sets X1 and X2\n    Args:\n        X1 (np.array): first dataset of input vectors\n        X2 (np.array): second dataset of input vectors\n    Returns:\n        kernel_matrix (np.array): matrix of size (len(X1),len(X2)) with elements K(x_1,x_2)\n    \"\"\"\n    dim1 = len(X1)\n    dim2 = len(X2)\n\n    # concatenate all pairs of vectors\n    Z = np.array([np.concatenate((X1[i], X2[j])) for i in range(dim1) for j in range(dim2)])\n    self.construct_circuit()\n    kernel_values = [self.forward(z) for z in Z]\n    # reshape the values into the kernel matrix\n    kernel_matrix = np.reshape(kernel_values, (dim1, dim2))\n\n    return kernel_matrix\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred</code> <p>Predicted labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/separable.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred (np.ndarray): Predicted labels of shape (n_samples,)\n    \"\"\"\n    X = self.transform(X)\n    kernel_matrix = self.precompute_kernel(X, self.params_[\"x_train\"])\n    return self.svm.predict(kernel_matrix)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X. note that this may be inconsistent with predict; see the sklearn docummentation for details.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/separable.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n    note that this may be inconsistent with predict; see the sklearn docummentation for details.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    if \"x_train\" not in self.params_:\n        raise ValueError(\"Model cannot predict without fitting to data first.\")\n\n    X = self.transform(X)\n    kernel_matrix = self.precompute_kernel(X, self.params_[\"x_train\"])\n    return self.svm.predict_proba(kernel_matrix)\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/separable.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n            self.scaler.fit(X)\n        X = self.scaler.transform(X)\n    return X * self.scaling\n</code></pre> <code></code> SeparableVariationalClassifier <pre><code>SeparableVariationalClassifier(encoding_layers=1, learning_rate=0.001, batch_size=32, max_vmap=None, jit=True, max_steps=10000, random_state=42, scaling=1.0, convergence_interval=200, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax'})\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Variational model that uses only separable operations (i.e. there is no entanglement in the model). The circuit consists of layers of encoding gates and parameterised unitaries followed by measurement of an observable.</p> <p>Each encoding layer consists of a trainiable arbitrary qubit rotation on each qubit followed by a product angle embedding of the input data, using RY gates. A final layer of trainable qubit rotations is applied at the end of the circuit.</p> <p>The obserable O is the mean value of Pauli Z observables on each of the output qubits. The value of this observable is used to predict the probability for class 1 as :math:<code>P(+1)=\\sigma(6\\langle O \\rangle)</code> where :math<code>\\sigma</code> is the logistic funciton. The model is then fit using the cross entropy loss.</p> PARAMETER DESCRIPTION <code>encoding_layers</code> <p>number of layers in the data encoding circuit.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>learning_rate</code> <p>learning rate for gradient descent.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.001</code> </p> <code>batch_size</code> <p>Size of batches used for computing parameter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>convergence_interval</code> <p>The number of loss values to consider to decide convergence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>max_steps</code> <p>Maximum number of training steps. A warning will be raised if training did not converge.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the keyword arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax'}</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/separable.py</code> <pre><code>def __init__(\n    self,\n    encoding_layers=1,\n    learning_rate=0.001,\n    batch_size=32,\n    max_vmap=None,\n    jit=True,\n    max_steps=10000,\n    random_state=42,\n    scaling=1.0,\n    convergence_interval=200,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax\"},\n):\n    r\"\"\"\n    Variational model that uses only separable operations (i.e. there is no entanglement in the model). The circuit\n    consists of layers of encoding gates and parameterised unitaries followed by measurement of an observable.\n\n    Each encoding layer consists of a trainiable arbitrary qubit rotation on each qubit followed by\n    a product angle embedding of the input data, using RY gates. A final layer of trainable qubit rotations is\n    applied at the end of the circuit.\n\n    The obserable O is the mean value of Pauli Z observables on each of the output qubits. The value of this\n    observable is used to predict the probability for class 1 as :math:`P(+1)=\\sigma(6\\langle O \\rangle)`\n    where :math`\\sigma` is the logistic funciton. The model is then fit using the cross entropy loss.\n\n    Args:\n        encoding_layers (int): number of layers in the data encoding circuit.\n        learning_rate (float): learning rate for gradient descent.\n        batch_size (int): Size of batches used for computing parameter updates.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        convergence_interval (int): The number of loss values to consider to decide convergence.\n        max_steps (int): Maximum number of training steps. A warning will be raised if training did not converge.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'.\n        qnode_kwargs (str): the keyword arguments passed to the circuit qnode.\n        random_state (int): Seed used for pseudorandom number generation\n    \"\"\"\n    # attributes that do not depend on data\n    self.encoding_layers = encoding_layers\n    self.learning_rate = learning_rate\n    self.batch_size = batch_size\n    self.max_steps = max_steps\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.jit = jit\n    self.convergence_interval = convergence_interval\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # Dictionary containing the trainable parameters\n    self.n_qubits_ = None\n    self.scaler = None  # data scaler will be fitted on training data\n    self.circuit = None\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/separable.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n    self.initialize(X.shape[1], np.unique(y))\n\n    self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    optimizer = optax.adam\n\n    def loss_fn(params, X, y):\n        # we multiply by 6 because a relevant domain of the sigmoid function is [-6,6]\n        vals = self.forward(params, X) * 6\n        y = jax.nn.relu(y)  # convert to 0,1\n        return jnp.mean(optax.sigmoid_binary_cross_entropy(vals, y))\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        X,\n        y,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels.</p> PARAMETER DESCRIPTION <code>n_features</code> <p>Number of features that the classifier expects</p> <p> TYPE: <code>int</code> </p> <code>classes</code> <p>class labels that the classifier expects</p> <p> TYPE: <code>array - like</code> DEFAULT: <code>None</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/separable.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_qubits_ = n_features\n    self.initialize_params()\n    self.construct_model()\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred</code> <p>Predicted labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/separable.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred (np.ndarray): Predicted labels of shape (n_samples,)\n    \"\"\"\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/separable.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    predictions = self.chunked_forward(self.params_, X)\n    predictions_2d = np.c_[(1 - predictions) / 2, (1 + predictions) / 2]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/separable.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n            self.scaler.fit(X)\n        X = self.scaler.transform(X)\n    return X * self.scaling\n</code></pre> <code></code> chunk_grad <pre><code>chunk_grad(grad_fn, max_vmap)\n</code></pre> <p>Convert a <code>jax.grad</code> function to an equivalent version that evaluated in chunks of size max_vmap.</p> <p><code>grad_fn</code> should be of the form <code>jax.grad(fn(params, X, y), argnums=0)</code>, where <code>params</code> is a dictionary of <code>jnp.arrays</code>, <code>X, y</code> are <code>jnp.arrays</code> with the same-size leading axis, and <code>grad_fn</code> is a function that is vectorised along these axes (i.e. <code>in_axes = (None,0,0)</code>).</p> <p>The returned function evaluates the original function by splitting the batch evaluation into smaller chunks of size <code>max_vmap</code>, and has a lower memory footprint.</p> PARAMETER DESCRIPTION <code>model</code> <p>gradient function with the functional form jax.grad(loss(params, X,y), argnums=0)</p> <p> TYPE: <code>func</code> </p> <code>max_vmap</code> <p>the size of the chunks</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <p>chunked version of the function</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def chunk_grad(grad_fn, max_vmap):\n    \"\"\"\n    Convert a `jax.grad` function to an equivalent version that evaluated in chunks of size max_vmap.\n\n    `grad_fn` should be of the form `jax.grad(fn(params, X, y), argnums=0)`, where `params` is a\n    dictionary of `jnp.arrays`, `X, y` are `jnp.arrays` with the same-size leading axis, and `grad_fn`\n    is a function that is vectorised along these axes (i.e. `in_axes = (None,0,0)`).\n\n    The returned function evaluates the original function by splitting the batch evaluation into smaller chunks\n    of size `max_vmap`, and has a lower memory footprint.\n\n    Args:\n        model (func): gradient function with the functional form jax.grad(loss(params, X,y), argnums=0)\n        max_vmap (int): the size of the chunks\n\n    Returns:\n        chunked version of the function\n    \"\"\"\n\n    def chunked_grad(params, X, y):\n        batch_slices = list(gen_batches(len(X), max_vmap))\n        grads = [grad_fn(params, X[slice], y[slice]) for slice in batch_slices]\n        grad_dict = {}\n        for key_list in get_nested_keys(params):\n            set_in_dict(\n                grad_dict,\n                key_list,\n                jnp.mean(jnp.array([get_from_dict(grad, key_list) for grad in grads]), axis=0),\n            )\n        return grad_dict\n\n    return chunked_grad\n</code></pre> <code></code> chunk_loss <pre><code>chunk_loss(loss_fn, max_vmap)\n</code></pre> <p>Converts a loss function of the form <code>loss_fn(params, array1, array2)</code> to an equivalent version that evaluates <code>loss_fn</code> in chunks of size max_vmap. <code>loss_fn</code> should batch evaluate along the leading axis of <code>array1, array2</code> (i.e. <code>in_axes = (None,0,0)</code>).</p> PARAMETER DESCRIPTION <code>loss_fn</code> <p>function of form loss_fn(params, array1, array2)</p> <p> TYPE: <code>func</code> </p> <code>max_vmap</code> <p>maximum chunk size</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <p>chunked version of the function</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def chunk_loss(loss_fn, max_vmap):\n    \"\"\"\n    Converts a loss function of the form `loss_fn(params, array1, array2)` to an equivalent version that\n    evaluates `loss_fn` in chunks of size max_vmap. `loss_fn` should batch evaluate along the leading\n    axis of `array1, array2` (i.e. `in_axes = (None,0,0)`).\n\n    Args:\n        loss_fn (func): function of form loss_fn(params, array1, array2)\n        max_vmap (int): maximum chunk size\n\n    Returns:\n        chunked version of the function\n    \"\"\"\n\n    def chunked_loss(params, X, y):\n        batch_slices = list(gen_batches(len(X), max_vmap))\n        res = jnp.array([loss_fn(params, *[X[slice], y[slice]]) for slice in batch_slices])\n        return jnp.mean(res)\n\n    return chunked_loss\n</code></pre> <code></code> chunk_vmapped_fn <pre><code>chunk_vmapped_fn(vmapped_fn, start, max_vmap)\n</code></pre> <p>Convert a vmapped function to an equivalent function that evaluates in chunks of size max_vmap. The behaviour of chunked_fn should be the same as vmapped_fn, but with a lower memory cost.</p> <p>The input vmapped_fn should have in_axes = (None, None, ..., 0,0,...,0)</p> PARAMETER DESCRIPTION <code>vmapped</code> <p>vmapped function with in_axes = (None, None, ..., 0,0,...,0)</p> <p> TYPE: <code>func</code> </p> <code>start</code> <p>The index where the first 0 appears in in_axes</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <p>chunked version of the function</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def chunk_vmapped_fn(vmapped_fn, start, max_vmap):\n    \"\"\"\n    Convert a vmapped function to an equivalent function that evaluates in chunks of size\n    max_vmap. The behaviour of chunked_fn should be the same as vmapped_fn, but with a\n    lower memory cost.\n\n    The input vmapped_fn should have in_axes = (None, None, ..., 0,0,...,0)\n\n    Args:\n        vmapped (func): vmapped function with in_axes = (None, None, ..., 0,0,...,0)\n        start (int): The index where the first 0 appears in in_axes\n        max_vmap (int) The max chunk size with which to evaluate the function\n\n    Returns:\n        chunked version of the function\n    \"\"\"\n\n    def chunked_fn(*args):\n        batch_len = len(args[start])\n        batch_slices = list(gen_batches(batch_len, max_vmap))\n        res = [\n            vmapped_fn(*args[:start], *[arg[slice] for arg in args[start:]])\n            for slice in batch_slices\n        ]\n        # jnp.concatenate needs to act on arrays with the same shape, so pad the last array if necessary\n        if batch_len / max_vmap % 1 != 0.0:\n            diff = max_vmap - len(res[-1])\n            res[-1] = jnp.pad(res[-1], [(0, diff), *[(0, 0)] * (len(res[-1].shape) - 1)])\n            return jnp.concatenate(res)[:-diff]\n        return jnp.concatenate(res)\n\n    return chunked_fn\n</code></pre> <code></code> get_batch <pre><code>get_batch(X, y, rnd_key, batch_size=32)\n</code></pre> <p>A generator to get random batches of the data (X, y)</p> PARAMETER DESCRIPTION <code>X</code> <p>Input data with shape (n_samples, n_features).</p> <p> TYPE: <code>array[float]</code> </p> <code>y</code> <p>Target labels with shape (n_samples,)</p> <p> TYPE: <code>array[float]</code> </p> <code>rnd_key</code> <p>A jax random key object</p> <p> </p> <code>batch_size</code> <p>Number of elements in batch</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> RETURNS DESCRIPTION <p>array[float]: A batch of input data shape (batch_size, n_features)</p> <p>array[float]: A batch of target labels shaped (batch_size,)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def get_batch(X, y, rnd_key, batch_size=32):\n    \"\"\"\n    A generator to get random batches of the data (X, y)\n\n    Args:\n        X (array[float]): Input data with shape (n_samples, n_features).\n        y (array[float]): Target labels with shape (n_samples,)\n        rnd_key: A jax random key object\n        batch_size (int): Number of elements in batch\n\n    Returns:\n        array[float]: A batch of input data shape (batch_size, n_features)\n        array[float]: A batch of target labels shaped (batch_size,)\n    \"\"\"\n    all_indices = jnp.array(range(len(X)))\n    rnd_indices = jax.random.choice(key=rnd_key, a=all_indices, shape=(batch_size,), replace=True)\n    return X[rnd_indices], y[rnd_indices]\n</code></pre> <code></code> get_from_dict <pre><code>get_from_dict(dict, key_list)\n</code></pre> <p>Access a value from a nested dictionary. Inspired by https://stackoverflow.com/questions/14692690/access-nested-dictionary-items-via-a-list-of-keys</p> PARAMETER DESCRIPTION <code>dict</code> <p>nested dictionary</p> <p> TYPE: <code>dict</code> </p> <code>key_list</code> <p>list of keys to be accessed</p> <p> TYPE: <code>list</code> </p> RETURNS DESCRIPTION <p>the requested value</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def get_from_dict(dict, key_list):\n    \"\"\"\n    Access a value from a nested dictionary.\n    Inspired by https://stackoverflow.com/questions/14692690/access-nested-dictionary-items-via-a-list-of-keys\n\n    Args:\n        dict (dict): nested dictionary\n        key_list (list): list of keys to be accessed\n\n    Returns:\n         the requested value\n    \"\"\"\n    return reduce(operator.getitem, key_list, dict)\n</code></pre> <code></code> get_nested_keys <pre><code>get_nested_keys(d, parent_keys=[])\n</code></pre> <p>Returns the nested keys of a nested dictionary.</p> PARAMETER DESCRIPTION <code>d</code> <p>nested dictionary</p> <p> TYPE: <code>dict</code> </p> RETURNS DESCRIPTION <p>list where each element is a list of nested keys</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def get_nested_keys(d, parent_keys=[]):\n    \"\"\"\n    Returns the nested keys of a nested dictionary.\n\n    Args:\n        d (dict): nested dictionary\n\n    Returns:\n        list where each element is a list of nested keys\n    \"\"\"\n    keys_list = []\n    for key, value in d.items():\n        current_keys = parent_keys + [key]\n        if isinstance(value, dict):\n            keys_list.extend(get_nested_keys(value, current_keys))\n        else:\n            keys_list.append(current_keys)\n    return keys_list\n</code></pre> <code></code> set_in_dict <pre><code>set_in_dict(dict, keys, value)\n</code></pre> <p>Set a value in a nested dictionary.</p> PARAMETER DESCRIPTION <code>dict</code> <p>nested dictionary</p> <p> TYPE: <code>dict</code> </p> <code>keys</code> <p>list of keys in nested dictionary</p> <p> TYPE: <code>list</code> </p> <code>value</code> <p>value to be set</p> <p> TYPE: <code>Any</code> </p> RETURNS DESCRIPTION <p>nested dictionary with new value</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def set_in_dict(dict, keys, value):\n    \"\"\"\n    Set a value in a nested dictionary.\n\n    Args:\n        dict (dict): nested dictionary\n        keys (list): list of keys in nested dictionary\n        value (Any): value to be set\n\n    Returns:\n        nested dictionary with new value\n    \"\"\"\n    for key in keys[:-1]:\n        dict = dict.setdefault(key, {})\n    dict[keys[-1]] = value\n</code></pre> <code></code> train <pre><code>train(model, loss_fn, optimizer, X, y, random_key_generator, convergence_interval=200)\n</code></pre> <p>Trains a model using an optimizer and a loss function via gradient descent. We assume that the loss function is of the form <code>loss(params, X, y)</code> and that the trainable parameters are stored in model.params_ as a dictionary of jnp.arrays. The optimizer should be an Optax optimizer (e.g. optax.adam). <code>model</code> must have an attribute <code>learning_rate</code> to set the initial learning rate for the gradient descent.</p> <p>The model is trained until a convergence criterion is met that corresponds to the loss curve being flat over a number of optimization steps given by <code>convergence_inteval</code> (see plots for details).</p> <p>To reduce precompilation time and memory cost, the loss function and gradient functions are evaluated in chunks of size model.max_vmap.</p> PARAMETER DESCRIPTION <code>model</code> <p>Classifier class object to train. Trainable parameters must be stored in model.params_.</p> <p> TYPE: <code>class</code> </p> <code>loss_fn</code> <p>Loss function to be minimised. Must be of the form loss_fn(params, X, y).</p> <p> TYPE: <code>Callable</code> </p> <code>optimizer</code> <p>Optax optimizer (e.g. optax.adam).</p> <p> TYPE: <code>optax optimizer</code> </p> <code>X</code> <p>Input data array of shape (n_samples, n_features)</p> <p> TYPE: <code>array</code> </p> <code>y</code> <p>Array of shape (n_samples) containing the labels.</p> <p> TYPE: <code>array</code> </p> <code>random_key_generator</code> <p>JAX key generator object for pseudo-randomness generation.</p> <p> TYPE: <code>PRNGKey</code> </p> <code>convergence_interval</code> <p>Number of optimization steps over which to decide convergence. Larger values give a higher confidence that the model has converged but may increase training time.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> RETURNS DESCRIPTION <code>params</code> <p>The new parameters after training has completed.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def train(model, loss_fn, optimizer, X, y, random_key_generator, convergence_interval=200):\n    \"\"\"\n    Trains a model using an optimizer and a loss function via gradient descent. We assume that the loss function\n    is of the form `loss(params, X, y)` and that the trainable parameters are stored in model.params_ as a dictionary\n    of jnp.arrays. The optimizer should be an Optax optimizer (e.g. optax.adam). `model` must have an attribute\n    `learning_rate` to set the initial learning rate for the gradient descent.\n\n    The model is trained until a convergence criterion is met that corresponds to the loss curve being flat over\n    a number of optimization steps given by `convergence_inteval` (see plots for details).\n\n    To reduce precompilation time and memory cost, the loss function and gradient functions are evaluated in\n    chunks of size model.max_vmap.\n\n    Args:\n        model (class): Classifier class object to train. Trainable parameters must be stored in model.params_.\n        loss_fn (Callable): Loss function to be minimised. Must be of the form loss_fn(params, X, y).\n        optimizer (optax optimizer): Optax optimizer (e.g. optax.adam).\n        X (array): Input data array of shape (n_samples, n_features)\n        y (array): Array of shape (n_samples) containing the labels.\n        random_key_generator (jax.random.PRNGKey): JAX key generator object for pseudo-randomness generation.\n        convergence_interval (int, optional): Number of optimization steps over which to decide convergence. Larger\n            values give a higher confidence that the model has converged but may increase training time.\n\n    Returns:\n        params (dict): The new parameters after training has completed.\n    \"\"\"\n\n    if not model.batch_size / model.max_vmap % 1 == 0:\n        raise Exception(\"Batch size must be multiple of max_vmap.\")\n\n    params = model.params_\n    opt = optimizer(learning_rate=model.learning_rate)\n    opt_state = opt.init(params)\n    grad_fn = jax.grad(loss_fn)\n\n    # jitting through the chunked_grad function can take a long time,\n    # so we jit here and chunk after\n    if model.jit:\n        grad_fn = jax.jit(grad_fn)\n\n    # note: assumes that the loss function is a sample mean of\n    # some function over the input data set\n    chunked_grad_fn = chunk_grad(grad_fn, model.max_vmap)\n    chunked_loss_fn = chunk_loss(loss_fn, model.max_vmap)\n\n    def update(params, opt_state, x, y):\n        grads = chunked_grad_fn(params, x, y)\n        loss_val = chunked_loss_fn(params, x, y)\n        updates, opt_state = opt.update(grads, opt_state)\n        params = optax.apply_updates(params, updates)\n        return params, opt_state, loss_val\n\n    loss_history = []\n    converged = False\n    start = time.time()\n    for step in range(model.max_steps):\n        key = random_key_generator()\n        X_batch, y_batch = get_batch(X, y, key, batch_size=model.batch_size)\n        params, opt_state, loss_val = update(params, opt_state, X_batch, y_batch)\n        loss_history.append(loss_val)\n        logging.debug(f\"{step} - loss: {loss_val}\")\n\n        if np.isnan(loss_val):\n            logging.info(\"nan encountered. Training aborted.\")\n            break\n\n        # decide convergence\n        if step &gt; 2 * convergence_interval:\n            # get means of last two intervals and standard deviation of last interval\n            average1 = np.mean(loss_history[-convergence_interval:])\n            average2 = np.mean(loss_history[-2 * convergence_interval : -convergence_interval])\n            std1 = np.std(loss_history[-convergence_interval:])\n            # if the difference in averages is small compared to the statistical fluctuations, stop training.\n            if np.abs(average2 - average1) &lt;= std1 / np.sqrt(convergence_interval) / 2:\n                logging.info(f\"Model {model.__class__.__name__} converged after {step} steps.\")\n                converged = True\n                break\n\n    end = time.time()\n    loss_history = np.array(loss_history)\n    model.loss_history_ = loss_history / np.max(np.abs(loss_history))\n    model.training_time_ = end - start\n\n    if not converged:\n        print(\"Loss did not converge:\", loss_history)\n        raise ConvergenceWarning(\n            f\"Model {model.__class__.__name__} has not converged after the maximum number of {model.max_steps} steps.\"\n        )\n\n    return params\n</code></pre> <code></code> tree_tensor CLASS DESCRIPTION <code>TreeTensorClassifier</code> <code></code> TreeTensorClassifier <pre><code>TreeTensorClassifier(learning_rate=0.01, batch_size=32, max_steps=10000, convergence_interval=200, random_state=42, max_vmap=None, jit=True, scaling=1.0, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax-jit'})\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Tree tensor network classifier from https://arxiv.org/abs/2011.06258v2 (see figure 1)</p> <p>This is a variational model where data is amplitude embedded and a trainiable circuit with a tree like structure is used for prediction. Due to the tree structure, the number of qubits must be a power of 2.</p> <p>In the plots, the data encoding state for a given input x is approximated  using a variational circuit. In practice, this means one has to train an encoding circuit for every training and test input. Since this is very expensive, we just use the exact amplitude encoded state. If the input data dimension is smaller than the state vector dimension, we pad with a value :math:<code>1/2^{n_qubits}</code>.</p> <p>The classification is performed via a Z measurement on the first qubit plus a trainable bias. Training is via the square loss.</p> PARAMETER DESCRIPTION <code>learning_rate</code> <p>Initial learning rate for gradient descent.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.01</code> </p> <code>batch_size</code> <p>Size of batches used for computing parameter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>convergence_interval</code> <p>The number of loss values to consider to decide convergence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>max_steps</code> <p>Maximum number of training steps. A warning will be raised if training did not converge.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the key word arguments passed to the circuit qnode</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax-jit'}</code> </p> METHOD DESCRIPTION <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>The feature vectors padded to the next power of 2 and then normalised.</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/tree_tensor.py</code> <pre><code>def __init__(\n    self,\n    learning_rate=0.01,\n    batch_size=32,\n    max_steps=10000,\n    convergence_interval=200,\n    random_state=42,\n    max_vmap=None,\n    jit=True,\n    scaling=1.0,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax-jit\"},\n):\n    r\"\"\"\n    Tree tensor network classifier from https://arxiv.org/abs/2011.06258v2 (see figure 1)\n\n    This is a variational model where data is amplitude embedded and a trainiable circuit with a tree like\n    structure is used for prediction. Due to the tree structure, the number of qubits must be a power of 2.\n\n    In the plots, the data encoding state for a given input x is approximated  using a variational circuit.\n    In practice, this means one has to train an encoding circuit for every training and test\n    input. Since this is very expensive, we just use the exact amplitude encoded state. If the input data\n    dimension is smaller than the state vector dimension, we pad with a value :math:`1/2^{n_qubits}`.\n\n    The classification is performed via a Z measurement on the first qubit plus a trainable bias. Training\n    is via the square loss.\n\n    Args:\n        learning_rate (float): Initial learning rate for gradient descent.\n        batch_size (int): Size of batches used for computing parameter updates.\n        convergence_interval (int): The number of loss values to consider to decide convergence.\n        max_steps (int): Maximum number of training steps. A warning will be raised if training did not converge.\n        random_state (int): Seed used for pseudorandom number generation\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        scaling (float): Factor by which to scale the input data.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'\n        qnode_kwargs (str): the key word arguments passed to the circuit qnode\n    \"\"\"\n\n    # attributes that do not depend on data\n    self.learning_rate = learning_rate\n    self.max_steps = max_steps\n    self.convergence_interval = convergence_interval\n    self.batch_size = batch_size\n    self.dev_type = dev_type\n    self.jit = jit\n    self.qnode_kwargs = qnode_kwargs\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # Dictionary containing the trainable parameters\n    self.circuit = None\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/tree_tensor.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n    self.initialize(X.shape[1], classes=np.unique(y))\n\n    X = self.transform(X)\n\n    def loss_fn(params, X, y):\n        # square loss\n        predictions = self.forward(params, X)\n        return jnp.mean((predictions - y) ** 2)\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    optimizer = optax.adam\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        X,\n        y,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels.</p> PARAMETER DESCRIPTION <code>n_features</code> <p>Number of features that the classifier expects</p> <p> TYPE: <code>int</code> </p> <code>classes</code> <p>class labels that the classifier expects</p> <p> TYPE: <code>array - like</code> DEFAULT: <code>None</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/tree_tensor.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    if n_features == 1:\n        self.n_qubits = 1\n    else:\n        n_qubits_ae = int(\n            np.ceil(np.log2(n_features))\n        )  # the num qubits needed to amplitude encode\n        n_qubits = 2 ** int(\n            np.ceil(np.log2(n_qubits_ae))\n        )  # the model needs 2**m qubits, for some m\n        self.n_qubits = n_qubits\n\n    self.initialize_params()\n    self.construct_model()\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred</code> <p>Predicted labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/tree_tensor.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred (np.ndarray): Predicted labels of shape (n_samples,)\n    \"\"\"\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/tree_tensor.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    predictions = self.chunked_forward(self.params_, X)\n    predictions_2d = np.c_[(1 - predictions) / 2, (1 + predictions) / 2]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> <p>The feature vectors padded to the next power of 2 and then normalised.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/tree_tensor.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    The feature vectors padded to the next power of 2 and then normalised.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    n_features = X.shape[1]\n    X = X * self.scaling\n\n    n_qubits_ae = int(np.ceil(np.log2(n_features)))  # the num qubits needed to amplitude encode\n    n_qubits = 2 ** int(\n        np.ceil(np.log2(n_qubits_ae))\n    )  # the model needs 2**m qubits, for some m\n    max_n_features = 2**n_qubits\n    n_padding = max_n_features - n_features\n    padding = np.ones(shape=(len(X), n_padding)) / max_n_features\n\n    X_padded = np.c_[X, padding]\n    X_normalised = np.divide(X_padded, np.expand_dims(np.linalg.norm(X_padded, axis=1), axis=1))\n    return X_normalised\n</code></pre> <code></code> vanilla_qnn CLASS DESCRIPTION <code>VanillaQNN</code> FUNCTION DESCRIPTION <code>chunk_grad</code> <p>Convert a <code>jax.grad</code> function to an equivalent version that evaluated in chunks of size max_vmap.</p> <code>chunk_loss</code> <p>Converts a loss function of the form <code>loss_fn(params, array1, array2)</code> to an equivalent version that</p> <code>chunk_vmapped_fn</code> <p>Convert a vmapped function to an equivalent function that evaluates in chunks of size</p> <code>get_batch</code> <p>A generator to get random batches of the data (X, y)</p> <code>get_from_dict</code> <p>Access a value from a nested dictionary.</p> <code>get_nested_keys</code> <p>Returns the nested keys of a nested dictionary.</p> <code>set_in_dict</code> <p>Set a value in a nested dictionary.</p> <code>train</code> <p>Trains a model using an optimizer and a loss function via gradient descent. We assume that the loss function</p> <code></code> VanillaQNN <pre><code>VanillaQNN(embedding_layers=2, variational_layers=3, learning_rate=0.01, batch_size=32, max_vmap=None, jit=True, max_steps=10000, convergence_threshold=1e-06, random_state=42, scaling=1.0, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax'})\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>A vanilla implementation of a quantum neural network with layer-wise angle embedding and a layered variational circuit.</p> PARAMETER DESCRIPTION <code>embedding_layers</code> <p>number of times to repeat the embedding circuit structure.</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>variational_layers</code> <p>number of layers in the variational part of the circuit.</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>learning_rate</code> <p>learning rate for gradient descent.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.01</code> </p> <code>batch_size</code> <p>Number of data points to subsample.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>max_vmap</code> <p>The largest size of vmap used (to control memory)</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>convergence_threshold</code> <p>If loss changes less than this threshold for 10 consecutive steps we stop training.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-06</code> </p> <code>max_steps</code> <p>Maximum number of training steps. A warning will be raised if training did not converge.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the keyword arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax'}</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> METHOD DESCRIPTION <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>predict</code> <p>Predict labels for data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/vanilla_qnn.py</code> <pre><code>def __init__(\n    self,\n    embedding_layers=2,\n    variational_layers=3,\n    learning_rate=0.01,\n    batch_size=32,\n    max_vmap=None,\n    jit=True,\n    max_steps=10000,\n    convergence_threshold=1e-6,\n    random_state=42,\n    scaling=1.0,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax\"},\n):\n    \"\"\"\n    A vanilla implementation of a quantum neural network with layer-wise angle embedding and a layered\n    variational circuit.\n\n    Args:\n        embedding_layers (int): number of times to repeat the embedding circuit structure.\n        variational_layers (int): number of layers in the variational part of the circuit.\n        learning_rate (float): learning rate for gradient descent.\n        batch_size (int): Number of data points to subsample.\n        max_vmap (int): The largest size of vmap used (to control memory)\n        jit (bool): Whether to use just in time compilation.\n        convergence_threshold (float): If loss changes less than this threshold for 10 consecutive steps we stop training.\n        max_steps (int): Maximum number of training steps. A warning will be raised if training did not converge.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'.\n        qnode_kwargs (str): the keyword arguments passed to the circuit qnode.\n        random_state (int): Seed used for pseudorandom number generation.\n    \"\"\"\n\n    # attributes that do not depend on data\n    self.embedding_layers = embedding_layers\n    self.variational_layers = variational_layers\n    self.learning_rate = learning_rate\n    self.batch_size = batch_size\n    self.max_steps = max_steps\n    self.convergence_threshold = convergence_threshold\n    self.batch_size = batch_size\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.jit = jit\n    self.scaling = scaling\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.params_ = None  # Dictionary containing the trainable parameters\n    self.n_qubits_ = None\n    self.scaler = None  # data scaler will be fitted on training data\n    self.circuit = None\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/vanilla_qnn.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n\n    self.initialize(n_features=X.shape[1], classes=np.unique(y))\n\n    self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n    self.scaler.fit(X)\n    X = self.transform(X)\n\n    optimizer = optax.adam\n\n    def loss_fn(params, X, y):\n        # we multiply by 6 because a relevant domain of the sigmoid function is [-6,6]\n        vals = self.forward(params, X) * 6\n        y = jax.nn.relu(y)  # convert to 0,1\n        return jnp.mean(optax.sigmoid_binary_cross_entropy(vals, y))\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    self.params_ = train(self, loss_fn, optimizer, X, y, self.generate_key)\n\n    return self\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels.</p> PARAMETER DESCRIPTION <code>n_features</code> <p>Number of features that the classifier expects</p> <p> TYPE: <code>int</code> </p> <code>classes</code> <p>class labels that the classifier expects</p> <p> TYPE: <code>array - like</code> DEFAULT: <code>None</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/vanilla_qnn.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    self.n_features = n_features\n    self.n_qubits_ = n_features\n    self.initialize_params()\n    self.construct_model()\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred</code> <p>Predicted labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/vanilla_qnn.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred (np.ndarray): Predicted labels of shape (n_samples,)\n    \"\"\"\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/vanilla_qnn.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    predictions = self.chunked_forward(self.params_, X)\n    predictions_2d = np.c_[(1 - predictions) / 2, (1 + predictions) / 2]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/vanilla_qnn.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n    if preprocess:\n        if self.scaler is None:\n            # if the model is unfitted, initialise the scaler here\n            self.scaler = MinMaxScaler(feature_range=(-np.pi / 2, np.pi / 2))\n            self.scaler.fit(X)\n        X = self.scaler.transform(X)\n    return X * self.scaling\n</code></pre> <code></code> chunk_grad <pre><code>chunk_grad(grad_fn, max_vmap)\n</code></pre> <p>Convert a <code>jax.grad</code> function to an equivalent version that evaluated in chunks of size max_vmap.</p> <p><code>grad_fn</code> should be of the form <code>jax.grad(fn(params, X, y), argnums=0)</code>, where <code>params</code> is a dictionary of <code>jnp.arrays</code>, <code>X, y</code> are <code>jnp.arrays</code> with the same-size leading axis, and <code>grad_fn</code> is a function that is vectorised along these axes (i.e. <code>in_axes = (None,0,0)</code>).</p> <p>The returned function evaluates the original function by splitting the batch evaluation into smaller chunks of size <code>max_vmap</code>, and has a lower memory footprint.</p> PARAMETER DESCRIPTION <code>model</code> <p>gradient function with the functional form jax.grad(loss(params, X,y), argnums=0)</p> <p> TYPE: <code>func</code> </p> <code>max_vmap</code> <p>the size of the chunks</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <p>chunked version of the function</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def chunk_grad(grad_fn, max_vmap):\n    \"\"\"\n    Convert a `jax.grad` function to an equivalent version that evaluated in chunks of size max_vmap.\n\n    `grad_fn` should be of the form `jax.grad(fn(params, X, y), argnums=0)`, where `params` is a\n    dictionary of `jnp.arrays`, `X, y` are `jnp.arrays` with the same-size leading axis, and `grad_fn`\n    is a function that is vectorised along these axes (i.e. `in_axes = (None,0,0)`).\n\n    The returned function evaluates the original function by splitting the batch evaluation into smaller chunks\n    of size `max_vmap`, and has a lower memory footprint.\n\n    Args:\n        model (func): gradient function with the functional form jax.grad(loss(params, X,y), argnums=0)\n        max_vmap (int): the size of the chunks\n\n    Returns:\n        chunked version of the function\n    \"\"\"\n\n    def chunked_grad(params, X, y):\n        batch_slices = list(gen_batches(len(X), max_vmap))\n        grads = [grad_fn(params, X[slice], y[slice]) for slice in batch_slices]\n        grad_dict = {}\n        for key_list in get_nested_keys(params):\n            set_in_dict(\n                grad_dict,\n                key_list,\n                jnp.mean(jnp.array([get_from_dict(grad, key_list) for grad in grads]), axis=0),\n            )\n        return grad_dict\n\n    return chunked_grad\n</code></pre> <code></code> chunk_loss <pre><code>chunk_loss(loss_fn, max_vmap)\n</code></pre> <p>Converts a loss function of the form <code>loss_fn(params, array1, array2)</code> to an equivalent version that evaluates <code>loss_fn</code> in chunks of size max_vmap. <code>loss_fn</code> should batch evaluate along the leading axis of <code>array1, array2</code> (i.e. <code>in_axes = (None,0,0)</code>).</p> PARAMETER DESCRIPTION <code>loss_fn</code> <p>function of form loss_fn(params, array1, array2)</p> <p> TYPE: <code>func</code> </p> <code>max_vmap</code> <p>maximum chunk size</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <p>chunked version of the function</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def chunk_loss(loss_fn, max_vmap):\n    \"\"\"\n    Converts a loss function of the form `loss_fn(params, array1, array2)` to an equivalent version that\n    evaluates `loss_fn` in chunks of size max_vmap. `loss_fn` should batch evaluate along the leading\n    axis of `array1, array2` (i.e. `in_axes = (None,0,0)`).\n\n    Args:\n        loss_fn (func): function of form loss_fn(params, array1, array2)\n        max_vmap (int): maximum chunk size\n\n    Returns:\n        chunked version of the function\n    \"\"\"\n\n    def chunked_loss(params, X, y):\n        batch_slices = list(gen_batches(len(X), max_vmap))\n        res = jnp.array([loss_fn(params, *[X[slice], y[slice]]) for slice in batch_slices])\n        return jnp.mean(res)\n\n    return chunked_loss\n</code></pre> <code></code> chunk_vmapped_fn <pre><code>chunk_vmapped_fn(vmapped_fn, start, max_vmap)\n</code></pre> <p>Convert a vmapped function to an equivalent function that evaluates in chunks of size max_vmap. The behaviour of chunked_fn should be the same as vmapped_fn, but with a lower memory cost.</p> <p>The input vmapped_fn should have in_axes = (None, None, ..., 0,0,...,0)</p> PARAMETER DESCRIPTION <code>vmapped</code> <p>vmapped function with in_axes = (None, None, ..., 0,0,...,0)</p> <p> TYPE: <code>func</code> </p> <code>start</code> <p>The index where the first 0 appears in in_axes</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <p>chunked version of the function</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def chunk_vmapped_fn(vmapped_fn, start, max_vmap):\n    \"\"\"\n    Convert a vmapped function to an equivalent function that evaluates in chunks of size\n    max_vmap. The behaviour of chunked_fn should be the same as vmapped_fn, but with a\n    lower memory cost.\n\n    The input vmapped_fn should have in_axes = (None, None, ..., 0,0,...,0)\n\n    Args:\n        vmapped (func): vmapped function with in_axes = (None, None, ..., 0,0,...,0)\n        start (int): The index where the first 0 appears in in_axes\n        max_vmap (int) The max chunk size with which to evaluate the function\n\n    Returns:\n        chunked version of the function\n    \"\"\"\n\n    def chunked_fn(*args):\n        batch_len = len(args[start])\n        batch_slices = list(gen_batches(batch_len, max_vmap))\n        res = [\n            vmapped_fn(*args[:start], *[arg[slice] for arg in args[start:]])\n            for slice in batch_slices\n        ]\n        # jnp.concatenate needs to act on arrays with the same shape, so pad the last array if necessary\n        if batch_len / max_vmap % 1 != 0.0:\n            diff = max_vmap - len(res[-1])\n            res[-1] = jnp.pad(res[-1], [(0, diff), *[(0, 0)] * (len(res[-1].shape) - 1)])\n            return jnp.concatenate(res)[:-diff]\n        return jnp.concatenate(res)\n\n    return chunked_fn\n</code></pre> <code></code> get_batch <pre><code>get_batch(X, y, rnd_key, batch_size=32)\n</code></pre> <p>A generator to get random batches of the data (X, y)</p> PARAMETER DESCRIPTION <code>X</code> <p>Input data with shape (n_samples, n_features).</p> <p> TYPE: <code>array[float]</code> </p> <code>y</code> <p>Target labels with shape (n_samples,)</p> <p> TYPE: <code>array[float]</code> </p> <code>rnd_key</code> <p>A jax random key object</p> <p> </p> <code>batch_size</code> <p>Number of elements in batch</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> RETURNS DESCRIPTION <p>array[float]: A batch of input data shape (batch_size, n_features)</p> <p>array[float]: A batch of target labels shaped (batch_size,)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def get_batch(X, y, rnd_key, batch_size=32):\n    \"\"\"\n    A generator to get random batches of the data (X, y)\n\n    Args:\n        X (array[float]): Input data with shape (n_samples, n_features).\n        y (array[float]): Target labels with shape (n_samples,)\n        rnd_key: A jax random key object\n        batch_size (int): Number of elements in batch\n\n    Returns:\n        array[float]: A batch of input data shape (batch_size, n_features)\n        array[float]: A batch of target labels shaped (batch_size,)\n    \"\"\"\n    all_indices = jnp.array(range(len(X)))\n    rnd_indices = jax.random.choice(key=rnd_key, a=all_indices, shape=(batch_size,), replace=True)\n    return X[rnd_indices], y[rnd_indices]\n</code></pre> <code></code> get_from_dict <pre><code>get_from_dict(dict, key_list)\n</code></pre> <p>Access a value from a nested dictionary. Inspired by https://stackoverflow.com/questions/14692690/access-nested-dictionary-items-via-a-list-of-keys</p> PARAMETER DESCRIPTION <code>dict</code> <p>nested dictionary</p> <p> TYPE: <code>dict</code> </p> <code>key_list</code> <p>list of keys to be accessed</p> <p> TYPE: <code>list</code> </p> RETURNS DESCRIPTION <p>the requested value</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def get_from_dict(dict, key_list):\n    \"\"\"\n    Access a value from a nested dictionary.\n    Inspired by https://stackoverflow.com/questions/14692690/access-nested-dictionary-items-via-a-list-of-keys\n\n    Args:\n        dict (dict): nested dictionary\n        key_list (list): list of keys to be accessed\n\n    Returns:\n         the requested value\n    \"\"\"\n    return reduce(operator.getitem, key_list, dict)\n</code></pre> <code></code> get_nested_keys <pre><code>get_nested_keys(d, parent_keys=[])\n</code></pre> <p>Returns the nested keys of a nested dictionary.</p> PARAMETER DESCRIPTION <code>d</code> <p>nested dictionary</p> <p> TYPE: <code>dict</code> </p> RETURNS DESCRIPTION <p>list where each element is a list of nested keys</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def get_nested_keys(d, parent_keys=[]):\n    \"\"\"\n    Returns the nested keys of a nested dictionary.\n\n    Args:\n        d (dict): nested dictionary\n\n    Returns:\n        list where each element is a list of nested keys\n    \"\"\"\n    keys_list = []\n    for key, value in d.items():\n        current_keys = parent_keys + [key]\n        if isinstance(value, dict):\n            keys_list.extend(get_nested_keys(value, current_keys))\n        else:\n            keys_list.append(current_keys)\n    return keys_list\n</code></pre> <code></code> set_in_dict <pre><code>set_in_dict(dict, keys, value)\n</code></pre> <p>Set a value in a nested dictionary.</p> PARAMETER DESCRIPTION <code>dict</code> <p>nested dictionary</p> <p> TYPE: <code>dict</code> </p> <code>keys</code> <p>list of keys in nested dictionary</p> <p> TYPE: <code>list</code> </p> <code>value</code> <p>value to be set</p> <p> TYPE: <code>Any</code> </p> RETURNS DESCRIPTION <p>nested dictionary with new value</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def set_in_dict(dict, keys, value):\n    \"\"\"\n    Set a value in a nested dictionary.\n\n    Args:\n        dict (dict): nested dictionary\n        keys (list): list of keys in nested dictionary\n        value (Any): value to be set\n\n    Returns:\n        nested dictionary with new value\n    \"\"\"\n    for key in keys[:-1]:\n        dict = dict.setdefault(key, {})\n    dict[keys[-1]] = value\n</code></pre> <code></code> train <pre><code>train(model, loss_fn, optimizer, X, y, random_key_generator, convergence_interval=200)\n</code></pre> <p>Trains a model using an optimizer and a loss function via gradient descent. We assume that the loss function is of the form <code>loss(params, X, y)</code> and that the trainable parameters are stored in model.params_ as a dictionary of jnp.arrays. The optimizer should be an Optax optimizer (e.g. optax.adam). <code>model</code> must have an attribute <code>learning_rate</code> to set the initial learning rate for the gradient descent.</p> <p>The model is trained until a convergence criterion is met that corresponds to the loss curve being flat over a number of optimization steps given by <code>convergence_inteval</code> (see plots for details).</p> <p>To reduce precompilation time and memory cost, the loss function and gradient functions are evaluated in chunks of size model.max_vmap.</p> PARAMETER DESCRIPTION <code>model</code> <p>Classifier class object to train. Trainable parameters must be stored in model.params_.</p> <p> TYPE: <code>class</code> </p> <code>loss_fn</code> <p>Loss function to be minimised. Must be of the form loss_fn(params, X, y).</p> <p> TYPE: <code>Callable</code> </p> <code>optimizer</code> <p>Optax optimizer (e.g. optax.adam).</p> <p> TYPE: <code>optax optimizer</code> </p> <code>X</code> <p>Input data array of shape (n_samples, n_features)</p> <p> TYPE: <code>array</code> </p> <code>y</code> <p>Array of shape (n_samples) containing the labels.</p> <p> TYPE: <code>array</code> </p> <code>random_key_generator</code> <p>JAX key generator object for pseudo-randomness generation.</p> <p> TYPE: <code>PRNGKey</code> </p> <code>convergence_interval</code> <p>Number of optimization steps over which to decide convergence. Larger values give a higher confidence that the model has converged but may increase training time.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> RETURNS DESCRIPTION <code>params</code> <p>The new parameters after training has completed.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/model_utils.py</code> <pre><code>def train(model, loss_fn, optimizer, X, y, random_key_generator, convergence_interval=200):\n    \"\"\"\n    Trains a model using an optimizer and a loss function via gradient descent. We assume that the loss function\n    is of the form `loss(params, X, y)` and that the trainable parameters are stored in model.params_ as a dictionary\n    of jnp.arrays. The optimizer should be an Optax optimizer (e.g. optax.adam). `model` must have an attribute\n    `learning_rate` to set the initial learning rate for the gradient descent.\n\n    The model is trained until a convergence criterion is met that corresponds to the loss curve being flat over\n    a number of optimization steps given by `convergence_inteval` (see plots for details).\n\n    To reduce precompilation time and memory cost, the loss function and gradient functions are evaluated in\n    chunks of size model.max_vmap.\n\n    Args:\n        model (class): Classifier class object to train. Trainable parameters must be stored in model.params_.\n        loss_fn (Callable): Loss function to be minimised. Must be of the form loss_fn(params, X, y).\n        optimizer (optax optimizer): Optax optimizer (e.g. optax.adam).\n        X (array): Input data array of shape (n_samples, n_features)\n        y (array): Array of shape (n_samples) containing the labels.\n        random_key_generator (jax.random.PRNGKey): JAX key generator object for pseudo-randomness generation.\n        convergence_interval (int, optional): Number of optimization steps over which to decide convergence. Larger\n            values give a higher confidence that the model has converged but may increase training time.\n\n    Returns:\n        params (dict): The new parameters after training has completed.\n    \"\"\"\n\n    if not model.batch_size / model.max_vmap % 1 == 0:\n        raise Exception(\"Batch size must be multiple of max_vmap.\")\n\n    params = model.params_\n    opt = optimizer(learning_rate=model.learning_rate)\n    opt_state = opt.init(params)\n    grad_fn = jax.grad(loss_fn)\n\n    # jitting through the chunked_grad function can take a long time,\n    # so we jit here and chunk after\n    if model.jit:\n        grad_fn = jax.jit(grad_fn)\n\n    # note: assumes that the loss function is a sample mean of\n    # some function over the input data set\n    chunked_grad_fn = chunk_grad(grad_fn, model.max_vmap)\n    chunked_loss_fn = chunk_loss(loss_fn, model.max_vmap)\n\n    def update(params, opt_state, x, y):\n        grads = chunked_grad_fn(params, x, y)\n        loss_val = chunked_loss_fn(params, x, y)\n        updates, opt_state = opt.update(grads, opt_state)\n        params = optax.apply_updates(params, updates)\n        return params, opt_state, loss_val\n\n    loss_history = []\n    converged = False\n    start = time.time()\n    for step in range(model.max_steps):\n        key = random_key_generator()\n        X_batch, y_batch = get_batch(X, y, key, batch_size=model.batch_size)\n        params, opt_state, loss_val = update(params, opt_state, X_batch, y_batch)\n        loss_history.append(loss_val)\n        logging.debug(f\"{step} - loss: {loss_val}\")\n\n        if np.isnan(loss_val):\n            logging.info(\"nan encountered. Training aborted.\")\n            break\n\n        # decide convergence\n        if step &gt; 2 * convergence_interval:\n            # get means of last two intervals and standard deviation of last interval\n            average1 = np.mean(loss_history[-convergence_interval:])\n            average2 = np.mean(loss_history[-2 * convergence_interval : -convergence_interval])\n            std1 = np.std(loss_history[-convergence_interval:])\n            # if the difference in averages is small compared to the statistical fluctuations, stop training.\n            if np.abs(average2 - average1) &lt;= std1 / np.sqrt(convergence_interval) / 2:\n                logging.info(f\"Model {model.__class__.__name__} converged after {step} steps.\")\n                converged = True\n                break\n\n    end = time.time()\n    loss_history = np.array(loss_history)\n    model.loss_history_ = loss_history / np.max(np.abs(loss_history))\n    model.training_time_ = end - start\n\n    if not converged:\n        print(\"Loss did not converge:\", loss_history)\n        raise ConvergenceWarning(\n            f\"Model {model.__class__.__name__} has not converged after the maximum number of {model.max_steps} steps.\"\n        )\n\n    return params\n</code></pre> <code></code> weinet CLASS DESCRIPTION <code>WeiNet</code> <code></code> WeiNet <pre><code>WeiNet(filter_name='edge_detect', learning_rate=0.1, max_steps=10000, convergence_interval=200, random_state=42, max_vmap=None, jit=True, scaling=1.0, dev_type='default.qubit.jax', qnode_kwargs={'interface': 'jax-jit'}, batch_size=32)\n</code></pre> <p>               Bases: <code>BaseEstimator</code>, <code>ClassifierMixin</code></p> <p>Quantum convolutional neural network from https://arxiv.org/abs/2104.06918v3  (see fig 2 of plots)</p> <p>The model has two registers: the ancilliary register and the work register. The ancilliary register is used to parameterise a 4 qubit state which in turn controls a number of unitaries that act on the work register, where the data is encoded via amplitude encoding.</p> <p>The qubits with index -1 and height-1 are then traced out, which is equivalent to a type of pooling. All single and double correlators  and  are measured, and a linear model on these values is used for classification.</p> <p>The plots does not specify the loss: we use the binary cross entropy.</p> <p>The input data X should have shape (dataset_size,height*width) and will be reshaped to (dataset_size,1, height, width) in the model. We assume height=width.</p> <p>Note that in figure 2 of the plots, the Hadamards on the ancilla register have no effect since we trace this register out. The effect of this register is then to simply perform  a classical mixture of the unitaries Q_i on the work register. For simplicity (and to save qubit numbers), we parameterise this distribution via params_['s'] rather than model the ancilla qubits themselves.</p> PARAMETER DESCRIPTION <code>filter_name</code> <p>The classical filter that defines the unitaries Q_i. either 'edge_detect', 'smooth', or 'sharpen'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'edge_detect'</code> </p> <code>learning_rate</code> <p>Initial learning rate for gradient descent.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>convergence_interval</code> <p>The number of loss values to consider to decide convergence.</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>max_steps</code> <p>Maximum number of training steps. A warning will be raised if training did not converge.</p> <p> TYPE: <code>int</code> DEFAULT: <code>10000</code> </p> <code>batch_size</code> <p>Size of batches used for computing parameter updates.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>max_vmap</code> <p>The maximum size of a chunk to vectorise over. Lower values use less memory. must divide batch_size.</p> <p> TYPE: <code>int or None</code> DEFAULT: <code>None</code> </p> <code>jit</code> <p>Whether to use just in time compilation.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dev_type</code> <p>string specifying the pennylane device type; e.g. 'default.qubit'.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'default.qubit.jax'</code> </p> <code>qnode_kwargs</code> <p>the key word arguments passed to the circuit qnode.</p> <p> TYPE: <code>str</code> DEFAULT: <code>{'interface': 'jax-jit'}</code> </p> <code>random_state</code> <p>Seed used for pseudorandom number generation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>42</code> </p> <code>scaling</code> <p>Factor by which to scale the input data.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> METHOD DESCRIPTION <code>construct_models</code> <p>constructs the 9 circuits used for the convolutional layer (Q_k in the plots).</p> <code>construct_unitaries</code> <p>Construct the unitaries V' defined in the plots</p> <code>fit</code> <p>Fit the model to data X and labels y.</p> <code>forward_fn</code> <p>We have taken some shortcuts here compared to the plots description but the result is the same.</p> <code>initialize</code> <p>Initialize attributes that depend on the number of features and the class labels.</p> <code>initialize_params</code> <p>initialise the trainable parameters</p> <code>predict</code> <p>Predict labels for data X.</p> <code>predict_proba</code> <p>Predict label probabilities for data X.</p> <code>transform</code> <p>Args:</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/weinet.py</code> <pre><code>def __init__(\n    self,\n    filter_name=\"edge_detect\",\n    learning_rate=0.1,\n    max_steps=10000,\n    convergence_interval=200,\n    random_state=42,\n    max_vmap=None,\n    jit=True,\n    scaling=1.0,\n    dev_type=\"default.qubit.jax\",\n    qnode_kwargs={\"interface\": \"jax-jit\"},\n    batch_size=32,\n):\n    \"\"\"\n    Quantum convolutional neural network from https://arxiv.org/abs/2104.06918v3  (see fig 2 of plots)\n\n    The model has two registers: the ancilliary register and the work register. The ancilliary register is used to\n    parameterise a 4 qubit state which in turn controls a number of unitaries that act on the work register, where\n    the data is encoded via amplitude encoding.\n\n    The qubits with index -1 and height-1 are then traced out, which is equivalent to a type of pooling. All single\n    and double correlators &lt;Z&gt; and &lt;ZZ&gt; are measured, and a linear model on these values is used for classification.\n\n    The plots does not specify the loss: we use the binary cross entropy.\n\n    The input data X should have shape (dataset_size,height*width) and will be reshaped to\n    (dataset_size,1, height, width) in the model. We assume height=width.\n\n    Note that in figure 2 of the plots, the Hadamards on the ancilla register have no effect since we trace this\n    register out. The effect of this register is then to simply perform  a classical mixture of the unitaries\n    Q_i on the work register. For simplicity (and to save qubit numbers), we parameterise this distribution\n    via params_['s'] rather than model the ancilla qubits themselves.\n\n    Args:\n        filter_name (str): The classical filter that defines the unitaries Q_i. either 'edge_detect', 'smooth', or\n            'sharpen'.\n        learning_rate (float): Initial learning rate for gradient descent.\n        convergence_interval (int): The number of loss values to consider to decide convergence.\n        max_steps (int): Maximum number of training steps. A warning will be raised if training did not converge.\n        batch_size (int): Size of batches used for computing parameter updates.\n        max_vmap (int or None): The maximum size of a chunk to vectorise over. Lower values use less memory.\n            must divide batch_size.\n        jit (bool): Whether to use just in time compilation.\n        dev_type (str): string specifying the pennylane device type; e.g. 'default.qubit'.\n        qnode_kwargs (str): the key word arguments passed to the circuit qnode.\n        random_state (int): Seed used for pseudorandom number generation.\n        scaling (float): Factor by which to scale the input data.\n    \"\"\"\n    # attributes that do not depend on data\n    self.learning_rate = learning_rate\n    self.max_steps = max_steps\n    self.filter_name = filter_name\n    self.convergence_interval = convergence_interval\n    self.dev_type = dev_type\n    self.qnode_kwargs = qnode_kwargs\n    self.scaling = scaling\n    self.batch_size = batch_size\n    self.jit = jit\n    self.random_state = random_state\n    self.rng = np.random.default_rng(random_state)\n    self.unitaries = []\n\n    if filter_name == \"edge_detect\":\n        self.filter = jnp.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]])\n    elif filter_name == \"smooth\":\n        self.filter = jnp.array([[1, 1, 1], [1, 5, 1], [1, 1, 1]]) / 13\n    elif filter_name == \"sharpen\":\n        self.filter = jnp.array([[-2, -2, -2], [-2, 32, -2], [-2, -2, -2]]) / 16\n\n    if max_vmap is None:\n        self.max_vmap = self.batch_size\n    else:\n        self.max_vmap = max_vmap\n\n    # data-dependant attributes\n    # which will be initialised by calling \"fit\"\n    self.n_qubits_ = None\n    self.params_ = None  # Dictionary containing the trainable parameters\n    self.height_ = None  # height of image data\n    self.width_ = None  # width of image data\n    self.circuit = None\n</code></pre> <code></code> construct_models <pre><code>construct_models()\n</code></pre> <p>constructs the 9 circuits used for the convolutional layer (Q_k in the plots).</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/weinet.py</code> <pre><code>def construct_models(self):\n    \"\"\"\n    constructs the 9 circuits used for the convolutional layer (Q_k in the plots).\n    \"\"\"\n\n    # get the operators that are used for prediction. We don't include the self.height_ qubit or the last\n    # qubit as per the plots, since there are lost in the pooling layer\n    operators = []\n    for i in range(self.n_qubits_ - 1):\n        if i != int(self.n_qubits_ / 2) - 1:\n            operators.append(qml.PauliZ(wires=i))\n            for j in range(i + 1, self.n_qubits_ - 1):\n                if j != int(self.n_qubits_ / 2) - 1:\n                    operators.append(qml.PauliZ(wires=i) @ qml.PauliZ(wires=j))\n    operators.append(qml.Identity(wires=0))\n\n    wires = range(self.n_qubits_)\n    dev = qml.device(self.dev_type, wires=wires)\n    circuits = []\n    for nu in range(3):\n        for mu in range(3):\n\n            @qml.qnode(dev, **self.qnode_kwargs)\n            def circuit(x):\n                qml.AmplitudeEmbedding(\n                    jnp.reshape(x, -1), wires=wires, normalize=True, pad_with=0.0\n                )\n                qml.QubitUnitary(\n                    jnp.kron(self.unitaries[nu][nu], jnp.array(self.unitaries[nu][mu])),\n                    wires=wires,\n                )\n                return [qml.expval(op) for op in operators]\n\n            self.circuit = circuit  # we use the last one of the circuits here as an example\n\n            if self.jit:\n                circuit = jax.jit(circuit)\n            circuits.append(circuit)\n\n    self.circuits = circuits\n</code></pre> <code></code> construct_unitaries <pre><code>construct_unitaries()\n</code></pre> <p>Construct the unitaries V' defined in the plots</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/weinet.py</code> <pre><code>def construct_unitaries(self):\n    \"\"\"\n    Construct the unitaries V' defined in the plots\n    \"\"\"\n    self.unitaries = [[None for __ in range(3)] for __ in range(3)]\n    for mu in range(3):\n        for nu, k in enumerate([-1, 0, 1]):\n            V = np.zeros([self.height_, self.width_])\n            for i in range(self.height_):\n                V[i, (i + k) % self.height_] = self.filter[nu, mu]\n            self.unitaries[nu][mu] = V / self.filter[nu, mu]\n</code></pre> <code></code> fit <pre><code>fit(X, y)\n</code></pre> <p>Fit the model to data X and labels y.</p> PARAMETER DESCRIPTION <code>X</code> <p>Image data of shape (n_samples, height**2)</p> <p> TYPE: <code>ndarray</code> </p> <code>y</code> <p>Labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/weinet.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"Fit the model to data X and labels y.\n\n    Args:\n        X (np.ndarray): Image data of shape (n_samples, height**2)\n        y (np.ndarray): Labels of shape (n_samples,)\n    \"\"\"\n\n    self.initialize(X.shape[1], classes=np.unique(y))\n    y = jnp.array(y, dtype=int)\n    X = self.transform(X)\n\n    # initialise the model\n    self.construct_unitaries()\n    self.construct_models()\n    self.forward = jax.vmap(self.forward_fn, in_axes=(None, 0))\n    self.chunked_forward = chunk_vmapped_fn(self.forward, 1, self.max_vmap)\n\n    def loss_fn(params, X, y):\n        # we use the usual cross entropy\n        y = jax.nn.relu(y)  # convert to 0,1 labels\n        vals = self.forward(params, X)\n        loss = jnp.mean(optax.sigmoid_binary_cross_entropy(vals, y))\n        return loss\n\n    if self.jit:\n        loss_fn = jax.jit(loss_fn)\n    optimizer = optax.adam\n    self.params_ = train(\n        self,\n        loss_fn,\n        optimizer,\n        X,\n        y,\n        self.generate_key,\n        convergence_interval=self.convergence_interval,\n    )\n\n    return self\n</code></pre> <code></code> forward_fn <pre><code>forward_fn(params, x)\n</code></pre> <p>We have taken some shortcuts here compared to the plots description but the result is the same. Since we trace out the ancilla register, the final hadamards in the circuit diagram have no effect, and the process is equivalent to classically sampling one of the unitaries Q_i, parameterised by params['s'].</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/weinet.py</code> <pre><code>def forward_fn(self, params, x):\n    \"\"\"\n    We have taken some shortcuts here compared to the plots description but the result is the same.\n    Since we trace out the ancilla register, the final hadamards in the circuit diagram have no effect, and the process\n    is equivalent to classically sampling one of the unitaries Q_i, parameterised by params['s'].\n    \"\"\"\n    probs = jax.nn.softmax(params[\"s\"])\n    expvals = jnp.array([probs[i] * jnp.array(self.circuits[i](x)).T for i in range(9)])\n    expvals = jnp.sum(expvals, axis=0)\n    out = jnp.sum(params[\"weights\"] * expvals)\n    # out = jax.nn.sigmoid(out)  # convert to a probability\n    # out = jnp.vstack((out, 1 - out)).T  # convert to 'two neurons'\n    # out = jnp.reshape(out, (2))\n    return out\n</code></pre> <code></code> initialize <pre><code>initialize(n_features, classes=None)\n</code></pre> <p>Initialize attributes that depend on the number of features and the class labels.</p> PARAMETER DESCRIPTION <code>n_features</code> <p>Number of features that the classifier expects</p> <p> TYPE: <code>int</code> </p> <code>classes</code> <p>class labels that the classifier expects</p> <p> TYPE: <code>array - like</code> DEFAULT: <code>None</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/weinet.py</code> <pre><code>def initialize(self, n_features, classes=None):\n    \"\"\"Initialize attributes that depend on the number of features and the class labels.\n\n    Args:\n        n_features (int): Number of features that the classifier expects\n        classes (array-like): class labels that the classifier expects\n    \"\"\"\n    if classes is None:\n        classes = [-1, 1]\n\n    self.classes_ = classes\n    self.n_classes_ = len(self.classes_)\n    assert self.n_classes_ == 2\n    assert 1 in self.classes_ and -1 in self.classes_\n\n    im_height = int(jnp.sqrt(n_features))\n    self.n_qubits_ = 2 * ceil(jnp.log2(im_height))\n    self.height_ = 2 ** (self.n_qubits_ // 2)\n    self.width_ = 2 ** (self.n_qubits_ // 2)\n    self.initialize_params()\n    self.construct_unitaries()\n    self.construct_models()\n</code></pre> <code></code> initialize_params <pre><code>initialize_params()\n</code></pre> <p>initialise the trainable parameters</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/weinet.py</code> <pre><code>def initialize_params(self):\n    \"\"\"\n    initialise the trainable parameters\n    \"\"\"\n    # no of expvals that are combined with weights\n    n_expvals = int(\n        self.n_qubits_ - 1 + factorial(self.n_qubits_ - 2) / 2 / factorial(self.n_qubits_ - 4)\n    )\n\n    self.params_ = {\n        \"s\": jax.random.normal(self.generate_key(), shape=(9,)),\n        \"weights\": jax.random.normal(self.generate_key(), shape=(n_expvals,)) / n_expvals,\n    }\n</code></pre> <code></code> predict <pre><code>predict(X)\n</code></pre> <p>Predict labels for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred</code> <p>Predicted labels of shape (n_samples,)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/weinet.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict labels for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred (np.ndarray): Predicted labels of shape (n_samples,)\n    \"\"\"\n    predictions = self.predict_proba(X)\n    mapped_predictions = np.argmax(predictions, axis=1)\n    return np.take(self.classes_, mapped_predictions)\n</code></pre> <code></code> predict_proba <pre><code>predict_proba(X)\n</code></pre> <p>Predict label probabilities for data X.</p> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> RETURNS DESCRIPTION <code>y_pred_proba</code> <p>Predicted label probabilities of shape</p> <p> TYPE: <code>ndarray</code> </p> <p>(n_samples, n_classes)</p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/weinet.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict label probabilities for data X.\n\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n\n    Returns:\n        y_pred_proba (np.ndarray): Predicted label probabilities of shape\n        (n_samples, n_classes)\n    \"\"\"\n    X = self.transform(X)\n    p1 = jax.nn.sigmoid(self.chunked_forward(self.params_, X))\n    predictions_2d = jnp.c_[1 - p1, p1]\n    return predictions_2d\n</code></pre> <code></code> transform <pre><code>transform(X, preprocess=True)\n</code></pre> PARAMETER DESCRIPTION <code>X</code> <p>Data of shape (n_samples, n_features)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/quoptuna/backend/base/pennylane_models/qml_benchmarks/models/weinet.py</code> <pre><code>def transform(self, X, preprocess=True):\n    \"\"\"\n    Args:\n        X (np.ndarray): Data of shape (n_samples, n_features)\n    \"\"\"\n\n    # put in NCHW format. We assume square images\n    im_height = int(jnp.sqrt(X.shape[1]))\n    X = jnp.reshape(X, (X.shape[0], 1, im_height, im_height))\n\n    X = self.scaling * X\n\n    padded_X = np.zeros([X.shape[0], X.shape[1], self.height_, self.width_])\n    padded_X[: X.shape[0], : X.shape[1], : X.shape[2], : X.shape[3]] = X\n    return jnp.array(padded_X)\n</code></pre>"},{"location":"api_docs.html#quoptuna.backend.tuners","title":"tuners","text":"MODULE DESCRIPTION <code>optimizer</code>"},{"location":"api_docs.html#quoptuna.backend.tuners.optimizer","title":"optimizer","text":"CLASS DESCRIPTION <code>Optimizer</code>"},{"location":"api_docs.html#quoptuna.backend.tuners.optimizer.Optimizer","title":"Optimizer","text":"<pre><code>Optimizer(db_name: str, dataset_name: str = '', data: Optional[dict] = None, study_name: str = '')\n</code></pre> <p>Initialize the Optimizer class.</p> PARAMETER DESCRIPTION <code>db_name</code> <p>The name of the database to be used for storing optimization results.</p> <p> TYPE: <code>str</code> </p> <code>dataset_name</code> <p>The name of the dataset.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>data</code> <p>A dictionary containing training and testing data.</p> <p> TYPE: <code>Optional[dict]</code> DEFAULT: <code>None</code> </p> <code>study_name</code> <p>The name of the study for Optuna.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> ATTRIBUTE DESCRIPTION <code>db_name</code> <p>The name of the database.</p> <p> TYPE: <code>str</code> </p> <code>dataset_name</code> <p>The name of the dataset.</p> <p> TYPE: <code>str</code> </p> <code>data_path</code> <p>The path to the dataset CSV file or an empty string</p> <p> TYPE: <code>str</code> </p> <code>data</code> <p>The data dictionary containing training and testing data.</p> <p> TYPE: <code>dict</code> </p> <code>train_x</code> <p>The training features.</p> <p> TYPE: <code>Optional[ndarray]</code> </p> <code>test_x</code> <p>The testing features.</p> <p> TYPE: <code>Optional[ndarray]</code> </p> <code>train_y</code> <p>The training labels.</p> <p> TYPE: <code>Optional[ndarray]</code> </p> <code>test_y</code> <p>The testing labels.</p> <p> TYPE: <code>Optional[ndarray]</code> </p> <code>storage_location</code> <p>The storage location for the Optuna study.</p> <p> TYPE: <code>str</code> </p> <code>study_name</code> <p>The name of the Optuna study.</p> <p> TYPE: <code>str</code> </p> <code>study</code> <p>The Optuna study object.</p> <p> TYPE: <code>Optional[Study]</code> </p> Source code in <code>src/quoptuna/backend/tuners/optimizer.py</code> <pre><code>def __init__(\n    self,\n    db_name: str,\n    dataset_name: str = \"\",\n    data: Optional[dict] = None,  # noqa: FA100\n    study_name: str = \"\",\n):\n    \"\"\"\n\n    Initialize the Optimizer class.\n\n    Args:\n        db_name (str): The name of the database to be used for storing optimization results.\n        dataset_name (str, optional): The name of the dataset.\n        If provided, the data will be loaded from a CSV file located in the 'notebook'\n        directory.Defaults to an empty string.\n        data (Optional[dict], optional): A dictionary containing training and testing data.\n        If not provided, an empty dictionary will be used.\n        Expected keys are 'train_x', 'test_x', 'train_y', and 'test_y'.\n        study_name (str, optional): The name of the study for Optuna.\n        Defaults to an empty string.\n\n    Attributes:\n        db_name (str): The name of the database.\n        dataset_name (str): The name of the dataset.\n        data_path (str): The path to the dataset CSV file or an empty string\n        if no dataset name is provided.\n        data (dict): The data dictionary containing training and testing data.\n        train_x (Optional[np.ndarray]): The training features.\n        test_x (Optional[np.ndarray]): The testing features.\n        train_y (Optional[np.ndarray]): The training labels.\n        test_y (Optional[np.ndarray]): The testing labels.\n        storage_location (str): The storage location for the Optuna study.\n        study_name (str): The name of the Optuna study.\n        study (Optional[Study]): The Optuna study object.\n    \"\"\"\n    self.db_name = db_name\n    self.dataset_name = dataset_name\n    if len(self.dataset_name) &gt; 0:\n        self.data_path = f\"notebook/{self.dataset_name}.csv\"\n    else:\n        self.data_path = \"\"\n    self.data = data or {}  # Use an empty dictionary if no data is provided\n    self.train_x = self.data.get(\"train_x\")\n    self.test_x = self.data.get(\"test_x\")\n    self.train_y = self.data.get(\"train_y\")\n    self.test_y = self.data.get(\"test_y\")\n    self.data_path = f\"db/{self.db_name}.db\"\n    if not os.path.exists(\"db\"):  # noqa: PTH110\n        os.makedirs(\"db\")  # noqa: PTH103\n    self.storage_location = f\"sqlite:///{self.data_path}\"\n    self.study_name = study_name\n    self.study = None\n</code></pre>"},{"location":"api_docs.html#quoptuna.backend.typing","title":"typing","text":"MODULE DESCRIPTION <code>data_typing</code>"},{"location":"api_docs.html#quoptuna.backend.typing.data_typing","title":"data_typing","text":"CLASS DESCRIPTION <code>DataVariable</code> <p>DataVariable class for storing the data variables.</p>"},{"location":"api_docs.html#quoptuna.backend.typing.data_typing.DataVariable","title":"DataVariable","text":"<p>               Bases: <code>TypedDict</code></p> <p>DataVariable class for storing the data variables.</p>"},{"location":"api_docs.html#quoptuna.backend.utils","title":"utils","text":"MODULE DESCRIPTION <code>data_utils</code>"},{"location":"api_docs.html#quoptuna.backend.utils.data_utils","title":"data_utils","text":"MODULE DESCRIPTION <code>data</code> <code>prepare</code>"},{"location":"api_docs.html#quoptuna.backend.utils.data_utils.data","title":"data","text":"FUNCTION DESCRIPTION <code>find_free_port</code> <p>Find a port number that is not in use and returns the port number.</p> <code>load_data</code> <p>Load the data from the file path.</p> <code>preprocess_data</code> <p>Preprocess the data.</p> <code></code> find_free_port <pre><code>find_free_port()\n</code></pre> <p>Find a port number that is not in use and returns the port number.</p> Source code in <code>src/quoptuna/backend/utils/data_utils/data.py</code> <pre><code>def find_free_port():\n    \"\"\"\n    Find a port number that is not in use and returns the port number.\n    \"\"\"\n    import socket\n\n    for port in range(6000, 7000):\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        result = sock.connect_ex((\"localhost\", port))\n        if result == 0:\n            sock.close()\n            continue\n        return port\n    return None\n</code></pre> <code></code> load_data <pre><code>load_data(file_path)\n</code></pre> <p>Load the data from the file path.</p> Source code in <code>src/quoptuna/backend/utils/data_utils/data.py</code> <pre><code>def load_data(file_path):\n    \"\"\"\n    Load the data from the file path.\n    \"\"\"\n    data_frame = pd.read_csv(file_path)\n    y = data_frame[\"Crystal\"]\n    x = data_frame.drop(columns=[\"Crystal\"])\n    return x, y\n</code></pre> <code></code> preprocess_data <pre><code>preprocess_data(x, y)\n</code></pre> <p>Preprocess the data.</p> Source code in <code>src/quoptuna/backend/utils/data_utils/data.py</code> <pre><code>def preprocess_data(x, y):\n    \"\"\"\n    Preprocess the data.\n    \"\"\"\n    scaler = StandardScaler()\n    x = scaler.fit_transform(x)\n    classes = np.unique(y)\n    y = np.where(y == classes[0], 1, -1)\n    return train_test_split(x, y, random_state=42)\n</code></pre>"},{"location":"api_docs.html#quoptuna.backend.utils.data_utils.prepare","title":"prepare","text":"CLASS DESCRIPTION <code>DataPreparation</code> <code></code> DataPreparation <pre><code>DataPreparation(dataset: DataSet | None = None, file_path: str | None = None, x_cols: list[str] | None = None, y_col: str | None = None, scaler=None)\n</code></pre> METHOD DESCRIPTION <code>create_dataset</code> <p>Creates a dataset from raw data.</p> <code>prepare_data</code> <p>Selects columns and preprocesses the data.</p> <code>preprocess</code> <p>Preprocess the features and target.</p> <code>read_csv</code> <p>Reads a CSV file and returns a raw dataset.</p> <code>select_columns</code> <p>Selects specified columns and splits the dataset into features and target.</p> <code>update_column_names</code> <p>Update column names in x_cols if they are single length after conversion to string.</p> Source code in <code>src/quoptuna/backend/utils/data_utils/prepare.py</code> <pre><code>def __init__(\n    self,\n    dataset: DataSet | None = None,\n    file_path: str | None = None,\n    x_cols: list[str] | None = None,\n    y_col: str | None = None,\n    scaler=None,\n):\n    self.x_cols = x_cols\n    self.y_col = y_col\n    self.scaler = scaler or StandardScaler()\n    if dataset is not None:\n        x = self.update_column_names(dataset.get(\"x\"))\n        self.set_x_cols(x.columns)\n        self.dataset = {\"x\": x, \"y\": dataset.get(\"y\")}\n    elif file_path is not None:\n        if x_cols is None or y_col is None:\n            msg = \"x_cols and y_col must be provided when file_path is used\"\n            raise ValueError(msg)\n        self.dataset = self.create_dataset(self.read_csv(file_path), x_cols, y_col)\n    else:\n        msg = \"Either dataset or file_path must be provided\"\n        raise ValueError(msg)\n    self.x_train, self.x_test, self.y_train, self.y_test = self.prepare_data()\n</code></pre> <code></code> create_dataset <pre><code>create_dataset(raw_data: DataFrame, x_cols: list[str], y_col: str) -&gt; DataSet\n</code></pre> <p>Creates a dataset from raw data.</p> Source code in <code>src/quoptuna/backend/utils/data_utils/prepare.py</code> <pre><code>def create_dataset(self, raw_data: pd.DataFrame, x_cols: list[str], y_col: str) -&gt; DataSet:\n    \"\"\"Creates a dataset from raw data.\"\"\"\n    x = raw_data[x_cols]\n    y = raw_data[y_col]\n    x = self.update_column_names(x)\n    self.set_x_cols(x.columns)\n    return {\"x\": x, \"y\": y}\n</code></pre> <code></code> prepare_data <pre><code>prepare_data()\n</code></pre> <p>Selects columns and preprocesses the data.</p> Source code in <code>src/quoptuna/backend/utils/data_utils/prepare.py</code> <pre><code>def prepare_data(self):\n    \"\"\"Selects columns and preprocesses the data.\"\"\"\n    if self.x_cols is None or self.y_col is None:\n        msg = \"x_cols and y_col must be provided\"\n        raise ValueError(msg)\n    x, y = self.select_columns()\n    return self.preprocess(x, y)\n</code></pre> <code></code> preprocess <pre><code>preprocess(x: DataFrame, y: Series, train_size: float = 0.75)\n</code></pre> <p>Preprocess the features and target.</p> Source code in <code>src/quoptuna/backend/utils/data_utils/prepare.py</code> <pre><code>def preprocess(self, x: pd.DataFrame, y: pd.Series, train_size: float = 0.75):\n    \"\"\"Preprocess the features and target.\"\"\"\n    x = pd.DataFrame(self.scaler.fit_transform(x), columns=x.columns)\n    classes = np.unique(y)\n    y = pd.DataFrame(\n        np.where(y == classes[0], 1, -1),\n        columns=[self.y_col] if not isinstance(self.y_col, list) else self.y_col,\n    )\n    return train_test_split(x, y, train_size=train_size, random_state=42)\n</code></pre> <code></code> read_csv <code>staticmethod</code> <pre><code>read_csv(file_path: str) -&gt; DataFrame\n</code></pre> <p>Reads a CSV file and returns a raw dataset.</p> Source code in <code>src/quoptuna/backend/utils/data_utils/prepare.py</code> <pre><code>@staticmethod\ndef read_csv(file_path: str) -&gt; pd.DataFrame:\n    \"\"\"Reads a CSV file and returns a raw dataset.\"\"\"\n    return pd.read_csv(file_path)\n</code></pre> <code></code> select_columns <pre><code>select_columns()\n</code></pre> <p>Selects specified columns and splits the dataset into features and target.</p> Source code in <code>src/quoptuna/backend/utils/data_utils/prepare.py</code> <pre><code>def select_columns(self):\n    \"\"\"Selects specified columns and splits the dataset into features and target.\"\"\"\n    if self.x_cols is None or self.y_col is None:\n        msg = \"x_cols and y_col must be provided\"\n        raise ValueError(msg)\n    x = self.dataset.get(\"x\")\n    y = self.dataset.get(\"y\")\n    return x, y\n</code></pre> <code></code> update_column_names <pre><code>update_column_names(dataframe: DataFrame | None = None)\n</code></pre> <p>Update column names in x_cols if they are single length after conversion to string. Also updates the corresponding DataFrame if provided.</p> Source code in <code>src/quoptuna/backend/utils/data_utils/prepare.py</code> <pre><code>def update_column_names(self, dataframe: pd.DataFrame | None = None):\n    \"\"\"Update column names in x_cols if they are single length after conversion to string.\n    Also updates the corresponding DataFrame if provided.\n    \"\"\"\n    if self.x_cols is not None:\n        for i, col in enumerate(self.x_cols):\n            if len(str(col)) == 1:\n                self.x_cols[i] = f\"feat: {col}\"\n                if dataframe is not None and col in dataframe.columns:\n                    dataframe = dataframe.rename(columns={col: f\"feat: {col}\"})\n    return dataframe\n</code></pre>"},{"location":"api_docs.html#quoptuna.backend.xai","title":"xai","text":"MODULE DESCRIPTION <code>xai</code>"},{"location":"api_docs.html#quoptuna.backend.xai.xai","title":"xai","text":"CLASS DESCRIPTION <code>XAI</code>"},{"location":"api_docs.html#quoptuna.backend.xai.xai.XAI","title":"XAI","text":"<pre><code>XAI(model: BaseEstimator, data: DataSet, config: XAIConfig | None = None)\n</code></pre> METHOD DESCRIPTION <code>generate_report_with_langchain</code> <p>Generate comprehensive report using LangChain and multimodal LLM.</p> <code>get_average_precision_score</code> <p>Get the average precision score of the model.</p> <code>get_classes</code> <p>Get model classes.</p> <code>get_classification_report</code> <p>Get the classification report of the model.</p> <code>get_cohens_kappa</code> <p>Get the cohens kappa of the model.</p> <code>get_confusion_matrix</code> <p>Get the confusion matrix of the model.</p> <code>get_f1_score</code> <p>Get the f1 score of the model.</p> <code>get_log_loss</code> <p>Get the log loss of the model.</p> <code>get_mcc</code> <p>Get the mcc of the model.</p> <code>get_plot</code> <p>Generate plot with given configuration.</p> <code>get_precision</code> <p>Get the precision of the model.</p> <code>get_precision_recall_curve</code> <p>Get the precision recall curve of the model.</p> <code>get_recall</code> <p>Get the recall of the model.</p> <code>get_report</code> <p>Get the report of the model.</p> <code>get_roc_auc_score</code> <p>Get the roc auc score of the model.</p> <code>get_roc_curve</code> <p>Get the roc curve of the model.</p> <code>load_state</code> <p>Loads the state of the class from a pkl file.</p> <code>plot_confusion_matrix</code> <p>Plot confusion matrix with given configuration.</p> <code>save_state</code> <p>Saves the state of the class and its variables in a pkl file.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def __init__(\n    self,\n    model: BaseEstimator,\n    data: DataSet,\n    config: XAIConfig | None = None,\n) -&gt; None:\n    if model is None:\n        msg = \"Model cannot be None\"\n        raise TypeError(msg)\n\n    self.config = config or XAIConfig()\n    self.model = model\n    self.data = data\n\n    # Explicitly declare instance attributes\n    self.use_proba: bool = self.config.use_proba\n    self.onsubset: bool = self.config.onsubset\n    self.feature_names: list[str] | None = self.config.feature_names\n    self.subset_size: int = self.config.subset_size\n    self.max_display: int = self.config.max_display\n    self.data_key: str = self.config.data_key\n    self.x_test_key: str = self.config.x_test_key\n    self.y_test_key: str = self.config.y_test_key\n\n    self._classes = self.get_classes\n    data_frame = self.data.get(self.data_key)\n    if self.feature_names is None and isinstance(data_frame, pd.DataFrame):\n        self.feature_names = list(data_frame.columns)\n\n    if self.use_proba:\n        self.validate_predict_proba()\n\n    # Initialize these as None, they'll be computed on demand\n    self._explainer: Explainer | None = None\n    self._shap_values: shap.Explanation | None = None\n    self._shap_values_each_class: dict[str, shap.Explanation] | None = None\n    self._x_test: pd.DataFrame | None = None\n    self._y_test: pd.Series | None = None\n    self._predictions: pd.Series | None = None\n    self._predictions_proba: pd.DataFrame | None = None\n</code></pre> <code></code> generate_report_with_langchain <pre><code>generate_report_with_langchain(api_key: str, model_name: str = 'gpt-4o', provider: str = 'google', num_waterfall_plots: int = 5)\n</code></pre> <p>Generate comprehensive report using LangChain and multimodal LLM.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def generate_report_with_langchain(\n    self,\n    api_key: str,\n    model_name: str = \"gpt-4o\",\n    provider: str = \"google\",\n    num_waterfall_plots: int = 5,\n):\n    \"\"\"Generate comprehensive report using LangChain and multimodal LLM.\"\"\"\n    chat = self._initialize_chat(api_key, model_name, provider)\n\n    report = self.get_report()\n    images = self._generate_report_images(num_waterfall_plots)\n\n    prompt2 = Path(\"prompt.txt\").read_text()\n    return self._generate_final_report(chat, report, images, prompt2)\n</code></pre> <code></code> get_average_precision_score <pre><code>get_average_precision_score()\n</code></pre> <p>Get the average precision score of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_average_precision_score(self):\n    \"\"\"Get the average precision score of the model.\"\"\"\n    return average_precision_score(self.y_test, self.predictions_proba)\n</code></pre> <code></code> get_classes <pre><code>get_classes() -&gt; dict[int, str]\n</code></pre> <p>Get model classes.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_classes(self) -&gt; dict[int, str]:\n    \"\"\"Get model classes.\"\"\"\n    if not hasattr(self.model, \"classes_\"):\n        msg = \"Model does not have a classes_ attribute\"\n        raise TypeError(msg)\n    return self.model.classes_\n</code></pre> <code></code> get_classification_report <pre><code>get_classification_report()\n</code></pre> <p>Get the classification report of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_classification_report(self):\n    \"\"\"Get the classification report of the model.\"\"\"\n    return classification_report(self.y_test, self.predictions)\n</code></pre> <code></code> get_cohens_kappa <pre><code>get_cohens_kappa()\n</code></pre> <p>Get the cohens kappa of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_cohens_kappa(self):\n    \"\"\"Get the cohens kappa of the model.\"\"\"\n    return cohen_kappa_score(self.y_test, self.predictions)\n</code></pre> <code></code> get_confusion_matrix <pre><code>get_confusion_matrix()\n</code></pre> <p>Get the confusion matrix of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_confusion_matrix(self):\n    \"\"\"Get the confusion matrix of the model.\"\"\"\n    return confusion_matrix(self.y_test, self.predictions)\n</code></pre> <code></code> get_f1_score <pre><code>get_f1_score()\n</code></pre> <p>Get the f1 score of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_f1_score(self):\n    \"\"\"Get the f1 score of the model.\"\"\"\n    return f1_score(self.y_test, self.predictions)\n</code></pre> <code></code> get_log_loss <pre><code>get_log_loss()\n</code></pre> <p>Get the log loss of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_log_loss(self):\n    \"\"\"Get the log loss of the model.\"\"\"\n    return log_loss(self.y_test, self.predictions_proba)\n</code></pre> <code></code> get_mcc <pre><code>get_mcc()\n</code></pre> <p>Get the mcc of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_mcc(self):\n    \"\"\"Get the mcc of the model.\"\"\"\n    return matthews_corrcoef(self.y_test, self.predictions)\n</code></pre> <code></code> get_plot <pre><code>get_plot(plot_type: PlotType, max_display: int = DEFAULT_MAX_DISPLAY, class_index: int = -1, index: int = 0, save_config: dict | None = None)\n</code></pre> <p>Generate plot with given configuration.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_plot(\n    self,\n    plot_type: PlotType,\n    max_display: int = DEFAULT_MAX_DISPLAY,\n    class_index: int = -1,\n    index: int = 0,\n    save_config: dict | None = None,\n):\n    \"\"\"Generate plot with given configuration.\"\"\"\n    try:\n        values = self._get_plot_values(class_index)\n        return self._generate_plot(plot_type, values, max_display, index, save_config)\n    except (ValueError, TypeError, KeyError, RuntimeError) as e:\n        self._handle_plot_error(plot_type, e)\n</code></pre> <code></code> get_precision <pre><code>get_precision()\n</code></pre> <p>Get the precision of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_precision(self):\n    \"\"\"Get the precision of the model.\"\"\"\n    return precision_score(self.y_test, self.predictions)\n</code></pre> <code></code> get_precision_recall_curve <pre><code>get_precision_recall_curve()\n</code></pre> <p>Get the precision recall curve of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_precision_recall_curve(self):\n    \"\"\"Get the precision recall curve of the model.\"\"\"\n    return precision_recall_curve(self.y_test, self.predictions_proba)\n</code></pre> <code></code> get_recall <pre><code>get_recall()\n</code></pre> <p>Get the recall of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_recall(self):\n    \"\"\"Get the recall of the model.\"\"\"\n    return recall_score(self.y_test, self.predictions)\n</code></pre> <code></code> get_report <pre><code>get_report()\n</code></pre> <p>Get the report of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_report(self):\n    \"\"\"Get the report of the model.\"\"\"\n    report = {}\n    metrics = {\n        \"confusion_matrix\": self.get_confusion_matrix,\n        \"classification_report\": self.get_classification_report,\n        \"roc_curve\": self.get_roc_curve,\n        \"roc_auc_score\": self.get_roc_auc_score,\n        \"precision_recall_curve\": self.get_precision_recall_curve,\n        \"average_precision_score\": self.get_average_precision_score,\n        \"f1_score\": self.get_f1_score,\n        \"mcc\": self.get_mcc,\n        \"log_loss\": self.get_log_loss,\n        \"cohens_kappa\": self.get_cohens_kappa,\n        \"precision\": self.get_precision,\n        \"recall\": self.get_recall,\n    }\n\n    try:\n        for key, func in metrics.items():\n            report[key] = func()\n    except (ValueError, TypeError) as e:\n        report[key] = str(e)\n    return report\n</code></pre> <code></code> get_roc_auc_score <pre><code>get_roc_auc_score()\n</code></pre> <p>Get the roc auc score of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_roc_auc_score(self):\n    \"\"\"Get the roc auc score of the model.\"\"\"\n    return roc_auc_score(self.y_test, self.predictions_proba)\n</code></pre> <code></code> get_roc_curve <pre><code>get_roc_curve()\n</code></pre> <p>Get the roc curve of the model.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def get_roc_curve(self):\n    \"\"\"Get the roc curve of the model.\"\"\"\n    return roc_curve(self.y_test, self.predictions_proba)\n</code></pre> <code></code> load_state <code>classmethod</code> <pre><code>load_state(file_path: str)\n</code></pre> <p>Loads the state of the class from a pkl file. Warning: Only use this method with trusted data sources as pickle can be unsafe.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>@classmethod\ndef load_state(cls, file_path: str):\n    \"\"\"Loads the state of the class from a pkl file.\n    Warning: Only use this method with trusted data sources as pickle can be unsafe.\n    \"\"\"\n    with Path(file_path).open(\"rb\") as f:\n        return pickle.load(f)  # noqa: S301\n</code></pre> <code></code> plot_confusion_matrix <pre><code>plot_confusion_matrix(plot_config: dict | None = None)\n</code></pre> <p>Plot confusion matrix with given configuration.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def plot_confusion_matrix(self, plot_config: dict | None = None):\n    \"\"\"Plot confusion matrix with given configuration.\"\"\"\n    from sklearn.metrics import ConfusionMatrixDisplay\n\n    config = {\n        \"include_values\": True,\n        \"cmap\": \"viridis\",\n        \"xticks_rotation\": \"horizontal\",\n        \"values_format\": None,\n        \"ax\": None,\n        \"colorbar\": True,\n        \"im_kw\": None,\n        \"text_kw\": None,\n        **(plot_config or {}),\n    }\n\n    cm = self.get_confusion_matrix()\n    ConfusionMatrixDisplay(cm).plot(**config)\n\n    if plot_config and plot_config.get(\"save_path\"):\n        plt.savefig(\n            Path(plot_config[\"save_path\"]) / plot_config[\"save_name\"],\n            format=plot_config.get(\"save_format\", \"png\"),\n            dpi=plot_config.get(\"save_dpi\", 300),\n            bbox_inches=\"tight\",\n        )\n\n    return plt.gcf()\n</code></pre> <code></code> save_state <pre><code>save_state(file_path: str)\n</code></pre> <p>Saves the state of the class and its variables in a pkl file.</p> Source code in <code>src/quoptuna/backend/xai/xai.py</code> <pre><code>def save_state(self, file_path: str):\n    \"\"\"Saves the state of the class and its variables in a pkl file.\"\"\"\n    with Path(file_path).open(\"wb\") as f:\n        pickle.dump(self, f)\n</code></pre>"},{"location":"api_docs.html#quoptuna.frontend","title":"frontend","text":"MODULE DESCRIPTION <code>main_page</code> <code>pages</code> <code>sidebar</code> <code>support</code>"},{"location":"api_docs.html#quoptuna.frontend.main_page","title":"main_page","text":"FUNCTION DESCRIPTION <code>main_page</code> <p>This is the main page of the app.</p>"},{"location":"api_docs.html#quoptuna.frontend.main_page.main_page","title":"main_page","text":"<pre><code>main_page()\n</code></pre> <p>This is the main page of the app.</p> Source code in <code>src/quoptuna/frontend/main_page.py</code> <pre><code>def main_page():\n    \"\"\"This is the main page of the app.\"\"\"\n    st.markdown(\n        '&lt;div class=\"main-title\"&gt;QuOptuna: Quantum-Enhanced ML Optimization&lt;/div&gt;',\n        unsafe_allow_html=True,\n    )\n\n    st.markdown(\n        \"\"\"\n        &lt;div class=\"description\"&gt;\n        Welcome to QuOptuna! A comprehensive platform for quantum-enhanced machine learning\n        with automated hyperparameter optimization, model training, and explainable AI.\n        &lt;/div&gt;\n        \"\"\",\n        unsafe_allow_html=True,\n    )\n\n    # Introduction\n    st.markdown(\"## \ud83d\ude80 Getting Started\")\n    st.markdown(\n        \"\"\"\n        QuOptuna provides a complete workflow for training and analyzing quantum and classical machine learning models:\n\n        1. **\ud83d\udcca Dataset Selection** - Load datasets from UCI ML Repository or upload your own\n        2. **\ud83c\udfaf Optimization** - Automated hyperparameter tuning with Optuna\n        3. **\ud83d\udd0d SHAP Analysis** - Explainable AI with comprehensive visualizations\n        4. **\ud83d\udcdd Report Generation** - AI-powered analysis reports\n        \"\"\"\n    )\n\n    # Feature highlights\n    col1, col2, col3 = st.columns(3)\n\n    with col1:\n        st.markdown(\"### \ud83c\udfaf Optimization\")\n        st.markdown(\n            \"\"\"\n            - Multi-objective optimization\n            - Support for quantum &amp; classical models\n            - Automatic trial management\n            - Real-time visualization\n            \"\"\"\n        )\n\n    with col2:\n        st.markdown(\"### \ud83d\udd0d Explainability\")\n        st.markdown(\n            \"\"\"\n            - SHAP value analysis\n            - Multiple visualization types\n            - Feature importance ranking\n            - Individual prediction explanations\n            \"\"\"\n        )\n\n    with col3:\n        st.markdown(\"### \ud83d\udcca Analytics\")\n        st.markdown(\n            \"\"\"\n            - Performance metrics\n            - Confusion matrices\n            - Model comparisons\n            - AI-generated reports\n            \"\"\"\n        )\n\n    # Quick start guide\n    st.markdown(\"## \ud83d\udcd6 Quick Start Guide\")\n\n    with st.expander(\"\ud83c\udf93 How to use QuOptuna\", expanded=True):\n        st.markdown(\n            \"\"\"\n            ### Step-by-Step Workflow:\n\n            #### 1. Dataset Selection\n            - Navigate to **\ud83d\udcca Dataset Selection** in the sidebar\n            - Choose a dataset from UCI ML Repository or upload your own CSV\n            - Configure target and feature columns\n            - Transform target values to -1 and 1 for binary classification\n\n            #### 2. Data Preparation &amp; Optimization\n            - Go to **\ud83c\udfaf Optimization** page\n            - Prepare your data (automatic train/test split)\n            - Configure optimization parameters (number of trials, database name)\n            - Run hyperparameter optimization\n            - Review best performing models\n\n            #### 3. SHAP Analysis &amp; Reporting\n            - Navigate to **\ud83d\udd0d SHAP Analysis** page\n            - Select a trial from the best performers\n            - Train the model with optimized parameters\n            - Run SHAP analysis to understand feature importance\n            - Generate visualizations (bar, beeswarm, violin, heatmap, waterfall)\n            - Create AI-powered analysis reports with your preferred LLM provider\n\n            ### Supported Models:\n\n            **Quantum Models:**\n            - Data Reuploading Classifier\n            - Circuit-Centric Classifier\n            - Quantum Kitchen Sinks\n            - Quantum Metric Learner\n            - Dressed Quantum Circuit Classifier\n\n            **Classical Models:**\n            - Support Vector Classifier (SVC)\n            - Multi-Layer Perceptron (MLP)\n            - Perceptron\n            \"\"\"\n        )\n\n    # Tips and best practices\n    with st.expander(\"\ud83d\udca1 Tips &amp; Best Practices\"):\n        st.markdown(\n            \"\"\"\n            ### Optimization Tips:\n            - Start with 50-100 trials for initial exploration\n            - Use more trials (100-200) for fine-tuning\n            - Monitor both quantum and classical model performance\n\n            ### SHAP Analysis Tips:\n            - Use subset of data (50-100 samples) for faster computation\n            - Generate multiple plot types for comprehensive understanding\n            - Use waterfall plots to explain individual predictions\n\n            ### Report Generation Tips:\n            - Provide dataset context for better AI-generated insights\n            - Use faster models (like Gemini Flash) for quick reports\n            - Use advanced models (like GPT-4 or Gemini Pro) for detailed analysis\n            \"\"\"\n        )\n\n    # Footer\n    st.markdown(\"---\")\n    st.markdown(\n        \"\"\"\n        &lt;div style=\"text-align: center; color: #888;\"&gt;\n        &lt;p&gt;Built with \u2764\ufe0f by the QuOptuna Team |\n        &lt;a href=\"https://github.com/Qentora/quoptuna\" target=\"_blank\"&gt;GitHub&lt;/a&gt; |\n        &lt;a href=\"https://github.com/Qentora/quoptuna/issues\" target=\"_blank\"&gt;Report Issues&lt;/a&gt;\n        &lt;/p&gt;\n        &lt;/div&gt;\n        \"\"\",\n        unsafe_allow_html=True,\n    )\n</code></pre>"},{"location":"api_docs.html#quoptuna.frontend.pages","title":"pages","text":"MODULE DESCRIPTION <code>1_dataset_selection</code> <p>Dataset Selection Page - Select UCI dataset or upload custom data.</p> <code>2_optimization</code> <p>Optimization Page - Prepare data and run hyperparameter optimization.</p> <code>3_shap_analysis</code> <p>SHAP Analysis &amp; Report Generation Page.</p> <code>shap</code>"},{"location":"api_docs.html#quoptuna.frontend.pages.1_dataset_selection","title":"1_dataset_selection","text":"<p>Dataset Selection Page - Select UCI dataset or upload custom data.</p> FUNCTION DESCRIPTION <code>configure_dataset</code> <p>Configure target and feature columns.</p> <code>fetch_uci_dataset</code> <p>Fetch dataset from UCI ML Repository.</p> <code>initialize_session_state</code> <p>Initialize session state variables.</p> <code>main</code> <p>Main function for dataset selection page.</p> <code>upload_custom_dataset</code> <p>Upload custom CSV dataset.</p>"},{"location":"api_docs.html#quoptuna.frontend.pages.1_dataset_selection.configure_dataset","title":"configure_dataset","text":"<pre><code>configure_dataset()\n</code></pre> <p>Configure target and feature columns.</p> Source code in <code>src/quoptuna/frontend/pages/1_dataset_selection.py</code> <pre><code>def configure_dataset():\n    \"\"\"Configure target and feature columns.\"\"\"\n    if not st.session_state[\"dataset_loaded\"]:\n        return\n\n    df = st.session_state[\"dataset_df\"]\n\n    st.subheader(\"\u2699\ufe0f Configure Dataset\")\n\n    # Show data preview\n    with st.expander(\"\ud83d\udc41\ufe0f Data Preview\", expanded=True):\n        st.dataframe(df.head(10), use_container_width=True)\n\n        col1, col2, col3 = st.columns(3)\n        with col1:\n            st.metric(\"Rows\", len(df))\n        with col2:\n            st.metric(\"Columns\", len(df.columns))\n        with col3:\n            missing = df.isnull().sum().sum()\n            st.metric(\"Missing Values\", missing)\n\n    # Column selection\n    st.markdown(\"### Select Target Column\")\n    target_col = st.selectbox(\n        \"Target Column (y):\",\n        df.columns.tolist(),\n        help=\"Select the column you want to predict\",\n    )\n\n    st.markdown(\"### Feature Columns\")\n    feature_cols = st.multiselect(\n        \"Feature Columns (X):\",\n        [col for col in df.columns if col != target_col],\n        default=[col for col in df.columns if col != target_col],\n        help=\"Select the features to use for prediction\",\n    )\n\n    # Target transformation\n    st.markdown(\"### Target Transformation\")\n    st.info(\"\u26a0\ufe0f QuOptuna requires binary classification targets to be encoded as -1 and 1\")\n\n    unique_values = df[target_col].unique()\n    st.write(f\"Current unique values in target: {unique_values}\")\n\n    transform_target = st.checkbox(\n        \"Transform target values to -1 and 1\",\n        value=True,\n        help=\"Automatically transform binary targets to -1 and 1\",\n    )\n\n    if transform_target and len(unique_values) == 2:\n        col1, col2 = st.columns(2)\n        with col1:\n            negative_value = st.selectbox(\"Map to -1:\", unique_values)\n        with col2:\n            positive_value = st.selectbox(\n                \"Map to 1:\",\n                [v for v in unique_values if v != negative_value],\n            )\n\n    # Save configuration\n    if st.button(\"\ud83d\udcbe Save Configuration\", type=\"primary\"):\n        if not feature_cols:\n            st.error(\"Please select at least one feature column!\")\n            return\n\n        # Apply transformations\n        processed_df = df.copy()\n\n        # Handle missing values\n        if processed_df.isnull().sum().sum() &gt; 0:\n            st.warning(\"\u26a0\ufe0f Removing rows with missing values...\")\n            processed_df = processed_df.dropna()\n\n        # Transform target if needed\n        if transform_target and len(unique_values) == 2:\n            processed_df[target_col] = processed_df[target_col].replace(\n                {negative_value: -1, positive_value: 1}\n            )\n            st.success(f\"\u2705 Transformed target: {negative_value} \u2192 -1, {positive_value} \u2192 1\")\n\n        # Rename target column to 'target'\n        if target_col != \"target\":\n            processed_df = processed_df.rename(columns={target_col: \"target\"})\n            target_col = \"target\"\n\n        # Keep only selected columns\n        processed_df = processed_df[feature_cols + [target_col]]\n\n        # Save to file\n        data_dir = \"data\"\n        if not os.path.exists(data_dir):\n            os.makedirs(data_dir)\n\n        file_path = mock_csv_data(\n            processed_df,\n            tmp_path=data_dir,\n            file_name=st.session_state[\"dataset_name\"],\n        )\n\n        # Update session state\n        st.session_state[\"file_path\"] = file_path\n        st.session_state[\"target_column\"] = target_col\n        st.session_state[\"feature_columns\"] = feature_cols\n        st.session_state[\"dataset_df\"] = processed_df\n\n        st.success(f\"\u2705 Configuration saved! Data saved to: {file_path}\")\n        st.info(\"\ud83d\udc49 Proceed to the next page: **Data Preparation &amp; Optimization**\")\n</code></pre>"},{"location":"api_docs.html#quoptuna.frontend.pages.1_dataset_selection.fetch_uci_dataset","title":"fetch_uci_dataset","text":"<pre><code>fetch_uci_dataset()\n</code></pre> <p>Fetch dataset from UCI ML Repository.</p> Source code in <code>src/quoptuna/frontend/pages/1_dataset_selection.py</code> <pre><code>def fetch_uci_dataset():\n    \"\"\"Fetch dataset from UCI ML Repository.\"\"\"\n    st.subheader(\"\ud83d\udcca UCI ML Repository\")\n\n    # Popular datasets for quick access\n    popular_datasets = {\n        \"Statlog (Australian Credit Approval)\": 143,\n        \"Blood Transfusion Service Center\": 176,\n        \"Banknote Authentication\": 267,\n        \"Heart Disease\": 45,\n        \"Ionosphere\": 225,\n    }\n\n    dataset_choice = st.radio(\n        \"Choose dataset source:\",\n        [\"Popular Datasets\", \"Custom UCI ID\"],\n        horizontal=True,\n    )\n\n    dataset_id = None\n    dataset_name = None\n\n    if dataset_choice == \"Popular Datasets\":\n        selected = st.selectbox(\n            \"Select a dataset:\",\n            list(popular_datasets.keys()),\n        )\n        if selected:\n            dataset_id = popular_datasets[selected]\n            dataset_name = selected\n    else:\n        dataset_id = st.number_input(\n            \"Enter UCI Dataset ID:\",\n            min_value=1,\n            max_value=1000,\n            value=143,\n            help=\"Find dataset IDs at https://archive.ics.uci.edu/datasets\",\n        )\n        dataset_name = f\"UCI_Dataset_{dataset_id}\"\n\n    if st.button(\"Load UCI Dataset\", type=\"primary\"):\n        try:\n            with st.spinner(f\"Fetching dataset {dataset_id}...\"):\n                dataset = fetch_ucirepo(id=dataset_id)\n\n                # Combine features and targets\n                X = dataset.data.features\n                y = dataset.data.targets\n                df = pd.concat([X, y], axis=1)\n\n                # Store in session state\n                st.session_state[\"dataset_df\"] = df\n                st.session_state[\"dataset_name\"] = dataset_name\n                st.session_state[\"dataset_metadata\"] = dataset.metadata\n                st.session_state[\"dataset_loaded\"] = True\n\n                st.success(f\"\u2705 Dataset '{dataset_name}' loaded successfully!\")\n\n                # Display metadata\n                with st.expander(\"\ud83d\udccb Dataset Metadata\", expanded=True):\n                    metadata = dataset.metadata\n                    col1, col2 = st.columns(2)\n                    with col1:\n                        st.metric(\"Instances\", metadata.get(\"num_instances\", \"N/A\"))\n                        st.metric(\"Features\", metadata.get(\"num_features\", \"N/A\"))\n                    with col2:\n                        st.metric(\"Area\", metadata.get(\"area\", \"N/A\"))\n                        st.metric(\"Tasks\", \", \".join(metadata.get(\"tasks\", [])))\n\n                    st.write(\"**Abstract:**\", metadata.get(\"abstract\", \"N/A\"))\n\n        except Exception as e:\n            st.error(f\"\u274c Error loading dataset: {e}\")\n</code></pre>"},{"location":"api_docs.html#quoptuna.frontend.pages.1_dataset_selection.initialize_session_state","title":"initialize_session_state","text":"<pre><code>initialize_session_state()\n</code></pre> <p>Initialize session state variables.</p> Source code in <code>src/quoptuna/frontend/pages/1_dataset_selection.py</code> <pre><code>def initialize_session_state():\n    \"\"\"Initialize session state variables.\"\"\"\n    defaults = {\n        \"dataset_loaded\": False,\n        \"dataset_df\": None,\n        \"dataset_name\": None,\n        \"dataset_metadata\": None,\n        \"file_path\": None,\n        \"target_column\": None,\n        \"feature_columns\": None,\n    }\n    for key, value in defaults.items():\n        if key not in st.session_state:\n            st.session_state[key] = value\n</code></pre>"},{"location":"api_docs.html#quoptuna.frontend.pages.1_dataset_selection.main","title":"main","text":"<pre><code>main()\n</code></pre> <p>Main function for dataset selection page.</p> Source code in <code>src/quoptuna/frontend/pages/1_dataset_selection.py</code> <pre><code>def main():\n    \"\"\"Main function for dataset selection page.\"\"\"\n    st.set_page_config(\n        page_title=\"Dataset Selection - QuOptuna\",\n        page_icon=\"\ud83d\udcca\",\n        layout=\"wide\",\n    )\n\n    initialize_session_state()\n\n    st.title(\"\ud83d\udcca Dataset Selection\")\n    st.markdown(\n        \"\"\"\n        Select a dataset from the UCI ML Repository or upload your own CSV file.\n        This is the first step in the QuOptuna workflow.\n        \"\"\"\n    )\n\n    # Dataset source selection\n    tab1, tab2 = st.tabs([\"\ud83c\udf10 UCI ML Repository\", \"\ud83d\udcc1 Upload Custom Dataset\"])\n\n    with tab1:\n        fetch_uci_dataset()\n\n    with tab2:\n        upload_custom_dataset()\n\n    # Show configuration if dataset is loaded\n    if st.session_state[\"dataset_loaded\"]:\n        st.divider()\n        configure_dataset()\n</code></pre>"},{"location":"api_docs.html#quoptuna.frontend.pages.1_dataset_selection.upload_custom_dataset","title":"upload_custom_dataset","text":"<pre><code>upload_custom_dataset()\n</code></pre> <p>Upload custom CSV dataset.</p> Source code in <code>src/quoptuna/frontend/pages/1_dataset_selection.py</code> <pre><code>def upload_custom_dataset():\n    \"\"\"Upload custom CSV dataset.\"\"\"\n    st.subheader(\"\ud83d\udcc1 Upload Custom Dataset\")\n\n    uploaded_file = st.file_uploader(\n        \"Choose a CSV file\",\n        type=[\"csv\"],\n        help=\"Upload a CSV file with your dataset\",\n    )\n\n    if uploaded_file:\n        try:\n            df = pd.read_csv(uploaded_file)\n\n            # Store in session state\n            st.session_state[\"dataset_df\"] = df\n            st.session_state[\"dataset_name\"] = uploaded_file.name.replace(\".csv\", \"\")\n            st.session_state[\"dataset_loaded\"] = True\n\n            st.success(f\"\u2705 File '{uploaded_file.name}' uploaded successfully!\")\n\n        except Exception as e:\n            st.error(f\"\u274c Error reading file: {e}\")\n</code></pre>"},{"location":"api_docs.html#quoptuna.frontend.pages.2_optimization","title":"2_optimization","text":"<p>Optimization Page - Prepare data and run hyperparameter optimization.</p> FUNCTION DESCRIPTION <code>display_results</code> <p>Display optimization results.</p> <code>initialize_session_state</code> <p>Initialize session state variables.</p> <code>main</code> <p>Main function for optimization page.</p> <code>prepare_data</code> <p>Prepare data for optimization.</p> <code>run_optimization</code> <p>Run hyperparameter optimization.</p>"},{"location":"api_docs.html#quoptuna.frontend.pages.2_optimization.display_results","title":"display_results","text":"<pre><code>display_results()\n</code></pre> <p>Display optimization results.</p> Source code in <code>src/quoptuna/frontend/pages/2_optimization.py</code> <pre><code>def display_results():\n    \"\"\"Display optimization results.\"\"\"\n    if not st.session_state.get(\"optimization_complete\"):\n        return\n\n    st.subheader(\"\ud83c\udfc6 Optimization Results\")\n\n    best_trials = st.session_state[\"best_trials\"]\n\n    if not best_trials:\n        st.warning(\"No best trials found.\")\n        return\n\n    # Format trial information\n    def format_trial(trial):\n        quantum_f1 = trial.user_attrs.get(\"Quantum_f1_score\", 0)\n        classical_f1 = trial.user_attrs.get(\"Classical_f1_score\", 0)\n        f1_score = quantum_f1 if quantum_f1 != 0 else classical_f1\n        model_type = trial.params.get(\"model_type\", \"Unknown\")\n        return f\"Trial {trial.number} - {model_type} - F1 Score: {f1_score:.4f}\"\n\n    # Display best trials\n    st.write(f\"**Found {len(best_trials)} best trial(s):**\")\n\n    for i, trial in enumerate(best_trials[:5]):  # Show top 5\n        with st.expander(format_trial(trial), expanded=(i == 0)):\n            col1, col2 = st.columns(2)\n\n            with col1:\n                st.write(\"**Performance:**\")\n                st.write(f\"- Quantum F1: {trial.user_attrs.get('Quantum_f1_score', 0):.4f}\")\n                st.write(f\"- Classical F1: {trial.user_attrs.get('Classical_f1_score', 0):.4f}\")\n\n            with col2:\n                st.write(\"**Key Parameters:**\")\n                st.write(f\"- Model Type: {trial.params.get('model_type')}\")\n                important_params = {\n                    k: v\n                    for k, v in trial.params.items()\n                    if k in [\"learning_rate\", \"n_layers\", \"batch_size\", \"C\", \"gamma\"]\n                }\n                for k, v in important_params.items():\n                    st.write(f\"- {k}: {v}\")\n\n            with st.expander(\"All Parameters\"):\n                st.json(trial.params)\n\n    st.info(\"\ud83d\udc49 Proceed to the next page: **Model Training &amp; Evaluation**\")\n</code></pre>"},{"location":"api_docs.html#quoptuna.frontend.pages.2_optimization.initialize_session_state","title":"initialize_session_state","text":"<pre><code>initialize_session_state()\n</code></pre> <p>Initialize session state variables.</p> Source code in <code>src/quoptuna/frontend/pages/2_optimization.py</code> <pre><code>def initialize_session_state():\n    \"\"\"Initialize session state variables.\"\"\"\n    defaults = {\n        \"data_dict\": None,\n        \"optimizer\": None,\n        \"study\": None,\n        \"best_trials\": None,\n        \"optimization_complete\": False,\n        \"db_name\": None,\n        \"study_name\": None,\n    }\n    for key, value in defaults.items():\n        if key not in st.session_state:\n            st.session_state[key] = value\n</code></pre>"},{"location":"api_docs.html#quoptuna.frontend.pages.2_optimization.main","title":"main","text":"<pre><code>main()\n</code></pre> <p>Main function for optimization page.</p> Source code in <code>src/quoptuna/frontend/pages/2_optimization.py</code> <pre><code>def main():\n    \"\"\"Main function for optimization page.\"\"\"\n    st.set_page_config(\n        page_title=\"Optimization - QuOptuna\",\n        page_icon=\"\ud83c\udfaf\",\n        layout=\"wide\",\n    )\n\n    initialize_session_state()\n\n    st.title(\"\ud83c\udfaf Data Preparation &amp; Optimization\")\n    st.markdown(\n        \"\"\"\n        Prepare your data and run hyperparameter optimization to find the best model configuration.\n        \"\"\"\n    )\n\n    # Data preparation section\n    data_ready = prepare_data()\n\n    if data_ready:\n        st.divider()\n        # Optimization section\n        run_optimization()\n\n        # Display results if available\n        if st.session_state.get(\"optimization_complete\"):\n            st.divider()\n            display_results()\n</code></pre>"},{"location":"api_docs.html#quoptuna.frontend.pages.2_optimization.prepare_data","title":"prepare_data","text":"<pre><code>prepare_data()\n</code></pre> <p>Prepare data for optimization.</p> Source code in <code>src/quoptuna/frontend/pages/2_optimization.py</code> <pre><code>def prepare_data():\n    \"\"\"Prepare data for optimization.\"\"\"\n    st.subheader(\"\ud83d\udd27 Data Preparation\")\n\n    if \"file_path\" not in st.session_state or not st.session_state[\"file_path\"]:\n        st.warning(\"\u26a0\ufe0f Please complete Dataset Selection first!\")\n        st.info(\"\ud83d\udc48 Go to the **Dataset Selection** page to load a dataset.\")\n        return False\n\n    st.success(f\"\u2705 Dataset loaded: {st.session_state['dataset_name']}\")\n    st.write(f\"\ud83d\udcc1 File path: `{st.session_state['file_path']}`\")\n\n    # Show dataset info\n    if st.session_state.get(\"dataset_df\") is not None:\n        df = st.session_state[\"dataset_df\"]\n        with st.expander(\"\ud83d\udcca Dataset Summary\"):\n            col1, col2, col3 = st.columns(3)\n            with col1:\n                st.metric(\"Total Samples\", len(df))\n            with col2:\n                st.metric(\"Features\", len(st.session_state[\"feature_columns\"]))\n            with col3:\n                target_dist = df[\"target\"].value_counts()\n                st.write(\"**Target Distribution:**\")\n                st.write(target_dist)\n\n    # Prepare data button\n    if st.button(\"\ud83d\udd28 Prepare Data for Training\", type=\"primary\"):\n        try:\n            with st.spinner(\"Preparing data...\"):\n                data_prep = DataPreparation(\n                    file_path=st.session_state[\"file_path\"],\n                    x_cols=st.session_state[\"feature_columns\"],\n                    y_col=st.session_state[\"target_column\"],\n                )\n\n                # Get data in the format required by Optimizer\n                data_dict = data_prep.get_data(output_type=\"2\")\n\n                # Convert to numpy arrays\n                data_dict[\"train_x\"] = data_dict[\"train_x\"].values\n                data_dict[\"test_x\"] = data_dict[\"test_x\"].values\n                data_dict[\"train_y\"] = data_dict[\"train_y\"].values\n                data_dict[\"test_y\"] = data_dict[\"test_y\"].values\n\n                st.session_state[\"data_dict\"] = data_dict\n\n                st.success(\"\u2705 Data prepared successfully!\")\n\n                # Show split information\n                col1, col2 = st.columns(2)\n                with col1:\n                    st.metric(\"Training Samples\", len(data_dict[\"train_x\"]))\n                with col2:\n                    st.metric(\"Test Samples\", len(data_dict[\"test_x\"]))\n\n        except Exception as e:\n            st.error(f\"\u274c Error preparing data: {e}\")\n            return False\n\n    return st.session_state.get(\"data_dict\") is not None\n</code></pre>"},{"location":"api_docs.html#quoptuna.frontend.pages.2_optimization.run_optimization","title":"run_optimization","text":"<pre><code>run_optimization()\n</code></pre> <p>Run hyperparameter optimization.</p> Source code in <code>src/quoptuna/frontend/pages/2_optimization.py</code> <pre><code>def run_optimization():\n    \"\"\"Run hyperparameter optimization.\"\"\"\n    st.subheader(\"\ud83c\udfaf Hyperparameter Optimization\")\n\n    if st.session_state.get(\"data_dict\") is None:\n        st.warning(\"\u26a0\ufe0f Please prepare data first!\")\n        return\n\n    # Optimization settings\n    col1, col2 = st.columns(2)\n\n    with col1:\n        db_name = st.text_input(\n            \"Database Name:\",\n            value=st.session_state[\"dataset_name\"],\n            help=\"Name for the Optuna database\",\n        )\n\n    with col2:\n        study_name = st.text_input(\n            \"Study Name:\",\n            value=st.session_state[\"dataset_name\"],\n            help=\"Name for this optimization study\",\n        )\n\n    n_trials = st.slider(\n        \"Number of Trials:\",\n        min_value=10,\n        max_value=200,\n        value=100,\n        step=10,\n        help=\"Number of optimization trials to run\",\n    )\n\n    st.info(\n        f\"\ud83d\udca1 This will run {n_trials} trials to find the best hyperparameters \"\n        \"for both classical and quantum models.\"\n    )\n\n    # Run optimization\n    if st.button(\"\ud83d\ude80 Start Optimization\", type=\"primary\"):\n        try:\n            with st.spinner(f\"Running optimization with {n_trials} trials...\"):\n                # Create optimizer\n                optimizer = Optimizer(\n                    db_name=db_name,\n                    study_name=study_name,\n                    data=st.session_state[\"data_dict\"],\n                )\n\n                # Progress bar\n                progress_bar = st.progress(0)\n                status_text = st.empty()\n\n                # Run optimization\n                study, best_trials = optimizer.optimize(n_trials=n_trials)\n\n                progress_bar.progress(100)\n                status_text.text(\"Optimization complete!\")\n\n                # Store results\n                st.session_state[\"optimizer\"] = optimizer\n                st.session_state[\"study\"] = study\n                st.session_state[\"best_trials\"] = best_trials\n                st.session_state[\"optimization_complete\"] = True\n                st.session_state[\"db_name\"] = db_name\n                st.session_state[\"study_name\"] = study_name\n\n                st.success(\"\u2705 Optimization completed successfully!\")\n\n                # Show best trials\n                display_results()\n\n        except Exception as e:\n            st.error(f\"\u274c Error during optimization: {e}\")\n            st.exception(e)\n</code></pre>"},{"location":"api_docs.html#quoptuna.frontend.pages.3_shap_analysis","title":"3_shap_analysis","text":"<p>SHAP Analysis &amp; Report Generation Page.</p> FUNCTION DESCRIPTION <code>display_shap_plots</code> <p>Display SHAP visualizations.</p> <code>generate_report</code> <p>Generate AI report using LLM.</p> <code>initialize_session_state</code> <p>Initialize session state variables.</p> <code>main</code> <p>Main function for SHAP analysis page.</p> <code>run_shap_analysis</code> <p>Run SHAP analysis on the trained model.</p> <code>save_plot</code> <p>Save plot to file.</p> <code>select_trial</code> <p>Select a trial for SHAP analysis.</p> <code>train_model</code> <p>Train the model with selected parameters.</p>"},{"location":"api_docs.html#quoptuna.frontend.pages.3_shap_analysis.display_shap_plots","title":"display_shap_plots","text":"<pre><code>display_shap_plots()\n</code></pre> <p>Display SHAP visualizations.</p> Source code in <code>src/quoptuna/frontend/pages/3_shap_analysis.py</code> <pre><code>def display_shap_plots():\n    \"\"\"Display SHAP visualizations.\"\"\"\n    st.subheader(\"\ud83d\udcca SHAP Visualizations\")\n\n    if not st.session_state.get(\"xai\"):\n        st.warning(\"Please run SHAP analysis first!\")\n        return\n\n    xai = st.session_state[\"xai\"]\n\n    # Create tabs for different plot types\n    tabs = st.tabs(\n        [\n            \"\ud83d\udcca Bar Plot\",\n            \"\ud83d\udc1d Beeswarm Plot\",\n            \"\ud83c\udfbb Violin Plot\",\n            \"\ud83d\udd25 Heatmap\",\n            \"\ud83d\udca7 Waterfall Plot\",\n            \"\ud83d\udcc8 Confusion Matrix\",\n        ]\n    )\n\n    # Bar Plot\n    with tabs[0]:\n        st.markdown(\"### Feature Importance (Bar Plot)\")\n        st.info(\"Shows the mean absolute SHAP value for each feature\")\n\n        try:\n            bar_plot = xai.get_plot(\"bar\", max_display=10, class_index=1)\n            st.image(bar_plot, use_container_width=True)\n\n            if st.button(\"\ud83d\udcbe Save Bar Plot\", key=\"save_bar\"):\n                save_plot(bar_plot, \"bar_plot.png\")\n\n        except Exception as e:\n            st.error(f\"Error generating bar plot: {e}\")\n\n    # Beeswarm Plot\n    with tabs[1]:\n        st.markdown(\"### Feature Impact Distribution (Beeswarm Plot)\")\n        st.info(\"Shows how feature values affect predictions\")\n\n        try:\n            beeswarm_plot = xai.get_plot(\"beeswarm\", max_display=10, class_index=1)\n            st.image(beeswarm_plot, use_container_width=True)\n\n            if st.button(\"\ud83d\udcbe Save Beeswarm Plot\", key=\"save_beeswarm\"):\n                save_plot(beeswarm_plot, \"beeswarm_plot.png\")\n\n        except Exception as e:\n            st.error(f\"Error generating beeswarm plot: {e}\")\n\n    # Violin Plot\n    with tabs[2]:\n        st.markdown(\"### Feature Distribution (Violin Plot)\")\n        st.info(\"Shows the distribution of SHAP values for each feature\")\n\n        try:\n            violin_plot = xai.get_plot(\"violin\", max_display=10, class_index=1)\n            st.image(violin_plot, use_container_width=True)\n\n            if st.button(\"\ud83d\udcbe Save Violin Plot\", key=\"save_violin\"):\n                save_plot(violin_plot, \"violin_plot.png\")\n\n        except Exception as e:\n            st.error(f\"Error generating violin plot: {e}\")\n\n    # Heatmap\n    with tabs[3]:\n        st.markdown(\"### Instance-Level Analysis (Heatmap)\")\n        st.info(\"Shows SHAP values for individual instances\")\n\n        try:\n            heatmap_plot = xai.get_plot(\"heatmap\", max_display=50, class_index=1)\n            st.image(heatmap_plot, use_container_width=True)\n\n            if st.button(\"\ud83d\udcbe Save Heatmap\", key=\"save_heatmap\"):\n                save_plot(heatmap_plot, \"heatmap_plot.png\")\n\n        except Exception as e:\n            st.error(f\"Error generating heatmap: {e}\")\n\n    # Waterfall Plot\n    with tabs[4]:\n        st.markdown(\"### Individual Prediction Explanation (Waterfall Plot)\")\n        st.info(\"Shows how features contribute to a single prediction\")\n\n        sample_idx = st.number_input(\n            \"Sample Index:\",\n            min_value=0,\n            max_value=min(50, len(xai.x_test) - 1),\n            value=0,\n        )\n\n        try:\n            waterfall_plot = xai.get_plot(\"waterfall\", index=sample_idx, class_index=1)\n            st.image(waterfall_plot, use_container_width=True)\n\n            if st.button(\"\ud83d\udcbe Save Waterfall Plot\", key=\"save_waterfall\"):\n                save_plot(waterfall_plot, f\"waterfall_plot_{sample_idx}.png\")\n\n        except Exception as e:\n            st.error(f\"Error generating waterfall plot: {e}\")\n\n    # Confusion Matrix\n    with tabs[5]:\n        st.markdown(\"### Model Performance (Confusion Matrix)\")\n        st.info(\"Shows model classification performance\")\n\n        try:\n            fig = xai.plot_confusion_matrix()\n            st.pyplot(fig)\n\n        except Exception as e:\n            st.error(f\"Error generating confusion matrix: {e}\")\n</code></pre>"},{"location":"api_docs.html#quoptuna.frontend.pages.3_shap_analysis.generate_report","title":"generate_report","text":"<pre><code>generate_report()\n</code></pre> <p>Generate AI report using LLM.</p> Source code in <code>src/quoptuna/frontend/pages/3_shap_analysis.py</code> <pre><code>def generate_report():\n    \"\"\"Generate AI report using LLM.\"\"\"\n    st.subheader(\"\ud83d\udcdd AI-Generated Report\")\n\n    if not st.session_state.get(\"xai\"):\n        st.warning(\"Please run SHAP analysis first!\")\n        return\n\n    st.markdown(\"### Report Configuration\")\n\n    # LLM provider selection\n    provider = st.selectbox(\n        \"LLM Provider:\",\n        [\"google\", \"openai\", \"anthropic\"],\n        help=\"Select the LLM provider for report generation\",\n    )\n\n    api_key = st.text_input(\n        \"API Key:\",\n        type=\"password\",\n        help=\"Enter your API key for the selected provider\",\n    )\n\n    model_name = st.text_input(\n        \"Model Name:\",\n        value=\"models/gemini-2.0-flash-exp\" if provider == \"google\" else \"\",\n        help=\"Enter the model name (e.g., 'models/gemini-2.0-flash-exp' for Google)\",\n    )\n\n    # Dataset information\n    with st.expander(\"\ud83d\udccb Dataset Information (Optional)\", expanded=False):\n        dataset_url = st.text_input(\"Dataset URL:\", value=\"\")\n        dataset_description = st.text_area(\"Dataset Description:\", value=\"\")\n\n    # Generate report\n    if st.button(\"\u2728 Generate Report\", type=\"primary\"):\n        if not api_key:\n            st.error(\"Please provide an API key!\")\n            return\n\n        try:\n            with st.spinner(\"Generating report with AI... This may take a minute.\"):\n                xai = st.session_state[\"xai\"]\n\n                # Prepare dataset info\n                dataset_info = {\n                    \"Name\": st.session_state.get(\"dataset_name\", \"Unknown\"),\n                    \"URL\": dataset_url if dataset_url else \"N/A\",\n                    \"Description\": dataset_description if dataset_description else \"N/A\",\n                    \"Features\": st.session_state.get(\"feature_columns\", []),\n                    \"Target\": st.session_state.get(\"target_column\", \"target\"),\n                }\n\n                # Get basic report\n                report_data = xai.get_report()\n\n                # Generate images for report\n                images = {}\n                plot_types: list[PlotType] = [\"bar\", \"beeswarm\", \"violin\", \"heatmap\"]\n\n                for plot_type in plot_types:\n                    try:\n                        images[plot_type] = xai.get_plot(plot_type, class_index=1)\n                    except Exception as e:\n                        st.warning(f\"Could not generate {plot_type} plot: {e}\")\n\n                # Add confusion matrix\n                try:\n                    fig = xai.plot_confusion_matrix()\n                    img_buf = BytesIO()\n                    fig.savefig(img_buf, format=\"png\")\n                    img_buf.seek(0)\n                    img_base64 = base64.b64encode(img_buf.getvalue()).decode(\"utf-8\")\n                    images[\"confusion_matrix\"] = f\"data:image/png;base64,{img_base64}\"\n                except Exception as e:\n                    st.warning(f\"Could not generate confusion matrix: {e}\")\n\n                # Generate final report with LLM\n                report = xai.generate_report_with_langchain(\n                    provider=provider,\n                    api_key=api_key,\n                    model_name=model_name,\n                    dataset_info=dataset_info,\n                )\n\n                st.session_state[\"report\"] = report\n                st.session_state[\"shap_images\"] = images\n\n                st.success(\"\u2705 Report generated successfully!\")\n\n        except Exception as e:\n            st.error(f\"\u274c Error generating report: {e}\")\n            st.exception(e)\n\n    # Display report\n    if st.session_state.get(\"report\"):\n        st.divider()\n        st.markdown(\"### \ud83d\udcc4 Generated Report\")\n\n        # Download button\n        report_text = st.session_state[\"report\"]\n        st.download_button(\n            label=\"\ud83d\udce5 Download Report (Markdown)\",\n            data=report_text,\n            file_name=f\"{st.session_state['dataset_name']}_report.md\",\n            mime=\"text/markdown\",\n        )\n\n        # Display report\n        st.markdown(report_text)\n</code></pre>"},{"location":"api_docs.html#quoptuna.frontend.pages.3_shap_analysis.initialize_session_state","title":"initialize_session_state","text":"<pre><code>initialize_session_state()\n</code></pre> <p>Initialize session state variables.</p> Source code in <code>src/quoptuna/frontend/pages/3_shap_analysis.py</code> <pre><code>def initialize_session_state():\n    \"\"\"Initialize session state variables.\"\"\"\n    defaults = {\n        \"selected_trial\": None,\n        \"trained_model\": None,\n        \"xai\": None,\n        \"report\": None,\n        \"shap_images\": None,\n    }\n    for key, value in defaults.items():\n        if key not in st.session_state:\n            st.session_state[key] = value\n</code></pre>"},{"location":"api_docs.html#quoptuna.frontend.pages.3_shap_analysis.main","title":"main","text":"<pre><code>main()\n</code></pre> <p>Main function for SHAP analysis page.</p> Source code in <code>src/quoptuna/frontend/pages/3_shap_analysis.py</code> <pre><code>def main():\n    \"\"\"Main function for SHAP analysis page.\"\"\"\n    st.set_page_config(\n        page_title=\"SHAP Analysis - QuOptuna\",\n        page_icon=\"\ud83d\udd0d\",\n        layout=\"wide\",\n    )\n\n    initialize_session_state()\n\n    st.title(\"\ud83d\udd0d SHAP Analysis &amp; Report Generation\")\n    st.markdown(\n        \"\"\"\n        Analyze model behavior with SHAP (SHapley Additive exPlanations) and generate\n        comprehensive AI-powered reports.\n        \"\"\"\n    )\n\n    # Trial selection\n    trial = select_trial()\n\n    if trial:\n        st.divider()\n\n        # Model training\n        model_trained = train_model()\n\n        if model_trained:\n            st.divider()\n\n            # SHAP analysis\n            shap_ready = run_shap_analysis()\n\n            if shap_ready:\n                st.divider()\n\n                # Display SHAP plots\n                display_shap_plots()\n\n                st.divider()\n\n                # Generate report\n                generate_report()\n</code></pre>"},{"location":"api_docs.html#quoptuna.frontend.pages.3_shap_analysis.run_shap_analysis","title":"run_shap_analysis","text":"<pre><code>run_shap_analysis()\n</code></pre> <p>Run SHAP analysis on the trained model.</p> Source code in <code>src/quoptuna/frontend/pages/3_shap_analysis.py</code> <pre><code>def run_shap_analysis():\n    \"\"\"Run SHAP analysis on the trained model.\"\"\"\n    st.subheader(\"\ud83d\udd0d SHAP Analysis\")\n\n    if not st.session_state.get(\"trained_model\"):\n        st.warning(\"Please train the model first!\")\n        return False\n\n    # SHAP configuration\n    st.markdown(\"### Configuration\")\n\n    col1, col2 = st.columns(2)\n\n    with col1:\n        use_proba = st.checkbox(\n            \"Use Probability Predictions\",\n            value=True,\n            help=\"Use probability predictions instead of class predictions\",\n        )\n\n    with col2:\n        onsubset = st.checkbox(\n            \"Use Subset of Data\",\n            value=True,\n            help=\"Use a subset of test data for faster computation\",\n        )\n\n    if onsubset:\n        subset_size = st.slider(\n            \"Subset Size:\",\n            min_value=10,\n            max_value=min(200, len(st.session_state[\"data_dict\"][\"test_x\"])),\n            value=50,\n            help=\"Number of samples to use for SHAP analysis\",\n        )\n    else:\n        subset_size = None\n\n    # Run SHAP analysis\n    if st.button(\"\ud83d\udd2c Run SHAP Analysis\", type=\"primary\"):\n        try:\n            with st.spinner(\"Calculating SHAP values...\"):\n                # Create XAI config\n                config = XAIConfig(\n                    use_proba=use_proba,\n                    onsubset=onsubset,\n                    subset_size=subset_size if onsubset else None,\n                )\n\n                # Create XAI instance\n                xai = XAI(\n                    model=st.session_state[\"trained_model\"],\n                    data=st.session_state[\"data_dict\"],\n                    config=config,\n                )\n\n                st.session_state[\"xai\"] = xai\n\n                st.success(\"\u2705 SHAP analysis completed!\")\n\n        except Exception as e:\n            st.error(f\"\u274c Error during SHAP analysis: {e}\")\n            st.exception(e)\n            return False\n\n    return st.session_state.get(\"xai\") is not None\n</code></pre>"},{"location":"api_docs.html#quoptuna.frontend.pages.3_shap_analysis.save_plot","title":"save_plot","text":"<pre><code>save_plot(plot_data, filename)\n</code></pre> <p>Save plot to file.</p> Source code in <code>src/quoptuna/frontend/pages/3_shap_analysis.py</code> <pre><code>def save_plot(plot_data, filename):\n    \"\"\"Save plot to file.\"\"\"\n    output_dir = \"outputs\"\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    filepath = os.path.join(output_dir, filename)\n\n    # If plot_data is base64 encoded image\n    if isinstance(plot_data, str) and plot_data.startswith(\"data:image\"):\n        img_data = plot_data.split(\",\")[1]\n        with open(filepath, \"wb\") as f:\n            f.write(base64.b64decode(img_data))\n    else:\n        # Assume it's a bytes object or similar\n        with open(filepath, \"wb\") as f:\n            f.write(plot_data)\n\n    st.success(f\"\u2705 Saved to {filepath}\")\n</code></pre>"},{"location":"api_docs.html#quoptuna.frontend.pages.3_shap_analysis.select_trial","title":"select_trial","text":"<pre><code>select_trial()\n</code></pre> <p>Select a trial for SHAP analysis.</p> Source code in <code>src/quoptuna/frontend/pages/3_shap_analysis.py</code> <pre><code>def select_trial():\n    \"\"\"Select a trial for SHAP analysis.\"\"\"\n    st.subheader(\"\ud83c\udfaf Select Trial for Analysis\")\n\n    if not st.session_state.get(\"optimization_complete\"):\n        st.warning(\"\u26a0\ufe0f Please complete optimization first!\")\n        st.info(\"\ud83d\udc48 Go to the **Optimization** page to run optimization.\")\n        return None\n\n    best_trials = st.session_state[\"best_trials\"]\n\n    if not best_trials:\n        st.error(\"No trials found. Please run optimization first.\")\n        return None\n\n    # Format trial for dropdown\n    def format_trial(trial):\n        quantum_f1 = trial.user_attrs.get(\"Quantum_f1_score\", 0)\n        classical_f1 = trial.user_attrs.get(\"Classical_f1_score\", 0)\n        f1_score = quantum_f1 if quantum_f1 != 0 else classical_f1\n        model_type = trial.params.get(\"model_type\", \"Unknown\")\n        return f\"Trial {trial.number} - {model_type} - F1: {f1_score:.4f}\"\n\n    # Trial selection\n    selected_trial = st.selectbox(\n        \"Select a trial:\",\n        best_trials,\n        format_func=format_trial,\n    )\n\n    if selected_trial:\n        with st.expander(\"\ud83d\udccb Trial Details\", expanded=True):\n            col1, col2 = st.columns(2)\n\n            with col1:\n                st.write(\"**Performance Metrics:**\")\n                st.metric(\n                    \"Quantum F1 Score\",\n                    f\"{selected_trial.user_attrs.get('Quantum_f1_score', 0):.4f}\",\n                )\n                st.metric(\n                    \"Classical F1 Score\",\n                    f\"{selected_trial.user_attrs.get('Classical_f1_score', 0):.4f}\",\n                )\n\n            with col2:\n                st.write(\"**Model Information:**\")\n                st.write(f\"**Model Type:** {selected_trial.params.get('model_type')}\")\n                st.write(f\"**Trial Number:** {selected_trial.number}\")\n\n            with st.expander(\"All Parameters\"):\n                st.json(selected_trial.params)\n\n        st.session_state[\"selected_trial\"] = selected_trial\n\n    return selected_trial\n</code></pre>"},{"location":"api_docs.html#quoptuna.frontend.pages.3_shap_analysis.train_model","title":"train_model","text":"<pre><code>train_model()\n</code></pre> <p>Train the model with selected parameters.</p> Source code in <code>src/quoptuna/frontend/pages/3_shap_analysis.py</code> <pre><code>def train_model():\n    \"\"\"Train the model with selected parameters.\"\"\"\n    st.subheader(\"\ud83c\udf93 Model Training\")\n\n    if not st.session_state.get(\"selected_trial\"):\n        st.warning(\"Please select a trial first!\")\n        return False\n\n    trial = st.session_state[\"selected_trial\"]\n\n    if st.button(\"\ud83d\ude80 Train Model\", type=\"primary\"):\n        try:\n            with st.spinner(\"Training model...\"):\n                # Get model parameters\n                model_params = trial.params.copy()\n\n                # Create and train model\n                model = create_model(**model_params)\n\n                # Train the model\n                data_dict = st.session_state[\"data_dict\"]\n                model.fit(data_dict[\"train_x\"], data_dict[\"train_y\"])\n\n                st.session_state[\"trained_model\"] = model\n\n                st.success(\"\u2705 Model trained successfully!\")\n\n        except Exception as e:\n            st.error(f\"\u274c Error training model: {e}\")\n            st.exception(e)\n            return False\n\n    return st.session_state.get(\"trained_model\") is not None\n</code></pre>"},{"location":"api_docs.html#quoptuna.frontend.pages.shap","title":"shap","text":"FUNCTION DESCRIPTION <code>select_data_columns</code> <p>Select x and y columns from the uploaded data.</p>"},{"location":"api_docs.html#quoptuna.frontend.pages.shap.select_data_columns","title":"select_data_columns","text":"<pre><code>select_data_columns(data)\n</code></pre> <p>Select x and y columns from the uploaded data.</p> Source code in <code>src/quoptuna/frontend/pages/shap.py</code> <pre><code>def select_data_columns(data):\n    \"\"\"Select x and y columns from the uploaded data.\"\"\"\n    st.markdown(\"### Select Features\")\n    x_columns, y_column = select_columns(data)  # Assuming select_columns is available\n    if x_columns and y_column:\n        x = data[x_columns]\n        y = data[y_column]\n        return x, y\n    return None, None\n</code></pre>"},{"location":"api_docs.html#quoptuna.frontend.sidebar","title":"sidebar","text":"FUNCTION DESCRIPTION <code>handle_sidebar</code> <p>Handle the sidebar interactions.</p> <code>run_optimization</code> <p>Run the optimization process.</p> <code>select_data</code> <p>Select features from the uploaded data.</p>"},{"location":"api_docs.html#quoptuna.frontend.sidebar.handle_sidebar","title":"handle_sidebar","text":"<pre><code>handle_sidebar()\n</code></pre> <p>Handle the sidebar interactions.</p> Source code in <code>src/quoptuna/frontend/sidebar.py</code> <pre><code>def handle_sidebar():\n    \"\"\"Handle the sidebar interactions.\"\"\"\n    st.title(\"QuOptuna: Optimizing Quantum Models with Optuna\")\n    st.write(\"Please upload your data file below.\")\n    data = upload_and_display_data()\n    if data is not None:\n        session_state = {\n            \"x_columns\": None,\n            \"y_column\": None,\n            \"DB_NAME\": st.text_input(\n                \"Enter database name\", help=\"Name of the database to store results\"\n            ),\n            \"study_name\": st.text_input(\n                \"Enter study name\", help=\"Name of the study for optimization\"\n            ),\n            \"n_trials\": st.number_input(\n                \"Number of trials\",\n                min_value=1,\n                max_value=100,\n                value=100,\n                help=\"Number of optimization trials\",\n            ),\n        }\n        (x_train, x_test, y_train, y_test), x_columns, y_column = select_data(data)\n        if x_train is not None:\n            data_dict = {\n                \"train_x\": x_train,\n                \"test_x\": x_test,\n                \"train_y\": y_train,\n                \"test_y\": y_test,\n            }\n            st.session_state.update(session_state)\n            run_optimization(session_state, data_dict)\n            if not st.session_state.get(\"start_visualization\", False):\n                st.button(\n                    \"Start Visualization\",\n                    on_click=lambda: st.session_state.update({\"start_visualization\": True}),\n                    help=\"Start the visualization with the latest data.\",\n                )\n                st.info(\"Click to start visualization.\")\n            else:\n                st.success(\"Visualization is already running.\")\n</code></pre>"},{"location":"api_docs.html#quoptuna.frontend.sidebar.run_optimization","title":"run_optimization","text":"<pre><code>run_optimization(session_state, data_dict)\n</code></pre> <p>Run the optimization process.</p> Source code in <code>src/quoptuna/frontend/sidebar.py</code> <pre><code>def run_optimization(session_state, data_dict):\n    \"\"\"Run the optimization process.\"\"\"\n    if session_state[\"DB_NAME\"] and all(\n        len(data_dict[key]) &gt; 0 for key in [\"train_x\", \"test_x\", \"train_y\", \"test_y\"]\n    ):\n        optimizer = Optimizer(\n            db_name=session_state[\"DB_NAME\"],\n            data=data_dict,\n            study_name=session_state[\"study_name\"],\n        )\n        st.session_state[\"optimizer\"] = optimizer\n        st.markdown(\"### Actions\")\n        if st.button(\"Run Optimization\", help=\"Start the optimization process\"):\n            try:\n                run_optimization_in_background(optimizer, session_state[\"n_trials\"])\n            except Exception as e:  # noqa: BLE001\n                st.error(f\"Failed to start optimization: {e}\")\n                st.error(f\"Exception type: {type(e).__name__}\")\n</code></pre>"},{"location":"api_docs.html#quoptuna.frontend.sidebar.select_data","title":"select_data","text":"<pre><code>select_data(data)\n</code></pre> <p>Select features from the uploaded data.</p> Source code in <code>src/quoptuna/frontend/sidebar.py</code> <pre><code>def select_data(data):\n    \"\"\"Select features from the uploaded data.\"\"\"\n    st.markdown(\"### Select Features\")\n    x_columns, y_column = select_columns(data)\n    if x_columns and y_column:\n        x = data[x_columns]\n        y = data[y_column]\n        return preprocess_data(x, y), x_columns, y_column\n    return None, None, None\n</code></pre>"},{"location":"api_docs.html#quoptuna.frontend.support","title":"support","text":"FUNCTION DESCRIPTION <code>display_study_control_panel</code> <p>Displays the study control panel and handles visualization.</p> <code>handle_input</code> <p>Handles input for optimizer, study name, and database name.</p> <code>initialize_session_state</code> <p>Initialize session state keys.</p> <code>plot_visualization</code> <p>Handles the visualization of the optimization results.</p> <code>run_optimization_in_background</code> <p>Run the optimization process in a separate thread.</p> <code>select_columns</code> <p>Allows user to select columns for X and y.</p> <code>update_plot</code> <p>Update the plot with the latest optimization results.</p> <code>upload_and_display_data</code> <p>Handles file upload and displays the data.</p>"},{"location":"api_docs.html#quoptuna.frontend.support.display_study_control_panel","title":"display_study_control_panel","text":"<pre><code>display_study_control_panel(study_name, optimizer)\n</code></pre> <p>Displays the study control panel and handles visualization.</p> Source code in <code>src/quoptuna/frontend/support.py</code> <pre><code>def display_study_control_panel(study_name, optimizer):\n    \"\"\"Displays the study control panel and handles visualization.\"\"\"\n    st.toast(\"Study Control Panel\")\n    st.markdown(\"### Study Control Panel\")\n    st.text_input(\"Study name\", value=st.session_state[\"study_name\"], disabled=True)\n    col1, col2 = st.columns([1, 1])\n    with col1:\n        st.button(\n            \"Start Visualization\",\n            on_click=lambda: st.session_state.update({\"start_visualization\": True}),\n            help=\"Start the visualization with the latest data.\",\n            key=\"start_visualization_button\",  # Add a unique key\n        )\n    with col2:\n        st.button(\n            \"Stop Visualization\",\n            on_click=lambda: st.session_state.update({\"start_visualization\": False}),\n            help=\"Stop the visualization updates.\",\n            key=\"stop_visualization_button\",  # Add a unique key\n        )\n\n    plot_visualization(optimizer, study_name)\n</code></pre>"},{"location":"api_docs.html#quoptuna.frontend.support.handle_input","title":"handle_input","text":"<pre><code>handle_input()\n</code></pre> <p>Handles input for optimizer, study name, and database name.</p> Source code in <code>src/quoptuna/frontend/support.py</code> <pre><code>def handle_input():\n    \"\"\"Handles input for optimizer, study name, and database name.\"\"\"\n    optimizer = st.session_state[\"optimizer\"]\n    study_name = st.session_state.get(\"study_name\", \"\")  # Get study_name from session state\n    db_name = st.session_state.get(\"DB_NAME\", \"\")  # Get DB_NAME from session state\n\n    with st.expander(\"Load Optimizer\", expanded=(optimizer is None)):\n        db_name = st.text_input(\n            \"Enter database name\",\n            value=db_name,  # Set the default value to the existing DB name\n            help=\"Name of the database to load optimizer from\",\n        )\n        study_name = st.text_input(\n            \"Enter study name\",\n            value=study_name,  # Set the default value to the existing study name\n            help=\"Name of the study to load optimizer from\",\n        )\n        uploaded_db = st.file_uploader(\"Upload DB file\", type=[\"db\"])\n\n        if uploaded_db:\n            db_path = f\"./db/{uploaded_db.name}\"\n            if not os.path.exists(\"./db\"):  # noqa: PTH110\n                os.makedirs(\"./db\")  # noqa: PTH103\n            with open(db_path, \"wb\") as f:  # noqa: PTH123\n                f.write(uploaded_db.getvalue())\n            st.session_state[\"file_location\"] = db_path\n        if st.button(\"Load Optimizer\") and study_name:\n            from quoptuna import Optimizer  # Import Optimizer here to avoid lint error\n\n            optimizer = Optimizer(db_name=db_name, study_name=study_name)\n            st.session_state[\"optimizer\"] = optimizer\n            st.session_state[\"DB_NAME\"] = db_name\n            st.session_state[\"study_name\"] = study_name  # Store study_name in session state\n            st.session_state[\"data_loaded_from_file\"] = True\n    return optimizer, study_name, db_name\n</code></pre>"},{"location":"api_docs.html#quoptuna.frontend.support.initialize_session_state","title":"initialize_session_state","text":"<pre><code>initialize_session_state()\n</code></pre> <p>Initialize session state keys.</p> Source code in <code>src/quoptuna/frontend/support.py</code> <pre><code>def initialize_session_state():\n    \"\"\"Initialize session state keys.\"\"\"\n    session_defaults = {\n        \"uploaded_file\": None,\n        \"uploaded_file_name\": None,\n        \"file_location\": None,\n        \"x_columns\": None,\n        \"y_column\": None,\n        \"DB_NAME\": None,\n        \"study_name\": None,\n        \"n_trials\": 100,\n        \"optimizer\": None,\n        \"process_running\": False,\n        \"start_visualization\": False,\n    }\n    for key, default in session_defaults.items():\n        if key not in st.session_state:\n            st.session_state[key] = default\n</code></pre>"},{"location":"api_docs.html#quoptuna.frontend.support.plot_visualization","title":"plot_visualization","text":"<pre><code>plot_visualization(optimizer, study_name)\n</code></pre> <p>Handles the visualization of the optimization results.</p> Source code in <code>src/quoptuna/frontend/support.py</code> <pre><code>def plot_visualization(optimizer, study_name):\n    \"\"\"Handles the visualization of the optimization results.\"\"\"\n    st.toast(\"Plotting Visualization\")\n    plot_placeholder_timeline = st.empty()\n    plot_placeholder_importances = st.empty()\n    plot_placeholder_optimization_history = st.empty()\n    trials_placeholder = st.empty()\n    trails_title = st.empty()\n    best_trials_placeholder = st.empty()\n    best_trials_title = st.empty()\n    counter = 0  # Initialize a counter for unique keys\n    while st.session_state[\"start_visualization\"]:\n        loaded_study = optuna.load_study(study_name=study_name, storage=optimizer.storage_location)\n        try:\n            fig_timeline = optuna.visualization.plot_timeline(loaded_study)\n            plot_placeholder_timeline.plotly_chart(fig_timeline, key=f\"timeline_chart_{counter}\")\n        except ValueError as e:\n            st.error(f\"Error in plotting timeline: {e}\")\n\n        try:\n            fig_importances = optuna.visualization.plot_param_importances(loaded_study)\n            plot_placeholder_importances.plotly_chart(\n                fig_importances, key=f\"importances_chart_{counter}\"\n            )\n        except ValueError as e:\n            st.error(f\"Error in plotting parameter importances: {e}\")\n\n        try:\n            fig_optimization_history = optuna.visualization.plot_optimization_history(loaded_study)\n            plot_placeholder_optimization_history.plotly_chart(\n                fig_optimization_history,\n                key=f\"optimization_history_chart_{counter}\",\n            )\n        except ValueError as e:\n            st.error(f\"Error in plotting optimization history: {e}\")\n\n        trials = loaded_study.get_trials(deepcopy=False)\n        trials_data = [\n            {\n                \"Trial Number\": trial.number,\n                \"State\": trial.state,\n                \"Value\": trial.value,\n                \"Params\": trial.params,\n                \"Start Time\": trial.datetime_start,\n                \"End Time\": trial.datetime_complete,\n                \"Classical F1 Score\": trial.user_attrs.get(\n                    \"Classical_f1_score\", None\n                ),  # Access user attributes\n                \"Quantum F1 Score\": trial.user_attrs.get(\n                    \"Quantum_f1_score\", None\n                ),  # Access user attributes\n            }\n            for trial in trials\n        ]\n        trails_title.write(\"Trials Data\")\n        # make a header and title for the dataframe\n        trials_placeholder.dataframe(\n            trials_data,\n            use_container_width=True,\n            column_config={\n                \"Classical F1 Score\": st.column_config.NumberColumn(\n                    \"Classical F1 Score\", help=\"Classical F1 Score\"\n                ),\n                \"Quantum F1 Score\": st.column_config.NumberColumn(\n                    \"Quantum F1 Score\", help=\"Quantum F1 Score\"\n                ),\n            },\n        )\n        # check if the best trial in study is not none\n        if loaded_study.best_trials is not None:\n            best_trials_title.write(\"Best Trials Data\")\n            best_trial = loaded_study.best_trials\n            best_trials_data = [\n                {\n                    \"Trial Number\": trial.number,\n                    \"State\": trial.state,\n                    \"Value\": trial.value,\n                    \"Params\": trial.params,\n                    \"Start Time\": trial.datetime_start,\n                    \"End Time\": trial.datetime_complete,\n                    \"Classical F1 Score\": trial.user_attrs.get(\n                        \"Classical_f1_score\", None\n                    ),  # Access user attributes\n                    \"Quantum F1 Score\": trial.user_attrs.get(\n                        \"Quantum_f1_score\", None\n                    ),  # Access user attributes\n                }\n                for trial in best_trial\n            ]\n            best_trials_placeholder.dataframe(best_trials_data, use_container_width=True)\n        counter += 1  # Increment the counter for the next iteration\n        if not st.session_state[\"process_running\"]:\n            break\n        time.sleep(10)\n</code></pre>"},{"location":"api_docs.html#quoptuna.frontend.support.run_optimization_in_background","title":"run_optimization_in_background","text":"<pre><code>run_optimization_in_background(optimizer, n_trials)\n</code></pre> <p>Run the optimization process in a separate thread.</p> Source code in <code>src/quoptuna/frontend/support.py</code> <pre><code>def run_optimization_in_background(optimizer, n_trials):\n    \"\"\"Run the optimization process in a separate thread.\"\"\"\n\n    def optimization_task():\n        study, best_trials = optimizer.optimize(n_trials=n_trials)\n        for trial in best_trials:\n            st.write(str(trial))\n\n    optimization_thread = threading.Thread(target=optimization_task)\n    optimization_thread.start()\n    st.success(\"Optimization started successfully.\")\n    st.session_state[\"process_running\"] = True\n</code></pre>"},{"location":"api_docs.html#quoptuna.frontend.support.select_columns","title":"select_columns","text":"<pre><code>select_columns(data)\n</code></pre> <p>Allows user to select columns for X and y.</p> Source code in <code>src/quoptuna/frontend/support.py</code> <pre><code>def select_columns(data):\n    \"\"\"Allows user to select columns for X and y.\"\"\"\n    x_columns = st.multiselect(\"Select columns for X\", data.columns.tolist())\n    y_column = st.selectbox(\"Select column for y\", data.columns.tolist())\n    return x_columns, y_column\n</code></pre>"},{"location":"api_docs.html#quoptuna.frontend.support.update_plot","title":"update_plot","text":"<pre><code>update_plot()\n</code></pre> <p>Update the plot with the latest optimization results.</p> Source code in <code>src/quoptuna/frontend/support.py</code> <pre><code>def update_plot():\n    \"\"\"Update the plot with the latest optimization results.\"\"\"\n    optimizer, study_name, db_name = handle_input()\n    st.toast(\"Updating Plot\")\n    st.toast(f\"Study Name: {study_name}, DB Name: {db_name}\")\n    if st.session_state[\"optimizer\"] and st.session_state[\"study_name\"]:\n        display_study_control_panel(study_name, optimizer)\n</code></pre>"},{"location":"api_docs.html#quoptuna.frontend.support.upload_and_display_data","title":"upload_and_display_data","text":"<pre><code>upload_and_display_data()\n</code></pre> <p>Handles file upload and displays the data.</p> Source code in <code>src/quoptuna/frontend/support.py</code> <pre><code>def upload_and_display_data():\n    \"\"\"Handles file upload and displays the data.\"\"\"\n    uploaded_file = st.file_uploader(\"Choose a CSV file\", type=[\"csv\"])\n    if uploaded_file:\n        if not os.path.exists(\"./uploaded_data\"):  # noqa: PTH110\n            os.makedirs(\"./uploaded_data\")  # noqa: PTH103\n        with open(f\"./uploaded_data/{uploaded_file.name}\", \"wb\") as f:  # noqa: PTH123\n            f.write(uploaded_file.getvalue())\n        st.session_state[\"uploaded_file\"] = uploaded_file\n        st.session_state[\"uploaded_file_name\"] = uploaded_file.name\n        file_location = f\"./uploaded_data/{uploaded_file.name}\"\n        st.session_state[\"file_location\"] = file_location\n        return pd.read_csv(file_location)\n    return None\n</code></pre>"},{"location":"api_reference.html","title":"API Reference","text":""},{"location":"api_reference.html#api-reference","title":"API Reference","text":""},{"location":"api_reference.html#overview","title":"Overview","text":"<p>QuOptuna provides a comprehensive Python API for quantum-enhanced machine learning optimization. This reference covers the main classes and functions available for programmatic use.</p>"},{"location":"api_reference.html#core-classes","title":"Core Classes","text":""},{"location":"api_reference.html#datapreparation","title":"DataPreparation","text":"<p>Handles data loading, preprocessing, and splitting.</p> <pre><code>from quoptuna import DataPreparation\n\ndata_prep = DataPreparation(\n    file_path=\"path/to/data.csv\",\n    x_cols=[\"feature1\", \"feature2\", \"feature3\"],\n    y_col=\"target\"\n)\n\n# Get preprocessed data\ndata_dict = data_prep.get_data(output_type=\"2\")\n</code></pre> <p>Parameters: - <code>file_path</code> (str): Path to the CSV data file - <code>x_cols</code> (list): List of feature column names - <code>y_col</code> (str): Target column name - <code>test_size</code> (float, optional): Proportion of data for testing (default: 0.25) - <code>random_state</code> (int, optional): Random seed for reproducibility</p> <p>Methods:</p>"},{"location":"api_reference.html#get_dataoutput_type2","title":"<code>get_data(output_type=\"2\")</code>","text":"<p>Returns preprocessed data dictionary.</p> <p>Parameters: - <code>output_type</code> (str): Format of output   - <code>\"1\"</code>: Returns pandas DataFrames   - <code>\"2\"</code>: Returns numpy arrays (recommended for optimization)</p> <p>Returns: - dict: Dictionary with keys <code>train_x</code>, <code>test_x</code>, <code>train_y</code>, <code>test_y</code></p>"},{"location":"api_reference.html#optimizer","title":"Optimizer","text":"<p>Manages hyperparameter optimization using Optuna.</p> <pre><code>from quoptuna import Optimizer\n\noptimizer = Optimizer(\n    db_name=\"my_experiment\",\n    study_name=\"trial_001\",\n    data=data_dict\n)\n\n# Run optimization\nstudy, best_trials = optimizer.optimize(n_trials=100)\n</code></pre> <p>Parameters: - <code>db_name</code> (str): Database name for storing results - <code>study_name</code> (str): Unique study identifier - <code>data</code> (dict): Data dictionary from DataPreparation - <code>dataset_name</code> (str, optional): Human-readable dataset name</p> <p>Attributes: - <code>storage_location</code> (str): SQLite database URI - <code>study</code> (optuna.Study): Optuna study object - <code>best_trials</code> (list): List of best performing trials</p> <p>Methods:</p>"},{"location":"api_reference.html#optimizen_trials100-timeoutnone","title":"<code>optimize(n_trials=100, timeout=None)</code>","text":"<p>Run hyperparameter optimization.</p> <p>Parameters: - <code>n_trials</code> (int): Number of optimization trials - <code>timeout</code> (int, optional): Maximum optimization time in seconds</p> <p>Returns: - <code>study</code> (optuna.Study): Completed study - <code>best_trials</code> (list): List of Pareto-optimal trials</p>"},{"location":"api_reference.html#load_study","title":"<code>load_study()</code>","text":"<p>Load previously saved study from database.</p> <p>Returns: - <code>study</code> (optuna.Study): Loaded study object</p>"},{"location":"api_reference.html#model-creation","title":"Model Creation","text":"<p>Create models with optimized hyperparameters.</p> <pre><code>from quoptuna.backend.models import create_model\n\n# From trial parameters\nmodel = create_model(**trial.params)\n\n# Or specify directly\nmodel = create_model(\n    model_type=\"DataReuploadingClassifier\",\n    n_layers=10,\n    learning_rate=0.1,\n    batch_size=32\n)\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Make predictions\npredictions = model.predict(X_test)\n</code></pre> <p>Supported Models:</p>"},{"location":"api_reference.html#quantum-models","title":"Quantum Models","text":"<p>DataReuploadingClassifier - <code>n_layers</code> (int): Number of quantum layers - <code>learning_rate</code> (float): Learning rate for optimization - <code>batch_size</code> (int): Batch size for training - <code>n_input_copies</code> (int): Number of data reuploads - <code>observable_type</code> (str): Type of observable (\"all\" or \"half\")</p> <p>CircuitCentricClassifier - <code>n_layers</code> (int): Circuit depth - <code>learning_rate</code> (float): Learning rate - <code>n_qubits</code> (int): Number of qubits</p> <p>QuantumKitchenSinks - <code>n_episodes</code> (int): Number of training episodes - <code>learning_rate</code> (float): Learning rate - <code>gamma</code> (float): Kernel parameter</p>"},{"location":"api_reference.html#classical-models","title":"Classical Models","text":"<p>SVC (Support Vector Classifier) - <code>C</code> (float): Regularization parameter - <code>gamma</code> (str or float): Kernel coefficient - <code>kernel</code> (str): Kernel type</p> <p>MLPClassifier - <code>hidden_layer_sizes</code> (tuple): Hidden layer configuration - <code>learning_rate</code> (str): Learning rate schedule - <code>alpha</code> (float): L2 regularization parameter</p>"},{"location":"api_reference.html#xai-explainable-ai","title":"XAI (Explainable AI)","text":"<p>Generate SHAP explanations and visualizations.</p> <pre><code>from quoptuna import XAI\nfrom quoptuna.backend.xai.xai import XAIConfig\n\n# Configure XAI\nconfig = XAIConfig(\n    use_proba=True,\n    onsubset=True,\n    subset_size=50\n)\n\n# Create XAI instance\nxai = XAI(\n    model=trained_model,\n    data=data_dict,\n    config=config\n)\n\n# Get evaluation report\nreport = xai.get_report()\n</code></pre> <p>XAIConfig Parameters: - <code>use_proba</code> (bool): Use probability predictions - <code>onsubset</code> (bool): Use subset of data - <code>subset_size</code> (int): Size of subset</p> <p>XAI Methods:</p>"},{"location":"api_reference.html#get_report","title":"<code>get_report()</code>","text":"<p>Generate classification report.</p> <p>Returns: - dict: Contains confusion matrix, classification report, and ROC curve data</p>"},{"location":"api_reference.html#get_plotplot_type-kwargs","title":"<code>get_plot(plot_type, **kwargs)</code>","text":"<p>Generate SHAP visualization.</p> <p>Parameters: - <code>plot_type</code> (str): Type of plot   - <code>\"bar\"</code>: Feature importance bar plot   - <code>\"beeswarm\"</code>: SHAP value distribution   - <code>\"violin\"</code>: Violin plot of SHAP values   - <code>\"heatmap\"</code>: Instance-level SHAP heatmap   - <code>\"waterfall\"</code>: Individual prediction explanation - <code>max_display</code> (int): Maximum features to display - <code>class_index</code> (int): Class to explain (for binary: 0 or 1) - <code>index</code> (int): Sample index (for waterfall plot) - <code>save_config</code> (dict, optional): Configuration for saving plot</p> <p>Returns: - str: Base64 encoded image</p>"},{"location":"api_reference.html#plot_confusion_matrix","title":"<code>plot_confusion_matrix()</code>","text":"<p>Generate confusion matrix plot.</p> <p>Returns: - matplotlib.figure.Figure: Confusion matrix figure</p>"},{"location":"api_reference.html#generate_report_with_langchainprovider-api_key-model_name-dataset_infonone","title":"<code>generate_report_with_langchain(provider, api_key, model_name, dataset_info=None)</code>","text":"<p>Generate AI-powered analysis report.</p> <p>Parameters: - <code>provider</code> (str): LLM provider (\"google\", \"openai\", \"anthropic\") - <code>api_key</code> (str): API key for the provider - <code>model_name</code> (str): Model identifier - <code>dataset_info</code> (dict, optional): Dataset metadata</p> <p>Returns: - str: Markdown formatted report</p>"},{"location":"api_reference.html#utility-functions","title":"Utility Functions","text":""},{"location":"api_reference.html#mock_csv_data","title":"mock_csv_data","text":"<p>Save DataFrame to CSV file.</p> <pre><code>from quoptuna.backend.utils.data_utils.data import mock_csv_data\n\nfile_path = mock_csv_data(\n    dataframe,\n    tmp_path=\"data\",\n    file_name=\"my_dataset\"\n)\n</code></pre> <p>Parameters: - <code>dataframe</code> (pd.DataFrame): Data to save - <code>tmp_path</code> (str): Directory path - <code>file_name</code> (str): File name (without .csv extension)</p> <p>Returns: - str: Full path to saved file</p>"},{"location":"api_reference.html#complete-example","title":"Complete Example","text":"<p>Here's a complete example workflow:</p> <pre><code>import pandas as pd\nfrom ucimlrepo import fetch_ucirepo\nfrom quoptuna import DataPreparation, Optimizer, XAI\nfrom quoptuna.backend.models import create_model\nfrom quoptuna.backend.xai.xai import XAIConfig\nfrom quoptuna.backend.utils.data_utils.data import mock_csv_data\n\n# 1. Load dataset\ndataset = fetch_ucirepo(id=143)  # Statlog dataset\nX = dataset.data.features\ny = dataset.data.targets\ndf = pd.concat([X, y], axis=1)\n\n# 2. Prepare data\ndf[\"target\"] = df[\"A15\"].replace({0: -1, 1: 1})\ndf = df.drop(columns=[\"A15\"])\ndf = df.dropna()\n\n# 3. Save to file\nfile_path = mock_csv_data(df, tmp_path=\"data\", file_name=\"Statlog\")\n\n# 4. Prepare for training\ndata_prep = DataPreparation(\n    file_path=file_path,\n    x_cols=list(df.columns.difference([\"target\"])),\n    y_col=\"target\"\n)\ndata_dict = data_prep.get_data(output_type=\"2\")\n\n# Convert to numpy arrays\ndata_dict[\"train_x\"] = data_dict[\"train_x\"].values\ndata_dict[\"test_x\"] = data_dict[\"test_x\"].values\ndata_dict[\"train_y\"] = data_dict[\"train_y\"].values\ndata_dict[\"test_y\"] = data_dict[\"test_y\"].values\n\n# 5. Run optimization\noptimizer = Optimizer(\n    db_name=\"Statlog\",\n    study_name=\"Statlog\",\n    data=data_dict\n)\nstudy, best_trials = optimizer.optimize(n_trials=100)\n\n# 6. Train best model\nbest_trial = best_trials[0]\nmodel = create_model(**best_trial.params)\nmodel.fit(data_dict[\"train_x\"], data_dict[\"train_y\"])\n\n# 7. SHAP analysis\nconfig = XAIConfig(use_proba=True, onsubset=True, subset_size=50)\nxai = XAI(model=model, data=data_dict, config=config)\n\n# 8. Generate visualizations\nbar_plot = xai.get_plot(\"bar\", max_display=10, class_index=1)\nbeeswarm_plot = xai.get_plot(\"beeswarm\", max_display=10, class_index=1)\n\n# 9. Generate report\nreport = xai.generate_report_with_langchain(\n    provider=\"google\",\n    api_key=\"your-api-key\",\n    model_name=\"models/gemini-2.0-flash-exp\",\n    dataset_info={\n        \"Name\": \"Statlog Credit Approval\",\n        \"URL\": \"https://archive.ics.uci.edu/dataset/143\",\n        \"Description\": \"Credit card application dataset\"\n    }\n)\n\nprint(report)\n</code></pre>"},{"location":"api_reference.html#data-format-requirements","title":"Data Format Requirements","text":""},{"location":"api_reference.html#input-data","title":"Input Data","text":"<p>CSV Format: - Must have header row with column names - Target column should contain binary values - Features can be numeric or categorical - Missing values will be removed</p> <p>Target Encoding: - Binary classification: Must use <code>-1</code> and <code>1</code> - QuOptuna does not currently support multi-class classification</p>"},{"location":"api_reference.html#data-dictionary-format","title":"Data Dictionary Format","text":"<p>After preprocessing, data should be in this format:</p> <pre><code>data_dict = {\n    \"train_x\": np.ndarray,  # Shape: (n_train_samples, n_features)\n    \"test_x\": np.ndarray,   # Shape: (n_test_samples, n_features)\n    \"train_y\": np.ndarray,  # Shape: (n_train_samples,)\n    \"test_y\": np.ndarray    # Shape: (n_test_samples,)\n}\n</code></pre>"},{"location":"api_reference.html#advanced-usage","title":"Advanced Usage","text":""},{"location":"api_reference.html#custom-optimization-objectives","title":"Custom Optimization Objectives","text":"<p>You can customize the optimization objective:</p> <pre><code>import optuna\n\ndef custom_objective(trial):\n    # Define your custom objective\n    params = {\n        \"model_type\": trial.suggest_categorical(\"model_type\", [\"SVC\", \"MLPClassifier\"]),\n        \"C\": trial.suggest_float(\"C\", 0.1, 10.0)\n    }\n\n    model = create_model(**params)\n    model.fit(train_x, train_y)\n\n    # Return custom metric\n    return custom_metric(model, test_x, test_y)\n\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(custom_objective, n_trials=100)\n</code></pre>"},{"location":"api_reference.html#parallel-optimization","title":"Parallel Optimization","text":"<p>Run multiple trials in parallel:</p> <pre><code>optimizer = Optimizer(db_name=\"my_db\", study_name=\"my_study\", data=data_dict)\n\n# Use n_jobs for parallel execution\nstudy, best_trials = optimizer.optimize(\n    n_trials=100,\n    n_jobs=4  # Run 4 trials in parallel\n)\n</code></pre>"},{"location":"api_reference.html#saving-and-loading-models","title":"Saving and Loading Models","text":"<pre><code>import joblib\n\n# Save model\njoblib.dump(model, \"model.pkl\")\n\n# Load model\nloaded_model = joblib.load(\"model.pkl\")\n</code></pre>"},{"location":"api_reference.html#error-handling","title":"Error Handling","text":"<p>Common errors and solutions:</p> <pre><code>try:\n    optimizer.optimize(n_trials=100)\nexcept ValueError as e:\n    # Handle data validation errors\n    print(f\"Data error: {e}\")\nexcept RuntimeError as e:\n    # Handle optimization errors\n    print(f\"Optimization error: {e}\")\nexcept Exception as e:\n    # Handle unexpected errors\n    print(f\"Unexpected error: {e}\")\n</code></pre>"},{"location":"api_reference.html#performance-tips","title":"Performance Tips","text":"<ol> <li>Use Subsets for SHAP: Analyze 50-100 samples for faster computation</li> <li>Increase Trials Gradually: Start with 50 trials, increase as needed</li> <li>Use Caching: Reuse loaded studies when possible</li> <li>Monitor Memory: Large datasets may require subset analysis</li> <li>Parallel Processing: Use <code>n_jobs</code> parameter for faster optimization</li> </ol>"},{"location":"api_reference.html#see-also","title":"See Also","text":"<ul> <li>User Guide - Step-by-step usage instructions</li> <li>Examples - Common use cases and tutorials</li> <li>GitHub Repository - Source code and issues</li> </ul>"},{"location":"changelog.html","title":"Changelog","text":""},{"location":"changelog.html#changelog","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog.html#unreleased","title":"Unreleased","text":""},{"location":"examples.html","title":"Examples","text":""},{"location":"examples.html#examples","title":"Examples","text":""},{"location":"examples.html#introduction","title":"Introduction","text":"<p>This page provides practical examples for common QuOptuna use cases. Each example includes complete, runnable code.</p>"},{"location":"examples.html#table-of-contents","title":"Table of Contents","text":"<ol> <li>Basic Workflow</li> <li>UCI Dataset Analysis</li> <li>Custom Dataset</li> <li>SHAP Analysis</li> <li>Comparing Models</li> <li>Report Generation</li> <li>Batch Processing</li> </ol>"},{"location":"examples.html#basic-workflow","title":"Basic Workflow","text":"<p>Complete workflow from data loading to SHAP analysis:</p> <pre><code>from quoptuna import DataPreparation, Optimizer, XAI\nfrom quoptuna.backend.models import create_model\nfrom quoptuna.backend.xai.xai import XAIConfig\nfrom quoptuna.backend.utils.data_utils.data import mock_csv_data\nimport pandas as pd\n\n# Load your data\ndf = pd.read_csv(\"your_data.csv\")\n\n# Ensure target is -1 and 1\ndf[\"target\"] = df[\"target\"].replace({0: -1, 1: 1})\n\n# Save to file\nfile_path = mock_csv_data(df, tmp_path=\"data\", file_name=\"my_data\")\n\n# Prepare data\ndata_prep = DataPreparation(\n    file_path=file_path,\n    x_cols=[col for col in df.columns if col != \"target\"],\n    y_col=\"target\"\n)\ndata_dict = data_prep.get_data(output_type=\"2\")\n\n# Convert to numpy\nfor key in [\"train_x\", \"test_x\", \"train_y\", \"test_y\"]:\n    data_dict[key] = data_dict[key].values\n\n# Optimize\noptimizer = Optimizer(db_name=\"my_experiment\", study_name=\"run_1\", data=data_dict)\nstudy, best_trials = optimizer.optimize(n_trials=50)\n\n# Train best model\nbest_model = create_model(**best_trials[0].params)\nbest_model.fit(data_dict[\"train_x\"], data_dict[\"train_y\"])\n\n# SHAP analysis\nxai = XAI(\n    model=best_model,\n    data=data_dict,\n    config=XAIConfig(use_proba=True, onsubset=True, subset_size=50)\n)\n\n# Generate plots\nbar_plot = xai.get_plot(\"bar\", max_display=10, class_index=1)\nprint(\"Analysis complete!\")\n</code></pre>"},{"location":"examples.html#uci-dataset-analysis","title":"UCI Dataset Analysis","text":"<p>Working with UCI ML Repository datasets:</p> <pre><code>from ucimlrepo import fetch_ucirepo\nfrom quoptuna import DataPreparation, Optimizer\nfrom quoptuna.backend.utils.data_utils.data import mock_csv_data\nimport pandas as pd\n\n# Fetch dataset from UCI\ndataset = fetch_ucirepo(id=143)  # Statlog Credit Approval\n\n# Get metadata\nprint(\"Dataset:\", dataset.metadata[\"name\"])\nprint(\"Instances:\", dataset.metadata[\"num_instances\"])\nprint(\"Features:\", dataset.metadata[\"num_features\"])\n\n# Prepare data\nX = dataset.data.features\ny = dataset.data.targets\ndf = pd.concat([X, y], axis=1)\n\n# Transform target\ntarget_col = dataset.metadata[\"target_col\"][0]\ndf[\"target\"] = df[target_col].replace({0: -1, 1: 1})\ndf = df.drop(columns=[target_col])\ndf = df.dropna()\n\n# Save and prepare\nfile_path = mock_csv_data(df, tmp_path=\"data\", file_name=\"Statlog\")\n\ndata_prep = DataPreparation(\n    file_path=file_path,\n    x_cols=list(df.columns.difference([\"target\"])),\n    y_col=\"target\"\n)\ndata_dict = data_prep.get_data(output_type=\"2\")\n\n# Convert to numpy\nfor key in data_dict.keys():\n    data_dict[key] = data_dict[key].values\n\n# Run optimization\noptimizer = Optimizer(db_name=\"Statlog\", study_name=\"Statlog\", data=data_dict)\nstudy, best_trials = optimizer.optimize(n_trials=100)\n\n# Show results\nfor i, trial in enumerate(best_trials[:3]):\n    print(f\"\\n=== Best Trial {i+1} ===\")\n    print(f\"Model: {trial.params['model_type']}\")\n    print(f\"Quantum F1: {trial.user_attrs.get('Quantum_f1_score', 0):.4f}\")\n    print(f\"Classical F1: {trial.user_attrs.get('Classical_f1_score', 0):.4f}\")\n</code></pre>"},{"location":"examples.html#custom-dataset","title":"Custom Dataset","text":"<p>Loading and processing a custom CSV file:</p> <pre><code>import pandas as pd\nfrom quoptuna import DataPreparation, Optimizer\nfrom quoptuna.backend.utils.data_utils.data import mock_csv_data\n\n# Load custom dataset\ndf = pd.read_csv(\"my_dataset.csv\")\n\n# Explore data\nprint(\"Shape:\", df.shape)\nprint(\"Columns:\", df.columns.tolist())\nprint(\"Missing values:\", df.isnull().sum().sum())\n\n# Handle missing values\ndf = df.dropna()\n\n# Transform target to -1 and 1\n# Example: If target is 'Yes'/'No'\ndf[\"target\"] = df[\"outcome\"].map({\"Yes\": 1, \"No\": -1})\n\n# Drop original target column\ndf = df.drop(columns=[\"outcome\"])\n\n# Select features\nfeature_cols = [\"age\", \"income\", \"credit_score\", \"debt_ratio\"]\n\n# Keep only selected columns\ndf = df[feature_cols + [\"target\"]]\n\n# Save processed data\nfile_path = mock_csv_data(df, tmp_path=\"data\", file_name=\"custom_dataset\")\n\n# Prepare for training\ndata_prep = DataPreparation(\n    file_path=file_path,\n    x_cols=feature_cols,\n    y_col=\"target\"\n)\ndata_dict = data_prep.get_data(output_type=\"2\")\n\n# Convert to numpy\nfor key in data_dict.keys():\n    data_dict[key] = data_dict[key].values\n\n# Optimize\noptimizer = Optimizer(\n    db_name=\"custom_experiment\",\n    study_name=\"trial_001\",\n    data=data_dict\n)\nstudy, best_trials = optimizer.optimize(n_trials=100)\n\nprint(f\"\\nFound {len(best_trials)} best trials\")\n</code></pre>"},{"location":"examples.html#shap-analysis","title":"SHAP Analysis","text":"<p>Comprehensive SHAP analysis with all plot types:</p> <pre><code>from quoptuna import XAI\nfrom quoptuna.backend.models import create_model\nfrom quoptuna.backend.xai.xai import XAIConfig\nimport os\n\n# Assuming you have optimized model and data_dict from previous steps\n# Load best trial parameters\nbest_params = best_trials[0].params\n\n# Train model\nmodel = create_model(**best_params)\nmodel.fit(data_dict[\"train_x\"], data_dict[\"train_y\"])\n\n# Configure XAI\nconfig = XAIConfig(\n    use_proba=True,\n    onsubset=True,\n    subset_size=100\n)\n\n# Create XAI instance\nxai = XAI(model=model, data=data_dict, config=config)\n\n# Create output directory\nos.makedirs(\"outputs/shap_plots\", exist_ok=True)\n\n# Generate and save all plot types\nplot_types = [\"bar\", \"beeswarm\", \"violin\", \"heatmap\"]\n\nfor plot_type in plot_types:\n    print(f\"Generating {plot_type} plot...\")\n\n    plot = xai.get_plot(\n        plot_type,\n        max_display=10,\n        class_index=1,\n        save_config={\n            \"save_path\": \"outputs/shap_plots\",\n            \"save_name\": f\"{plot_type}_plot\",\n            \"save_format\": \"png\",\n            \"save_dpi\": 300\n        }\n    )\n\n    print(f\"Saved {plot_type} plot\")\n\n# Generate waterfall plots for first 5 samples\nfor i in range(5):\n    waterfall = xai.get_plot(\n        \"waterfall\",\n        index=i,\n        class_index=1,\n        save_config={\n            \"save_path\": \"outputs/shap_plots\",\n            \"save_name\": f\"waterfall_sample_{i}\",\n            \"save_format\": \"png\",\n            \"save_dpi\": 300\n        }\n    )\n\n    print(f\"Saved waterfall plot for sample {i}\")\n\n# Get classification report\nreport = xai.get_report()\n\nprint(\"\\n=== Classification Report ===\")\nprint(report[\"classification_report\"])\n\n# Plot confusion matrix\nimport matplotlib.pyplot as plt\n\nfig = xai.plot_confusion_matrix()\nplt.savefig(\"outputs/shap_plots/confusion_matrix.png\", dpi=300, bbox_inches=\"tight\")\nplt.close()\n\nprint(\"\\nAll SHAP plots saved to outputs/shap_plots/\")\n</code></pre>"},{"location":"examples.html#comparing-models","title":"Comparing Models","text":"<p>Compare quantum vs classical models:</p> <pre><code>from quoptuna import Optimizer\nfrom quoptuna.backend.models import create_model\nimport pandas as pd\n\n# Run optimization (assumes data_dict is prepared)\noptimizer = Optimizer(db_name=\"comparison\", study_name=\"quantum_vs_classical\", data=data_dict)\nstudy, best_trials = optimizer.optimize(n_trials=100)\n\n# Separate quantum and classical trials\nquantum_trials = []\nclassical_trials = []\n\nfor trial in study.get_trials():\n    model_type = trial.params.get(\"model_type\", \"\")\n\n    # Determine if quantum or classical\n    if \"Classifier\" in model_type and any(\n        q in model_type\n        for q in [\"Reuploading\", \"Circuit\", \"Quantum\", \"Kitchen\", \"Dressed\"]\n    ):\n        quantum_trials.append(trial)\n    else:\n        classical_trials.append(trial)\n\n# Compare performance\ndef get_f1_score(trial):\n    q_f1 = trial.user_attrs.get(\"Quantum_f1_score\", 0)\n    c_f1 = trial.user_attrs.get(\"Classical_f1_score\", 0)\n    return max(q_f1, c_f1)\n\n# Get best from each category\nbest_quantum = max(quantum_trials, key=get_f1_score) if quantum_trials else None\nbest_classical = max(classical_trials, key=get_f1_score) if classical_trials else None\n\nprint(\"=== Model Comparison ===\\n\")\n\nif best_quantum:\n    print(\"Best Quantum Model:\")\n    print(f\"  Type: {best_quantum.params['model_type']}\")\n    print(f\"  F1 Score: {get_f1_score(best_quantum):.4f}\")\n    print(f\"  Trial: {best_quantum.number}\")\n\nif best_classical:\n    print(\"\\nBest Classical Model:\")\n    print(f\"  Type: {best_classical.params['model_type']}\")\n    print(f\"  F1 Score: {get_f1_score(best_classical):.4f}\")\n    print(f\"  Trial: {best_classical.number}\")\n\n# Create comparison DataFrame\ncomparison_data = []\n\nfor trial in quantum_trials + classical_trials:\n    comparison_data.append({\n        \"Trial\": trial.number,\n        \"Model Type\": trial.params[\"model_type\"],\n        \"Category\": \"Quantum\" if trial in quantum_trials else \"Classical\",\n        \"F1 Score\": get_f1_score(trial),\n        \"State\": trial.state.name\n    })\n\ndf_comparison = pd.DataFrame(comparison_data)\ndf_comparison = df_comparison.sort_values(\"F1 Score\", ascending=False)\n\nprint(\"\\n=== Top 10 Models ===\")\nprint(df_comparison.head(10))\n\n# Save results\ndf_comparison.to_csv(\"outputs/model_comparison.csv\", index=False)\n</code></pre>"},{"location":"examples.html#report-generation","title":"Report Generation","text":"<p>Generate comprehensive AI reports:</p> <pre><code>from quoptuna import XAI\nfrom quoptuna.backend.xai.xai import XAIConfig\nimport os\n\n# Train model (from previous steps)\nmodel = create_model(**best_trials[0].params)\nmodel.fit(data_dict[\"train_x\"], data_dict[\"train_y\"])\n\n# Create XAI instance\nxai = XAI(\n    model=model,\n    data=data_dict,\n    config=XAIConfig(use_proba=True, onsubset=True, subset_size=50)\n)\n\n# Dataset information for better reports\ndataset_info = {\n    \"Name\": \"Credit Card Approval\",\n    \"URL\": \"https://archive.ics.uci.edu/dataset/143\",\n    \"Description\": \"\"\"\n        This dataset concerns credit card applications.\n        It contains a mix of continuous and categorical features\n        for predicting credit approval decisions.\n    \"\"\",\n    \"Features\": [\"Age\", \"Income\", \"Credit Score\", \"Employment Status\"],\n    \"Target\": \"Approval Decision\",\n    \"Instances\": 690,\n    \"Task\": \"Binary Classification\"\n}\n\n# Generate report with Google Gemini\nreport = xai.generate_report_with_langchain(\n    provider=\"google\",\n    api_key=os.getenv(\"GOOGLE_API_KEY\"),\n    model_name=\"models/gemini-2.0-flash-exp\",\n    dataset_info=dataset_info\n)\n\n# Save report\nwith open(\"outputs/analysis_report.md\", \"w\") as f:\n    f.write(report)\n\nprint(\"Report saved to outputs/analysis_report.md\")\n\n# Generate with OpenAI GPT\nreport_gpt = xai.generate_report_with_langchain(\n    provider=\"openai\",\n    api_key=os.getenv(\"OPENAI_API_KEY\"),\n    model_name=\"gpt-4\",\n    dataset_info=dataset_info\n)\n\nwith open(\"outputs/analysis_report_gpt4.md\", \"w\") as f:\n    f.write(report_gpt)\n\nprint(\"GPT-4 report saved to outputs/analysis_report_gpt4.md\")\n</code></pre>"},{"location":"examples.html#batch-processing","title":"Batch Processing","text":"<p>Process multiple datasets:</p> <pre><code>from quoptuna import DataPreparation, Optimizer\nfrom quoptuna.backend.utils.data_utils.data import mock_csv_data\nimport pandas as pd\nimport os\n\n# List of datasets to process\ndatasets = [\n    {\"id\": 143, \"name\": \"Statlog\"},\n    {\"id\": 176, \"name\": \"Blood\"},\n    {\"id\": 267, \"name\": \"Banknote\"},\n]\n\nresults = []\n\nfor dataset_info in datasets:\n    print(f\"\\n{'='*50}\")\n    print(f\"Processing: {dataset_info['name']}\")\n    print('='*50)\n\n    try:\n        # Fetch dataset\n        from ucimlrepo import fetch_ucirepo\n        dataset = fetch_ucirepo(id=dataset_info[\"id\"])\n\n        # Prepare data\n        X = dataset.data.features\n        y = dataset.data.targets\n        df = pd.concat([X, y], axis=1)\n\n        # Get target column name\n        target_col = dataset.metadata[\"target_col\"][0]\n        df[\"target\"] = df[target_col].replace({0: -1, 1: 1})\n        df = df.drop(columns=[target_col])\n        df = df.dropna()\n\n        # Save\n        file_path = mock_csv_data(\n            df,\n            tmp_path=\"data/batch\",\n            file_name=dataset_info[\"name\"]\n        )\n\n        # Prepare\n        data_prep = DataPreparation(\n            file_path=file_path,\n            x_cols=list(df.columns.difference([\"target\"])),\n            y_col=\"target\"\n        )\n        data_dict = data_prep.get_data(output_type=\"2\")\n\n        # Convert to numpy\n        for key in data_dict.keys():\n            data_dict[key] = data_dict[key].values\n\n        # Optimize\n        optimizer = Optimizer(\n            db_name=f\"batch_{dataset_info['name']}\",\n            study_name=dataset_info[\"name\"],\n            data=data_dict\n        )\n        study, best_trials = optimizer.optimize(n_trials=50)\n\n        # Record results\n        best_f1 = max(\n            best_trials[0].user_attrs.get(\"Quantum_f1_score\", 0),\n            best_trials[0].user_attrs.get(\"Classical_f1_score\", 0)\n        )\n\n        results.append({\n            \"Dataset\": dataset_info[\"name\"],\n            \"Best Model\": best_trials[0].params[\"model_type\"],\n            \"Best F1\": best_f1,\n            \"Trials\": len(study.trials),\n            \"Status\": \"Success\"\n        })\n\n        print(f\"\u2713 Completed: {dataset_info['name']}\")\n        print(f\"  Best F1: {best_f1:.4f}\")\n        print(f\"  Model: {best_trials[0].params['model_type']}\")\n\n    except Exception as e:\n        print(f\"\u2717 Failed: {dataset_info['name']}\")\n        print(f\"  Error: {e}\")\n\n        results.append({\n            \"Dataset\": dataset_info[\"name\"],\n            \"Best Model\": None,\n            \"Best F1\": None,\n            \"Trials\": 0,\n            \"Status\": f\"Failed: {str(e)}\"\n        })\n\n# Save summary\ndf_results = pd.DataFrame(results)\ndf_results.to_csv(\"outputs/batch_processing_results.csv\", index=False)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"BATCH PROCESSING COMPLETE\")\nprint(\"=\"*50)\nprint(df_results)\n</code></pre>"},{"location":"examples.html#advanced-custom-objective-function","title":"Advanced: Custom Objective Function","text":"<p>Define custom optimization objectives:</p> <pre><code>import optuna\nfrom quoptuna.backend.models import create_model\nfrom sklearn.metrics import f1_score, precision_score, recall_score\n\ndef custom_objective(trial, data_dict):\n    \"\"\"Custom objective balancing F1 score and model complexity.\"\"\"\n\n    # Suggest model type\n    model_type = trial.suggest_categorical(\n        \"model_type\",\n        [\"SVC\", \"MLPClassifier\", \"DataReuploadingClassifier\"]\n    )\n\n    # Suggest hyperparameters based on model type\n    if model_type == \"SVC\":\n        params = {\n            \"model_type\": model_type,\n            \"C\": trial.suggest_float(\"C\", 0.1, 10.0),\n            \"gamma\": trial.suggest_categorical(\"gamma\", [\"scale\", \"auto\"])\n        }\n    elif model_type == \"MLPClassifier\":\n        params = {\n            \"model_type\": model_type,\n            \"hidden_layer_sizes\": trial.suggest_categorical(\n                \"hidden_layer_sizes\",\n                [\"(10,)\", \"(50,)\", \"(10, 10)\"]\n            ),\n            \"alpha\": trial.suggest_float(\"alpha\", 0.0001, 0.1, log=True)\n        }\n    else:  # DataReuploadingClassifier\n        params = {\n            \"model_type\": model_type,\n            \"n_layers\": trial.suggest_int(\"n_layers\", 2, 10),\n            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.5)\n        }\n\n    # Create and train model\n    model = create_model(**params)\n    model.fit(data_dict[\"train_x\"], data_dict[\"train_y\"])\n\n    # Evaluate\n    y_pred = model.predict(data_dict[\"test_x\"])\n    y_true = data_dict[\"test_y\"]\n\n    # Calculate metrics\n    f1 = f1_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred)\n    recall = recall_score(y_true, y_pred)\n\n    # Store metrics as user attributes\n    trial.set_user_attr(\"precision\", precision)\n    trial.set_user_attr(\"recall\", recall)\n\n    # Return weighted score\n    # Prefer higher F1 but penalize complex models\n    complexity_penalty = 0.01 if model_type == \"DataReuploadingClassifier\" else 0\n    return f1 - complexity_penalty\n\n# Create study\nstudy = optuna.create_study(direction=\"maximize\")\n\n# Optimize\nstudy.optimize(\n    lambda trial: custom_objective(trial, data_dict),\n    n_trials=100\n)\n\n# Show results\nprint(f\"Best F1 Score: {study.best_value:.4f}\")\nprint(f\"Best Parameters: {study.best_params}\")\nprint(f\"Precision: {study.best_trial.user_attrs['precision']:.4f}\")\nprint(f\"Recall: {study.best_trial.user_attrs['recall']:.4f}\")\n</code></pre>"},{"location":"examples.html#next-steps","title":"Next Steps","text":"<ul> <li>Review the API Reference for detailed class documentation</li> <li>Check the User Guide for the Streamlit interface</li> <li>Visit GitHub for more examples</li> </ul>"},{"location":"user_guide.html","title":"User Guide","text":""},{"location":"user_guide.html#user-guide","title":"User Guide","text":""},{"location":"user_guide.html#introduction","title":"Introduction","text":"<p>QuOptuna is a comprehensive platform for quantum-enhanced machine learning optimization. This guide will walk you through the complete workflow from dataset selection to model analysis and report generation.</p>"},{"location":"user_guide.html#workflow-overview","title":"Workflow Overview","text":"<p>The QuOptuna workflow consists of four main stages:</p> <ol> <li>Dataset Selection - Load and prepare your data</li> <li>Optimization - Find the best hyperparameters</li> <li>Model Training - Train models with optimized parameters</li> <li>SHAP Analysis - Understand and explain model behavior</li> </ol>"},{"location":"user_guide.html#getting-started","title":"Getting Started","text":""},{"location":"user_guide.html#installation","title":"Installation","text":"<p>Install QuOptuna using UV (recommended) or pip:</p> <pre><code># Using UV (recommended)\nuv pip install quoptuna\n\n# Using pip\npip install quoptuna\n</code></pre>"},{"location":"user_guide.html#launching-the-application","title":"Launching the Application","text":"<p>Start the Streamlit interface:</p> <pre><code>quoptuna --start\n</code></pre> <p>Or using Python:</p> <pre><code>python -m quoptuna.frontend.app run\n</code></pre>"},{"location":"user_guide.html#dataset-selection","title":"Dataset Selection","text":""},{"location":"user_guide.html#uci-ml-repository","title":"UCI ML Repository","text":"<p>QuOptuna provides easy access to datasets from the UCI Machine Learning Repository:</p> <ol> <li>Navigate to the Dataset Selection page</li> <li>Select UCI ML Repository tab</li> <li>Choose from popular datasets or enter a custom UCI ID</li> <li>Click Load UCI Dataset</li> </ol> <p>Popular Datasets: - Statlog (Australian Credit Approval) - ID: 143 - Blood Transfusion Service Center - ID: 176 - Banknote Authentication - ID: 267 - Heart Disease - ID: 45 - Ionosphere - ID: 225</p>"},{"location":"user_guide.html#custom-dataset-upload","title":"Custom Dataset Upload","text":"<p>To use your own dataset:</p> <ol> <li>Navigate to the Upload Custom Dataset tab</li> <li>Upload a CSV file</li> <li>Configure target and feature columns</li> <li>Apply target transformation if needed</li> </ol>"},{"location":"user_guide.html#data-configuration","title":"Data Configuration","text":"<p>Important: QuOptuna requires binary classification targets to be encoded as <code>-1</code> and <code>1</code>.</p> <ol> <li>Select Target Column: Choose the column you want to predict</li> <li>Select Features: Choose the features to use for prediction</li> <li>Target Transformation: Map your target values to -1 and 1</li> <li>Handle Missing Values: QuOptuna will automatically remove rows with missing values</li> </ol> <p>Click Save Configuration to proceed to the next step.</p>"},{"location":"user_guide.html#data-preparation-optimization","title":"Data Preparation &amp; Optimization","text":""},{"location":"user_guide.html#data-preparation","title":"Data Preparation","text":"<p>Once your dataset is configured:</p> <ol> <li>Review the dataset summary (rows, columns, target distribution)</li> <li>Click Prepare Data for Training</li> <li>QuOptuna will automatically:</li> <li>Split data into training and test sets</li> <li>Scale features</li> <li>Convert to the format required by models</li> </ol>"},{"location":"user_guide.html#hyperparameter-optimization","title":"Hyperparameter Optimization","text":"<p>Configure and run optimization:</p> <ol> <li>Database Name: Name for storing optimization results</li> <li>Study Name: Unique identifier for this optimization study</li> <li>Number of Trials: How many hyperparameter combinations to try (recommended: 50-200)</li> </ol> <p>Click Start Optimization to begin. This will: - Test multiple model types (both quantum and classical) - Try different hyperparameter combinations - Track the best performing configurations</p> <p>Model Types Tested: - Data Reuploading Classifier (Quantum) - Circuit-Centric Classifier (Quantum) - Quantum Kitchen Sinks (Quantum) - Support Vector Classifier (Classical) - Multi-Layer Perceptron (Classical) - And more...</p>"},{"location":"user_guide.html#understanding-results","title":"Understanding Results","text":"<p>After optimization completes, you'll see: - Best Trials: Top performing configurations - F1 Scores: Performance metrics for quantum and classical approaches - Hyperparameters: The configuration for each trial</p>"},{"location":"user_guide.html#shap-analysis-reporting","title":"SHAP Analysis &amp; Reporting","text":""},{"location":"user_guide.html#trial-selection","title":"Trial Selection","text":"<ol> <li>Navigate to the SHAP Analysis page</li> <li>Select a trial from the dropdown (sorted by performance)</li> <li>Review the trial details and parameters</li> </ol>"},{"location":"user_guide.html#model-training","title":"Model Training","text":"<ol> <li>Click Train Model to train the selected model</li> <li>The model will be trained on your data with the optimized hyperparameters</li> </ol>"},{"location":"user_guide.html#shap-analysis","title":"SHAP Analysis","text":"<p>Configure SHAP analysis:</p> <ul> <li>Use Probability Predictions: Use probability outputs instead of class predictions</li> <li>Use Subset of Data: Analyze a subset for faster computation</li> <li>Subset Size: Number of samples to analyze (recommended: 50-100)</li> </ul> <p>Click Run SHAP Analysis to calculate SHAP values.</p>"},{"location":"user_guide.html#shap-visualizations","title":"SHAP Visualizations","text":"<p>QuOptuna provides multiple visualization types:</p>"},{"location":"user_guide.html#bar-plot","title":"Bar Plot","text":"<p>Shows the mean absolute SHAP value for each feature, indicating overall importance.</p> <p>Use Case: Quick overview of feature importance</p>"},{"location":"user_guide.html#beeswarm-plot","title":"Beeswarm Plot","text":"<p>Shows the distribution of SHAP values, with color indicating feature value (red = high, blue = low).</p> <p>Use Case: Understanding how feature values affect predictions</p>"},{"location":"user_guide.html#violin-plot","title":"Violin Plot","text":"<p>Shows the distribution of SHAP values for each feature.</p> <p>Use Case: Understanding the variability in feature impact</p>"},{"location":"user_guide.html#heatmap","title":"Heatmap","text":"<p>Shows SHAP values for individual instances.</p> <p>Use Case: Instance-level analysis, finding patterns in predictions</p>"},{"location":"user_guide.html#waterfall-plot","title":"Waterfall Plot","text":"<p>Explains how features contribute to a single prediction.</p> <p>Use Case: Understanding individual predictions in detail</p>"},{"location":"user_guide.html#confusion-matrix","title":"Confusion Matrix","text":"<p>Shows classification performance.</p> <p>Use Case: Evaluating overall model accuracy</p>"},{"location":"user_guide.html#report-generation","title":"Report Generation","text":"<p>Generate comprehensive AI-powered reports:</p> <ol> <li>Select LLM Provider: Google (Gemini), OpenAI (GPT), or Anthropic (Claude)</li> <li>Enter API Key: Your API key for the selected provider</li> <li>Model Name: Specific model to use (e.g., \"models/gemini-2.0-flash-exp\")</li> <li>Dataset Information (optional): Add context about your dataset</li> </ol> <p>Click Generate Report to create a detailed analysis report.</p> <p>Report Includes: - Performance metrics analysis - SHAP value interpretation - Feature importance ranking - Risk and fairness assessment - Governance recommendations</p>"},{"location":"user_guide.html#best-practices","title":"Best Practices","text":""},{"location":"user_guide.html#optimization","title":"Optimization","text":"<ul> <li>Start Small: Begin with 50-100 trials to get quick results</li> <li>Increase Gradually: Use 100-200 trials for production models</li> <li>Monitor Performance: Check both quantum and classical model scores</li> <li>Save Studies: Use descriptive names for databases and studies</li> </ul>"},{"location":"user_guide.html#shap-analysis_1","title":"SHAP Analysis","text":"<ul> <li>Use Subsets: Analyze 50-100 samples for faster computation</li> <li>Multiple Plots: Generate several plot types for comprehensive understanding</li> <li>Document Findings: Save plots and reports for future reference</li> <li>Understand Context: Consider domain knowledge when interpreting SHAP values</li> </ul>"},{"location":"user_guide.html#report-generation_1","title":"Report Generation","text":"<ul> <li>Provide Context: Add dataset URL and description for better AI insights</li> <li>Choose Appropriate Models:</li> <li>Fast models (Gemini Flash): Quick exploratory reports</li> <li>Advanced models (GPT-4, Gemini Pro): Detailed production reports</li> <li>Review Carefully: AI-generated reports should be reviewed by domain experts</li> </ul>"},{"location":"user_guide.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"user_guide.html#common-issues","title":"Common Issues","text":"<p>Dataset Loading Fails - Check UCI dataset ID is correct - Ensure CSV file is properly formatted - Verify file encoding (UTF-8 recommended)</p> <p>Optimization Errors - Ensure data has no missing values - Check target column has exactly 2 unique values - Verify sufficient samples for train/test split</p> <p>SHAP Analysis Slow - Reduce subset size - Use simpler model types - Check available memory</p> <p>Report Generation Fails - Verify API key is valid - Check internet connection - Ensure model name is correct - Try a different LLM provider</p>"},{"location":"user_guide.html#advanced-features","title":"Advanced Features","text":""},{"location":"user_guide.html#loading-previous-studies","title":"Loading Previous Studies","text":"<p>You can load and analyze previously run optimizations:</p> <ol> <li>Go to the Optimization page</li> <li>Enter the database name and study name</li> <li>Click Load Optimizer</li> <li>Results will be available for analysis</li> </ol>"},{"location":"user_guide.html#batch-processing","title":"Batch Processing","text":"<p>For multiple datasets, you can: 1. Use the Python API directly (see API documentation) 2. Script the workflow using QuOptuna classes 3. Save results to different databases</p>"},{"location":"user_guide.html#custom-models","title":"Custom Models","text":"<p>Advanced users can integrate custom models by: 1. Following the model interface in <code>quoptuna.backend.models</code> 2. Adding model configurations to the optimizer 3. See API documentation for details</p>"},{"location":"user_guide.html#next-steps","title":"Next Steps","text":"<ul> <li>Explore the API Documentation for programmatic usage</li> <li>Check out Examples for common use cases</li> <li>Review Development Guide for contributing</li> </ul>"},{"location":"user_guide.html#support","title":"Support","text":"<ul> <li>GitHub Issues: Report bugs or request features</li> <li>Documentation: Full documentation</li> <li>Community: Join our discussions on GitHub</li> </ul>"}]}